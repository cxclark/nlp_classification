title,created_utc,selftext,subreddit,author,media_only,permalink
Advice on RL for Robotics Graduate Studies,1602132231,"I am not sure if this the right place to seek advice on Graduate studies, but wanted someone's two cents on this as I am really confused. 

My research focus is in Reinforcement Learning and it's applications (mostly, robotics) - - - Controlling quadrocopters, Robotic arm manipulation, autonomous vehicle control etc. Most of my projects/publications is along the same lines. I am looking for research based programs (masters or PhD). Though I have done my extensive research for the programs, professors and Labs to apply , I am confused whether I should apply for Phd/Master's in robotics or PhD/master's in CS and later take up RL as specialization (Not sure how this work). 

Also , it is worth mentioning that I have a CS background with good exposure to Robotics , ROS, C++, Moveit, Gazebo, Rviz , etc

Any advice would really be help me make a well informed decision.",reinforcementlearning,rlmlsavant,False,/r/reinforcementlearning/comments/j76ueh/advice_on_rl_for_robotics_graduate_studies/
NeurIPS (NIPS) 2020 Accepted Paper List is available,1602123524,[https://neurips.cc/Conferences/2020/AcceptedPapersInitial](https://neurips.cc/Conferences/2020/AcceptedPapersInitial),reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/j74lz6/neurips_nips_2020_accepted_paper_list_is_available/
Still don't understand policy loss...,1602103406,"I've been working on a custom gym implementation using stable-baselines SAC. I've seen improvement in the reward, the Q-losses converge to \~0 and the entropy decrease (slightly). However the policy loss just increases.

To be honest I don't have a great understanding of what the policy loss represents. I know that the equation is:

policy loss = -log(pr(a | s) \* (Q(s,a) - V(s))

so if the Q function for an action is higher for some action a\\\* then the Advantage (Q(s,a\\\*) -V(s)) is positive.... ok. Then the log(pr(a\* | s)) negative since pr(x) &lt; 1.  If the probability of taking action a\\\* is not that high (entropy is \~=0.8) then -log(pr(a\* | s)) \~= 0.5.

Does that mean that the policy loss should be positive/increasing if the entropy never converges to a sufficiently low number??? The environment I created changes pretty dramatically over time so I'm not that concerned if the entropy doesn't lock in on a number and I'm happy with the rewards obtained.

Here's my tensorboard outputs for a couple runs if it helps:

https://preview.redd.it/etlcdioxeqr51.png?width=1571&amp;format=png&amp;auto=webp&amp;s=ce1b53e6f246a05b50a5927cbd510aa94423f6fa",reinforcementlearning,little_grey_mare,False,/r/reinforcementlearning/comments/j6z0dx/still_dont_understand_policy_loss/
Learn Logic Gates with a Game,1602089219,"Hi everyone! I have developed an educative game called “Logic Gates” which aims to teach logic and logic gates in a fun and interactive way. Everybody can understand and complete the educational levels in which you learn the basics of logic and experience with logic gates in an interactive simulator. After learning the basics, you can solve the puzzles which are more challenging problems. If you are already familiar with logic, you can directly try to solve puzzles. Also, there is a sandbox mod in game where you can create your own logic circuits and gates.

If you are interested in mathematics or computer science, you should **definitely** play the game because logic is the most fundamental topic of those areas. Here are the links:  
 [https://apps.apple.com/tr/app/mantik-kapilari/id1533790336?l=tr](https://apps.apple.com/tr/app/mantik-kapilari/id1533790336?l=tr)  
 [https://play.google.com/store/apps/details?id=com.KeremOner.LogicGame](https://play.google.com/store/apps/details?id=com.KeremOner.LogicGame)

I would be glad if I can receive some feedbacks from you.",reinforcementlearning,bugattieb,False,/r/reinforcementlearning/comments/j6ue8i/learn_logic_gates_with_a_game/
Deep RL in Portfolio Optimization: Help Needed,1602082606,"I've tried to optimize the portfolio value using PPO Algorithm. I'm working on high-performing stocks like MSFT, and AAPL.  

My problem is that the model captures a very naive policy; try to buy indefinitely.

Heres my model setup, 

Action : 0,1,2

States: log normalized stock -price, number of stock in the portfolio, and some features to capture stock trend. 

Reward: log normalized (current portfolio value - initial balance)

Parameters:

epsilon: 0.99(linear decay)

learning rate: 3e-2

&amp;#x200B;

I've trained my model on daily closing price data from 2014-2018 while my test data is from 2018-2019. On testing, I get all 252 buy signals after a single epoch.  

Although all buy signals give a sharpe ratio of 2.5 on MSFT data, I wanted to know if I could tweak something in my model to get a more sophisticated policy.",reinforcementlearning,SalarySlow4581,False,/r/reinforcementlearning/comments/j6sc6f/deep_rl_in_portfolio_optimization_help_needed/
Continual Learning Project,1602079133," Hey, guys so recently I made a project on a continual learning AI and tried using it on this API called continuum. I think it is performing really well but I am new to this so it may just be that I am calculating metrics incorrectly. So could you guys take a look at it?

It is in this Google Colab: [https://colab.research.google.com/drive/1I6gfI3VcnF1dny2CaX2GOMmazpzfJdLT?usp=sharing](https://colab.research.google.com/drive/1I6gfI3VcnF1dny2CaX2GOMmazpzfJdLT?usp=sharing)

PS. The reason I am sharing here is because it uses reinforcement learning to encode information about the past",reinforcementlearning,OptimalDendrite,False,/r/reinforcementlearning/comments/j6rbbw/continual_learning_project/
Ph.D. in reinforcement learning,1602077288,What are the best colleges to apply for Ph.D. to do research in reinforcement learning,reinforcementlearning,sandy_005,False,/r/reinforcementlearning/comments/j6qt95/phd_in_reinforcement_learning/
How to extract components which construct the value function in Q-learning?,1602059440,"I've built a simple agent which learns to sail upwind using Q-learning and average reward instead of discounting. Currently, the most complex thing it can do is tacking to sail upwind. 

It is rewarded every time it moves north and the value of the reward is equal to the instantaneous velocity it the northern direction and is punished when it crashes against a pier.

I am wondering how to go about extracting the approximate velocity function out of the value function. Is that even possible? Or is it easier to learn the velocity function and the remaining rewards separately and just sum them to get the value function?

*Value = expected velocity + expected rewards*

The project is described in detail here: [https://github.com/PPierzc/ai-learns-to-sail](https://github.com/PPierzc/ai-learns-to-sail)

Any help will be much appreciated!",reinforcementlearning,ppierzc,False,/r/reinforcementlearning/comments/j6n8sc/how_to_extract_components_which_construct_the/
Mathematical Background,1602054988,"I plan to go through the math parts of RL. What are the books I should follow? Let’s say, I’m starting with Intro to Stat. Any suggestions? Thanks.",reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j6mhnj/mathematical_background/
I asked RL-expert what and why he logs/tracks in reinforcement learning experiments for training and debug. Here is what he said.,1602050733,"For effective training of reinforcement learning agent it's quite important to see what is going on in the training and understand *why* agent behaves in particular way. This is challenging especially when the agent doesn’t behave the way we would like it to behave, which is like always.

**He shared categorized list of metrics/statistics that you want to keep track of to inspect/debug your agent learning trajectory.**

**I. How is the agent doing?**

* Episode return -&gt; *care about this one most, and try to find sane baseline for your problem.*
* Episode length
* Solve rate

**II. Progress of training**

* Total environment steps
* Training steps
* Wall time -&gt; *tells you how fast you can progress or try new ideas.*
* Steps per second

**III. What is the agent thinking/doing?**

* State/Action value function
* Policy entropy

**IV. How the training goes?**

* KL divergence
* Network weights/gradients/activations histograms -&gt; *Beware Dying ReLUs / Vanishing or Exploding gradients or activations.*
* Policy/Value/Quality/... heads losses

**V. Aggregated statistics**

* Average and standard deviation
* Minimum/Maximum value -&gt; *inspecting extremes can help spot a bug.*
* Median

What other categories or metrics you observe that help you fix or improve your policy?

I hope you find it helpful! Deep dive in the [Article](https://neptune.ai/blog/how-to-make-sense-of-the-reinforcement-learning-agents-what-and-why-i-log-during-training-and-debug?utm_source=reddit&amp;utm_medium=post&amp;utm_campaign=blog-how-to-make-sense-of-the-reinforcement-learning-agents-what-and-why-i-log-during-training-and-debug).",reinforcementlearning,kk_ai,False,/r/reinforcementlearning/comments/j6lp7v/i_asked_rlexpert_what_and_why_he_logstracks_in/
I need an RL / Deep Learning specialist (a full-time paid position),1602027794,"I'm not sure this belongs here, but I am looking for an RL / Deep Learning specialist for a full-time paid position. You must be familiar enough with the theory of SOTA techniques (e.g. MuZero, DreamerV2, Agent57, etc.) to be able to implement from a paper. Experience with CNNs, RNNs, transformers, and data augmentation is a huge plus. Top-end compensation plus profit sharing.

You would be working with a team of folks who have ML experience but are not (yet) cutting edge researchers (though we have almost recreated GPT3). You would be expected to be the lead researcher / engineer.

We work on a variety of topics including NLP, images, time series, and generative systems. Control systems to be added to the list within the next two years.

DM me if you are interested.

If you have suggestions on good places to find candidates, please let me know - I would greatly appreciate it!",reinforcementlearning,chazzmoney,False,/r/reinforcementlearning/comments/j6gar5/i_need_an_rl_deep_learning_specialist_a_fulltime/
DDPG in America Stock,1602023199,"Hi,

Hope this finds you well guys!

Recently I am trying to implement the paper  [""Adversarial Deep Reinforcement Learning in Portfolio Management""](https://arxiv.org/abs/1808.09940) with one of the methods mentioned in the paper -- Deep Deterministic Policy Gradient. The code of the paper can be found [here](https://github.com/liangzp/Reinforcement-learning-in-portfolio-management-). It also includes the dataset.  I am using TensorFlow 2.0 / Keras. I created a small network instead of a full network in the paper for debugging purposes. My code could be found [here](https://github.com/XimingFeng/ddpg-stock). You can open to Jupyter notebook ""[DDPGTest2.ipynp](https://github.com/XimingFeng/ddpg-stock/blob/master/DDPGTest2.ipynb)"" for the running result.

My issue right now:  somehow the actor starts to produce high confidence with only one class/stock after a few steps, then it stays that way forever unless I set the noise on the actor output to be really high. Even if I force the network to learn the same dataset for multiple episodes, still does not make any change. I tried different types of networks such as CNN, Fully-Connected. With that, I don't think the network type is the issue. I have also tried different sets of hyper-parameter. I feel stuck right now. Not sure what to check next. If you are interested in the project, please let me know. Any help is appreciated!",reinforcementlearning,royfeng123,False,/r/reinforcementlearning/comments/j6f1hj/ddpg_in_america_stock/
Need help with SARSA in Cartpole,1602017928,"I'm looking to start learning RL and read most of Sutton and Barto's Intro to RL book. Now I'm trying to implement some of their algorithms into gym environments but I've been having some difficulty; specifically with implementing the SARSA algorithm in the Cartpole environment.

I've tried looking for answers online and came across this:  [https://gist.github.com/martinholub/c4860006d0cf3fbe87a79a054a9c98cd](https://gist.github.com/martinholub/c4860006d0cf3fbe87a79a054a9c98cd) 

But as people have pointed out in that thread, the update rule on line 35 in the first code block (example.py) ,  `w += alpha*(reward - discount*q_hat_next)*q_hat_grad` , should instead be ,  `w += alpha*(reward + discount*q_hat_next - q_hat)*q_hat_grad`...at least, according to my understanding of the algorithm (I've uploaded the pseudocode here:  [https://imgur.com/a/bUdcBYz](https://imgur.com/a/bUdcBYz) , and it can be found in the Intro to RL book on page 244).

What's weird is that that implementation seems to work, and moreover if I switch the update rule to `w += alpha*(reward + discount*q_hat_next - q_hat)*q_hat_grad` , the algorithm doesn't learn the correct action value function.

Anyone have experience / could shed some light on this? Much appreciated, thanks!",reinforcementlearning,thechiamp,False,/r/reinforcementlearning/comments/j6dh9j/need_help_with_sarsa_in_cartpole/
Recent overview of state-of-the-art ?,1601994990,"Is there any recent paper (last 2 years or so) that provides a good overview of Reinforcement Learning algorithms? Even better if a more extensive work, such as a thesis or a book.",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/j665j1/recent_overview_of_stateoftheart/
Why isn’t RL research considered theoretical?,1601994515,"I am interested in RL and have done some work in it but I’m not huge on coding all the time. I really love math and proofs. But my adviser said RL isn’t a classic theory problem, which I get, but can it not become a theory problem? 
I guess my main question is, is what the main problem is in RL. If we’re still having problems, shouldn’t we be looking at the math behind it and looking at it theoretically? Sorry if this sounds dumb",reinforcementlearning,nobbithrowaway,False,/r/reinforcementlearning/comments/j660mt/why_isnt_rl_research_considered_theoretical/
[R] Mastering Atari with Discrete World Models,1601989751,,reinforcementlearning,Caffeinated-Scholar,False,/r/reinforcementlearning/comments/j64qm7/r_mastering_atari_with_discrete_world_models/
Update Rule Actor Critic Policy Gradient,1601988357,"Hey everyone, 

I am currently doing my Master thesis and i have a question regarding the theory of the policy gradient Methode wich use an actor and a critic.

The Basic Update rule states invokes the gradient of the Policy(actor output) and the approximated value of the state Action value(critic Output).

Both networks Input the current State. The actor then Outputs the probability for the actions depending on the current State- This Makes Sense to me 

But the critic Network Inputs also the State but outputs its estimation for the Q(s,a). This is a scalar.

I dont Unterstand to wich Action the value corresponds since the critic also just Inputs the State and not the State and the Action on wich the Q value is defined.


I Hope one Unterstands my issue with This Concept.",reinforcementlearning,slippy_1993,False,/r/reinforcementlearning/comments/j64e46/update_rule_actor_critic_policy_gradient/
How many months it will take to complete an ICML/Neurips/ICLR paper for top PhD students and Researchers?,1601987197,"Normally, how many months it will take to complete an ICML/Neurips/ICLR paper for top PhD students and Researchers?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/j644a5/how_many_months_it_will_take_to_complete_an/
Difficulty of agent's learning with increasing dimensions of continuous actions.,1601979268,"Dear Reinforcement Learning Redditors!

I have been working on some RL project, where the policy is controlling the robot using its joint angles. 

Throughout the project I have noticed some phenomenon, which caught my attention. I have decided to create a very simplified script to investigate the problem. There it goes:

**The environment**

There is a robot, with two rotational joints, so 2 degrees of freedom. This means its continuous action space (joint rotation angle) has a dimensionality of 2. Let's denote this action vector by **a**. I vary the maximum joint rotation angle per step from 11 to 1 degrees and make sure that the environment is allowed to do a reasonable amounts of steps before the episode is forced to terminate on time-out.

Our goal is to move the robot by getting its current joint configuration **c** closer to the goal joint angle configuration **g** (also two dimensional input vector).   
Hence, the reward I have chosen is e\*\*(-L2\_distance(**c**, **g**)). The smaller the L2\_distance, the exponentially higher the reward, so I am sure that the robot is properly incentivised to reach the goal quickly.

&amp;#x200B;

[y-axis: reward, x-axis: L2 distance](https://preview.redd.it/vvqgcp3yufr51.png?width=766&amp;format=png&amp;auto=webp&amp;s=b56b2af7513dada039b882dc4eab0c3f6893c579)

So the pseudocode for every step goes like: 

\- move the joints by predicted joint angle delta

\- collect the reward

\- if time-out or joint deviates too much into some unrealistic configuration: terminate.

Very simple environment, not to have too many moving parts in our problem.

**RL algorithm** 

I use Catalyst framework to train my agent in the actor-critic setting using TD3 algorithm. By using a tested framework, which I am quite familiar with, I am quite sure that there are no implementational bugs.

The policy is goal-driven so the actor consumes the concatenated current and goal joint configuration **a**= policy(\[**c**,**g**\])

**The big question**

When the robot has only two degrees of freedom, the training quickly converges and the robots learns to solve the task with high accuracy (final L2 distance smaller than 0.01). 

[Performance of the converged 2D agent. y-axis: joint angle value, x-axis: no of episodes. Crosses denote the desired goal state of the robot.](https://preview.redd.it/z9azrsy74gr51.png?width=640&amp;format=png&amp;auto=webp&amp;s=e5ddccca2793cf86dc8935a608f79fedad988254)

However, if the problem gets more complicated - I increase the joint dimensions to 4D or 6D, the robot initially learns to approach the target, but it never ""fine-tunes"" its movement. Some joints tend to oscillate around the end-point, some of them tend to overshoot. I have been experimenting with different ideas: making the network wider and deeper, changing the action step. I have not tried optimizer scheduling yet. No matter how many samples the agent receives or how long it trains, it never learns to approach targets with required degree of accuracy (L2\_distance smaller than 0.05).

[Performance of the converged 4D agent. y-axis: joint angle value, x-axis: no of episodes. Crosses denote the desired goal state of the robot.](https://preview.redd.it/nl7zv1n8yfr51.png?width=640&amp;format=png&amp;auto=webp&amp;s=fd20c81d833d57faa8d5e3b96ca393e35ac21f9b)

&amp;#x200B;

[Training curve for 2D agent \(red\) and 4D agent \(orange\). 2D agent quickly minimises the L2 distance to something smaller than 0.05, while the 4D agent struggles to go below 0.1.](https://preview.redd.it/vmge1od63gr51.png?width=2030&amp;format=png&amp;auto=webp&amp;s=573b3ff8d7f0a10e5274e00dc4f690d42abb7e65)

**Literature research** 

I have looked into papers which describe motion planning in joint space using TD3 algorithm. There are not many differences from my approach:

[https://arxiv.org/abs/1906.00214](https://arxiv.org/abs/1906.00214) 

[https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiy4KCU3Z\_sAhXEnVwKHXzfBZcQFjABegQIBRAC&amp;url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F10%2F2%2F575%2Fpdf&amp;usg=AOvVaw2-D78jGxqQspBGrhbGR0Yk](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiy4KCU3Z_sAhXEnVwKHXzfBZcQFjABegQIBRAC&amp;url=https%3A%2F%2Fwww.mdpi.com%2F2076-3417%2F10%2F2%2F575%2Fpdf&amp;usg=AOvVaw2-D78jGxqQspBGrhbGR0Yk)

Their problem is much more difficult because the policy needs to also learn the model of the obstacles in joint space, not only the notion of the goal. The only thing which is special about them is that they use quite wide and shallow networks. But this is the only peculiar thing.

I am really interested, what do you guys would advise me to do, so that the robot can reach high accuracy in higher joint configuration dimensions? What am I missing here?!",reinforcementlearning,dtransposed,False,/r/reinforcementlearning/comments/j62ivn/difficulty_of_agents_learning_with_increasing/
RL algos struggle with simple environment,1601970444," I am trying to use an RL algorithm as controller for electrical grid storages. The final goal is to train an algorithm to be able to control five storages as optimally as possible. As a first step, I built an environment based on OpenAI gym with a small distribution grid consisting of only one storage, one PV and one load. For PV generation and load consumption one year of data is available (sampled every 15 minutes à 4/h x 24 h x 365 days = 35,040 time steps). 

The observation contains four values: 

\- storage State of Charge

\- storage power

\- PV power 

\- load power

The action is only one value:

\- storage power for the next time step

Since the overall goal is to minimize the loading at the transformer the house is connected to, the reward is calculated from the trafo power. In addition to that, the storage SoC is penalized if it is above 100 / below 0 percent:

reward = - 1e5 \* p\_trafo\^2 - storage\_rew

storage\_rew =  abs(storage\_soc) if storage\_soc &lt; 0

storage\_soc – 100  if storage\_soc &gt; 100

0 else

A large factor (1e5) for the trafo power is necessary because the trafo power is very small (0.00x to 0.0x range before square). The transformer power is (leaving out small losses) equal to p\_pv - p\_load – p\_storage.

I tested the environment using several dummy controllers (e.g. charging when the sun is shining, discharging when not shining; charging when the transformer load is high, discharging when low, …) and the environment behaved as expected in all tests. Therefore, I expect it to function properly (of course I can not be 100 percent sure).

Then I tried RL using stable-baselines3. I tried to solve the problem using PPO and A2C. Additionally, I modified the action space from Box to Discrete to be able to test DQN. But regardless of the algorithm used I don’t get a consistent result. I had a rather good result (still not great) once but am not able to reproduce it (random seed is always the same). The major problem I observe is that the agent doesn’t learn to take full advantage of the storage’s capacity. Most of the trained agents are close to 0 / 100 percent SoC most of the time and only charge / discharge to about 25 / 75 percent, maybe 50. I can not explain this behavior. Things that I varied during training include:

\- episode\_length: 96 (1 day) to 2880 (30 days)

\- training episodes: 1e6 to 20e6

\- MlpPolicy: from standard two-layer fully connected \[64, 64\] to \[16, 16\] or \[8, 8\]

In my eyes I made the environment as simple as possible and I am wondering why none of the agents is able to find a satisfying solution. Did anyone encounter a similar problem where an apparently simple problem could not be solved? I am grateful for any ideas and tips",reinforcementlearning,not-a-researcher,False,/r/reinforcementlearning/comments/j610v0/rl_algos_struggle_with_simple_environment/
anyone implemented Hindsight Experience Replay for any of the Gym env's? e.g. mountain car?,1601962688,I cant seem to get it work.,reinforcementlearning,wolfzwolf,False,/r/reinforcementlearning/comments/j5zm1y/anyone_implemented_hindsight_experience_replay/
How to train policy gradient on multi-GPUs?,1601958080,"While I was working on creating a few baseline models for my project, I came up with the question, how can we train the model with multi-GPUs?

When I looked up using multi-GPU for training, almost all of them uses batch training for their model i.e. CNN, LSTM,... 
Which makes sense, because batches are independent of the others and it is possible to train the model with multi-GPU as we can deploy each batch on different GPUs to compute gradients and average them. 

But, in Reinforcement Learning, in a policy-based model-free single-agent scenario, an agent only takes one step each time and trains the policy. I couldn't think of how we can train the model using multi-GPUs when we can not use more than one agent.

Do we need to set up the environment manually so that we have multiple agents in order to train them using multiple-GPUs? Or is there a way to train the model with only using a single agent with multi-GPUs to boost up the training speed? 

Thank you for reading!",reinforcementlearning,g6ssgs,False,/r/reinforcementlearning/comments/j5yo7o/how_to_train_policy_gradient_on_multigpus/
Model-free vs model based?,1601953501,I was reading about the differences. My understanding is that model free doesn’t need defined transition probability whether model-based needs the transition probability. Is it correct?,reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j5xm3v/modelfree_vs_model_based/
How is reward backpropagated?,1601948666,"I did some googling and wasn't really able to find an intuitive explanation for how various algos like DQN back propagate the reward. Let's say we have an environment with sparse rewards/feedback. If we set the n-step value to 2, so that we don't actually compute the reward for the current step, until we've seen the reward in the future step, the reward for the current step is calculated as (reward of current time step + reward of next time step \* gamma). My understanding is that even though the reward is only seen one step back, that with enough iterations the reward will ""backpropagate"" so that time steps will ""see"" reward distributions further into the future than just 2 steps, despite these steps never actually giving any rewards themselves. How does this happen?",reinforcementlearning,Yogi_DMT,False,/r/reinforcementlearning/comments/j5wg9q/how_is_reward_backpropagated/
Silver's lectures good for RL env?,1601923750,"I am looking to work on an RL env. David Silver's lectures look good but does anyone know what # lecture(s) of his go over this, and the coding aspect specifically? I would rather not watch all of them right now.",reinforcementlearning,AwkwardRound,False,/r/reinforcementlearning/comments/j5pb3o/silvers_lectures_good_for_rl_env/
"""How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds"", Ammanabrolu et al 2020 {FB}",1601913157,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/j5lwct/how_to_motivate_your_dragon_teaching_goaldriven/
"Hello guys, I’m a master’s student in Electrical and Computer Engineering. I’m gonna do my thesis on rl. I have just opened a discord study group: https://discord.gg/zatvm2",1601906011,Let’s study together and help each other. Thanks.,reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j5jsfm/hello_guys_im_a_masters_student_in_electrical_and/
Unity ml-agents with Gym,1601905245,"Hello! Are there those who train ml-agents here? I am trying to link the unity environment to the Gym learning environment by following this tutorial: [https://github.com/Unity-Technologies/ml-agents/tree/master/gym-unity](https://github.com/Unity-Technologies/ml-agents/tree/master/gym-unity) . But apparently the code is hopelessly outdated. If I use the DQN algorithm, then the error ""'MultiDiscrete' object has no attribute 'n'"" is displayed, and if I use ppo, then the game is simply turned off and the error \[WinError 109\] is displayed. Can someone guide me on how to fix these errors or can someone have more recent code? Thanks.",reinforcementlearning,El_cartel,False,/r/reinforcementlearning/comments/j5jl6i/unity_mlagents_with_gym/
MADRaS : Multi Agent Driving Simulator,1601902339,,reinforcementlearning,Caffeinated-Scholar,False,/r/reinforcementlearning/comments/j5itkz/madras_multi_agent_driving_simulator/
Good PhD programs for Reinforcement Learning,1601838878,"I am in the last year of my Master studies in Machine Learning, and once I finish it I would love to do a PhD in reinforcement learning.

However, my university doesn't really do much research in the field. I want to do it somewhere else (preferably Europe) but I don't know which universities to aim at.

Can you point me to something interesting? Or give me some advices on how I can find interesting places? 

What I plan to do is to read more papers and try to understand where proficient RL researchers come from.",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/j54mza/good_phd_programs_for_reinforcement_learning/
"""Predictive Maps in Rats and Humans for Spatial Navigation"", de Cothi et al 2020",1601829400,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/j51w01/predictive_maps_in_rats_and_humans_for_spatial/
Need a study buddy for RL book by Sutton,1601824816,"Hey all,

I  have been going through Sutton and Barto slowly through the past weeks, and have trying out code up the programs.

But I am doing it alone

So, is anyone else doing the same?

DM me!

Thanks!",reinforcementlearning,yasserius,False,/r/reinforcementlearning/comments/j50lws/need_a_study_buddy_for_rl_book_by_sutton/
How to choose the number of steps in n-step Q-learning?,1601784011,How to choose the number of steps in n-step Q-learning?,reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/j4spuu/how_to_choose_the_number_of_steps_in_nstep/
Multi-agent Social Reinforcement Learning Improves Generalization,1601740809,,reinforcementlearning,Caffeinated-Scholar,False,/r/reinforcementlearning/comments/j4hh33/multiagent_social_reinforcement_learning_improves/
Looking for some help on climbing the RL mountain,1601707111,"Hello guys !

I've been trying to learn RL for about 3 months now, and the damn thing is so hard :)  My background is on the artistic side of computer graphics, but I'm mostly tech-oriented, so I work with the app's scripting languages and I know some python. Also I studied 2 years engineering in college, so I think my math is OK, but not sure if it's enough for RL !

Although I feel like I've learned quite a few fundamentals, I'm sort of stuck in my learning process, so I'm hoping to find someone who would be so kind in personally giving me some tips on what to do next, maybe ""read this"", ""do that"", ""watch this"", sort of holding my hand while I try to climb this steep mountain, hopefully checking on my progress once or twice a week.  I'm just looking for any help I can get because it's getting pretty frustrating.

I've managed to borrow from a friend over 40 books and video courses, so I have a ton of material. I skimmed through all of them, and I've started reading a few but I still haven't found something that I can see a clear path with.  The most promising seem to be The Lazy Programmer series, the Phil Tabor courses, the Lapan book Deep RL Hands-on, and the Zai-Brown book Deep RL in Action.

I understand if whoever wants to help doesn't have much time to share, I know how it is, I have a full-time job, a daughter, etc, haha.

Well, I hope this appealing. My whatsapp # is +569 4147 17 \[eight-zero\] Or I'll install whatever messaging app needed on the phone.

Cheers",reinforcementlearning,rickoster,False,/r/reinforcementlearning/comments/j4b5d8/looking_for_some_help_on_climbing_the_rl_mountain/
"Google AI Introduces Menger, A Massive Large-Scale Distributed Reinforcement Learning (RL) Infrastructure",1601692965,"Reinforcement learning (RL) is a significant area of machine learning, with the potential to solve a lot of real world problems in various fields, like [game theory](https://en.wikipedia.org/wiki/Game_theory), [control theory](https://en.wikipedia.org/wiki/Control_theory), [operations research](https://en.wikipedia.org/wiki/Operations_research), [information theory](https://en.wikipedia.org/wiki/Information_theory), [simulation-based optimization](https://en.wikipedia.org/wiki/Simulation-based_optimization), [multi-agent systems](https://en.wikipedia.org/wiki/Multi-agent_system), [swarm intelligence](https://en.wikipedia.org/wiki/Swarm_intelligence), and [statistics](https://en.wikipedia.org/wiki/Statistics). Reinforcement learning (RL) infrastructure is a loop system of data collection and training using actors as data sample collectors, and learners to train and update the model.  

Summary: [https://www.marktechpost.com/2020/10/02/google-ai-introduces-menger-a-massive-large-scale-distributed-reinforcement-learning-rl-infrastructure/](https://www.marktechpost.com/2020/10/02/google-ai-introduces-menger-a-massive-large-scale-distributed-reinforcement-learning-rl-infrastructure/)

Source: [https://ai.googleblog.com/2020/10/massively-large-scale-distributed.html](https://ai.googleblog.com/2020/10/massively-large-scale-distributed.html)",reinforcementlearning,ai-lover,False,/r/reinforcementlearning/comments/j4845g/google_ai_introduces_menger_a_massive_largescale/
Multi Agent environment for protein structure prediction on a bravais lattice.,1601687477,"TL;DR: Created a multi agent environment for protein folding with a Gym-like api, feel free to use!

&amp;#x200B;

Hi all, for my final dissertation I studied [mean-field multi agent reinforcement learning](https://arxiv.org/abs/1802.05438) and tried to apply with it with a combination of other techniques to the [protein structure prediction problem](https://en.wikipedia.org/wiki/Protein_structure_prediction) to predict tertiary structures after being inspired by the likes of [AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) and the results of [CASP13](https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery).

The original mean field multi-agent algorithm was given in Tensorflow and so I implemented it in Pytorch with a few experimental adjustments. Although final experiments did not yield, I verified the implementation of the environment with extensive debugging and so I felt as though it would be of use to the wider community incase they would like to experiment with a multi-agent environment for protein folding:

[https://github.com/honne23/Dissertation](https://github.com/honne23/Dissertation)

&amp;#x200B;

The environment can be found in Code/Environment/Bravais.py. It comes in the form of a bravais lattice where the actions available to each agent is the set of unit movement vectors along each dimension of space (27 in total for 3D). By default, it is configured in a 3D FCC configuration. 

You can interact with it through a similar interface as the OpenAI Gym API and there are plans to port it to the standard Gym interface in future. The catch is that the `step` function takes a vector where each component represents the index of the chosen action for each agent. 

For an extended explanation of any of the above topics, please see the report :).",reinforcementlearning,honne23,False,/r/reinforcementlearning/comments/j46u58/multi_agent_environment_for_protein_structure/
Basic Q learning approach,1601686334,"I recently began teaching myself about how reinforcement learning works and through that I developed a question that I cannot seem to understand - what is it that stops me from developing a simple Q learning algorithm and training it on each of OpenAI's ""Classic Control"" environments successfully?",reinforcementlearning,clarky103,False,/r/reinforcementlearning/comments/j46kfw/basic_q_learning_approach/
Advantage Weighted Actor Critic,1601681542,,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/j45e9t/advantage_weighted_actor_critic/
Does anyone know how to make custom environment?,1601661144,"I'm trying to implement RL to schedule the charging of EV based on the time of use price. My action is continous, so i plan to use TRPO. Any suggestions regarding the custom environment? thanks",reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j3zd79/does_anyone_know_how_to_make_custom_environment/
Kaggle features RL competition with Google Football and Manchester City F.C.,1601652404,"To the best of my knowledge it is first Kaggle RL competition that offers cash prizes.

Some time ago they began experimenting with so-called ""simulation"" competitions \[[1](https://www.kaggle.com/c/connectx), [2](https://www.kaggle.com/c/halite-iv-playground)\].

Here, problem is quite interesting: trained agent controls one player in the 11 vs 11 match. For evaluation agent plays with other agents and in this way can climb up the leaderboard.

&amp;#x200B;

**How would you approach this competition?**

[https://www.kaggle.com/c/google-football/overview](https://www.kaggle.com/c/google-football/overview)",reinforcementlearning,kk_ai,False,/r/reinforcementlearning/comments/j3wk8s/kaggle_features_rl_competition_with_google/
PPO + exploration bonusses? Stuck in local optimum,1601600121,"Hello!

I am making a 4 player 32 card game AI, it's a cooperative game (2x2players) and it can be played with or without trump.  
Without trump I got it working great, and with fewer cards it at least approaches a Nash equilibrium. Now, with trump he gets stuck in a local optimum pretty much after a couple of iterations. I have toyed around with parameters, optimizers, input, way of gathering samples, different sorts of actor and value networks etc for many hours. The 'problem' with the game is that there is high variance in how good an action in a certain state is so I guess PPO just quickly settles for safe decisions. Explicitly making it explore a lot when generating samples or using a higher entropy coefficient didn't do much. My actor and critic are standard MLPs, sharing layers or not doesn't make a difference.

I was looking into [Random Network Distillation](https://arxiv.org/pdf/1810.12894.pdf) which apparently should really help exploration and I will soon be implementing it. Do you guys have any tips on what other things I should possibly look at, pay attention to or try? I have put a lot of time in this and it's very frustrating tbh, almost at the brink of just giving up lol.

[https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html#key-exploration-problems](https://lilianweng.github.io/lil-log/2020/06/07/exploration-strategies-in-deep-reinforcement-learning.html#key-exploration-problems)

Here are multiple approaches described, from what I gather, RND would be one of the easiest to implement and possibly best in my PPO algorithm.

Any input is very much appreciated :)",reinforcementlearning,perpetualdough,False,/r/reinforcementlearning/comments/j3l2vf/ppo_exploration_bonusses_stuck_in_local_optimum/
Policy gradients and multiple gradient steps,1601596085,"In for example PPO, often multiple gradient steps are taken. 

In some implementations I have seen that the newest value function is used as the baseline. This seems wrong to me; it will drive all the advantages towards zero by training the value function to match the return (and breaks the assumption that the baseline should be independent of the action taken).

Is it ok (better?) to use the old value function as the baseline, and the new value function for the td-return if using GAE?",reinforcementlearning,roboputin,False,/r/reinforcementlearning/comments/j3k25i/policy_gradients_and_multiple_gradient_steps/
AI forgets when loading weights...,1601574959,"I am training a control AI using PPO (stable-baselines 3) and a custom open\[closed\]AI gym environment. I train the NN from scratch, and get decent results. Once it is done training, I load it with the goal of further training, and it seems to forget everything it learned in the previous training session. I have attached a picture for more clarity. Dark red was loaded into the bright red network.

Does anyone have any experience with these packages and loading models? How do I avoid this catastrophic forgetting?

[Reward per episode during training.](https://preview.redd.it/7it6po51viq51.png?width=1776&amp;format=png&amp;auto=webp&amp;s=fe4e2afb7722b091e2d556219697b1084be472a1)",reinforcementlearning,SupMathematician,False,/r/reinforcementlearning/comments/j3dn7u/ai_forgets_when_loading_weights/
How can we apply RL on a simple 5R + gripper robot,1601564679,"In the mechatronics lab in my university, I am allowed to work with 2 robots both of which are 5R(5 revolute joint) robots with a gripper, namely dobot ([https://www.dobot.us/](https://www.dobot.us/)) and scorobot-ER V Plus. 

I have successfully simulated a youbot using V-REP (as part of a capstone project) to pick and place a cube at desired location. I would like to use RL on these robots. What are the various ways in which RL can be used on these robots? I have some experience of RL but not related to robotics. Other equipments like Raspberry pi camera are also available.   Would be grateful for any suggestion.",reinforcementlearning,51times,False,/r/reinforcementlearning/comments/j3aan4/how_can_we_apply_rl_on_a_simple_5r_gripper_robot/
PettingZoo: Gym for Multi-Agent Reinforcement Learning,1601560802,,reinforcementlearning,Caffeinated-Scholar,False,/r/reinforcementlearning/comments/j3931k/pettingzoo_gym_for_multiagent_reinforcement/
I'm interested in using deep reinforcement learning for object detection,1601554543,Is there any code available? Or tutorial ? Can you please share it.,reinforcementlearning,harshaneo17,False,/r/reinforcementlearning/comments/j37ez1/im_interested_in_using_deep_reinforcement/
How to generate a synthetic dataset for a custom optimization problem?,1601542742,"Hey everyone,

I've been currently working on my master's thesis. I have an optimization problem but I don't know how to generate a dataset (AFAIK, academic papers apply their synthetic datasets). I'd really appreciate it if anyone could help and give me some general tips. (I found [a blog](https://www.samyzaf.com/ML/rl/qmaze.html#Q-Training) but I'm not able to figure it out).

Thanks,",reinforcementlearning,Fit_Art8427,False,/r/reinforcementlearning/comments/j34zxx/how_to_generate_a_synthetic_dataset_for_a_custom/
WatchCarsLearn - Cars learn to drive using NEAT,1601541538,,reinforcementlearning,Ringsofthekings,False,/r/reinforcementlearning/comments/j34sbv/watchcarslearn_cars_learn_to_drive_using_neat/
WatchCarsLearn - Cars learn to drive using NEAT,1601541227,,reinforcementlearning,Ringsofthekings,False,/r/reinforcementlearning/comments/j34qk2/watchcarslearn_cars_learn_to_drive_using_neat/
WatchCarsLearn - Cars learn to drive using NEAT,1601540919,,reinforcementlearning,Ringsofthekings,False,/r/reinforcementlearning/comments/j34omj/watchcarslearn_cars_learn_to_drive_using_neat/
Flocking Behavior Driven by Multi-Agent RL (code link in comments),1601516272,,reinforcementlearning,Familiar-Watercress2,False,/r/reinforcementlearning/comments/j2za2i/flocking_behavior_driven_by_multiagent_rl_code/
"Why noisy oscillation pattern on the Average reward plot for 10-armed Testbed ? Really confusing...Especially for greedy methond. Should the plot of greedy be smooth? It seems to be a constant ""randomness"" for both greedy and epsilon-greedy. Why?",1601514348,,reinforcementlearning,FatasticAI,False,/r/reinforcementlearning/comments/j2ysgn/why_noisy_oscillation_pattern_on_the_average/
Researchers Introduce New Algorithm For Faster Reinforcement Learning,1601501073,[https://analyticsindiamag.com/new-algorithm-awac-reinforcement-learning/](https://analyticsindiamag.com/new-algorithm-awac-reinforcement-learning/),reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/j2v2jg/researchers_introduce_new_algorithm_for_faster/
Which features are backpropagated in Actor network of PPO algorithm?,1601470166,"Hi everyone, 

I have implemented a PPO algorithm in PyTorch, but I am not sure about which features should I backpropagate to calculate its gradients after getting the actor loss. ATM I have detached everything from the computational graph apart from entropy and the clipped surrogate loss but not sure if this is correct because I suffer from exploding gradients in some envs as well. I thought that the reason for that might be falsy backpropagated gradients. Should it be both the clipped surrogate loss and the entropy or just the entropy of the distribution from which I select actions?",reinforcementlearning,k_ili,False,/r/reinforcementlearning/comments/j2lh56/which_features_are_backpropagated_in_actor/
"Tutor RL: I’m a master’s student and relatively new in the field of RL. Anyone interested to teach me RL? (Paid). If you’re expert at RL and interested to tach me, DM me or comment here. Thanks.",1601461591,,reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j2jlva/tutor_rl_im_a_masters_student_and_relatively_new/
This is really cool. I wonder if we can draw some inspiration from this for RL.,1601439709,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/j2faxb/this_is_really_cool_i_wonder_if_we_can_draw_some/
This is really cool. I wonder if we can draw some inspiration from this for RL.,1601439646,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/j2faek/this_is_really_cool_i_wonder_if_we_can_draw_some/
My lab has special funding for buying books. My supervisor have bought these for me. Do you guys have any more suggestions that increase my RL collections? Thanks.,1601436432,,reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j2ehgl/my_lab_has_special_funding_for_buying_books_my/
Energy-based Surprise Minimization for Multi-Agent Value Factorization,1601421440,[removed],reinforcementlearning,No_Advisor_9263,False,/r/reinforcementlearning/comments/j2akrh/energybased_surprise_minimization_for_multiagent/
Optmization steps and subsampling in PPO,1601416426,"Hello!  
I'm currently experimenting with PPO in a stock market environment. Since we don't have much time to implement our own version of the algorithm, we decided to go with [Tensorforce](https://github.com/tensorforce/tensorforce) implementation.  


There are two hyperparameters in the framework that I can't explain:

* **Subsampling fraction:** as far as I understand, it controls how many steps of the episode will be used to compute the backpropagation. If this is the case, some steps are ignored when adjusting the policy?
* **Optimization steps:** determines how many times back-propagation will run with the same gradient values. Doing it wouldn't imply in updating a policy with the experience of an older policy? How it impacts the model performance?",reinforcementlearning,fig0o,False,/r/reinforcementlearning/comments/j29434/optmization_steps_and_subsampling_in_ppo/
Training multi-agent RL with OpenAI gym interface,1601409416,"How does one train multi-agent RL systems using the OpenAI gym interface? Is this even possible?

As a recap, the gym interface makes training RL simple, by requiring the users defining the environment to implement the functions reset() and step(). The reset() function resets the environment state and returns a state/observation, and the step() function takes an action and returns the next state/observation.

In multi-agent RL, you have a sequential decision-making problem of multiple autonomous agents that operate in a common environment, each of which aims to optimize its own long-term return by interacting with the environment and other agents.

So let's say we're training a soccer environment, where you have two agents trying to kick a ball into each other's goals. How would you implement this in a gym environment? For example, what would the reset() function return? If the reset() function resets the environment and returns a state/observation, this implies that there is only one agent in the environment right?",reinforcementlearning,sporadic_chocolate,False,/r/reinforcementlearning/comments/j26vw6/training_multiagent_rl_with_openai_gym_interface/
"Quality, research implementation of SAC?",1601402947,"I’m looking for a good implementation of SAC that gets performance on-par with the results reported in the paper and I have yet to find something. 

I also need to be able to save a fully trained policy and then generate a dataset with it (I’m working on some offline RL style stuff). So it would be great if the implementation supports saving/loading policies. 

Does anyone know of a good implementation for this? I will be delighted to hear any recommendations. Thanks!",reinforcementlearning,OptimalOptimizer,False,/r/reinforcementlearning/comments/j24r2o/quality_research_implementation_of_sac/
Learning dynamics models vs using defined dynamics models in robotics,1601372603,"I want to know if there are any papers which show a comparison of using learnt models vs using predefined models for robot control. I know alot of engineering effort goes into system modeling hence these models would be definitely better in simulation. However, in real world I believe learnt models may perform better since they would account for real world factors such as slip/deformations in robot structure/non perfect DH parameters. 

Are there any papers in this ? I need this for a literature review and any help would be really appreciated.",reinforcementlearning,rs_dark_side,False,/r/reinforcementlearning/comments/j1w5xp/learning_dynamics_models_vs_using_defined/
Self-play learning in games,1601356223,"Guys, hello everyone. Are there people here who do reinforcement learning in games? Especially self-play learning. I really need help and would like to find someone from whom I could ask a couple of questions on this topic. I have very bad English and now I speak with you in the language of google translator, sorry)",reinforcementlearning,El_cartel,False,/r/reinforcementlearning/comments/j1sxet/selfplay_learning_in_games/
"Hey guys, I’m interested in safe reinforcement learning. Do you guys any contents that you think would be useful to look at?",1601352953,,reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j1s7gd/hey_guys_im_interested_in_safe_reinforcement/
Understanding where to go from Sutton's book,1601338893,"I've largely read Sutton's text on RL, and found it pretty interesting. In particular, I thought the first, say, 5-8 chapters were pretty clear and I was able to implement some of the examples from scratch in Python. However, after the tabular method chapter things started to get more complex and there were fewer coding exercises to do to follow along with. On top of this, there were just *so many* algorithms presented that I have trouble understanding what ones I should consider when solving a real-life problem. Similarly, a cursory glance at some of the packages for RL which are out there seem to use methods that are barely even mentioned in the text (DQN, for example). 

&amp;#x200B;

So now that I've largely (though not completely) finished the book, I'm wondering, where to go from here? I want to use RL to solve a battery charging problem where I have to charge a battery at variable prices against a backdrop of potential discharge events (of unknown size but I understand the underlying distribution). My state space would be a good deal larger than anything treated in the early parts of the text so it seems tabular methods are likely out, but I didn't get much practice with figuring out how approximate methods are used or if there are any good libraries for using them to understand if it would be easy to use them on my problem. 

Where would be a good next place to start? How do I go from the book's background to a real implementation? I realize this question is a bit vague, so apologies for that, but just trying to figure out how to bridge the gap from the simple examples in the book to the production level code I see in the RL libraries I've glanced at.",reinforcementlearning,compost_embedding,False,/r/reinforcementlearning/comments/j1ofow/understanding_where_to_go_from_suttons_book/
Why do my rewards reduce after extensive training using D3QN,1601331727,"I am running a drone simulator for collision avoidance using a slight variant of D3QN. The training is usually costly (runs for at least a week) and I have observed that reward function gradually increases during training and then drastically drops. In the simulator, this corresponds to the drone exhibiting cool collision avoidance after a few thousand episodes. However, after training for more iterations it starts taking *counterintuitive* actions such as simply crashing into a wall (I have checked to ensure that there is no exploration at play over here).

Does this have to do this overfitting? I am unable to understand why my rewards are falling this way.",reinforcementlearning,Academic-Rent7800,False,/r/reinforcementlearning/comments/j1mdd5/why_do_my_rewards_reduce_after_extensive_training/
I’m trying to solve a problem where my actions are both discrete and continuous. Which algorithm is better fit? Actor-critic?,1601318627,,reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j1i0tt/im_trying_to_solve_a_problem_where_my_actions_are/
Deal with states of different sizes,1601317051,"Hi everyone.

I'm working on a project where the size of my state is a vector but where the size can vary. And the size of the actions is correlated with the size of the state in input. 

For example :

\-  I can have a vector of size 6, so I want a action distribution of size 7,

\- next step vector of size 4 so I want a action distribution of size 5, etc.

Is there anyway to deal with this ? I tried to look for Conv1d but it doesn't to fit",reinforcementlearning,Krokodeale,False,/r/reinforcementlearning/comments/j1hh9c/deal_with_states_of_different_sizes/
Emergent Tool Use From Multi-Agent Autocurricula | Paper Explained,1601273507,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/j16t5z/emergent_tool_use_from_multiagent_autocurricula/
[QUESTION] Monte Carlo Policy Evaluation,1601238651,"I'm trying to implement a tabular version of First Visit MC for approximating the Value function for a particular policy. In my environment, there is one goal state with reward +1, and every other state has 0 reward. Once the agent reaches this goal state, the episode terminates.

If I initialize the Value function arbitrarily to 0 for all states, does it make sense that the approximated value of this goal state is zero? (Since I am not able to take any actions *from* this state to another?) Thanks for your help :)",reinforcementlearning,rapture_inc,False,/r/reinforcementlearning/comments/j0yae2/question_monte_carlo_policy_evaluation/
Environments has huge state spaces with known transition dynamics,1601214728,If we were to given an huge state action space environment that we know all transition dynamics (assume Markov property holds). Is taking model free approach still preferable over a model based approach,reinforcementlearning,massagre,False,/r/reinforcementlearning/comments/j0rftq/environments_has_huge_state_spaces_with_known/
MultiArm Bandits - Live Training Part 2: UCB Algorithms,1601212150,"I am hosting a live training session on multi arm bandits (MAB). This will be the part 2 of my session.  The video of the previous session is available here: [https://youtu.be/\_VvnEu\_2i2k?t=275](https://youtu.be/_VvnEu_2i2k?t=275). The sessions are interactive and you can ask questions and clarify your doubts. 

This time around we will continue to build the logic from the greedy algorithms to the variants of UCB algorithms. We will also touch upon some basics of Explore then Commit algorithms too. As usual, I will have the hands on session as well, besides just the lectures. 

I got great feedback from some reddit users too. See the comments here: [https://www.reddit.com/r/reinforcementlearning/comments/iwcrx4/doing\_a\_live\_training\_on\_multi\_arm\_bandits\_for/](https://www.reddit.com/r/reinforcementlearning/comments/iwcrx4/doing_a_live_training_on_multi_arm_bandits_for/)

You can find the meetup event here, though most of the time we do sessions relation to Microsoft AI offerings both commercial and Open source.

[https://www.meetup.com/Microsoft-AI-ML-Community/events/273314958/](https://www.meetup.com/Microsoft-AI-ML-Community/events/273314958/)

Or you can subscribe to the channel to get notifications. I go live every Tuesday at 7pm Singapore time.

YouTube: [https://www.youtube.com/setuchokshi](https://www.youtube.com/setuchokshi)

Twitch: [https://www.twitch.tv/setuchokshi/](https://www.twitch.tv/setuchokshi/)

&amp;#x200B;

https://preview.redd.it/rad658rxvop51.png?width=1576&amp;format=png&amp;auto=webp&amp;s=46fde87f456a1438b591be0800ee5baf37de075b",reinforcementlearning,setuc,False,/r/reinforcementlearning/comments/j0qtvy/multiarm_bandits_live_training_part_2_ucb/
a bird's eye view on RL,1601195374,"[https://medium.com/@maxim.volgin/the-many-flavours-of-reinforcement-learning-7f9eda6798eb](https://medium.com/@maxim.volgin/the-many-flavours-of-reinforcement-learning-7f9eda6798eb)

a quick reminder that not RL is DRL and there are simple solutions that simply work",reinforcementlearning,maxvol75,False,/r/reinforcementlearning/comments/j0no3v/a_birds_eye_view_on_rl/
Model Based RL library??,1601191829,"The common RL libraries like Stable Baselines, RLLib have implementations of algorithms, which I agree can be modified to be used in a model based setting. I was wondering if anyone has come accross a library that has model based algorithms also coded up and ready to use? TIA",reinforcementlearning,rpatr_54,False,/r/reinforcementlearning/comments/j0n0f7/model_based_rl_library/
Loss in REINFORCE algo Theory vs. Implementation,1601189041,"I was reading through the Policy Gradient methods in [Sutton](http://www.incompleteideas.net/book/the-book.html) where I came across the policy gradient theorem. The cost function is essentially the expectation of reward, while using the policy to take the actions.

Coming to my question, all the posts I've seen online on how to implement the REINFORCE algorithm use `G_t * log(π(a_t | s_t)) summed over all t` as the loss.

Why is that so? I did read the policy gradient theorem and understand the gradient of cost function is `G_t * ∇log(π(a_t | s_t)) summed over all t` (∇ added) but I'm not able to connect theory and the implementation parts.

&amp;#x200B;

I sincerely thank any feedback/answers.",reinforcementlearning,Moltres23,False,/r/reinforcementlearning/comments/j0mhb9/loss_in_reinforce_algo_theory_vs_implementation/
Gym-like Testbed of Multi-Agent RL,1601165306,"I was wondering if there are any testbeds or multi-agent RL. Specifically, it would be great to have gym-like interface and have environments that contain  (1) two-player zero-sum stochastic games, (2) multi-agent stochastic games. It would be ideal if I can program each player separately.",reinforcementlearning,sparsity_,False,/r/reinforcementlearning/comments/j0gwws/gymlike_testbed_of_multiagent_rl/
RL in Demand Response,1601146758,"Hey guys, I’m new to RL. I would like to use RL to schedule household appliances such as washing machine or EV. In this case, I have to consider both discrete and continuous action. How should I approach now? Is there anyone here worked on this topic before? Would really appreciate if you help me. Thanks.",reinforcementlearning,Same_Championship253,False,/r/reinforcementlearning/comments/j0bnkm/rl_in_demand_response/
Training Agents to teach Social Distancing and Self-Isolation with an Epidemic Simulation,1601143656,"Hi folks,

 I'm Ege Hoşgüngör, a recently graduated MSc Student from University of Sussex.

I have completed my dissertation and wanted to share.  The subject was creating a physics-based epidemic simulation and training AI agents by utilizing Reinforcement Learning techniques. The project presents clear evidence of how social distancing and self-isolation are mathematically correct ways for flattening the epidemic spread curve.  I would like to know what you guys think about it.  You can ask anything about the project. More information can be found in my LinkedIn:

[https://www.linkedin.com/posts/hosgungor\_cover-page-activity-6714173535100661760-uVP0](https://www.linkedin.com/posts/hosgungor_cover-page-activity-6714173535100661760-uVP0)

and  the GitHub Repo:

[https://github.com/Hsgngr/Pandemic\_Simulation](https://github.com/Hsgngr/Pandemic_Simulation)

Many Thanks, Ege",reinforcementlearning,WiseStrider,False,/r/reinforcementlearning/comments/j0arym/training_agents_to_teach_social_distancing_and/
Training Agent to make them find Social Distancing &amp; Self Isolation,1601143410,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/j0apkb/training_agent_to_make_them_find_social/
Next step in life,1601133419,"Hi. I'm a 23yo who recently finished his MSc in Artificial Intelligence and thinking about what to do next. I don't have any job experience, and I'm slowly realising that breaking into the AI industry is very hard. This COVID situation doesn't help either.

So here's my ""career"" idea: since getting a job at this time is even more difficult than before, I'm thinking about working a menial job (like McDonald's) part-time, so it pays the bills, it's easy and fast to get hired, and I keep some form of human contacts. The rest of my time will be devoted to self-study. I have created a learning curriculum with list of books and resources that I expect would take me 1-1.5yr to complete. At the end of this curriculum, I will have a solid math foundation, have implemented from scratch with numpy a handful of ML / RL algorithms, have solved games such as Poker and Connect4 using RL and what I read in papers/sutton book/others, participated in ML / RL online competitions, and hopefully starting to get good paper reading skills. 

I have self-studied a lot in my life and have no problem with finding the discipline to sit down and work by myself. 

My life goal is to become an expert at a particular sub-topic within AI. Which sub-topic I don't really know yet, but I'm interested in Reinforcement Learning, and even more foundational topics like new models of neural networks. There's the other idea of trying to enrol into a PhD program, but top programs and getting funding just seems insanely competitive. Plus, I want to work in the industry for a few years before. I don't feel stable enough to embark into a 6-yr journey phd program. 

This is why I want to do a year of self-study first, because to me it seems that after that year, my profile will be quite good and I can confidently approach companies for junior ML roles.

What do you think about my approach?",reinforcementlearning,pythonistaaaaaaa,False,/r/reinforcementlearning/comments/j07sv9/next_step_in_life/
Policy gradient standardization,1601119323,"I am trying to do a modification to this Karpathys code on policy gradient: [github link](https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5#file-pg-pong-py). The idea I would like to try is that could it for some problems work better to, instead of doing the standardization (substract mean and divide by deviation) across all timesteps of one trajectory, do it separately for each timestep and across all trajectories of the minibatch. 

My intuition tells that this should make the rewarding more fair especially in trajectories where some sets of states are only seen in some time windows of the trajectory. With the current system if the initial state is in an area of poor value or the reward is given only in the end then the action-state combinations that occur early will always be regarded poor even if they are correct choices and eventually lead to good states. 

So originally the standardization part is run after each episode (each trajectory):

    # standardize the rewards to be unit normal (helps control the gradient estimator variance)
        discounted_epr -= np.mean(discounted_epr)
        discounted_epr /= np.std(discounted_epr)

But I would like to run it only after we have gathered a whole minibatch of information. The problem is that I have poor Python programming skills and I would like to know what data types and loop structures would be the best. To be honest the Karpathys code is so elegant that I have had great difficulties in following it. I guess the sort of pythonic pseudo code should be something like this:

    for each episode:
      edplogp_buffer.append(epdlogp)
      discounted_epr_buffer.append(discounted_epr) 
      
      if episode_number % batch_size == 0: 
        for n, timestep in enumerate(discounted_epr_buffer): #Somehow index timesteps
          discounted_epr_buffer[n] -= np.mean(timestep) #Take the mean and deviation 
          discounted_epr_buffer[n] /= np.std(timestep)  #across all episodes
        epdlogp *= discounted_epr_buffer #This would need to be elementwise 
        for episode_edplogp in epdlogp: #Now iterate through episodes
          grad = policy_backward(eph, episode_epdlogp)
          for k in model: grad_buffer[k] += grad[k]
        #rest is the same  
        for k,v in model.iteritems():
          g = grad_buffer[k] # gradient
          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2
          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)
          grad_buffer[k] = n    
            
        clear edplogp_buffer and discounted_epr_buffer

Do you think my thinking makes any sense and how should I go about actually coding this thing? Especially how to deal with multidimensional arrays that can be of different sizes and how to index them? The number of episodes will of course always be the same across one minibatch but the number of time\_steps will differ.",reinforcementlearning,Apj_1,False,/r/reinforcementlearning/comments/j04cr9/policy_gradient_standardization/
A spaced repetition app for keeping your reinforcement learning knowledge fresh,1601059930,,reinforcementlearning,josemwas,False,/r/reinforcementlearning/comments/izpgcc/a_spaced_repetition_app_for_keeping_your/
"""DQN Zoo"": Jax/Haiku/RLax Python implementations of DQN/Double DQN/Prioritized sampling/C51/Quantile/Rainbow/IQN {DM}",1601050310,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/izmcjm/dqn_zoo_jaxhaikurlax_python_implementations_of/
Normalizing Advantage Estimates in PPO causing strange behavior. Why?,1600984014,"The other day I posted this about a strange issue I was having using PPO:

[https://www.reddit.com/r/reinforcementlearning/comments/ixi11d/any\_ideas\_about\_what\_could\_possibly\_cause/](https://www.reddit.com/r/reinforcementlearning/comments/ixi11d/any_ideas_about_what_could_possibly_cause/)

To reiterate, I am using actor-critic PPO to learn low-level controls for an aircraft guiding itself to a target final position. The input is the state of the aircraft in space: 3 dimensions for the position, 3 dimensions for velocity. The output is a single real-valued control command in \[-pi/2, pi/2\]. What was happening was that the mean reward during training for a batch of trajectories either just continuously oscillates and never settles or converges, or just keeps getting lower and lower. It would even oscillate up to a near-perfect value, then just slide back down. If it did converge it converged to a very low reward value even though it experienced very high reward states during training. The agent would achieve a near 0 reward (error from the target position, so perfect performance), yet still continuously oscillate, as if it is learning, then once it gets close to hitting its target state, diverges, then just repeats that cycle.

Well, I seem to have solved the problem. Before, I was normalizing the advantages in PPO via

u\_adv = advantages.mean()

std\_adv = advantages.std() +  1e-6

advantages = (advantages - u\_adv) / std\_adv 

All I did was take out this normalization and now the agent seems to be learning perfectly. Anyone know why this happened? My advantage estimate is simply (discounted sum of rewards - value function prediciton)",reinforcementlearning,ShittingTits,False,/r/reinforcementlearning/comments/iz6lpi/normalizing_advantage_estimates_in_ppo_causing/
5 Reasons Why Having Multiple Mentors Prepares Kids for the Future We understand that transitioning across different teachers or mentors can be a bit difficult for students and families at times... #JuniLearning#Kids#Mentors#Student#STEM#metacognitiveskills#education#news#blockgeni,1600955309,,reinforcementlearning,Blockgeni,False,/r/reinforcementlearning/comments/iyxlad/5_reasons_why_having_multiple_mentors_prepares/
Actor Critic Model in CartPole-v0 environment,1600945881,"Hey, i just tried to implement Actor Critic Model on CartPole environment and i noticed something strange and i don't know why it happened. May be you guys can help me out. I was just trying to check what happens when i increase the number of neurons in hidden layer in both actor and critic models. When the number of neurons in actor is increased no matter how many no of neurons is in critic network, the model performs horrible. While it performs really good when the number of neurons in hidden layers of critic model is high and that in actor is low. Can somebody have an explaination for this?",reinforcementlearning,Happy-Complaint-8171,False,/r/reinforcementlearning/comments/iyv8av/actor_critic_model_in_cartpolev0_environment/
CleanRL v0.4.0; added experimental Apex-DQN and CarRacing-v0,1600916274,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/iyozdi/cleanrl_v040_added_experimental_apexdqn_and/
"""Tasks, stability, architecture, and compute: Training more effective learned optimizers, and using them to train themselves"", Metz et al 2020 {GB} [beating Adam with a hierarchical LSTM]",1600914529,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iyoi8a/tasks_stability_architecture_and_compute_training/
Advice for beginners,1600909172,"Hi guys,

I'm new to RL, I just finished David silver's course on RL a couple of months ago. Well, it was a good experience and I understood a little more than half of what he taught. But now I'm left hanging, I don't know where to go after this.

I was wondering maybe you guy could suggest some projects, blogs or any other resources for absolute beginners, recommendations for someone with intermediate knowledge is also welcome. Basically I am trying to make a plan of things to do to get good at RL.",reinforcementlearning,gubberex,False,/r/reinforcementlearning/comments/iymyag/advice_for_beginners/
Does anybody understand this,1600900318," 

Say that the reward signal is amended to only give reward to the agent at the end of an episode. So, the reward is 0 for every time step, with the exception of the final time step. When the episode terminates, the agent receives a reward of **-1**. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.)

&amp;#x200B;

The discount rate is 1.

&amp;#x200B;

The discount rate is 0.9.

&amp;#x200B;

The discount rate is 0.5.

&amp;#x200B;

(None of these discount rates would help the agent, and there is a problem with the reward signal.)

**SOLUTION:**

* The discount rate is 0.9.
* The discount rate is 0.5.

Say that the reward signal is amended to only give reward to the agent at the end of an episode. So, the reward is 0 for every time step, with the exception of the final time step. When the episode terminates, the agent receives a reward of **+1**. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.)

&amp;#x200B;

The discount rate is 1.

&amp;#x200B;

The discount rate is 0.9.

&amp;#x200B;

The discount rate is 0.5.

&amp;#x200B;

(None of these discount rates would help the agent, and there is a problem with the reward signal.)

**SOLUTION:**

The discount rate is 0.9.

&amp;#x200B;

The discount rate is 0.5.",reinforcementlearning,SilverMorra,False,/r/reinforcementlearning/comments/iykt1x/does_anybody_understand_this/
"""An adaptive deep reinforcement learning framework enables curling robots with human-like performance in real-world conditions"", Won et al 2020",1600898043,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iyk2m8/an_adaptive_deep_reinforcement_learning_framework/
Reinforcement Learning Python Library Recommendation?,1600894842,"Hi, there. I'm taking the RL class on Coursera released by University of Alberta &amp; Alberta Machine Intelligence Institute. It is great. I was wondering whether I can download the RL-Glue library to my own Anaconda? I would like to use that library to build my own project, but unfortunately I cannot  find place where I can download. Most of the links are not valid anymore. Do anyone know where I can download the library? Or is there any new recommended library on RL? Appreciate any helpful response. Thank you.",reinforcementlearning,FatasticAI,False,/r/reinforcementlearning/comments/iyj198/reinforcement_learning_python_library/
Python library recommendation?,1600894761,"Hi, there. I'm taking the RL class on Coursera released by University of Alberta &amp; Alberta Machine Intelligence Institute. It is great. I was wondering whether I can download the RL-Glue library to my own Anaconda? I would like to use that library to build my own project, but unfortunately I cannot  find place where I can download. Most of the links are not valid anymore. Do anyone know where I can download the library? Or is there any new recommended library on RL? Appreciate any helpful response. Thank you.",reinforcementlearning,FatasticAI,False,/r/reinforcementlearning/comments/iyj07k/python_library_recommendation/
Reinforcement learning in Matlab,1600881337,Has anyone used the RL toolbox in MATLAB? I need help accessing a saved agent.,reinforcementlearning,sayakm330,False,/r/reinforcementlearning/comments/iyef3a/reinforcement_learning_in_matlab/
Tutorial for creating TF-agents environment,1600881255,"Hi! I’m new to RL and I’m trying to create my own environment with TF-agents. I’m using TF-agents because it seems very easy to train a DQN when the environment is built with this. I have followed TF’s own [tutorial](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial), however, I still have many questions. Do you know any great tutorial I can follow to fully understand how to build an environment with TF-agents?

Thanks in advance!",reinforcementlearning,Candpolit,False,/r/reinforcementlearning/comments/iyee37/tutorial_for_creating_tfagents_environment/
"Any ""trust region"" approach for value-based methods?",1600878676,"A big problem with value-based method is that a small change in the value function can lead to large changes to the policy (see eg https://arxiv.org/abs/1711.07478).

With Policy Gradient methods, a common way to avoid this is to restrict how much the policy can change.

I understand that this may not be so straight-forward with value-based methods as the policy is derived from a value function though a `max` operation.

Still, has there been any research in this direction? Naively, you could imagine that at each iteration you could update the VF multiple times, checking each time that the resulting policy didn't change too much (based for example on the action that would be picked by the new policy based on the last N experiences).",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/iydie3/any_trust_region_approach_for_valuebased_methods/
"Does PPO keep ""memory"" of observations it has not seen in the current iteration?",1600878525,"I am working with an environment that has short episodes, but there is a large set of potential variations for a particular episode. Ie. at the start of each episode, a ""level"" is drawn from a large selection of levels. All episodes are the same overarching problem but there are so many potential levels, that realistically i can only collect on a small fraction of levels for a single iteration of PPO.

Unlike other problems, where as your agent gets better, states that are considered suboptimal are less relevant as your agent should not be in these states. For the problem i am working on, the better my agent gets, the same likelihood of encountering any particular state so it is not a good to idea for PPO to forget how to deal with any particular state.

This being said, i am trying to understand how PPO keeps memory of various observations. As i understand the main idea of PPO is that you use the current policy to generate samples, you generate a new policy based on how the current policy performs, and you clip the evolution so that your policy doesn't change too much in a single iteration.

This means that the value function is rebuilt for every iteration, and it is built only from the samples seen in a particular iteration. Given this line of thought and how NN's work, it would seem that the very least the value function only knows how to handle observations it has seen in the current iteration.

What i am wondering, is if the new policy is an evolution of the previous policy, does it know how to handle observations for example that have been seen in the prior policy, or does it only know how to handle observations seen in the current iteration as mentioned above? Does the amount of collect episodes per iterations need to be high enough to cover the entire set of possible episodes/levels?",reinforcementlearning,Yogi_DMT,False,/r/reinforcementlearning/comments/iydgi8/does_ppo_keep_memory_of_observations_it_has_not/
Very nice research-oriented DRL overview,1600878402,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/iydezq/very_nice_researchoriented_drl_overview/
Policy Gradient vs Deep Q learning,1600871919,"Hi, I am very confused in implementation of reinforcement learning as policy gradient and as deep q networks. For example i was trying to run the CartPole-v0 using both algorithms. I am not sure what difference does it make? As mentioned in most of the articles, policy gradient is for continuous state and actions while for discrete we can use the deep Q learning. But my cartpole was performing very well with both these algorithms. So what difference does the policy gradient make?",reinforcementlearning,Happy-Complaint-8171,False,/r/reinforcementlearning/comments/iybcqq/policy_gradient_vs_deep_q_learning/
What does the value function of a state actually represent in value iteration,1600860826,"The Value iteration algorithm is used to find an optimal policy in a MDP where the transition probabilites are known. The value function will eventually converge  and the best action to take at a particular state is the one which maximizes the expectation over our transition matrix of the reward and the discounted value of the state we end up at, for all possible states we can reach given this particular action. However, does the value of a particular state actually represent something or is it just a number to make a ranking of how good the different states are.  For example, if we have a bandit problem with a time horizon of 3. Then the value function at the initial state would be how much reward we expect to obtain during the three pulls of the arms. This makes sense and the value function of a state actually represent a quantity of interest.  


In my particular problem at the moment, I have three processors and two different types of jobs which comes in to a queue at each timestep, each job having a known probability of entering the queue. Each processor has a known probability of finishing a job at a timestep. At each timestep my possible actions are to move the job from the queue to any of the processors or ignore the job in the queue, which will then be replaced by a new job at next timestep. If a processor succesfully finishes a job, a reward is obtained. I use value iteration to find the best policy for each state. However, in this case I don't understand what the quantity of the value function actually represent. There is no time horizon defined and if I were to have the process run for infinity I would obtain infinite reward. My guess is that the value in this context doesn't actually have any meaning other than being used for finding a policy. I find a lot of information about the value iteration algorithm, but none answer this question, so I have maybe misunderstood some trivial part of the concept.",reinforcementlearning,SlobodanTankovic,False,/r/reinforcementlearning/comments/iy8dkd/what_does_the_value_function_of_a_state_actually/
Isn't any multi player game partially observable?,1600837783,"Sorry if this is a dumb question, but I see examples of partially observable games being stuff with hidden cards, etc. I've also seen backgammon and chess being referred to as fully observable. But isn't the state of the other player by definition not observable? For instance, they may be trying to use a strategy that you must infer. This is similar to how a robot that can't see behind it might infer that there is someone behind it from a shadow, for example. It seems to me that the only fully observable environments are very simple single agent environments, like grid world.",reinforcementlearning,RusticScentedMale,False,/r/reinforcementlearning/comments/iy417a/isnt_any_multi_player_game_partially_observable/
Any ideas about what could possibly cause behavior like this?,1600755396,"I am using actor-critic PPO to learn low-level controls for an aircraft guiding itself to a target final position. The input is the state of the aircraft in space: 3 dimensions for the position, 3 dimensions for velocity. The output is a single real-valued control command in \[-pi/2, pi/2\]. What essentially happens is that **the mean reward during training for a batch of trajectories just continuously oscillates and never settles or converges. It will even oscillate up to a near-perfect value, then just slide back down. If it does converge it converges to a very low reward value even though it experienced very high reward states during training.** The agent will achieve a near 0 reward (error from the target position, so perfect performance), yet still continuously oscillate, as if it is learning, then once it gets close to hitting its target state, diverges, then just repeats that cycle.  

This seems like an obvious too large of a learning rate problem or perhaps too small a batch problem, but decreasing the learning rate and/or increasing the batch size doesn't stabilize it. I'm using a modestly sized GRU network with 60 recurrent steps (trajectories are of length 1600) for the policy and value function (two networks). For the value function fitting, I am using a discounted sum of rewards as the targets. For estimating the advantage, I am using the difference between a discounted sum of rewards and the value function estimate. I would use GAE but I am using two discount factors, one for the final reward, and one for the reward shaping. This is incompatible with GAE.

Is there anything obvious that I am unaware of that may cause behavior like this? I'm not too experienced with RL at this point.",reinforcementlearning,ShittingTits,False,/r/reinforcementlearning/comments/ixi11d/any_ideas_about_what_could_possibly_cause/
The custom loss function in Keras is not working,1600753937,"I am implementing the PPO algorithm using Keras but encountered some issues related to custom loss function in Keras.

&amp;#x200B;

Tensorflow version: 2.3.0 &lt;br&gt;

Keras version: 2.4.3

&amp;#x200B;

\*\*This is the custom loss function in Keras:\*\*

    ```python
    def ppo_loss(old_prediction, advantage, reward, value):
        def loss(y_true, y_pred):
            # print(type(y_true), type(y_pred), type(old_prediction), type(advantage), type(reward), type(value))
            newpolicy_probs = y_pred
            ratio = K.exp(K.log(newpolicy_probs + 1e-10) - K.log(old_prediction + 1e-10))
            clip_ratio = K.clip(ratio, min_value=1 - EPSILON, max_value=1 + EPSILON)
            surrogate1 = ratio * advantage
            surrogate2 = clip_ratio * advantage
            actor_loss = -K.mean(K.minimum(surrogate1, surrogate2))
            critic_loss = K.mean(K.square(reward - value))
            entropy_loss = K.mean(-(newpolicy_probs * K.log(newpolicy_probs + 1e-10)))
            total_loss = CRITIC_DISCOUNT * critic_loss + actor_loss - BETA * entropy_loss
            return total_loss
        return loss
    ```

\*\*This the actor\_model:\*\*

    ```python
        def actor_model(input_dims, output_dims):
            state = Input(shape=(input_dims,), name='state_input')
            old_prediction = Input(shape=(output_dims,), name='old_prediction_input')
            advantage = Input(shape=(1,), name='advantage_input')
            reward = Input(shape=(1,), name='reward_input')
            value = Input(shape=(1,), name='value_input')
    
            x = Dense(HIDDEN_UNITS, activation='tanh', name='fc1')(state)
            x = Dense(HIDDEN_UNITS, activation='tanh')(x)
            policy = Dense(output_dims, activation='tanh', name='policy')(x)
    
            actor_network = Model(inputs=[state, old_prediction, advantage, reward, value], outputs=[policy])
            actor_network.compile(optimizer=Adam(lr=LEARNING_RATE), 
                loss=ppo_loss(
                    old_prediction=old_prediction,
                    advantage=advantage,
                    reward=reward,
                    value=value),
                # run_eagerly=True
            )
            # actor_network.summary()
            return actor_network
    ```

\*\*And I am getting this error:\*\*

    ```bash
    Traceback (most recent call last):
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\execute.py"", line 59, in quick_execute
        tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
    TypeError: An op outside of the function building code is being passed
    a ""Graph"" tensor. It is possible to have Graph tensors
    leak out of the function building context by including a
    tf.init_scope in your function building code.
    For example, the following function will fail:
      u/tf.function
      def has_init_scope():
        my_constant = tf.constant(1.)
        with tf.init_scope():
          added = my_constant * 2
    The graph tensor has name: old_prediction_input:0
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""train.py"", line 426, in &lt;module&gt;
        agent.train()
      File ""train.py"", line 370, in train
        actor_loss = self.actor.fit(
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 108, in _method_wrapper
        return method(self, *args, **kwargs)
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1098, in fit
        tmp_logs = train_function(iterator)
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\def_function.py"", line 780, in __call__
        result = self._call(*args, **kwds)
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\def_function.py"", line 840, in _call
        return self._stateless_fn(*args, **kwds)
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 2829, in __call__
        return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 1843, in _filtered_call
        return self._call_flat(
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 1923, in _call_flat
        return self._build_call_outputs(self._inference_function.call(
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\function.py"", line 545, in call
        outputs = execute.execute(
      File ""C:\Users\rahul\Desktop\Train-ml-agents\python-envs\offline_training\lib\site-packages\tensorflow\python\eager\execute.py"", line 72, in quick_execute
        raise core._SymbolicException(
    tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors, but found [&lt;tf.Tensor 'old_prediction_input:0' shape=(None, 2) dtype=float32&gt;, &lt;tf.Tensor 'advantage_input:0' shape=(None, 1) dtype=float32&gt;, &lt;tf.Tensor 'reward_input:0' shape=(None, 1) dtype=float32&gt;, &lt;tf.Tensor 'value_input:0' shape=(None, 1) dtype=float32&gt;]
    ```

So after searching on the internet I found one solution i.e to add \`\`\`run\_eagerly=True\`\`\` to the model.compile() method as:

    ```python
    actor_model.compile(..., run_eagerly=True)
    ``` 

But after adding this I am getting 0 loss value from `actor_model.history['loss']`

&amp;#x200B;

Can someone point out the error, or am I implementing something wrong?",reinforcementlearning,dhyeythumar9,False,/r/reinforcementlearning/comments/ixhqf3/the_custom_loss_function_in_keras_is_not_working/
[D] Are custom reward functions 'cheating',1600719341,"I want to compare an algorithm I am using to something like SAC. For an example consider the [humanoid](https://gym.openai.com/envs/Humanoid-v2/) environment. Would it be an unfair comparison to use simply use the distance the agent has traveled as a reward function for my algorithm, but still compare the two on the basis of total reward that is received from the environment? Would you consider this an unfair advantage or a feature of my algorithm.

&amp;#x200B;

The reason I ask this is because using distance as the reward in the initial phases of my algorithm and then switching to optimizing the reward pulls the agent out of the local minima that is simply standing still. I am using the pybullet version of the environment (which is [considerably harder](https://github.com/bulletphysics/bullet3/issues/1718) than the mujoco version) and the agent often falls into local minima that is simply standing.",reinforcementlearning,sash-a,False,/r/reinforcementlearning/comments/ix8ctv/d_are_custom_reward_functions_cheating/
Action smoothing and time-based penalties?,1600715839,"I've been working on this for a bit now, and am reaching out for help. My agent has a continuous action (say 0-100%) While it's training, it seems to like to vary the action rapidly so that it resembles noise. The system is also such that this could cause damage over time. Im trying to figure out how to smooth this, and ideally, be able to encourage it to remain in an off state (0%) for 60 seconds, should it decide to go to that 0% state.

Any suggestions?",reinforcementlearning,Testy_Calls,False,/r/reinforcementlearning/comments/ix762s/action_smoothing_and_timebased_penalties/
Looking for specific RL environment,1600711783,Anyone know any environments with vectors as input observations and discrete action space? something similar to classic control tasks of OpenAI gym.,reinforcementlearning,modanesh,False,/r/reinforcementlearning/comments/ix5s4g/looking_for_specific_rl_environment/
How to minimize risk at optimize speed?,1600706393,"So I have a trading environment where the agent trades stocks, you can imagne what lays around for that environment. I just feed the returns as reward, however the agent isn’t able to work with negative trades causing it to take forever to cut it instead of dropping it earlier, same goes vice versa with positive trades, how can I make the agent minimize the risk while still being fast in episodes instead of it taking so long?

I tried gamma ranges from 0.95 till 0.99 without effect. I thought this would activate a behaviour that would make trades and cut long lasting episodes. Any ideas?",reinforcementlearning,Additional-Teach5180,False,/r/reinforcementlearning/comments/ix3zfj/how_to_minimize_risk_at_optimize_speed/
[R] GRAC: Self-Guided and Self-Regularized Actor-Critic,1600696457,,reinforcementlearning,Caffeinated-Scholar,False,/r/reinforcementlearning/comments/ix0uqv/r_grac_selfguided_and_selfregularized_actorcritic/
Decoupling Representation Learning From Reinforcement Learning | Paper Explained,1600670413,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/iwv7d4/decoupling_representation_learning_from/
Which language should I use to implement RL algorithms?,1600625879,"I know that most of the work on the RL frameworks is being done in **Python** language, but I'm thinking about starting to implement some RL algorithms, (starting from Richard Sutton's books and will move on from that). I'm thinking about using **Julia** because of its speed, focus on scientific computing and active community. There are other languages like Rust, Swift (thinking about using this as well because of TFSwift project), Clojure, and more. What do you guys think would be a better language to try to create a unified RL framework/library?

[View Poll](https://www.reddit.com/poll/iwj1ur)",reinforcementlearning,aadimator,False,/r/reinforcementlearning/comments/iwj1ur/which_language_should_i_use_to_implement_rl/
Is there any reliable reference mentioning the time complexity of an RL algorithm like tabular Q-learning?,1600613448,"References I found mostly address sample complexity but I'm rather interested in time complexity. I understand that the complexity in this case is coming from finding the max value in the Q-table. There is also the update step which has a max over actions.
But I need a citable resource rather than doing the explanation myself.
Thanks for any pointers to useful resources.",reinforcementlearning,OptimalMountain3,False,/r/reinforcementlearning/comments/iwf74c/is_there_any_reliable_reference_mentioning_the/
Custom Environment Long Reset Time,1600608309,"Hi!   


I have implemented a Custom Gym environment, in which the reset takes a lot of time. (incomparably more than a single step) - this is since the terminal reward takes a lot of time to calculate. This is also why it would be nice to be able to deploy many workers at once.  So far I have been using Stable Baselines, as it's easy to write a Custom Policy - my observation is just a long vector, but SB allows me to later slice the vector observation into two, to reshape the first part of it to apply a Convnet and apply a simple MLP head on the second one.   


However the multiple envs implementation in Stable Baselines is such that steps are taken simultaneously for all environments and thus if one environment is done and it resets, all others have to wait - this is a significant bottleneck to training time.  


I'd appreciate all the suggestions on an RL algorithm (implementation) which uses asynchronous workers and in which it would be easy to implement a custom policy.   


Thanks a lot in advance!",reinforcementlearning,andwhata,False,/r/reinforcementlearning/comments/iwdssj/custom_environment_long_reset_time/
Question about Thompson Sampling,1600608129,"For my Thesis i am researching some algorithms regarding the MAB-Problem. 

 I am researching the epsilon greedy, ucb and thompson sampling how they perform.

 I also want to research them on different reward distributions (Normal, Bernoulli and Exponential rewards) 

 For the UCB and epsilon greedy the different reward distributions dont turn out to be a problem at all.  

But for the Thompson Sampling there seems to be a huge problem because it is not possible to get the thetas (priors) out of a beta distribution because alpha or beta could be negative. 

 I found a paper ([http://proceedings.mlr.press/v33/honda14.pdf](http://proceedings.mlr.press/v33/honda14.pdf)) that tackles this problem but i dont seem to get the grasp on it.  

Is there a chance you guys could give me some advice how to handle it?

  Im not asking you to code the problem for me but maybe give me a little push in the right direction. 

&amp;#x200B;

Thanks in advance and have a great day!",reinforcementlearning,clinxno,False,/r/reinforcementlearning/comments/iwdr4g/question_about_thompson_sampling/
"Doing a live training on multi arm bandits, for free of course",1600604009,"I am hosting a live training session on multi arm bandits (MAB) starting this Tuesday, 15th September. I will start with absolute basics on the algorithms right from the greedy ones to some of the most current work on aggregation and boosting. In the course we will build the intuition on how they work (the flavours of Upper Confidence Bound algorithms UCB) so that you become confident on using them. Towards the end, I spend some time on the contextual bandits, especially the algorithms in Vowpal Wabbit. If you are interested in a particular topic related to reinforcement learning, would be happy to spend time on it.

You can find the meetup event here, though most of the time we do sessions relation to Microsoft AI offerings both commercial and Open source.

[https://www.meetup.com/Microsoft-AI-ML-Community/events/273314958/](https://www.meetup.com/Microsoft-AI-ML-Community/events/273314958/)

Or you can subscribe to the channel to get notifications. I go live every Tuesday at 7pm Singapore time.

YouTube: [https://www.youtube.com/setuchokshi](https://www.youtube.com/setuchokshi)

Twitch: [https://www.twitch.tv/setuchokshi/](https://www.twitch.tv/setuchokshi/)

Mods: If this is inappropriate please remove it. I wasn't sure if it was ok to post.",reinforcementlearning,setuc,False,/r/reinforcementlearning/comments/iwcrx4/doing_a_live_training_on_multi_arm_bandits_for/
How DeepMind design and plot figures in papers accepted by Nature and Science?,1600514600,"I read the paper: [https://science.sciencemag.org/content/364/6443/859](https://science.sciencemag.org/content/364/6443/859) I found the figures are awesome, but I do not know that tools they used to draw and plot these figures. Does anyone know it?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/ivqsl8/how_deepmind_design_and_plot_figures_in_papers/
Is the GAE Usable in Off-Policy Algorithms Like DDPG?,1600511418,"Howdy folks

Can someone tell me if the concept of the [Generalized Advantage Estimator](https://arxiv.org/abs/1506.02438) (or it's simpler k-step sibling) is applicable to an off-policy agent, like [DDPG](https://arxiv.org/abs/1509.02971v6)?

Gracias very mucho!",reinforcementlearning,AnotherForce,False,/r/reinforcementlearning/comments/ivq5jf/is_the_gae_usable_in_offpolicy_algorithms_like/
Rich Sutton came to our zoom lecture today.,1600461606,,reinforcementlearning,khan9813,False,/r/reinforcementlearning/comments/ive464/rich_sutton_came_to_our_zoom_lecture_today/
Challenges and Open Problems in Autonomous Driving,1600455415,"What are the current challenges and open problems in Autonomous Driving? Especially the learning and decision making domain?
Or put it another way, where is the state-of-the-art tech of top companies headed?

I am a student, curious to know more. There's not a lot of literature published by top companies for confidentiality I guess, so there's this entry barrier to figure out what's new and what problems are being solved right now. I found Chauffeurnet to be pretty interesting, but it's from 2018. What's happened in the past 2 years?
I understand that at some level, imitation learning plays a huge role. Andrej mentioned IL during one of Tesla's presentation. Drew Bagnell, CTO of Aurora, is a top researcher in IL (published DAgger). And a lot of other companies have their AVs being driven around to collect expert data. So, I guess almost everyone's going with IL. Does Reinforcement Learning come into the picture somewhere? Offline RL? Does Control Theory have a role to play?
What are the challenges, open problems? What's the SOTA? How safe is it in new situations or out-of-distribution states? Is it fast enough to react, time critical? What's the approach to the ethical paradox, the trolley problem? What is the next breakthrough everyone's working towards?lem? What is the next breakthrough everyone's working towards?",reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/ivc6nr/challenges_and_open_problems_in_autonomous_driving/
What RL framework do you propose ?,1600435972,"I have an RL project, where the environment is really custom and time expensive. I want to choose a framework, where I can easily apply an RL algorithm and check how it works in my environment.  
The framework should allow for using GPUs, multiple agents, and for sure a custom, external environment. It would be good if it can be distributed over multiple nodes with several GPUs (e.g. using MPI).

To this day I see two environments:  
TF-agents and ACME, both look similar. And here a question comes to my mind.

Do the frameworks provide what I described earlier?  
Which one is more flexible and modular?",reinforcementlearning,DanielWicz,False,/r/reinforcementlearning/comments/iv654b/what_rl_framework_do_you_propose/
Hard constraints in the reward?,1600358801,"I have an environment that is an EV charging station and I want to use DRL for scheduling the charging of the EVs with the objective to maximize the revenue. The problem that I face has to do with a constraint that all EVs have to leave with their charging demand fulfilled. What I am currently doing is that I penalize the amount of the unfulfilled demand in the reward by a large coefficient, namely if an EV leaves with 5 kW unfulfilled demand, I subtract from the revenue this amount multiplied by a coefficient. In the end, it cannot learn to fulfill all the demand as a hard constraint lets say. Is there a better way to incorporate hard constraints in the reward?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/iumgw8/hard_constraints_in_the_reward/
Gradient policy methods like PPO/AC for discrete action space,1600357168,"I was reading through [this post](https://www.reddit.com/r/reinforcementlearning/comments/fafi7j/actorcritic_for_discrete_action_space/?utm_source=amp&amp;utm_medium=&amp;utm_content=post_title) and was curious about some of the same things as the OP.

One of the main reasons i wanted to move away from DQN was that i liked the idea of optimizing directly on the policy instead of trying to predict Q values.

For my particular problem, i don't really care about predicting a specific return value, i really only care about knowing that action A is better than action B. My main concern is that i am afraid a lot of my model's capacity will be used trying to predict specific values when value predictions very hard to predict and/or are stochastic in nature. Knowing that action A is better than action B is a lot less random and more tractable of a problem to solve.

In addition to that i think that loss from predicting Q-distributions does not necessarily translate to higher reward in a direct way. For example a 50% decrease in loss predicting Q distributions might not equate to a 50% increase in reward. Obviously they will be generally correlated but i am worried the model will spend capacity to reduce loss in ways that don't really translate well to higher reward.

That being said it seems like a lot of policy gradient type methods like actor-critic or PPO do actually still need a value function. Seems like a lot of the responses seems to indicate there isn't much of a benefit to using actor-critic on discrete action spaces. We still have a value function so my concerns above still stand. Is my understanding correct in that gradient-methods still rely on predicting Q-values or is this not the case?

My only other concern is that i do like the idea of selecting an action based on probabilities. It fits well into stochastic/hidden information scenarios and it naturally allows exploration and ensures your policy doesn't get stuck as opposed to something like epsilon greedy where we need to try and pick a correct value of epsilon. Is there any reason why we can't make our distributions outputs into a similar probability based policy?",reinforcementlearning,Yogi_DMT,False,/r/reinforcementlearning/comments/iulx1e/gradient_policy_methods_like_ppoac_for_discrete/
Action saturation to max value in DDPG and Actor Critic settings,1600345314,"So, looking around the web there seems to be a fairly common issue when using DDPG with an environment with an action vector.

Basically it tends to saturate to either the maximum or the minimum action on each component.

here are a few links with people discussing about it:

[https://www.reddit.com/r/reinforcementlearning/comments/dh7e63/what\_is\_the\_reason\_for\_early\_saturation\_of\_values/](https://www.reddit.com/r/reinforcementlearning/comments/dh7e63/what_is_the_reason_for_early_saturation_of_values/)

[https://www.reddit.com/r/reinforcementlearning/comments/dr6d0m/action\_saturation\_in\_ddpg\_and\_td3/](https://www.reddit.com/r/reinforcementlearning/comments/dr6d0m/action_saturation_in_ddpg_and_td3/)

[https://github.com/rmst/ddpg/issues/10](https://github.com/rmst/ddpg/issues/10)

[https://github.com/m5823779/DDPG](https://github.com/m5823779/DDPG)

[https://www.mathworks.com/matlabcentral/answers/589159-ddpg-agent-has-saturated-actions-with-diverging-q-value](https://www.mathworks.com/matlabcentral/answers/589159-ddpg-agent-has-saturated-actions-with-diverging-q-value)

[https://www.reddit.com/r/reinforcementlearning/comments/hi2u53/scaling\_actor\_output\_in\_ddpg/?utm\_source=amp&amp;utm\_medium=&amp;utm\_content=post\_body](https://www.reddit.com/r/reinforcementlearning/comments/hi2u53/scaling_actor_output_in_ddpg/?utm_source=amp&amp;utm_medium=&amp;utm_content=post_body)  (here they propose a batchnorm in the first layer of actor and critics...why tho?)

  
I experienced the bug myself with this implementation [https://github.com/sfujim/TD3](https://github.com/sfujim/TD3)(it's from the original paper of TD3, so I'd say it is likely top-notch).

has any of you faced the same issue? how did you solve it? 

Any insight on what is happening in these scenarios?",reinforcementlearning,Aumanidol,False,/r/reinforcementlearning/comments/iuicn2/action_saturation_to_max_value_in_ddpg_and_actor/
"[Help] Trying to implement Policy Gradient in Pytorch on my own, not sure where I am going wrong.",1600324324,"I have been trying to implement PG(for continuous state and action) from scratch for past 2 months, but have not been successful yet. Most of the return trends look like this. Any idea what could be going wrong here or what generally leads to such behavior?  
[The code](https://github.com/aakash94/RLSamples/tree/master/PolicyGradient). Would appreciate any help.  


[average reward per episode Pendulum-v0 \(each epoch has 512 episodes\)](https://preview.redd.it/yjcp1n6zfnn51.png?width=700&amp;format=png&amp;auto=webp&amp;s=eb56c3680f4d1a3fcb0d0bb0b0c3b93491d3ddad)",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/iudzsn/help_trying_to_implement_policy_gradient_in/
Searching an algorithm off-policy and with continuous action space,1600273975,"For a project I would like to develop a reinforcement agent in an environment with this specification:  
\- Each Trajectory takes 1 minutes to be computed (with multiprocessing =&gt; \~10 samples per minute)  
\- Each Trajectory is composed of \~10 steps   
\- The action for each step is first to choose which action to do (discrete) then choose parameters (continuous or discrete) (there is maybe 15 actions and 0 to 5 parameters per action) The action space is therefore very large.  
\-The observation is in the contrary, rather small (1D array fewer than 50 elements)

Since it is quite long to produce samples, I would like to strongly optimize the use of the data generated. I am therefore looking for an algorithm that is possibly off-policy (to be able to reuse past data), that can handle both discrete and continuous action space.

I found maybe some paper (e.g. CAQL) and saw some works about batch reinforcement learning that can be interesting, but I would like you to know if you know some information that could help me (maybe a paper that is hot but not easy to find or else), anything would be welcome :)

Thanks!",reinforcementlearning,thomashirtz,False,/r/reinforcementlearning/comments/itz2e6/searching_an_algorithm_offpolicy_and_with/
"""Best"" environment with image-like state representation",1600269481,"For various reasons, I would like to develop RL algorithms similar to DQN where it takes as input an image, and outputs the best action to take.

I would like to test this on well-tested, open-sourced environments. Are the only options to use the Atari Learning Environment (ALE)? Are there any others that I may not be aware of?

My main problem with using ALE is that it takes rather long to run (I have a single CPU (i7), single GPU (2080 Ti) setup). If I then repeat these runs 10 times for reproducibility, it limits the pace I can iterate new algorithms at.

Is there like an image-based equivalent of CartPole? The problem with CartPole is that it represents the state as a 4-tuple as opposed to an image (has someone converted CartPole to an image-based representation???)",reinforcementlearning,rl_noob123,False,/r/reinforcementlearning/comments/itxj73/best_environment_with_imagelike_state/
Is there a way for one agent to propose goals to another agent from a certain distribution,1600256627,"Let's say I have two agents that are engaged in a self-play. One agent proposes goals of increasing difficulty that another agents is supposed to complete (something in the line of self-play described here [https://openreview.net/forum?id=SkT5Yg-RZ](https://openreview.net/forum?id=SkT5Yg-RZ)).   


I am looking for approaches for the proposing agent to be rewarded for proposing only 'interesting' goals to the fulfilling agent. So, is there maybe literature that proposes solutions to the questions of 1) what goals are interesting, and 2) how to propose goals from the pool of interesting goals...   


Thanks",reinforcementlearning,denis56,False,/r/reinforcementlearning/comments/ittttw/is_there_a_way_for_one_agent_to_propose_goals_to/
Solving Large Gridworld Task,1600244873,"I have a 20x20 grid. Now at the start of the episode the agent randomly spawns anywhere in the grid and so does a terminal cell. The rewards are -0.02 for every timestep and 1 for reaching the terminal cell. Max episode length is 50. The observation space is the (20,20,1) and in the observation the agent position is represented by 0.5 and the goal position by 0.1.   


To solve the task I wanted to use a hand crafted curriculum (since training on random I think would just give too sparse rewards). I created a hand crafted curriculum(the goals are first placed close to the agent and then subsequently further), but the agent just doesn't train that well. It seems the task is fairly easy, but the even after 30 mln steps in the agent still seems to not be able to solve a big portion of the tasks. I'm using stable baselines, here are the hyperparameters and CNN architecture:  


CNN (mlp head on top):  
activ = tf.nn.relu  
layer\_1 = activ(conv(scaled\_images, 'c1', n\_filters=32, filter\_size=8, stride=4, init\_scale=np.sqrt(2), \*\*kwargs))  
layer\_2 = activ(conv(layer\_1, 'c2', n\_filters=64, filter\_size=4, stride=2, init\_scale=np.sqrt(2), \*\*kwargs))  
last = conv\_to\_fc(layer\_2)  


hyperparameters:  
gamma=0.99, n\_steps=512, ent\_coef=0.00, learning\_rate=2.5e-4, vf\_coef=0.5,  
 max\_grad\_norm=0.5, lam=0.95, nminibatches=8, noptepochs=10, cliprange=0.2  


  
Is there something I'm missing - this seems to be an easy enough task?",reinforcementlearning,andwhata,False,/r/reinforcementlearning/comments/itrn9o/solving_large_gridworld_task/
"""Decisions from Data: How Offline Reinforcement Learning Will Change How We Use ML"", Sergey Levine",1600213678,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/itkgml/decisions_from_data_how_offline_reinforcement/
"""PPG: Phasic Policy Gradient"", Cobbe et al 2020 {OA}",1600196077,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/itf315/ppg_phasic_policy_gradient_cobbe_et_al_2020_oa/
"""PPG: Phasic Policy Gradient"", Cobb et al 2020 {OA}",1600196051,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/itf2qj/ppg_phasic_policy_gradient_cobb_et_al_2020_oa/
Is it possible to post RL Scientist vacancy here?,1600186936,"Hi there,

I am not sure that the group rules allow post about job opportunities,  but we are looking for a full-time RL specialist for our USA-based sports startup. We are Madison, Wisconsin based startup building an automated endurance sports training applications that leverage big data and an AI-based algorithm to produce truly individualized training plans. 

If you are interested or know somebody who can apply, please forward it. I have published details at [Kaggle page](https://www.kaggle.com/jobs/20099)",reinforcementlearning,Egor_Akimov,False,/r/reinforcementlearning/comments/itc3ie/is_it_possible_to_post_rl_scientist_vacancy_here/
Could I say the DQN loss function is to make the policy approach the target network?,1600142378,"The DQN loss function is: 

[DQN loss function.](https://preview.redd.it/fbcoz9rzi8n51.png?width=581&amp;format=png&amp;auto=webp&amp;s=4755844e9bb00ca843a7aea12ba2f57ba0e016f4)

Does minimizing this function mean that we want the Q network to go to the direction of the target network?",reinforcementlearning,alreadybetoken,False,/r/reinforcementlearning/comments/it19xb/could_i_say_the_dqn_loss_function_is_to_make_the/
Does anyone know some papers which is trying to optimize the hyperparameters of DQN?,1600139592,Such as the replay memory capacity .,reinforcementlearning,alreadybetoken,False,/r/reinforcementlearning/comments/it0jtm/does_anyone_know_some_papers_which_is_trying_to/
[P] For monitoring ML model training on your mobile phone,1600132926,,reinforcementlearning,hnipun,False,/r/reinforcementlearning/comments/isyq3b/p_for_monitoring_ml_model_training_on_your_mobile/
"""Aligning superhuman AI with human behaviour: chess as a model system"", McIlroy-Young et al 2020 (discussion by Adrian Colyer)",1600106281,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/isq61h/aligning_superhuman_ai_with_human_behaviour_chess/
Is a simple 30 neurons DQN able to hack numpys random.normal()?,1600082921,"To be honest, I didn't dare to ask the question at first ... But I have found indications that it is possible and would like to make sure that I don't just have a mistake somewhere.

To test my system, I create ""pseudo random numbers"" using numpy.random.normal (size=1000).

I divide them into 2 parts, the first 500 are for training, the rest are for testing. The system only sees the TRAIN data, the TEST data are not shown. During training, the agent is shown the last 20 values ​​or their changes (current-last). To my amazement, was the system able to generate positive results on the unseen TEST data?

Not on individual results, but on all of them. As far as I have been able to determine so far, it does not work if I reduce the input lag or use less data overall.

I had the following possible reasons for this:

1.) I accidentally look to the future at the inputs  
2.) these are just random hits  
3.) I have a bug in my reward system  
4.) the agent can reproduce the RNG

1.) checked, everything looks good, no look ahead  
2.) all TEST results are positive, it cannot be a coincidence  
3.) checked, everything looks good  
4.) I thought it was impossible until I found the following ...

[predict-the-next-pseudo-random-number](https://ai.stackexchange.com/questions/3850/can-a-neural-network-be-used-to-predict-the-next-pseudo-random-number)

[impossible-to-produce-truly-random-numbers](https://softwareengineering.stackexchange.com/questions/124233/why-is-it-impossible-to-produce-truly-random-numbers)

[How We Learned to Cheat at Online Poker](https://web.archive.org/web/20110322162237/http://www.cigital.com/papers/download/developer_gambling.php)

&amp;#x200B;

https://preview.redd.it/192855q9m3n51.png?width=1257&amp;format=png&amp;auto=webp&amp;s=702dbf1c08e2c557884e4c39c79b7606a6275720

So either I still have a bug in the system or is it really possible that a simple DQN with only 30 neurons and 1 hidden layer can decode the numpy.random.normal () and make pretty good predictions?",reinforcementlearning,iz8ft,False,/r/reinforcementlearning/comments/isj31w/is_a_simple_30_neurons_dqn_able_to_hack_numpys/
"Stable baselines, custom env help",1600080279,"I am trying to create an agent for this game: https://github.com/AbdullahGheith/BlockPuzzleGym

My problem is that it's not getting that many points as it should (barely getting more than 100 points) I can get a lot more myself. Is the problem in the gym or in the code

I am using stable baselines and running a code like this:

    import gym
    import blockpuzzlegym
    from stable_baselines import PPO2
    env = gym.make(""BlockPuzzleGym-v0"")
    model = PPO2('MlpPolicy', env, verbose=1)
    model.learn(250000) for _ in range(10000):
    action, _states = model.predict(obs) obs,
    rewards, dones, info = env.step(action)
    env.render()

any help is appreciated",reinforcementlearning,klaushansen,False,/r/reinforcementlearning/comments/isijnp/stable_baselines_custom_env_help/
Real life example,1600067774,,reinforcementlearning,onufrios,False,/r/reinforcementlearning/comments/isg6pi/real_life_example/
"""Physically Embedded Planning Problems: New Challenges for Reinforcement Learning"", Mirza et al 2020 {DM} [trying to solve tic-tac-toe/sokoban/Go with camera inputs + robot arms]",1600050284,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/isc46f/physically_embedded_planning_problems_new/
Job Question,1600022095,"Hello everyone hope all of you are well and safe.... I am really interested in robotics control (control theory) and machine learning (deep reinforcement learning). I am almost finished with my master degree but when i started looking for jobs i could only find jobs that needs either machine learning either control. My question, is there any jobs that combine these two fields, especially in UK?",reinforcementlearning,Hazem2037,False,/r/reinforcementlearning/comments/is3tbp/job_question/
So I recently finished this specialisation on RL course on Coursera which taught me the fundamentals and now I wanna work on some core projects. Can someone point in me in the right direction?,1600017621,"I did a couple of projects using open ai gym, but I still want to learn more.",reinforcementlearning,pinkman8144,False,/r/reinforcementlearning/comments/is2bih/so_i_recently_finished_this_specialisation_on_rl/
I published a car game to spice up your reinforcement learning life. What I did with it: SAC steering a car in GTA 5,1600004483,"When I was doing RL with the standard open-ai gyms I felt, that these libraries are good but cannot be transferred easily to real world problems. I was thinking which domain I would be interested in and then decided to make my own car game. Please check to code here: https://github.com/MatthiasSchinzel/Simple-Car-Game-For-Reinforcement-Learning

I then trained a soft actor critic to play the game: https://github.com/MatthiasSchinzel/Soft-Actor-Critic-For-Simple-Car-Game

And then used that to let SAC steer a car in GTA 5: https://github.com/MatthiasSchinzel/Soft-Actor-Critic-Playing-GTA

I hope that also other users in this area might find this car game useful, even though it is still at a early stage. With the GTA 5 implementation I want to show a proof of concept, that the trained reinforcement learning algorithm can be generalized to something more realistic.

Thanks for checking out the repos!",reinforcementlearning,mad_ger,False,/r/reinforcementlearning/comments/iryhh1/i_published_a_car_game_to_spice_up_your/
I published a open-ai like API car game to spice up your reinforcement learning life. What I did with it: SAC steering a car in GTA 5,1600002344,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/irxxam/i_published_a_openai_like_api_car_game_to_spice/
Why does RLLib use a very small standard deviation (0.01) for weight initialization in the final fully connected layers of its default fully connected model?,1599963700,"In all the prior layers they use a standard deviation of 1.0. In experimenting with this myself, I've found that if I use essentially the same fully connected architecture as their default model, but use Xavier initialization for all my layers, my training results are significantly worse. What is unique or important about the weight initialization scheme used by RLLib, particularly with the standard deviations used for the final layers? Thanks in advance!

The default RLLib model I'm referring to is here: https://github.com/ray-project/ray/blob/master/rllib/models/tf/fcnet.py",reinforcementlearning,1cedrake,False,/r/reinforcementlearning/comments/irpxfu/why_does_rllib_use_a_very_small_standard/
"Identify which action intervals reach at least a certain value function (or ""evaluate the value function over the entire continuos action space)",1599959616,"So I'm trying to solve the following problem:

I have an actor-critic setting, so continuous states and actions. let's say that the action is limited between -5 and 5.

I'd like to know which actions lead to a value function of at least 4.

In the discrete setting, I could simply enumerate them and run each action in the critic network to check if its output is greater than the value I seek. But what can I do in the continuos setting?

I thought about inverting the critic network but feels very weird, I thought about using a stochastic actor to obtain a distribution over the action set (and this does not sound so bad tbh), but so far I did not conclude anything useful.

&amp;#x200B;

Is there a know solution or does somebody in here have any idea?",reinforcementlearning,Aumanidol,False,/r/reinforcementlearning/comments/irox9v/identify_which_action_intervals_reach_at_least_a/
Is there any tool that tracks the recent research paper (not journal) citation count and trend in the field of machine learning and reinforcement learning?,1599951625,,reinforcementlearning,yy0318,False,/r/reinforcementlearning/comments/irmq9w/is_there_any_tool_that_tracks_the_recent_research/
"John Schulman, ""Optimizing Expectations: From Deep RL to Stochastic Computation Graphs"" on the Thesis Review podcast",1599915343,,reinforcementlearning,thesisreview,False,/r/reinforcementlearning/comments/irc3rl/john_schulman_optimizing_expectations_from_deep/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1599912027,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/irbbms/simple_learning_to_play_atari_with_only_2_hours/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1599911960,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/irbb1w/simple_learning_to_play_atari_with_only_2_hours/
Distributional Reinforcement Learning,1599899690,"Hey,   
I wrote two short articles introducing Distributional Reinforcement Learning and the Algorithms C51, QR-DQN, IQN, and FQF. Maybe they help others to understand the topic and give a small introduction.  


[Distributional Reinforcement Learning — Part 1 (C51 and QR-DQN)](https://medium.com/analytics-vidhya/distributional-reinforcement-learning-part-1-c51-and-qr-dqn-a04c96a258dc)

[Distributional Reinforcement Learning — Part 2 (IQN and FQF)](https://medium.com/analytics-vidhya/distributional-reinforcement-learning-part-2-iqn-and-fqf-567fbc7a04d7)",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/ir8z6m/distributional_reinforcement_learning/
The Sorcerer’s Apprentice Guide to Training LSTMs,1599899456,"Hi all. I wrote a blogpost about unconventional tricks and intuitions to train LSTM networks. Given the applicability of LSTMs to RL tasks, I thought this might also be relevant for members of this community. For example, I reference the [RUDDER work](https://arxiv.org/abs/1806.07857), which used many of the listed techniques to tackle environments with delayed rewards.

https://www.niklasschmidinger.com/posts/2020-09-09-lstm-tricks/",reinforcementlearning,smdrnks,False,/r/reinforcementlearning/comments/ir8xkr/the_sorcerers_apprentice_guide_to_training_lstms/
Is it possible to let RL agent observe environment without acting on it and learn some of the rules nevertheless?,1599895012,"There is some environment where the agent would benefit from understanding its dynamics before even acting in it. I am wondering whether it's possible (and how) to feed the various states of this environment and have the function approximator learn the rules. After some time, we can let the agent loose and it can start acting. One possible way to do that is to force no-opt actions for some duration of time, but maybe there is a smarter way of doing it..",reinforcementlearning,denis56,False,/r/reinforcementlearning/comments/ir83sp/is_it_possible_to_let_rl_agent_observe/
Tips for SAC performance,1599889593,"I have recently found the following post with implementation details for PPO:

[https://costa.sh/blog-the-32-implementation-details-of-ppo.html](https://costa.sh/blog-the-32-implementation-details-of-ppo.html)

I would like to ask if there is something similar for SAC or if there are some similar tips hold also for SAC.",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/ir72gj/tips_for_sac_performance/
Sample-Efficient Automated Deep Reinforcement Learning,1599871729,,reinforcementlearning,Science_Squid,False,/r/reinforcementlearning/comments/ir2zhg/sampleefficient_automated_deep_reinforcement/
Any communities working with learning automata?,1599857236,"I'm working on reinforcement learning and, by now, I have studied bandits and MDPs, which I believe make the bulk of the field, at least recently. A couple of times reviewers and colleagues (who are just applying RL and are not directly working on the field) have asked why I don't mention learning automata as alternatives and I honestly don't know what to answer. From a brief look I understood that it's a concept that arose independently from RL, looks pretty similar to it, concerns only stochastic environments and  does not employ value functions.

So I was wondering, is there is any field or application that has them as a first choice for modelling decision-making? Have you seen them in large ML conferences?",reinforcementlearning,dr_cosmicomical,False,/r/reinforcementlearning/comments/iqyyd1/any_communities_working_with_learning_automata/
SuperSuit- simple wrappers for all common preprocessing methods for Gym and PettingZoo environments- has hit its 2.0 release,1599855158,"A common source of minor nuisance in reinforcement learning is that, whenever you need to apply basic downscaling operations, make observations greyscale, apply frame stacking, etc. you typically have to write your own little wrappers. Gym includes some of this functionality, but it's limited and often not amazingly well done. Since this is such a common task, it's something a library where a library should include all popular functions and do right, and that's the idea behind SuperSuit: [https://github.com/PettingZoo-Team/SuperSuit](https://github.com/PettingZoo-Team/SuperSuit)",reinforcementlearning,justinkterry,False,/r/reinforcementlearning/comments/iqyauv/supersuit_simple_wrappers_for_all_common/
How to share state information across all critic networks in multiagent setting?,1599851960,"Hi everyone, 

I have a multiagent environment where I want to implement a centralized learning approach. So I need to share state information across all agents before feeding this state information into the critic networks to evaluate these states. 

How am I going to share these state information though?",reinforcementlearning,k_ili,False,/r/reinforcementlearning/comments/iqxabz/how_to_share_state_information_across_all_critic/
DQN code-level optimizations,1599835696,"I read a lot about code-level optimizations for policy gradient algorithms such as reward scaling, observation clipping, for example in [this paper](https://arxiv.org/abs/2005.12729).

Do you know about code-level optimizations in DQN? Or other techniques to improve the standard",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/iqrw4p/dqn_codelevel_optimizations/
Unity vs Unreal Engine in RL + simulation,1599813471,"Hi all :)   
Since no one at the r/unrealengine would like to discuss the pros and cons of using each engine, I thought maybe you guys are in a more similar position to mine.  
I am about to start a multi-agent project, and I am in the very early stage of deciding where to spend my time developing the environment. I have previously used both Unreal Engine and Unity in different project and feel comfortable in both engines so this is not the deciding factor for me.  


Here is my thought:  
**Unreal** **Engine pros:**

1. The engine is very powerful in creating realistic images beyond anything I have seen in Unity. This will help my project going form simulation to real-life tests. 
2. There exist plugins for getting all manner of images out of the engine in real-time. Depth, segmented, normals, RGB.
3. In 2021 the Unreal Engine 5 is released for better environmental quality and better processing.

**Unreal Engine cons:**

1. Besides images, it's rather complicated to get information out of the engine simultaneously with the images. Unity has very nice coupling with gym and ML-agents.
2. All the plugin are hooked up to use one kind of agent (UnrealCV = Camera) (Airsim = Car/UAV). For a multi-agent project, I would like not just control one type of actor but the entire environment.

**Unity pros:**

1. We have premade libraries for training agents where the flow of information is streamlined. Gym + ML-agent.
2. Unity is really easy to use and manipulate.  (Debugging, c#, scripting logic)

**Unity cons:**

1. Harder to optimize for good framerate. (I have no experience in the Entity Component System (**ECS**)  so if I am going with unity I need this for better performance.)
2. Lower out-of-the-box rendering quality.
3. The road map seems rather disappointing (Visual scripting, nice to have features). I feel like they are playing the catch-up game atm.

If anyone has any experience, please let me know, and I would love to discuss with you :)",reinforcementlearning,Zartris,False,/r/reinforcementlearning/comments/iqmqo6/unity_vs_unreal_engine_in_rl_simulation/
Is there any recommended textbook or resources to learn the theory (and analysis) of reinforcement learning?,1599785766,,reinforcementlearning,yy0318,False,/r/reinforcementlearning/comments/iqgf04/is_there_any_recommended_textbook_or_resources_to/
"""Cruise’s Continuous Learning Machine Predicts the Unpredictable on San Francisco Roads"" {Cruise}",1599784839,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iqg5qx/cruises_continuous_learning_machine_predicts_the/
"""Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess"", Tomašev et al 2020 {DM}",1599756337,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iq71xy/assessing_game_balance_with_alphazero_exploring/
"""Munchausen Reinforcement Learning"" - a simple tweak to improve DQN",1599738790,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/iq1umy/munchausen_reinforcement_learning_a_simple_tweak/
Dimitri Bertsekas's reinforcement learning book,1599721478,"I plan to buy the reinforcement learning books authored by  Dimitri Bertsekas. The book titles I am interested are 

Reinforcement Learning and Optimal Control ( [https://www.amazon.com/Reinforcement-Learning-Optimal-Control-Bertsekas/dp/1886529396/](https://www.amazon.com/Reinforcement-Learning-Optimal-Control-Bertsekas/dp/1886529396/) )

Dynamic Programming and Optimal Control ( [https://www.amazon.com/Dynamic-Programming-Optimal-Control-Vol/dp/1886529434/](https://www.amazon.com/Dynamic-Programming-Optimal-Control-Vol/dp/1886529434/) )

Is there anyone who read these two books? Are they similar? If I read Reinforcement Learning and Optimal Control, is it necessary to read Dynamic Programming and Optimal Control for studying reinforcement learning?",reinforcementlearning,yy0318,False,/r/reinforcementlearning/comments/ipyh3j/dimitri_bertsekass_reinforcement_learning_book/
"""A Unified Bellman Optimality Principle Combining Reward Maximization and Empowerment"", Leibfried et al 2019 {Prowler.io}",1599676902,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ipm7g4/a_unified_bellman_optimality_principle_combining/
Using Multi-Objective Deep Reinforcement Learning to Uncover a Pareto Front in Multi-Body Trajectory Design - an Extension of PPO,1599672483,,reinforcementlearning,aero_grad_student,False,/r/reinforcementlearning/comments/ipkhoj/using_multiobjective_deep_reinforcement_learning/
Necessity of log-prob correction for tanh-squashed continuous actions,1599665362,"In continuous control environments we typically parametrize a policy as a diagonal Gaussian and then squash actions to be between -1 and 1 using tanh. This requires modifying the log probability using the change of variables formula.

Since the tanh has no learnable parameters, can't we just apply the transformation to the action just-before stepping the environment, but instead storing the original untransformed action in the replay buffer and using the regular log-probability there?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/ipi2qd/necessity_of_logprob_correction_for_tanhsquashed/
Reward graph is almost a straight line sideways?,1599653191,"Hello,

I have an environment that tries to simulate driving, kinda, with text based environment Logic. So my reward scheme is based on “breadcrums” having small rewards (based around -1 till 1) for when the agent does things like taking the correct turns and not going off road, winning the race grants a bigger reward (10 for winning ans -10 for losing), however what my training shows is not what I expected...

The rewards are mostly just a flat line like —— with some movement, but any up/downwards trend, it also doesn’t to barely achieve the terminal big reward. I thought this breadcrums idea would aid towards achieving the goal, but it caused short lasted episodes and poorly executed actions. Tried all kinds of activation functions, units, gamma’s (0.95 till 0.9999), learning rates, batch sizes. My state is normalized and I have 3 possible actions, furthermore I use DQN, tried with and without double q and or prioritized experience replay. Both td Error and loss go down for the start of the run but form a flat line later in the run to just “hover” a decent amount above the 0 line, without any real movement (talking about the average) 

Does anybody have any idea perhaps on possible causes and or tips to check/tryout for possible outcomes?",reinforcementlearning,Additional-Teach5180,False,/r/reinforcementlearning/comments/ipem1g/reward_graph_is_almost_a_straight_line_sideways/
Best material/tutorials to learn reinforcement learning,1599649540,"This question has probably been asked already but I believe that such a fast pace field comes up often with new material for learning.
I have already experience with NNs and try to hack together some self playing games.

I wonder what worked best for you for learning deep RL.",reinforcementlearning,carthage_seif,False,/r/reinforcementlearning/comments/ipdt9z/best_materialtutorials_to_learn_reinforcement/
How to create a trading environment for tf-agents.,1599634103,,reinforcementlearning,Denis_Vo,False,/r/reinforcementlearning/comments/ipb14c/how_to_create_a_trading_environment_for_tfagents/
"""GPT-f: Generative Language Modeling for Automated Theorem Proving"", Polu &amp; Sutskever 2020 {OA} (GPT-2 for Metamath)",1599614388,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ip6h34/gptf_generative_language_modeling_for_automated/
"[D] ""Jane Wang: Evolving Altruism in AI"" on Brain Inspired podcast [Video]",1599574166,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/iou7wy/d_jane_wang_evolving_altruism_in_ai_on_brain/
"""DCEM: The Differentiable Cross-Entropy Method"", Amos &amp; Yarats 2020 {FB}",1599529740,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iokt1m/dcem_the_differentiable_crossentropy_method_amos/
Train Proximal Policy Optimization (PPO) with Swift for TensorFlow (S4TF),1599525294,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/iojn07/train_proximal_policy_optimization_ppo_with_swift/
A very easy to use optimization library for RL using ES and Pytorch,1599516252,"Hello everyone, 

I created a simple black-box optimization framework to train your models for optimizing any given objective. It is well suited well for RL problems and other machine learning problems when optimizing given non-differentiable objectives.

[https://github.com/rajcscw/pytorch-optimize](https://github.com/rajcscw/pytorch-optimize)

It provides simple wrappers for models and optimizers so that they can be used to optimize the provided objective function (including non-differentiable objectives). It also supports optimization of multiple objectives out-of-the-box. The optimizer itself is based on [Evolution strategies](https://arxiv.org/pdf/1703.03864.pdf) which estimate gradient using parallel workers so that it can scale well utilizing multiple cores.

A demo script for cart pole example can be found at:

[https://github.com/rajcscw/pytorch-optimize/blob/master/sample\_scripts/rl.py](https://github.com/rajcscw/pytorch-optimize/blob/master/sample_scripts/rl.py)

&amp;#x200B;

Please give a try and provide your feedback. It would be nice also to collaborate to add more features, more demo scripts and implement other ES variants.

&amp;#x200B;

Looking forward to hearing from you!",reinforcementlearning,Elk_Clean,False,/r/reinforcementlearning/comments/ioh7pn/a_very_easy_to_use_optimization_library_for_rl/
Neural ODE for Reinforcement Learning and Nonlinear Optimal Control: Cartpole Problem Revisited,1599508377,"Hello! I wrote a preprint with code on [Neural ODE for Reinforcement Learning and Nonlinear Optimal Control: Cartpole Problem Revisited](https://medium.com/swlh/neural-ode-for-reinforcement-learning-and-nonlinear-optimal-control-cartpole-problem-revisited-5408018b8d71). Feedback welcome :) 

https://i.redd.it/54txi3g26sl51.gif",reinforcementlearning,Karenina-IO,False,/r/reinforcementlearning/comments/ioeoof/neural_ode_for_reinforcement_learning_and/
Loss jumps to a big number and then zero (Rainbow DQN),1599501756,"I've working with the code [https://github.com/medipixel/rl\_algorithms](https://github.com/medipixel/rl_algorithms) trying in differences environments like pong and another NES games. But almost all the time I see the same pattern, the loss goes down normally but after some point it jumps to a very big value and then goes to zero. After that doesn't converge.  

Is it a catastrophic forgetful or is something else? Could improve with parameters tuning or is a bug in the code?

I tried a lot of parameters for a week but nothing changes, neither improvement nor worse. What else can I do?",reinforcementlearning,pacoflaco,False,/r/reinforcementlearning/comments/iocifn/loss_jumps_to_a_big_number_and_then_zero_rainbow/
A project based discussion?,1599497096,"Looking to have a discussion with anyone experienced in reinforcement learning.

Want to run some ideas about a project I'd like to pursue. Any help would be greatly appreciated.",reinforcementlearning,invisible_gent,False,/r/reinforcementlearning/comments/ioazlo/a_project_based_discussion/
Dynamic reward+action space for DRL in system optimization,1599475064,"Hi All,

I am working on DRL for system optimization. Based on agent decision the system performance changes. I have divided the system state into epochs (some time interval). At the end of an epoch, I assign reward to the agent. 

I came across two doubts:

1) Reward: I am assigning reward 1/current\_system\_performance\_epoch. The agent doesn't seem to converge with this reward because in certain epochs the performance could be higher than in the other, which could be because of background functions as well. So, is there any better way to assign the reward? also, is it better to assign a static reward than a dynamic?

2) Action space:  I have divided my states into epochs (certain time intervals) and inside that interval, the agent can see the same state multiple times. Should I implement a constraint action space to make the agent perform the same action when it sees the same state inside an epoch but in a different epoch, it can perform different actions? or I should let the agent explore and based on current system performance it will learn to stick to the best action inside an epoch?

Any suggestions/feedback would be appreciated!

&amp;#x200B;

Thanks!",reinforcementlearning,splurgein,False,/r/reinforcementlearning/comments/io56k4/dynamic_rewardaction_space_for_drl_in_system/
Plan2Explore: Planning to Explore via Self-Supervised World Models | Paper Explained,1599461096,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/io2lv8/plan2explore_planning_to_explore_via/
TD3 stable_baselines - model selects action identical to last action,1599432131,"Brief summary of objective. 

I am attempting to run Reinforcement learning on actual robotics hardware. 

&amp;#x200B;

My RL algorithm has a continuous action and observation space, both from 0 to 100 (representing the percentage of actuation)

&amp;#x200B;

I have a bunch of middle ware that translates this 0-100% actuation command to the hardware and the action is taken by the robot. The state is read via embedded sensors and translated back to the RL algorithm in the form of 0 or 100% actuation

&amp;#x200B;

[Robotic setup of pneumatic soft actuators with april tag monitoring with a webcam](https://preview.redd.it/5ldt3d4rull51.png?width=1993&amp;format=png&amp;auto=webp&amp;s=1e6f1cbb966fe5f305ba3708ce09b54aa6e275ed)

This is my first big reinforcement learning project and has been a bit of a doozy. I need help with some of the intuition aspects of my learning runs.

I have this problem where without any previous model the model seems to instantly ""converge"" on a given policy. I always thought there would be a lot more exploration. 

Being hardware things are very slow. I am still working out some bugs in the hardware to get really long runs with a LOT of steps in but I have been able to run for 10,000 steps consecutively. I waited to start learning until step 9,500 and my bot STILL converged on this problem mentioned above. I have manipulated the reward in countless variations and no matter how I do it, even giving a constant reward positive and negative produce the same results.

I really thought things would be random at first and then start to converge.

The goal is just a policy that can move forward so the reward is just the distance moved in the y direction. I have played with a few parameters like batch size etc, but I don’t have enough experience to really understand what meaningful effect that will have on learning. I understand what it does technically, just not intuitively. 

**Problem**

TLDR: no matter what I seem to do my model converges on the solution of repeating the same action over and over. Given enough time the average of the repeated action trends down toward 0. the reward from tensorboard has a sharp negative scope. There is no learning or exploration going on at all. 

Here is a result of one of my runs

&amp;#x200B;

[Sorry for the bad quality this is all i had from the place i was writing this post](https://preview.redd.it/61rol72xull51.png?width=1081&amp;format=png&amp;auto=webp&amp;s=c318f0b177bd4ccf14c0e8c3edfd372c15b7ac3b)

Any help is appreciated and I am happy to answer questions.

Below I smy code. I am happy to answer qusetions!!!!. Also here is an overview of the hardware connection

&amp;#x200B;

[System overview - happy to explain some parts but my focus here is on the RL algorithm. I have taken great care to isolate this problem to the model action generation](https://preview.redd.it/yhos2al6vll51.png?width=2187&amp;format=png&amp;auto=webp&amp;s=76d0ffd0aa1cedd8cc55d720250bfda5e3fd6ed6)",reinforcementlearning,csullivan107,False,/r/reinforcementlearning/comments/invox7/td3_stable_baselines_model_selects_action/
Learning about SLAM and Reinforcement learning,1599421994,"Hi everyone,

I'm a newbie when it comes to reinforcement learning. However, I have an ambitious goal for the end of the year. I'm trying to make a Doom bot that can find an exit, quickly, in never before seen levels quickly. I've been browsing the interwebs and have heard about the  CVPR 2020 - ObjectNav challenge.

Digging deeper I found this paper - [https://www.researchgate.net/publication/311299501\_Playing\_Doom\_with\_SLAM-Augmented\_Deep\_Reinforcement\_Learning](https://www.researchgate.net/publication/311299501_Playing_Doom_with_SLAM-Augmented_Deep_Reinforcement_Learning) which talks about building a game map from only visual input.

I have only basic knowledge of MDPs, Q-learning, and DQNs (Mostly from David Silver's course back in 2017 - [https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;ab\_channel=DeepMind](https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ&amp;ab_channel=DeepMind)) I'm trying to find out what is the next step I can take to understand SLAM based approaches and try to implement them myself.

Thank you in advance.",reinforcementlearning,Sahil231090,False,/r/reinforcementlearning/comments/inspcf/learning_about_slam_and_reinforcement_learning/
DDPG how to implement for the dynamic and continouse environment,1599385666," How to implement Deep Reinforcement Learning with the DDPG algorithm on how to implement it in python? how to implement for the dynamic environment as like millimeter-wave massive MIMO in wireless data communication networks, please?",reinforcementlearning,hussainalaaedi,False,/r/reinforcementlearning/comments/injdsk/ddpg_how_to_implement_for_the_dynamic_and/
"With using PPO on a continuous environment, is there any merit to sub-sampling your environment?",1599371440,"For signal environments (ie. Stocks), where the number of steps in one episode is potentially the entire history of our data, is there any merit in randomly sampling from a ""master"" environment to create smaller sub environments?

It just seems infeasible to step through the entire history for 30 separate episodes just to perform one train step.

My thinking is that since we are dealing with a continuous environment, sub-sampling might not violate any assumptions about the problem PPO is trying to solve. Each slice of the environment is technically just a different angle on the sample underlying game.

I can see many cases of reinforcement learning where an episode may differ from another episode in the same way. We are still trying to learn the underlying policy. But each episode will be a variation of the underlying function we are trying to solve.

Seems like sub sampling would be a lot more efficient. Does this sound like a good idea or is there some sort of flaw in my thinking?",reinforcementlearning,Yogi_DMT,False,/r/reinforcementlearning/comments/ingt5k/with_using_ppo_on_a_continuous_environment_is/
Reinforcement Learning for Long-Distance Quantum Communication,1599331310,,reinforcementlearning,aamelnikov,False,/r/reinforcementlearning/comments/in6fe5/reinforcement_learning_for_longdistance_quantum/
Deep RL for an agent interacting with a probabilistic learning model,1599254251,"I have a probabilistic model that models student learning and I have a tutoring agent (the RL agent) and I'm trying to find optimal tutoring sequencing decisions as the agent interacts with the student model. The student model, as mentioned earlier, is probabilistic (doesn't behave the same way every time even if all the other conditions are the same, but it is still constrained by Probability).   


Noe my question: are current RL techniques robust enough to learn even while interacting with the probabilistic (student) model mentioned above? Im still kinda new to RL",reinforcementlearning,SpareResist6324,False,/r/reinforcementlearning/comments/imoiz0/deep_rl_for_an_agent_interacting_with_a/
Elegant epsilon-greedy implementation,1599230323,[removed],reinforcementlearning,Lure_Angler,False,/r/reinforcementlearning/comments/imh3wd/elegant_epsilongreedy_implementation/
Combination of continuous and discrete action space,1599215148,"I have experience using stable baselines, but they cannot provide Tuple/Dict action spaces. Do you have an idea of a framework similar to stable baselines that I can use tuple action spaces?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/imdhxv/combination_of_continuous_and_discrete_action/
Value-Based vs Off-Policy Actor Critic in DRL.,1599209006,"Hello, I'm still having issues in understanding the actual benefits of value-based methods (e.g., DQN) vs off-policy actor-critic methods (e.g., DDPG).

&amp;#x200B;

Recent work (""Q-Learning in enormous action spaces via amortized approximate maximization"", Van de Wiele et al. 2020) designed a value-based method to handle discrete, continuous or hybrid high dimensional action spaces. They motivate this as they can maintain the benefits of Q-Learning, having a value-based solution for such high-dimensional problems. 

&amp;#x200B;

I suppose that I am missing what are these Q-Learning benefits that off-policy actor-critic methods does not present (as one of the only difference that comes in my mind is that DDPG (as example) uses 4 networks, resulting in a slower training)?",reinforcementlearning,emarche,False,/r/reinforcementlearning/comments/imcf71/valuebased_vs_offpolicy_actor_critic_in_drl/
[R] Grounded Language Learning Fast and Slow,1599198053,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/imacq9/r_grounded_language_learning_fast_and_slow/
Results keep getting worse after every iteration of policy gradient optimization,1599189511,"I am trying to implement image captioning code i found here: [https://github.com/chenxinpeng/Optimization\_of\_image\_description\_metrics\_using\_policy\_gradient\_methods/blob/master/image\_caption.py](https://github.com/chenxinpeng/Optimization_of_image_description_metrics_using_policy_gradient_methods/blob/master/image_caption.py)

its implements image captioning by applying policy gradient over a combination of metrics.

the function that I'm have trouble with is this:

    def SGD_update(self, batch_num_images=1000):     
    
        images = tf.placeholder(tf.float32, [batch_num_images, self.feats_dim])     
        images_embed = tf.matmul(images, self.encode_img_W) + self.encode_img_b     
        Q_rewards = tf.placeholder(tf.float32, [batch_num_images, self.lstm_step]) 
        Baselines = tf.placeholder(tf.float32, [batch_num_images, self.lstm_step])      
        
        state = self.lstm.zero_state(batch_size=batch_num_images, dtype=tf.float32)      
        
        loss = 0.0        
        
        with tf.variable_scope(""LSTM""):         
            tf.get_variable_scope().reuse_variables()         
            output, state = self.lstm(images_embed, state) 
            
            with tf.device(""/cpu:0""):             
                current_emb = tf.nn.embedding_lookup(self.Wemb, 
                                tf.ones([batch_num_images], dtype=tf.int64))
            
            for i in range(0, self.lstm_step):             
                output, state = self.lstm(current_emb, state)              
                logit_words = tf.matmul(output, self.embed_word_W) + self.embed_word_b
                logit_words_softmax = tf.nn.softmax(logit_words)             
                max_prob_word = tf.argmax(logit_words_softmax, 1)             
                max_prob = tf.reduce_max(logit_words_softmax, 1)              
                
                current_rewards = Q_rewards[:, i] - Baselines[:, i]                          
    
                loss = loss + tf.reduce_sum(-tf.log(max_prob) * current_rewards)
                
                with tf.device(""/cpu:0""):                 
                        current_emb = tf.nn.embedding_lookup(self.Wemb, max_prob_word) 
                        #current_emb = tf.expand_dims(current_emb, 0) 
    
    images, Q_rewards, Baselines, loss, max_prob, current_rewards, logit_words

I output the rewards each iteration (linear combination of bleu 1,2,3,4) and notice that it decreases every time.

I need help with figuring out what might be causing it, it could be my loss function or some faulty code as I'm not versed in tensorflow, or something else.

One thing I have noticed is that it prefers smaller captions and makes them very generalized, such as 'two dogs'",reinforcementlearning,AbhikA12,False,/r/reinforcementlearning/comments/im8cqt/results_keep_getting_worse_after_every_iteration/
How to handle invalid actions in RL?,1599169616,"Hi guys,

imagine you are building a trading bot, the bot can just sit and WAIT, open (BUY) a trade, HOLD it and CLOSE the trade.

So your possible ACTIONS are:

\[WAIT, BUY, HOLD, CLOSE\]

But how do you handle the situation if the bot says ""CLOSE"" but there is no open trade to close?

Or it says BUY and again BUY but you only want to open 1 trade?

Currently i got 2 ideas:

&amp;#x200B;

&gt;**1.) remap invalid actions to valid actions**  
&gt;  
&gt;Examples:   
&gt;  
&gt;CLOSE if no trade open is remapped to WAIT  
&gt;  
&gt;BUY if already BUY open is remapped to HOLD

Does this work or is it confusing the agent?  


&gt;2.) mask invalid actions depending on what is allowed  
&gt;  
&gt;Example: after getting my Q values i can set invalid actions to 0.0 and do   
&gt;  
&gt;np.argmax(self.q\_values\[0\], axis=None) on the other actions...

But i got negative rewards in my system. My concern is, that the agent will learn to give invalid commands instead of accepting a negative reward if it gives the allowed command...?  


How do i teach a RL agent to only give valid commands, depending on the current state?",reinforcementlearning,iz8ft,False,/r/reinforcementlearning/comments/im2zuf/how_to_handle_invalid_actions_in_rl/
Rewards in DQN RL for the NEXT or CURRENT state?,1599167814,"Hi guys, 

imagine you are building a trading bot and you are at the point to calculate the rewards.

1.) Here you can see how someone calculates the reward based on CURRENT - OPEN price:

[https://github.com/edwardhdlu/q-trader/blob/master/train.py#L36](https://github.com/edwardhdlu/q-trader/blob/master/train.py#L36)

    	for t in xrange(l):
    		action = agent.act(state)
    
    		# sit
    		next_state = getState(data, t + 1, window_size + 1)
    		reward = 0
    
    		if action == 1: # buy
    			agent.inventory.append(data[t]) # bought_price = price now
    
    		elif action == 2 and len(agent.inventory) &gt; 0: # sell
    			bought_price = agent.inventory.pop(0)
    			reward = max(data[t] - bought_price, 0)
    			total_profit += data[t] - bought_price
    
    		done = True if t == l - 1 else False
    		agent.memory.append((state, action, reward, next_state, done))
    		state = next_state

2.) Here you can see, that they use the price one step ahead, means NEXT - CURRENT price:

[https://github.com/ShuaiW/teach-machine-to-trade/blob/master/envs.py#L63](https://github.com/ShuaiW/teach-machine-to-trade/blob/master/envs.py#L63)  


      def _step(self, action):
        assert self.action_space.contains(action)
        prev_val = self._get_val() # here we get the CURRENT value, not prev!
        self.cur_step += 1 # go to NEXT data point
        self.stock_price = self.stock_price_history[:, self.cur_step] # update price
        self._trade(action)
        cur_val = self._get_val() # now they get the NEXT price
        reward = cur_val - prev_val # !!! they calculate NEXT - CURRENT !!!
        done = self.cur_step == self.n_step - 1
        info = {'cur_val': cur_val}
        return self._get_obs(), reward, done, info

In a labyrinth the reward is depending on the NEXT value, for example if the current action leads to the exit/goal/fruit.

In a trading environment, however, the reward depends on the current value, if I close a trade now, I will close it at the price now, not the next.

In reinforcement learning, are the rewards calculated from CURRENT - LAST value or from the NEXT-CURRENT value? 

Do we need to look ahead?",reinforcementlearning,iz8ft,False,/r/reinforcementlearning/comments/im2ftg/rewards_in_dqn_rl_for_the_next_or_current_state/
[Tutorial] Training a Double Deep Q-Network to Play Super Mario Bros,1599149580,"This post covers an introduction to reinforcement learning, Q-learning, and double Q-learning, followed by a tutorial with full Python code for building our model and training our agent to navigate the Super Mario Bros environment. 

Article link: [https://blog.paperspace.com/building-double-deep-q-network-super-mario-bros/](https://blog.paperspace.com/building-double-deep-q-network-super-mario-bros/) 

Run the code for free on Gradient:  [https://ml-showcase.paperspace.com/projects/super-mario-bros-double-deep-q-network](https://ml-showcase.paperspace.com/projects/super-mario-bros-double-deep-q-network)",reinforcementlearning,hellopaperspace,False,/r/reinforcementlearning/comments/ilwh23/tutorial_training_a_double_deep_qnetwork_to_play/
Adaptive Control Theory &amp; Reinforcement Learning,1599148901,Can someone please explain the connections between the two? I have a fair idea about RL and want to see how AC can help. Would really appreciate examples. Thanks.,reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/ilw8nb/adaptive_control_theory_reinforcement_learning/
A2C vs PPO and Agent's Generalization Ability in Procgen Benchmark,1599147091,"Hello everyone,

I'm currently working on a project comparing A2C and PPO in procedurally generated environments using ProcGen Benchmark. I'm following the following approach (1), as a simplified version of (2):

(1) [https://github.com/rgilman33/simple-A2C-PPO/blob/master/A2C%20PPO.ipynb](https://github.com/rgilman33/simple-A2C-PPO/blob/master/A2C%20PPO.ipynb)

(2) [https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c\_ppo\_acktr/algo/ppo.py](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/ppo.py)

Any recommendation on this topic would be valuable. Thanks!",reinforcementlearning,datascguy,False,/r/reinforcementlearning/comments/ilvn0u/a2c_vs_ppo_and_agents_generalization_ability_in/
[R] Document-editing Assistants and Model-based Reinforcement Learning as a Path to Conversational AI,1599129885,"**Abstract**:  Intelligent assistants that follow commands or answer simple questions, such as Siri and Google search, are among the most economically important applications of AI. Future conversational AI assistants promise even greater capabilities and better user experience through a deeper understanding of the domain, the user, or the user's purposes. But what domain and what methods are best suited to researching and realizing this promise?

 In this paper, researchers argue for the domain of voice document editing and for the methods of model-based reinforcement learning. The primary advantages of voice document editing are that the domain is tightly scoped and that it provides something for the conversation to be about (the document) that is delimited and fully accessible to the intelligent assistant. 

The advantages of reinforcement learning, in general, are that its methods are designed to learn from interaction without explicit instruction and that it formalizes the purposes of the assistant. Model-based reinforcement learning is needed in order to genuinely understand the domain of discourse and thereby work efficiently with the user to achieve their goals. Together, voice document editing and model-based reinforcement learning comprise a promising research direction for achieving conversational AI. 

Full paper: [https://arxiv.org/abs/2008.12095v1](https://arxiv.org/abs/2008.12095v1)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/ilr309/r_documentediting_assistants_and_modelbased/
Stable Baselines Change parameters during learning / save model intermittenly,1599120723,"Hey all,

I am struggling with a couple of things. I have tried many things and they do not seem to work right. 

Is there any way to do something like this.

    model.learn(1000 steps)
    #save model
    #change parameters
    model.learn(1000 steps)
    #save model (but I want to save the whole thing, all 2000 steps

every time i try something like this the model doesn't save the total steps taken (2000 in above example). It only saves the number of steps taken by the last model.learn when i open up tensorboard.

It is very late, and I am very tired and fed up with this problem. sorry if it is not well exlpained haha!",reinforcementlearning,csullivan107,False,/r/reinforcementlearning/comments/ilpfok/stable_baselines_change_parameters_during/
Reinforcement Q-Learning from Scratch in Python with OpenAI Gym,1599117486,,reinforcementlearning,sharkdeng,False,/r/reinforcementlearning/comments/iloukn/reinforcement_qlearning_from_scratch_in_python/
How to evaluate the robustness of a learned Reinforcement learning policy?,1599099016,"Current learned RL policy can get super-human performance in many fields, such as video games, GO game. The following is based on my intuition.

I think those policies are not robust. If we look through the learning process, the policy is going to the direction which is to get more and more rewards.  But can the policy can make good decisions in most situations even in a very bad situation?  In real-world applications, the environment may change dramatically in a very short time. Does the RL policy still make the best action?

I think the learned policy can deal with the relative ""good"" situation. With ith the training process going, the generated examples are ""good examples"" (state which is to get more rewards). But for some bad situations, it could be not ""well-learned"" during the training.  

 I am trying to make some research in this direction.",reinforcementlearning,alreadybetoken,False,/r/reinforcementlearning/comments/ilkq14/how_to_evaluate_the_robustness_of_a_learned/
Does it make sense to average Q values for DQN?,1599090658,"In the implementation of DQN that takes an input and output |A| values, which represent the list of Q estimates for |A| actions, where |A| is the number of actions. 

Given a state as input, if I average these Q values from my DQN, does it have any meaning for the averaged Q values?",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/iliilw/does_it_make_sense_to_average_q_values_for_dqn/
Offline RL vs KNN in healthcare,1599084909,"I'm an undergrad doing research for my capstone project and while reading this [https://arxiv.org/pdf/2005.01643.pdf](https://arxiv.org/pdf/2005.01643.pdf) I noticed it mentions offline RL being used in healthcare to determine treatment for patients. 

My question is, in what scenario would offline RL be advantageous versus a multi-class classifier algorithm such as KNN?",reinforcementlearning,ItsHampster,False,/r/reinforcementlearning/comments/ilgvtx/offline_rl_vs_knn_in_healthcare/
PPO: questions on trajectories and value loss,1599075857,"Hi everybody! I am currently developing the PPO algorithm for a multi-agent problem. I have some questions:

1) Is the definition of trajectory unique? I mean, can I consider an agent's trajectory terminated whenever it reaches its goal, even if this process requires many episodes and the environment is reset multiple times? I would answer no, but considering longer trajectories seems to perform better than truncating them at the end of the episode independently from the agent final outcome.

2) I've seen some implementations ([https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/f60ac80147d7fcd3aa7e9210e37d5734d9b6f4cd/a2c\_ppo\_acktr/algo/ppo.py#L77](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/f60ac80147d7fcd3aa7e9210e37d5734d9b6f4cd/a2c_ppo_acktr/algo/ppo.py#L77) and [https://github.com/tpbarron/pytorch-ppo/blob/master/main.py#L144](https://github.com/tpbarron/pytorch-ppo/blob/master/main.py#L144)) multiplying the value loss function with 0.5. At first I thought it was the coefficient but I am really not sure?",reinforcementlearning,-john--doe-,False,/r/reinforcementlearning/comments/ildyyf/ppo_questions_on_trajectories_and_value_loss/
"""ReBeL: Combining Deep Reinforcement Learning and Search for Imperfect-Information Games"", Brown et al 2020 {FB} [heads-up no-limit Texas hold'em poker]",1599067643,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ilb8ep/rebel_combining_deep_reinforcement_learning_and/
TD3 reward platue for Continuous Luner Lander OpenAI Gym environment,1599067132,"I've been trying to train [continuous lunar lander](https://gym.openai.com/envs/LunarLanderContinuous-v2/) OpenAI gym environment using TD3 for while now and the rewards during training seem to do well initially but then hit a wall at around 0.

&amp;#x200B;

https://preview.redd.it/s23awlrd8rk51.png?width=402&amp;format=png&amp;auto=webp&amp;s=1b15490a6e6e2190d54a219dc8e4a2eee903e6a8

I'm wondering if anyone has seen behavour like this. When I play the trained agent the lunar lander hovers in place at the top of the screen and doesn't make any attempt to descend to the surface. In earlier episodes it might get down to the surface and actually land but it later seems to unlearn that later. It seems like it decides that landing is too risky and so decideds just to stay where it is instead?

I've been over the code a lot of times so I'm pretty sure the algorithm is correct. The code is [here](https://github.com/mauicv/openai-gym-solns) if you want to take a look but I don't expect anyone to read it. I'm more just wondering if this is an observation others have had and if they figured out what was going on.

Thanks",reinforcementlearning,mauicv,False,/r/reinforcementlearning/comments/ilb2d4/td3_reward_platue_for_continuous_luner_lander/
Best Meta Learning algo for discrete actions,1598994730,"Hi, I wanna do meta reinforcement learning and I have vectors as states and discretes actions. 
Normally I would go for a DQN but I doubt its efficiency on Meta-RL. What the best algorithm for that kind of dilemma ? I was thinking of PPO",reinforcementlearning,Quetor,False,/r/reinforcementlearning/comments/ikt15g/best_meta_learning_algo_for_discrete_actions/
Implemented Deep Q Learning from scratch,1598991180,"Recently I read the original DQN paper by deep mind so implemented Deep Q Learning from scratch with the network in my [own library](https://github.com/ShivamShrirao/dnn_from_scratch) to play games from visual input. Currently I just trained on Breakout and Pong. Will try it out on more interesting games and also implement improvements to the algorithm.
Link: https://github.com/ShivamShrirao/deep_Q_learning_from_scratch",reinforcementlearning,0x00groot,False,/r/reinforcementlearning/comments/ikrvgi/implemented_deep_q_learning_from_scratch/
What makes Soft Actor-Critic able to be off-policy?,1598987320,"Soft Actor-Critic is based on maximum entropy learning and thus compared with the common objective, there's a entropy term added to it. But if we remove that, all those expectations in $J\_\\pi$ or $J\_Q$ are still valid. 

In the SAC paper, they also proved that the improvement is guaranteed every soft policy iteration, but doesn't that also hold for normal Actor-Critic?

Then why can't actor-critic be off-policy? What's the key difference that enables SAC to become off-policy?",reinforcementlearning,Gabr1e111,False,/r/reinforcementlearning/comments/ikqn6u/what_makes_soft_actorcritic_able_to_be_offpolicy/
GPU-accelerated MOBA environment,1598972744,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/iklxln/gpuaccelerated_moba_environment/
Trading a sine wave with DQN sparse/full rewards,1598905145,"Hi folks, I've been dealing with reinforcement learning for a few months now and have put together a system in which I can use Keras / PyTorch and openai.ai gyms (cartpole, lunarlander etc.). I also built my own environment, which is almost completely gym compatible.

I can load data in my environment and define own rules, call step(). One of my environments for testing the system is trading a simple sine wave. To keep it simple, imagine the agent only has 3 actions (0-WAIT / 1-BUY / 2-CLOSE).

[sine trading | data | reward | accumulated rewards](https://preview.redd.it/kvgyhpyj2ek51.png?width=800&amp;format=png&amp;auto=webp&amp;s=03dba9cadc8234ffb1a1dc36bcf67dc12244854a)

If the agent carries out an illegal action BUY / BUY or CLOSE / CLOSE or CLOSE if no BUY is open - he receives a penalty of -10. 

If I give the agent detailed rewards, everything works great. 

By detailed rewards I mean the following:

ACTIONS and REWARDS:  
BUY     =  reward 0.0 IF NO BUY else -10.0 IF BUY ALREADY OPEN  
WAIT   =  reward 0.0 IF NO BUY else difference (current  value - last value)  
CLOSE = reward -10.0 IF NO BUY else difference (current value - last value)

If a BUY is open, it is saved in status variables. This information is appended to the state array. Therefore my state array consists of 2 parts \[\[normal inputs\], \[status BUY - none = 0 / open = 1\]\].

**If I do it exactly like that, everything works great.** With the negative reward for illegal commands, the agent first learns the rules, then optimizes the rewards until he has the right solution.

[epsilon, learning rate, loss and score \(scores \&lt; 0 are removed\)](https://preview.redd.it/tdntda3q2ek51.png?width=969&amp;format=png&amp;auto=webp&amp;s=d015cf562db44547dc08a2505254a03535e8ba8c)

Normally it should do the same thing if I just give it the profit / loss per trade as a reward on the CLOSE, which is exactly what doesn't work and I don't understand why.

Here are some code parts of the **working** system:

    
        #////////////////////////////////////////////////
    
        def _get_price(self, pos):
    
            #////////////////////////////////////////////////
    
            return(round(self.data[pos, 0], ndigits=7)) # price in col 0
    
        #////////////////////////////////////////////////
    
        def _get_state(self, pos):
    
            #////////////////////////////////////////////////
            # np.append does not occur in-place: a new array is !!!allocated!!! and filled
            #////////////////////////////////////////////////
    
            return(np.append(self.data[pos, 1:], (self.b_open, self.s_open), axis=None).astype(self.dtype))
    
        #////////////////////////////////////////////////
    
        def step(self, action):
    
            #////////////////////////////////////////////////
            """""" run Action(t) and calculate the Reward(t), STEP to next State(t+1)""""""
            #////////////////////////////////////////////////
    
            self.action = action                    # current action
            self.astrng = self.actions[self.action] # get action string
    
            #////////////////////////////////////////////////
    
            self.state   = self._get_state(pos=self.pos)                  # get the current State(t)
            self.reward  = 0.0                                            # reset to 0
            self.n_state = np.zeros(self.state.shape).astype(self.dtype)  # new instance! or state=next_state connected
            self.done    = False                                          # end of episode
    
            #////////////////////////////////////////////////
            # DONE | last value is current value, stop 1 before
            #////////////////////////////////////////////////
    
            if(self.pos &gt;= self.data_len - 2): self.done = True # todo check
    
            #////////////////////////////////////////////////
    
            self.lcp = self._get_price(pos=self.pos-1)
            self.ccp = self._get_price(pos=self.pos)
    
            #////////////////////////////////////////////////
            # MANAGE BUY
            #////////////////////////////////////////////////
    
            if(self.b_open == 1):
    
                #////////////////////////////////////////////////
    
                if(self.astrng == 'WAIT'):
    
                    #////////////////////////////////////////////////
    
                    self.reward = round((self.ccp - self.lcp), 10) * 100.0
    
                #////////////////////////////////////////////////
    
                elif(self.astrng == 'BUY' or self.astrng == 'SELL'):
    
                    #////////////////////////////////////////////////
    
                    self.reward = self.reward_wrong
    
                #////////////////////////////////////////////////
    
                elif(self.astrng == 'CLOSE'):
    
                    #////////////////////////////////////////////////
    
                    self.reward = round((self.ccp - self.lcp), 10) * 100.0
    
                    #////////////////////////////////////////////////
    
                    self.b_open = 0
                    self.oob    = 0
                    self.oop    = 0.0
    
            #////////////////////////////////////////////////
            # WRONG CLOSE &gt;&gt;&gt; NO TRADE OPEN
            #////////////////////////////////////////////////
    
            elif(self.astrng == 'CLOSE'): self.reward = self.reward_wrong
    
            #////////////////////////////////////////////////
            # BUY OPEN
            #////////////////////////////////////////////////
    
            elif((self.b_open == 0) and(self.astrng == 'BUY')):
    
                #////////////////////////////////////////////////
    
                self.b_open = 1
                self.oob    = self.pos
                self.oop    = self.ccp
    
            #////////////////////////////////////////////////
    
            self.episode_rewards[self.pos] = self.reward
    
            #////////////////////////////////////////////////
            # TRANSITION TO NEXT STATE | inc pos
            #////////////////////////////////////////////////
    
            self.pos += 1
    
            #////////////////////////////////////////////////
            # DONE | todo check what todo
            #////////////////////////////////////////////////
    
            if(self.done):
    
                #////////////////////////////////////////////////
    
                pass
    
            #////////////////////////////////////////////////
    
            else:
    
                #////////////////////////////////////////////////
                # _get_state(pos) needs corresponding b_open &amp; s_open to be set
                #////////////////////////////////////////////////
    
                self.n_state[:] = self._get_state(pos=self.pos)
    
            #////////////////////////////////////////////////
    
            return((self.n_state, self.reward, self.done, 'info'))

The important part here is at ""# MANAGE BUY"", changing the following lines to only give a reward ""per trade"" on CLOSE results in wrong learning.

    
            #////////////////////////////////////////////////
            # MANAGE BUY
            #////////////////////////////////////////////////
    
            if(self.b_open == 1):
    
                #////////////////////////////////////////////////
    
                if(self.astrng == 'WAIT'):
    
                    #////////////////////////////////////////////////
    
                    self.reward = 0.0
    
                #////////////////////////////////////////////////
    
                elif(self.astrng == 'BUY' or self.astrng == 'SELL'):
    
                    #////////////////////////////////////////////////
    
                    self.reward = self.reward_wrong
    
                #////////////////////////////////////////////////
    
                elif(self.astrng == 'CLOSE'):
    
                    #////////////////////////////////////////////////
    
                    self.reward = round((self.ccp - self.oop), 10) * 100.0
    
                    #////////////////////////////////////////////////
    
                    self.b_open = 0
                    self.oob    = 0
                    self.oop    = 0.0

I know that is / was a lot of text but I have no idea how to describe the problem in a shorter way.

Actually, I have 2 questions about this:  
1. Why does my system work with detailed rewards but does not with sparse rewards?

2. How do I treat done (the last value) in this case?

Thank you in advance, I'm looking forward to your ideas!",reinforcementlearning,iz8ft,False,/r/reinforcementlearning/comments/ik5psw/trading_a_sine_wave_with_dqn_sparsefull_rewards/
Need help with resources,1598900824,"Hey guys, I am new to AI and ML and I was wondering if there are any resources where they discuss ML/AI/RL papers and its math behind it?",reinforcementlearning,Snoop_52275,False,/r/reinforcementlearning/comments/ik4bqh/need_help_with_resources/
AI discovers a whole new way to play pong. (sorry for the stuttering),1598892882,,reinforcementlearning,LeonShams,False,/r/reinforcementlearning/comments/ik1qkb/ai_discovers_a_whole_new_way_to_play_pong_sorry/
Any ideas on Crystal Generation using RL,1598877459,"Hey everyone! I am new to RL, and I am working on a project which is to predict the Crystal group structure. The task is trying to predict how monomer can connect with other monomer to get  polymer which exists in the real world. I can represent the molecule into a graph class, which contains the \`atom\_type\`, \`atom\_bond\`, \`bond\_length\`. But the number of data is really small, so I doubt it could use any deep learning method? Although I have some restrictions like the \`bond\_length\` cannot longer than x nm, something like that.

The picture is one example of the polymer which consists of two monomers.

I think this task could be solved by RL, but I am really new to this area and though I am working on DL for 2 years, mostly in Computer Vision. 

Could anyone give me some advice? Anything concerned is welcomed!

Thank you in advance!!",reinforcementlearning,Rasingue,False,/r/reinforcementlearning/comments/ijx6bo/any_ideas_on_crystal_generation_using_rl/
Pong perfect score of 21 in 80 episodes.,1598833568,I created my own version of a [Rainbow DQN](https://arxiv.org/pdf/1710.02298.pdf) (without the C51) with a slightly modified version of n-step. I tested it out on Atari 2600 Pong and I was able to get a perfect score of 21 after 80 episodes. Would this be considered good performance?,reinforcementlearning,LeonShams,False,/r/reinforcementlearning/comments/ijnxkn/pong_perfect_score_of_21_in_80_episodes/
MADDPG Unity Tennis - fastest completion time,1598822584,"Hey there,

I recently finished Udacity's Deep Reinforcement Learning nanodegree and as part of one of the assignments, we were supposed to implement a multi-agent reinforcement learning algorithm in the game of ping pong (table tennis).

My agent solves the environment in around 230 episodes. the next closest one is around 430 (as far as i know). I ran mine on CPU and it took about 10 minutes for completion. I've attached the code below in case someone find it useful. 

Algorithm used: MADDPG (Multi-Agent Deep Deterministic Policy Gradient).

Source: [https://github.com/QasimWani/RL-Unity](https://github.com/QasimWani/RL-Unity) (its in the MADDPG folder).

&amp;#x200B;

If any of you found it helpful, I'd really appreciate a star. this encourages me to commit to open source. feedback is always appreciated. 

thanks :D",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/ijkv75/maddpg_unity_tennis_fastest_completion_time/
DQN Settlers of Catan,1598816886,"I am using DQN in Pytorch to learn an agent the rules of Settlers of Catan. I am following Pytorch's tutorial ([https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)) as close as possible since I am quite new to RL. 

&amp;#x200B;

My question is more of the fundamental kind:

How do I handle the next\_state that is being pushed to the agent's memory and how do I progress the game when an illegal action given. There are 2 options I tried  to let the agent learn the rules of the game. 

\- Whenever 1 of the 4 players tries an illegal action (build a road while not having enough resources, build a settlement on a spot that is not connected to road for example) their turn immediately ends and next\_state = None, however the game continues. 

\- Whenever 1 of the 4 players tries an illegal action the game is over for them and next\_state = None. Other players still get the chance to perform an action until they perform an illegal action as well

Both of these options do not work. Using the first option makes the agents try even more illegal actions over time compared to random actions. I am thinking this is because next\_state is not really None, it just doesn't change. However setting next\_state = state gives even worse results. The second option makes none of the agents even able to build a new settlement even with infinite resources (they need to build 2 roads and then a settlement without trying any illegal action). 

So which of these options would be the correct way to let an agent learn the rules of a game? Is it the first one and I defining the next\_state wrong? Or is the second one and am I doing something else wrong? If you do not know the game of Catan, the same can be applied to chess. Restart the game at an illegal action or keep going?",reinforcementlearning,NickOTeenO,False,/r/reinforcementlearning/comments/ijj4yq/dqn_settlers_of_catan/
How to find the best coefficients for the reward function?,1598805680,"I have a reward function that consists of three components, where each component has a different order of magnitude from the others. Is there a way to find the best coefficients that lead to the highest reward  instead of try and error?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/ijfq09/how_to_find_the_best_coefficients_for_the_reward/
Facing error while trying to implement pendulum with DQN based on input of pixels,1598803825,"I am trying to implement DQN in pendulum from openaigym based on pixels . I have took the official tutorial code of pytorch-cartpole .

My code : [https://gist.github.com/ajithvallabai/b17a2848f77573f933f7586d465288b3](https://gist.github.com/ajithvallabai/b17a2848f77573f933f7586d465288b3) 

Reference code :  [https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) 

I am facing below error :

 Traceback (most recent call last):  
  File ""classic\_control/pendulum/pendulum\_screen\_2.py"", line 295, in &lt;module&gt;  
    optimize\_model()  
  File ""classic\_control/pendulum/pendulum\_screen\_2.py"", line 260, in optimize\_model  
    loss.backward()  
  File ""/home/robot/anaconda3/envs/tf3/lib/python3.6/site-packages/torch/tensor.py"", line 198, in backward  
    torch.autograd.backward(self, gradient, retain\_graph, create\_graph)  
  File ""/home/robot/anaconda3/envs/tf3/lib/python3.6/site-packages/torch/autograd/\_\_init\_\_.py"", line 100, in backward  
    allow\_unreachable=True)  # allow\_unreachable flag  
RuntimeError: expected dtype Double but got dtype Float (validate\_dtype at /pytorch/aten/src/ATen/native/TensorIterator.cpp:143) 

&amp;#x200B;

Could some one help me to run the program properly i searched in github also not able to find any code for DQN based on this method for pendulum-v0",reinforcementlearning,ajithvallabai,False,/r/reinforcementlearning/comments/ijf6wr/facing_error_while_trying_to_implement_pendulum/
Feature matching in IRL,1598794456,"I am trying to understand the concept of feature matching in IRL. This is used in MaxEnt IRL, Deep MaxEnt IRL and other versions. 
I understand that it is a vector that represents the sequence of states (or state-action pairs) observed.

Let's say the state is the one-hot-encoded version of a 2x2 gridworld: S1=[1,0,0,0], S2 =[0,1,0,0], ...
If the vector of observations B is: B = [S1, S3, S2], what would the empirical feature count be?",reinforcementlearning,thatpizzatho,False,/r/reinforcementlearning/comments/ijcr0w/feature_matching_in_irl/
Need help understanding AlphaZero,1598787346,"I read so many articles about AlphaZero and so many implementation about AlphaZero that I still don't understand some points.

1. Do you collect training data of your neural network as you self play? Or do you self play like a million times then train your neural net on the data? I believe it is the former but I seen implementations where it is the latter, which doesn't make sense to me.
2. Do you have to stimulate to the terminal state? I seen implementation where it does but most explanation make it seem like it doesn't need to?
3. If we are training as we play and we don't stimulate terminal state, how does learning even occur? How do we produce labels for our neural net? If I understand correct, we stimulate up to X number of moves ahead, then we use the neural net that we are training on to evaluate the value of this ""terminal"" state? For an untrained network, it is just garbage? 

So, just to make sure I get the big picture, AlphaGo basically:

1. Start building the MCT
2. Stimulate the next action picked using UCB
3. Repeat step 2 X number of times
4. Value of the leaf is the value outputted by the neural net at the leaf state
5. Backprop the value back to the root
6. Repeat 2-5 Y number of times
7. Pick next action based on the state with expected highest value
8. Train neural network using state, value pairs (is it on both stimulated and actual or just actual?)
9. Restart game and repeat 1-8 

So we will have 2 hyperparameters to limit search space: the number of stimulations and the depths of each simulation?",reinforcementlearning,idkname999,False,/r/reinforcementlearning/comments/ijbbia/need_help_understanding_alphazero/
How to run RLLab on MacOS,1598786866,"rllab requires a 1.31 version of mujoco, but that does not seem to compatible with macos later than 10.12.

Is there a workaround of some kind? I tried modifying the ""[mjlib.py](https://mjlib.py)"" file but more and more errors just keep popping up.",reinforcementlearning,Gabr1e111,False,/r/reinforcementlearning/comments/ijb8oq/how_to_run_rllab_on_macos/
"""Towards General and Autonomous Learning of Core Skills: A Case Study in Locomotion"", Hafner et al 2020 {DM}",1598734305,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iizyp0/towards_general_and_autonomous_learning_of_core/
Infinite episode Reinforcement learning?,1598727774,"Hello,

I am looking for more information in regards to the infinite episode concept. I’ve seen it in a paper, but I can’t remember where nor what it was about really, only the concept. I hope I explained enough and somebody could link me to resources and or answer with your opinion/experience.

The environment was poker where the agent his reward was the current money he owns, the agent just plays poker like we know, but instead an episode being 1 round in poker (sorry I can’t explain the entire game rn, Google) the agent has the entire game as his episode, i believe they did this as the agent would be better at understanding the risk that it brings to doing x causing the total reward to be y. I think this is an intresting concept as i think it gives more depth to risk if your total “stash” is affected by things u have done. 

Anybody knows what I am talking about or perhaps explain it correct/more in depth as I am doing this all from memory from 1-2 years ago. Any input is appreciated.",reinforcementlearning,Additional-Teach5180,False,/r/reinforcementlearning/comments/iiy405/infinite_episode_reinforcement_learning/
Freezing DQN policy layers in stable-baselines 2 for transfer learning purposes?,1598721217,"I'm trying to perform transfer learning of a policy learned using stable-baseline's DQN (the state features and actions are the same, but there's a change in the environment (Half Field Offense) - there's a team mate with different possible behaviors, not made by me, that I treat as part of the environment). The idea is, after training a policy with team mate A, when playing with team mate B (previously unknown), to load A's policy and start training it.

I had the idea of freezing some weights of the policy to help with the transfer, which currently takes way too long. With Keras one could do something like `layer1.trainable = False`, but I'm not too familiar with stable-baselines or Tensorflow (1.x) (used internally by the former), and I couldn't find an easy way to freeze the initial layers of my policy (loaded with SB's DQN.load()). Also [Tensorflow's documentation just tells me to use Keras](https://www.tensorflow.org/guide/keras/transfer_learning).

I also failed to implement my own DQN in Keras with prioritized experience replay, even using stable-baselines' PrioritizedMemory and SumSegmentTree/MinSegmentTree and looking at its code for reference (I haven't been able to successfully learn a policy for the full problem without PER; also I believe I correctly implemented Nature DQN + Double + Dueling.)

Any help is appreciated. Thank you!",reinforcementlearning,Pypirl,False,/r/reinforcementlearning/comments/iiw54w/freezing_dqn_policy_layers_in_stablebaselines_2/
Questions regarding MCTS,1598686867,"In Monte Carlo Tree Search, in the context of AlphaGo Zero, do you build a new tree for every action you take? 

If not, does that mean we need to store every state, action pair? For UCB, you need N(s,a), the visit count. If that is persistent across moves/simulations, doesn't that mean we need to store everything?

I'm a bit confused. Thanks for the help.",reinforcementlearning,idkname999,False,/r/reinforcementlearning/comments/iiojlu/questions_regarding_mcts/
"""How Close Are Computers to Automating Mathematical Reasoning? AI tools are shaping next-generation theorem provers, and with them the relationship between math and machine""",1598656886,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iihw24/how_close_are_computers_to_automating/
[Unity ML-Agents] Landing rockets with PPO,1598647135,,reinforcementlearning,alexandretorres_,False,/r/reinforcementlearning/comments/iif1dd/unity_mlagents_landing_rockets_with_ppo/
Reinforcement Learning for imbalanced classification,1598620603,"Hello everyone! I am currently working on a project where i want to use reinforcement learning to perform a classification task for an imbalanced dataset ( Fraud data to be exact ) . I have implemented the DQN algorithm and tested it on a balanced dataset (IRIS) to make sure everything is working and it successfully converges to acceptable solutions. When trying to scale the algorithm to a much larger and imbalanced dataset, the training seems to be much slower and although i ran it for like 8 hours (which makes roughly 700 episodes), the cumulative rewards for each episode don't show an increasing trend. Would anyone know if this is a normal issue where i need to train for longer or do i need to further tune my hyperparameters/NN architecture? Or are there any tricks to help the algorithm converge?

I am currently basing my work on 

[https://www.researchgate.net/publication/339996036\_Q-Credit\_Card\_Fraud\_Detector\_for\_Imbalanced\_Classification\_using\_Reinforcement\_Learning](https://www.researchgate.net/publication/339996036_Q-Credit_Card_Fraud_Detector_for_Imbalanced_Classification_using_Reinforcement_Learning)

but using a different and much larger dataset.

Thanks!",reinforcementlearning,BoOM_837,False,/r/reinforcementlearning/comments/ii6u52/reinforcement_learning_for_imbalanced/
How we are calculating average reward (r(π)) if the policy changes over time?,1598619753," In the average reward setting the quality of a policy is defined as: 

&amp;#x200B;

https://preview.redd.it/7g2qwbv6rqj51.png?width=201&amp;format=png&amp;auto=webp&amp;s=99cd42da627bc0a119b5f8ff2bd5e7fb3b5f38a2

 When we reach the steady state distribution we can write the above equation as follows: 

&amp;#x200B;

https://preview.redd.it/olusgksarqj51.png?width=191&amp;format=png&amp;auto=webp&amp;s=80e9f0c756d2873c820a60c94d83df74c3853d3b

 We can use the incremental update method to find r(π):

  
 

https://preview.redd.it/4mz0og8frqj51.png?width=284&amp;format=png&amp;auto=webp&amp;s=95d447395b47a9478e75e3e001206aeb52bd3f57

 where R¯t−1 is the estimate of the average reward r(π)r(π) at timestep t−1t−1. We use this incremental update rule in the SARSA algorithm: 

&amp;#x200B;

https://preview.redd.it/t69xun1mrqj51.png?width=512&amp;format=png&amp;auto=webp&amp;s=9cad940e4de271e4edc7397505decee0130c5600

 Now, In this above algorithm, we can see that the policy will change with respect to time. But to calculate the r(π), the agent should follow the policy π for a long period of time. Then how we are using r(π) if the policy changes with respect to time?",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/ii6m9q/how_we_are_calculating_average_reward_rπ_if_the/
How does GoalGan evaulate goals of intermediate difficulty?,1598608513,"Is it correct for me to think that the R^(g) in the paper is the ""success rate"" of the goal g? But its implementation is a bit different, using the average of reward(in a single trajectory) as its success rate.

If I understand the code correctly, then a success with a long trajectory has a low R\^g?

Am I interpreting the code wrongly or does this make sense?",reinforcementlearning,Gabr1e111,False,/r/reinforcementlearning/comments/ii4983/how_does_goalgan_evaulate_goals_of_intermediate/
Solution used by Covariant.ai,1598589764,,reinforcementlearning,matpoliquin,False,/r/reinforcementlearning/comments/ii0qq5/solution_used_by_covariantai/
"""Impact of Go AI on the professional Go world"", Haijin Lee",1598564023,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ihu34l/impact_of_go_ai_on_the_professional_go_world/
Systematic Exploration and Uncertainty Dominate Young Children’s Choices,1598562170,,reinforcementlearning,crlane,False,/r/reinforcementlearning/comments/ihtiso/systematic_exploration_and_uncertainty_dominate/
Reinforcement Learning Thesis Proposals,1598529773,"Hi  guys,

&amp;#x200B;

I have started doing research on Procedural Generation of Environments for an MSc Thesis that didn't finally work and I'm currently searching for solutions related to RL for a robust topic.

&amp;#x200B;

Any recommendation would be highly appreciated!

&amp;#x200B;

Thank you in advance.",reinforcementlearning,datascguy,False,/r/reinforcementlearning/comments/ihjrdy/reinforcement_learning_thesis_proposals/
What setup should I use? Anaconda + PyTorch + SpinningUp?,1598524059,"I want something general, I'm setting up an environment in which I can do RL / Deep RL.

People are mostly recommending PyTorch, I wanna ask what's the difference between Gym and SpinningUp. Which one to use? How compatible is it with PyTorch if at all? Just need to point in the right direction, thank you.",reinforcementlearning,clorky123,False,/r/reinforcementlearning/comments/ihil3a/what_setup_should_i_use_anaconda_pytorch/
GenRL: PyTorch-First Reinforcement Learning library,1598522575,"Github: [https://github.com/SforAiDl/genrl](https://github.com/SforAiDl/genrl)

Reinforcement learning research is moving faster than ever before. In order to keep up with the growing trend and ensure that RL research remains reproducible, GenRL aims to aid faster paper reproduction and benchmarking by providing the following main features:

* **PyTorch-first**: Modular, Extensible and Idiomatic Python
* **Tutorials and Documentation:** We have over 20 tutorials assuming no knowledge of RL concepts. Basic explanations of algorithms in Bandits, Contextual Bandits, RL, Deep RL, etc.
* **Unified Trainer and Logging class**: code reusability and high-level UI
* **Ready-made algorithm implementations**: ready-made implementations of popular RL algorithms.
* **Faster Benchmarking**: automated hyperparameter tuning, environment implementations, etc.

By integrating these features into GenRL, we aim to eventually support **any new algorithm implementation in less than 100 lines**. **We're also looking for more Open Source Contributors!**

Currently, the library has implementations of popular classical and Deep RL agents that ready to be deployed. Apart from these, various Bandit algorithms are a part of GenRL. It has various abstraction layers that make the addition of new algorithms easy for the user. Do give us a star!

[Vanilla DQN](https://preview.redd.it/9hcy6s37qij51.png?width=1548&amp;format=png&amp;auto=webp&amp;s=d9bc334aaf7a731678507627c798706eda5e4b24)

[Training a DoubleDQN would only require changing a single function](https://preview.redd.it/9oc5whh2qij51.png?width=1784&amp;format=png&amp;auto=webp&amp;s=c633f21fa594786157eaedb2b57a7e9cd2a9c883)

[Training a DuelingDQN would only require changing a single function](https://preview.redd.it/sjrt0eh2qij51.png?width=1682&amp;format=png&amp;auto=webp&amp;s=834db1bfab8c14012faec8b74ecfd126cd119558)",reinforcementlearning,sharadchitlangia,False,/r/reinforcementlearning/comments/ihibey/genrl_pytorchfirst_reinforcement_learning_library/
proximal policy gradient tensorflow pendulum issue !!!,1598471933,"import gym  
import numpy as np  
import tensorflow as tf  


class Memory(object):  
 def \_\_init\_\_(self):  
 self.ep\_obs, self.ep\_act, self.ep\_rwd, self.ep\_neglogp = \[\], \[\], \[\], \[\]  
 def store\_transition(self, obs0, act, rwd, neglogp):  
 self.ep\_obs.append(obs0)  
 self.ep\_act.append(act)  
 self.ep\_rwd.append(rwd)  
 self.ep\_neglogp.append(neglogp)  
 def covert\_to\_array(self):  
        array\_obs = np.vstack(self.ep\_obs)  
        array\_act = np.vstack(self.ep\_act)  
        array\_rwd = np.array(self.ep\_rwd)  
        array\_neglogp = np.array(self.ep\_neglogp).squeeze(axis=1)  
 return array\_obs, array\_act, array\_rwd, array\_neglogp  
 def reset(self):  
 self.ep\_obs, self.ep\_act, self.ep\_rwd, self.ep\_neglogp = \[\], \[\], \[\], \[\]  


class ActorNetwork(object):  
 def \_\_init\_\_(self, act\_dim, name):  
 self.act\_dim = act\_dim  
 self.name = name  
 def step(self, obs, reuse):  
 with tf.variable\_scope(self.name, reuse=reuse):  
            h1 = tf.layers.dense(obs, 100, activation=tf.nn.relu)  
            mu = 2 \* tf.layers.dense(h1, self.act\_dim, activation=tf.nn.tanh)  
            sigma = tf.layers.dense(h1, self.act\_dim, activation=tf.nn.softplus)  
            pd = tf.distributions.Normal(loc=mu, scale=sigma)  
 return pd  
 def choose\_action(self, obs, reuse=False):  
        pd = self.step(obs, reuse)  
        action = tf.squeeze(pd.sample(1), axis=0)  
        action = tf.clip\_by\_value(action, -2, 2)  
 return action  
 def get\_neglogp(self, obs, act, reuse=False):  
        pd = self.step(obs, reuse)  
        logp = pd.log\_prob(act)  
 return logp  


class ValueNetwork(object):  
 def \_\_init\_\_(self, name):  
 self.name = name  
 def step(self, obs, reuse):  
 with tf.variable\_scope(self.name, reuse=reuse):  
            h1 = tf.layers.dense(inputs=obs, units=100, activation=tf.nn.relu)  
            value = tf.layers.dense(inputs=h1, units=1)  
 return value  
 def get\_value(self, obs, reuse=False):  
        value = self.step(obs, reuse)  
 return value  


class PPO(object):  
 def \_\_init\_\_(self, act\_dim, obs\_dim, lr\_actor, lr\_value, gamma, clip\_range):  
 self.act\_dim = act\_dim  
 self.obs\_dim = obs\_dim  
 self.lr\_actor = lr\_actor  
 self.lr\_value = lr\_value  
 self.gamma = gamma  
 self.clip\_range = clip\_range  
 self.OBS = tf.placeholder(tf.float32, \[None, self.obs\_dim\], name=""observation"")  
 self.ACT = tf.placeholder(tf.float32, \[None, self.act\_dim\], name=""action"")  
 self.Q\_VAL = tf.placeholder(tf.float32, \[None, 1\], name=""q\_value"")  
 self.ADV = tf.placeholder(tf.float32, \[None, 1\], name=""advantage"")  
 self.NEGLOGP = tf.placeholder(tf.float32, \[None, 1\], name=""old\_neglogp"")  
        actor = ActorNetwork(self.act\_dim, 'actor')  
        value = ValueNetwork('critic')  
 self.memory = Memory()  
 self.action = actor.choose\_action(self.OBS)  
 self.neglogp = actor.get\_neglogp(self.OBS, self.ACT, reuse=True)  
        ratio = tf.math.exp(self.neglogp - self.NEGLOGP)  
        clip\_ratio = tf.clip\_by\_value(ratio, 1. - self.clip\_range, 1. + self.clip\_range)  
        actor\_loss = -tf.reduce\_mean((tf.minimum(ratio\* self.ADV, clip\_ratio\* self.ADV)) )  
 self.actor\_train\_op = tf.train.AdamOptimizer(self.lr\_actor).minimize(actor\_loss)  
 self.value = value.get\_value(self.OBS)  
 self.advantage = self.Q\_VAL - self.value  
        value\_loss = tf.reduce\_mean(tf.square(self.advantage))  
 self.value\_train\_op = tf.train.AdamOptimizer(self.lr\_value).minimize(value\_loss)  
 self.sess = tf.Session()  
 self.sess.run(tf.global\_variables\_initializer())  
 def step(self, obs):  
 if obs.ndim &lt; 2: obs = obs\[np.newaxis, :\]  
        action = self.sess.run(self.action, feed\_dict={self.OBS: obs})  
        action = np.squeeze(action, 1).clip(-2, 2)  
        neglogp = self.sess.run(self.neglogp, feed\_dict={self.OBS: obs, self.ACT: action\[np.newaxis, :\]})  
        value = self.sess.run(self.value, feed\_dict={self.OBS: obs})  
        value = np.squeeze(value, 1).squeeze(0)  
 return action, neglogp, value  
 def learn(self, last\_value, done):  
        obs, act, rwd, neglogp = self.memory.covert\_to\_array()  
        rwd = (rwd - rwd.mean()) / (rwd.std() + 1e-5)  
        q\_value = self.compute\_q\_value(last\_value, obs, rwd)  
        adv = self.sess.run(self.advantage, {self.OBS: obs, self.Q\_VAL: q\_value})  
        \[self.sess.run(self.value\_train\_op,  
                       {self.OBS: obs, self.Q\_VAL: q\_value}) for \_ in range(10)\]  
        \[self.sess.run(self.actor\_train\_op,  
                          {self.OBS: obs, self.ACT: act, self.ADV: adv, self.NEGLOGP: neglogp}) for \_ in range(10)\]  
 self.memory.reset()  
 def compute\_q\_value(self, last\_value, obs, rwd):  
        q\_value = np.zeros\_like(rwd)  
        value = self.sess.run(self.value, feed\_dict={self.OBS: obs})  
 for t in reversed(range(0, len(rwd)-1)):  
            q\_value\[t\] = value\[t+1\] \* self.gamma + rwd\[t\]  
 return q\_value\[:, np.newaxis\]  


env = gym.make('Pendulum-v0')  
env.seed(1)  
env = env.unwrapped  
agent = PPO(act\_dim=env.action\_space.shape\[0\], obs\_dim=env.observation\_space.shape\[0\],  
            lr\_actor=0.0004, lr\_value=0.0003, gamma=0.9, clip\_range=0.2)  
nepisode = 1000  
nstep = 200  
for i\_episode in range(nepisode):  
    obs0 = env.reset()  
    ep\_rwd = 0  
 for t in range(nstep):  
        act, neglogp, \_ = agent.step(obs0)  
        obs1, rwd, done, \_ = env.step(act)  
        agent.memory.store\_transition(obs0, act, rwd, neglogp)  
        obs0 = obs1  
        ep\_rwd += rwd  
 if (t + 1) % 32 == 0 or t == nstep - 1:  
            \_, \_, last\_value = agent.step(obs1)  
            agent.learn(last\_value, done)  
    print('Ep: %i' % i\_episode, ""|Ep\_r: %i"" % ep\_rwd)  
if there is something wrong above tell me, why god its not working, it's days that im trying to fix it, I dont see what can be wrong ?",reinforcementlearning,Cultural_Purchase276,False,/r/reinforcementlearning/comments/ih669i/proximal_policy_gradient_tensorflow_pendulum_issue/
What reinforcement learning method can be used for this problem?,1598471555,"I am trying to train the computer to draw the number 1. I have a 5 x 5 canvas (a matrix of 0s), and the computer must go through each pixel left-right top-down (it cannot go backwards) and choose between 0 (white) or 1 (black). This is pretty simple. The action space is discrete (you can only choose 2 colors),  and it's like a binary decision tree with 25 levels + the top level, left = 0/white, right = 1/black, and 2\^26 different resulting images. But the problem is that there are many different outcomes. There's many ways to draw 1.

    [[0, 1, 0, 0, 0]
     [0, 1, 0, 0, 0]
     [0, 1, 0, 0, 0]
     [0, 1, 0, 0, 0]
     [0, 0, 0, 0, 0]]
    or 
    [[0, 0, 0, 0, 1],
     [0, 0, 0, 0, 1],
     [0, 0, 0, 0, 1],
     [0, 0, 0, 0, 1],
     [0, 0, 0, 0, 1]]
    0 = white, 1 = black.

I'm thinking about using Q-learning to solve this, and was wondering if it is still possible if the range of outputs is not as discrete. I can probably just get a bunch of drawn examples of 1 (like say, 50) and implement them in the reward matrix.

I've also made a neural network that takes in a 5x5 image and can determine if it's a 1 or not. I was wondering if there's a reinforcement learning method that uses a neural network during the training, and if it's the best method for this problem. Thanks",reinforcementlearning,Lockonon2,False,/r/reinforcementlearning/comments/ih61w1/what_reinforcement_learning_method_can_be_used/
"""Curriculum Learning for Reinforcement Learning Domains: A Framework and Survey"", Narvekar et al 2020",1598466151,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ih49rf/curriculum_learning_for_reinforcement_learning/
Open AI gym vs unity ml agent,1598453917,I'm new to RL and recently read about these two toolkits bring used to create simulation of environments? Does anyone know how they compare and the contexts in which one is more useful than the other?,reinforcementlearning,MumenRidaa,False,/r/reinforcementlearning/comments/ih0cy5/open_ai_gym_vs_unity_ml_agent/
Centralized learning-decentralized execution clarification (engineering perspective on PPO algo),1598453176," Hi everyone,

I can understand the theoritical concept of the centralized learning-decentralized execution approach, but I am quite confused about the coding-engineering changes to be done in the update of the networks in the PPO algo.

I think that the actor network (I have seperate networks) will use each agent's actor loss to update the network, but how the critcs are updated? Should I calculate the cummulative critic loss (from all the agents) and backpropagate it in every single critic network?",reinforcementlearning,k_ili,False,/r/reinforcementlearning/comments/ih05bo/centralized_learningdecentralized_execution/
Contextual bandit in Continuous action with non-linear reward?,1598446831,"I recently tried to find algorithm about contextual bandit, especially in continuous action with non-linear reward. 

All I found was contextual bandit in discrete action.

Is there any research that I want exactly? or any keyword?

What I could try is just some of the RL algorithms now.",reinforcementlearning,LukasKyu,False,/r/reinforcementlearning/comments/igydf5/contextual_bandit_in_continuous_action_with/
Multiple moves per turn?,1598402679,"What is common practice when dealing with games that have multiple moves per turn like Risk, Catan, and many video games like Minecraft or League. I imagine for the video games it’s easier to just do one action per step and it works out bc of how fast the steps go. However, would you do the same with one of those board games?",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/igovma/multiple_moves_per_turn/
DDPG,1598388898,"Is there any tutorial that gives an intuitive understanding of what DDPG does. Mostly I see explanations with derivations which does not help. Any intuitive video or material out there.
Thanks",reinforcementlearning,Saty18,False,/r/reinforcementlearning/comments/igkvjh/ddpg/
DDPG vs TD3,1598366971,"Hi, 

Can someone explain the difference between DDPG and TD3. As far as I know TD3 addresses the defects of DDPG. But when I am using DDPG for my real time optimization problem my model converges albeit very slowly. But in TD3 for the same environment, the convergence never happened.

Thanks",reinforcementlearning,Saty18,False,/r/reinforcementlearning/comments/igdm98/ddpg_vs_td3/
Translating learning curves from Mujoco to Pybullet,1598352998,"Hey everyone,

I am currently trying to compare my approach to an already published one ([https://arxiv.org/pdf/1906.08253.pdf](https://arxiv.org/pdf/1906.08253.pdf)). However, they run their experiments in Mujoco and I am running mine in Pybullet.

The easiest way to compare those would be to use the same environments for both, but I do not have access to Mujoco, and I was not able to make their code ([https://github.com/JannerM/mbpo](https://github.com/JannerM/mbpo)) in work in Pybullet using Google Colab.

&amp;#x200B;

Is there anything out there to approximate a translation from Mujoco to Pybullet? I know that this solution is far from perfect, but I can't think of a better way to do this atm. Most ressource state that Pybullet environments are generally harder than Mujoco, but I can't find any direct comparisons.

&amp;#x200B;

Thanks!",reinforcementlearning,durotan97,False,/r/reinforcementlearning/comments/ig9v6i/translating_learning_curves_from_mujoco_to/
[Application] TD3 to derive a neural spacecraft attitude controller. The policy is still able to perform desirably in situations it wasn't explicitly trained on!,1598321848,,reinforcementlearning,theoryofjake,False,/r/reinforcementlearning/comments/ig3agy/application_td3_to_derive_a_neural_spacecraft/
What does the word primitive mean?,1598287022,"What does the word primitive mean in the context of HRL. 

An excerpt from the paper I am reading ‘the lower level operates at a higher temporal resolution and produces primitive actions’. 

So, what does primitive actions mean here?",reinforcementlearning,DefinitelyNot4Burner,False,/r/reinforcementlearning/comments/ifsg2k/what_does_the_word_primitive_mean/
Elon Musk's Open AI Hide &amp; Seek Revolutionizes Reinforcement Learning,1598240208,,reinforcementlearning,Snoo28889,False,/r/reinforcementlearning/comments/ifhluo/elon_musks_open_ai_hide_seek_revolutionizes/
Skyrocketing DDQN Loss,1598227508,"I'm working on a simple RL environment that's just a grid with the 'player' (a green square), and the target (a red square). I'm using tf\_agents for python. It's working ok when the grid size is up to 6x6, but if I go higher than that, the loss skyrockets to 10\^9+

The max reward for the network is 1.0, and the cost for illegal actions is -1.0

Just wondering if this is a well known problem and people have ideas to resolve it? I thought at first it was just a fluke of DQN networks and DDQN was supposed to address it, but DDQN doesn't seem to have resolved the problem.",reinforcementlearning,ogib2,False,/r/reinforcementlearning/comments/ifecuy/skyrocketing_ddqn_loss/
Reinforcement Learning through projects..,1598212590,"So, recently I saw some cool RL projects: one where a drone learns to fly, and one where a character in a game learns to move in a maze.

I have knowledge of deep learning, neural networks. But about reinforcement learning though, I'm very confused how to start as I prefer learning through projects. 

1. I saw a reinforcement learning course on Coursera offered by University of Alberta. Incase anyone has taken that, any suggestions on that course would be helpful. Or any other course you wish to suggest that focuses on developing an actual game at the end.

2. Will completing courses enable me, to build projects that I mentioned before The reason I doubt that is, the environment seemed too 'professional'. Or shall I learning game development first? Are there some simulation environment that I can use and focus only on my RL aspect.",reinforcementlearning,ugh_madlad,False,/r/reinforcementlearning/comments/if9zv4/reinforcement_learning_through_projects/
Multi-Agent: How to get a general policy ?,1598193583,"Hello, I'm working on a mutli-agent where agents have  to cooperate but also being competitive to share a network. It's working so far, but I forgot one criteria, is that at the end I want to extract and deploy one general policy for one agent. 

In my solution right now, each agent learns his own policy. 

So how could I transform this to get a general policy ? 

I didn't find much papers on this, hope you can help me.",reinforcementlearning,Krokodeale,False,/r/reinforcementlearning/comments/if4aed/multiagent_how_to_get_a_general_policy/
Looking for collaborators on implementing a trading environment in a game,1598180152,"Good morning (or whatever it is in your part of the globe)!

I'm working on a project to embed an API into the auction house of the source code of a private server of a popular MMORPG that rhymes with shmorld of shmorelaughs. That API will be used in a Gym environment to train agents using the in game financial data as a market.

I'm pretty invested in the project and I'm going to see it through to the finish but my free time is pretty not-existent. If you're at least modestly interested in the idea and have some C++ skills I would be interested in collaboration; maybe the project will get done some time this century.

The next part of the project involves socket programming in C++ and being able to walk through a very large open source code base without yelling at the lack of consistent design.

You'll probably find the former easier.

If you're interested and you're either out of work or a poor student, let me know. I certainly can't afford to pay your bills but we can talk about me possibly supporting your beer habit for a couple of weeks.",reinforcementlearning,gdpoc,False,/r/reinforcementlearning/comments/if19a1/looking_for_collaborators_on_implementing_a/
Double Deep Q Learning,1598168368,"For double deep Q learning, are we using the original model only for action selection? Not for action evaluation? 

That is what the original paper said. 

However, this implementation uses the target model for both evaluation and selection: [https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f](https://towardsdatascience.com/deep-q-learning-for-the-cartpole-44d761085c2f)

Another implementation uses the target model only for selection and not evaluation (opposite to original paper): [https://towardsdatascience.com/double-deep-q-networks-905dd8325412](https://towardsdatascience.com/double-deep-q-networks-905dd8325412)

&amp;#x200B;

Am I misunderstanding something here?",reinforcementlearning,idkname999,False,/r/reinforcementlearning/comments/iez77t/double_deep_q_learning/
Loading a reinforcement learning algorithm with saved weights using Tensorflow,1598158344,"Tensorflow allows users to save weights &amp; the model architecture, however, that will be insufficient unless the values of certain other variables are also stored. For instance, if epsilon is not stored the model will start exploring from scratch and a new model will have to be trained from scratch.

&amp;#x200B;

How does one go about this problem? Could you direct me to some code that has taken this into account?",reinforcementlearning,Academic-Rent7800,False,/r/reinforcementlearning/comments/iexcqh/loading_a_reinforcement_learning_algorithm_with/
I want to make a feature encoding of a state in a text-based game using LSTM and then input this feature encoding into another NN. What will be my input for the other NN - the hidden state of the LSTM or its output?,1598143125,,reinforcementlearning,noootrooo,False,/r/reinforcementlearning/comments/ietx06/i_want_to_make_a_feature_encoding_of_a_state_in_a/
Help needed for training a DQN algorithm.,1598116609,"Hello everyone,

I need to train a dqn algorithm in a mobile edge computing environment. State looks like this \[data1, data2, ... data7\]. All are numeric values and normalized. Action are \[0,1,2\]. I want to train a dqn to take the best action based on the states. I generated synthetic data for state variables and save them in a CSV file. There is around 40000 row in the CSV file. 

I am struggling to train dqn in these settings. For game like mountain car, cart pole, etc, each episode ends when the agent goes to the end of the game, and they train the dqn for several episodes. In my environment, the episode ends when a certain data variable value becomes smaller than a threshold value. So far, I tried these ways. 

&gt;for e in episodes:  
&gt;  
&gt;get initial state  
&gt;  
&gt;while True:  
&gt;  
&gt;get action(random or argmax)  
&gt;  
&gt;get reward, next state, done  
&gt;  
&gt;put in replay memory  
&gt;  
&gt;train dqn using minibatch and reply memory.  
&gt;  
&gt;if done:  
&gt;  
&gt;break

This way, I iterated over my data(40k) only once. and after 500 episodes, the dqn converge. However, when I evaluate, the dqn doesn't return the best actions. 

I also tried to train a small subset of my data over and over again. 

&gt;for e in episodes:  
&gt;  
&gt;get initial state  
&gt;  
&gt;for i in range(N): #N is length of dataset(say first 1000 from my csv file)  
&gt;  
&gt;get action(random or argmax)  
&gt;  
&gt;get reward, next state, done  
&gt;  
&gt;put in replay memory  
&gt;  
&gt;train dqn using mini-batch and replay memory. 

But, this way, the dqn never converge. 

I'm not sure what I'm doing wrong. How should I train dqn for this dataset so that it returns the best actions? How can I evaluate the network?",reinforcementlearning,raegartargaryan,False,/r/reinforcementlearning/comments/iemd1y/help_needed_for_training_a_dqn_algorithm/
Has anyone plotted the critic loss in the SAC algorithm?,1598116485,"I implemented the SAC algorithm and ran it on the Pendulum-v0 environment. The reward converges to around -200, which is ok i guess. But when I plot the critic loss, it's just going up and down, with occasional peaks at \~several hundred. Is this normal? Can someone who's got a working SAC help plot your critic loss(loss of the q-function) in this environment? THX!!",reinforcementlearning,Gabr1e111,False,/r/reinforcementlearning/comments/iembjv/has_anyone_plotted_the_critic_loss_in_the_sac/
Visualization of a survey of Planning and RL methods,1598111842,"I created a visualization of a survey of methods (planning and reinforcement learning) used to solve (sequential) decision problems here: https://github.com/amy12xx/ml_notes_and_reports/blob/master/solving_mdps/README.md

This is meant mostly as a cheatsheet, to support the wonderful S&amp;B textbook, and to help me keep up with the different settings that RL works in. 

I think I got a little carried away, and may need to break it down into individual slides to make it more readable. Anyway, if you find any errors or have feedback, I'd love to know.",reinforcementlearning,Punkter,False,/r/reinforcementlearning/comments/iekx3z/visualization_of_a_survey_of_planning_and_rl/
How to obtain the penetration term for the Fetch Environments in the OpenAI gym?,1598111573," In the experiments section of the [HER](https://arxiv.org/abs/1707.01495) paper, the authors mention that they added the square depth penetration term to the reward so as to penalize such behavior. Where do you get this quantity in the gym?  
I've gone through the source code of the Fetch environments from OpenAI's Gym github page, but still can't find what I'm looking for. Is there some way to calculate it using some other variables?",reinforcementlearning,TheNush07,False,/r/reinforcementlearning/comments/iekud8/how_to_obtain_the_penetration_term_for_the_fetch/
Solving multiple tasks in parallel using RL,1598111502,Do you know some good and easy to understand papers or tutorials on solving parallel tasks using RL?,reinforcementlearning,s927,False,/r/reinforcementlearning/comments/iektjg/solving_multiple_tasks_in_parallel_using_rl/
Self-play on an 2-player fighting game,1598107816,"Hey! I've been interested in making an AI for a 2-player fighting game (that doesn't have an in-game AI) where your goal is to fight to the end of a side-scrolling map. After looking around, it seemed like self-play might be a viable choice. Some digging then led me to Stable Baselines 3 (I'm not too familiar with RL, so I was hoping there was a plug n play solution that would allow me to do some tuning) and [this example project](https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py). I have a question regarding how I might approach this:

* [PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#stable_baselines3.ppo.PPO) takes a \` n\_steps \`. Is it correct to have PPO run for like 4096 steps, reach a game state, update the policy (where the game just stops since no inputs are coming in), then continue from that same game state? Wouldn't this mean that the same game could go on forever? 

I'm still trying to read up about this more, but I fear this is too challenging of a project for me (not going to give up just yet though :) )",reinforcementlearning,ShadyIronclad,False,/r/reinforcementlearning/comments/iejr7j/selfplay_on_an_2player_fighting_game/
Stable baselines DDPG,1598069487,I am working on a constrained optimization problem using DDPG algorithms of stable baselines. But anyone here who has knowledge about the library. How to tune the hyper parameters. Currently the optimization takes about 40 minutes. I want to make it converge faster. Any help is appreciated,reinforcementlearning,Saty18,False,/r/reinforcementlearning/comments/iebya3/stable_baselines_ddpg/
Difference between discount factor and gamma?,1598041221,They both seem to do the same thing,reinforcementlearning,Yogi_DMT,False,/r/reinforcementlearning/comments/ie4h14/difference_between_discount_factor_and_gamma/
"Experts Predict The Next Roadblocks in AI - Hear from Jeff Clune, Andriy Burkov, Jane Wang, Alexia Jolicoeur-Martineau and more.",1598010741,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/idva9k/experts_predict_the_next_roadblocks_in_ai_hear/
Question regarding Deep Q Learning using Frame Skipping,1597995351,"I  want to create an agent which is based on DQL to master the classic  game of snake. But instead of pre selecting the state features, I want  to give the entire 2D grid and use a Conv Network to predict the Q  values.

I read Deep-mind's paper  on Atari and the recommended frame skipping upto 4 frames to give a  sense of motion to the agent and follow the same action for all 4  frames. The reward at the end can be the average reward for the 4 steps.

So  the agent basically only predicts action every 4th frame. But if I want  to update my Q values, I need to supply the network with the current  state stack, average reward and the next state stack.

Because  the NN is designed in such a way that it takes 4 frames as a stack and  predicts the q value. My problem is that I also need to predict the Q  values of the next state to update the network. But what should I use as  my next state stack?

Say the current state stack is frames 0-3 then should the next state stack be frames 1-4 or 4-8?

Basically what frames should be there in the next state stack? Any help appreciated. Thanks.",reinforcementlearning,Slimy_Ranger,False,/r/reinforcementlearning/comments/idse8j/question_regarding_deep_q_learning_using_frame/
Train agent to explore unknown environment,1597973201,"Hi,

&amp;#x200B;

While reading a book on RL ""Deep Reinforcement Learning Hands-On"" By Maxim Lapan I tried to create my own agent and environment. The problem is that my agent does not seem to train properly.

&amp;#x200B;

I want to make an agent that can automatically explore (in the most efficient way) an indoor building. To do that, I created an Gym-like (and compatible) environment, here are the characteristics:

\- Size: 100px by 100px

\- Randomly generated at each reset using an Binary Space Partitioning algorithm (from here: [https://arcade.academy/examples/procedural\_caves\_bsp.html](https://arcade.academy/examples/procedural_caves_bsp.html))

\- All is in grayscale

\- Wall pixels correspond to a value of 0

\- The agent, a 3x3 px square, has a value of 192

\- The empty pixels have a value of 255

\- The explored pixels have a value of 64

\- For each action (up, down, left, right) the agent is moved by 1 pixel

&amp;#x200B;

I clipped the reward between -1 and 1 such as:

\- +1.0 if the agent explore new pixel

\- -0.1 if the agent does not explore new pixel

\- -1.0 if the agent hit a wall

&amp;#x200B;

The win condition is if the agent has explored at least 98% of the pixel with also a reward of +1. Or it looses if the agent does not explore new pixels during 8000 frames.

I used ray-casting to simulate the field-of-view of the agent. It starts with a blank screen and walls automatically appears in a radius of X pixels around the agent.

&amp;#x200B;

https://preview.redd.it/1ki16bu2c9i51.png?width=506&amp;format=png&amp;auto=webp&amp;s=8bda1eadce4f86f258acf8fae79d33ee3612d5d0

I tried using DQN with different enhancements, the code for those are from the book's author here: [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter08](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On-Second-Edition/tree/master/Chapter08)

&amp;#x200B;

The model for the neural network is the following:

&gt;(0): Conv2d(1, 32, kernel\_size=(8, 8), stride=(4, 4))  
&gt;  
&gt;(1): ReLU()  
&gt;  
&gt;(2): Conv2d(32, 64, kernel\_size=(4, 4), stride=(2, 2))  
&gt;  
&gt;(3): ReLU()  
&gt;  
&gt;(4): Conv2d(64, 64, kernel\_size=(3, 3), stride=(1, 1))  
&gt;  
&gt;(5): ReLU()  
&gt;  
&gt;  
&gt;  
&gt;(0): Linear(in\_features=5184, out\_features=512, bias=True)  
&gt;  
&gt;(1): ReLU()  
&gt;  
&gt;(2): Linear(in\_features=512, out\_features=4, bias=True)

&amp;#x200B;

Input to the neural network:

\- 1 frame of the environment: \[1, 1, 100, 100\] normalized from \[0, 255\] to \[0, 1\]

(I did not use frame stacking because there is no moving objects, except the agent at each state)

&amp;#x200B;

Hyperparameters (For basic DQN with replay buffer and target net):

\- lr: 10e-3

\- Replay size: 800 000 (max I can do with my ram)

\- Replay start size: 50 000

\- Sync target net: 10 000

\- Epsilon starting at 1.0 and ending at 0.1 after 1 million frames

\- Gamma: 0.99

\- Batch size: 32

&amp;#x200B;

I tried two things to force the agent to explore:

\- Leaving the environment as specified previously and hoping the rewards as well as the feedback the agent gets from the colored-coded ""explored pixels"" would be enough.

\- Removing the explored pixels color and reward and adding special pixels (1px by 1px) of gray value 128. Each time the agent hit those pixels it would get a reward of +1 (Like apple or points in Pacman, Snake, etc.). To try to force the agent to seek and explore to find those special points. Seen below:

&amp;#x200B;

https://preview.redd.it/acjlust0c9i51.png?width=371&amp;format=png&amp;auto=webp&amp;s=a31a32999037689811072bb1feae48ffc39ccca1

The problem is that the agent does not seems to learn to explore the environment. It gets better scores with the randomness added by epsilon (or noise using Noizy Layers). But after that the agent seems to explore less and less and finally gets mostly stuck in corners. Same with the special points, it does not seem to try to get them. I tried different hyperparameters/rewards, tried basic DQN, adding enhancement (N-step, noizy layers, Double DQN, Dueling, etc.). I ran my code ranging from a few hours to 10 hours (on a 2080 super).

&amp;#x200B;

So I was wondering if maybe there is something wrong with my idea on how to get the agent to explore, maybe the rewards are wrong (i tried varying those), maybe there is something I am missing to get the agent to explore the map. Also I did not really touch the model's network, I might need to add more layer ? Or maybe there is something wrong with the information I gave above. Also I know better methods exist now, A2C, A3C, PPO, etc. But my environment is not that different from an ATARI game that DQN can solve anyway.

&amp;#x200B;

If you have any ideas...

&amp;#x200B;

Thanks!",reinforcementlearning,alphaxenox,False,/r/reinforcementlearning/comments/idnc60/train_agent_to_explore_unknown_environment/
Deep reinforcement learning in Self driving car,1597952579,"Is DRL used in Self driving cars? If yes could you please describe how?

Thanks a lot",reinforcementlearning,chitrang6,False,/r/reinforcementlearning/comments/idhb72/deep_reinforcement_learning_in_self_driving_car/
Framework where RL should be applied,1597909106,"Hello community,

I have applied RL once through a desire to learn this technique. This was applied to a Pac-Man game. Classic.

Now, in my company, we have some R&amp;D budgets and some people wonder if this budget could be dedicated to a proof of concept (POC) using RL techniques. Now, this POC  should have a chance of having business impact i.e. the POC should be applied to a case close to our real business activities.

In the activities we have, all cases we have found can be summarized with : ""One wants to optimize how to organize things in locations with constraints (temperatures, ...)""  (think backpack problem).

Personally, I don't see RL being  the right technique for that. I would rather see ""standard"" softwares implementing optimization under constraints like Gurobi for example. 

Am I right ? 

And do you have ideas or resources where I could find industrial applications of RL techniques ? Because every time I try to think about it, I can't find real life examples except maybe industrial processes but we don't have those kind of business cases.  In each case I can think of, I think to myself ""ok but this is not RL framework, this is ""standard"" optimization framework"".

Thank you !",reinforcementlearning,FenryrMKIII,False,/r/reinforcementlearning/comments/id5wkd/framework_where_rl_should_be_applied/
Intermittent reinforcement,1597891503,"I have come accross this concept of intermittent reinforcement (IR) in psychology in a course by professor Robert Sapolsky. It is a method that has been determined to yield the greatest effort from the subject. The subject does not receive a reward each time they perform a desired behavior or according to any regular schedule but at seemingly random intervals. 

Is it something that has already been tackled in the RL research community ? If not, do you  find it worth the time to explore in order to achieve better performance with existing agents ?",reinforcementlearning,white_noise212,False,/r/reinforcementlearning/comments/id26po/intermittent_reinforcement/
How to make this Double Deep Q Network converge to optimal policy? (Python Keras),1597877323," Hello,  
I've been struggling with this problem (for a school project) for weeks. I managed to solve many issues but this one got me stumped.

**- The goal:** making a RL stock trading agent that can make optimal trading decisions based on price movements within the time-series.

**- The setup:** In order to simplify the problem I was suggested to use deterministic price movements (simulated from a sine function and so not real noisy price) and there are 2 inputs: last action taken by the agent and an indicator binary variable that takes 1 if the next day's price goes up and -1 if it goes down (to simplify the prediction problem/randomness using a ""perfect"" indicator that has a 100% prediction accuracy of the next day's price). The outputs are the 4 possible actions: Buy, Hold, Close current position and stay in Cash (do nothing).

The network is made using Keras' Functional API: it is 2 hidden layers deep the first has a ReLU activation function, the second hidden layer has a softmax activation while the output layer uses a linear activation. I tried many reward signals: variation in price, portfolio value, the profit/loss after each action is taken (both in absolute and percentage points) but the convergence problem persists (I thought about using Inverse RL/Imitation Learning but I can't only start geting into it now given the deadlines). I use an Epsilon-Greedy framework and slowly anneal the exploration rate towards 0 with each time-step. I use the Double DQN framework too (but without Experience Replay). Each observation (last action and 1/-1 pair) is fed into the network individually (no mini-batches), the reward is estimated and SGD is used to minimize the cost (Huber) between the target Q-value and the estimated Q-value (computed from the target network that is frozen for every 10 iterations). And then the next observation from the next point of the time-series is passed and so on.

**- Expected behavior:** The actions are supposed to make sense. Ideally, when the inputs is for example -1 (next day's price goes down) and 0 (last action was ""buy"") the Q value output should be so that the agent closes the position to avoid the loss. When it's in cash and the price is predicted to go up it should buy in order to gain, etc.

**- The problem:** What ends up actually happening is either the agent keeps collecting negative rewards or when it does collect positive rewards the outputs are similar/indentical and usually one predicted Q-value will always be higher than the others no matter the input which makes the agent almost always take that same action (which seems optimal but wouldn't generalize to other observations).

I'm not sure if the problem is in the application of the theory or in the code (since this is my first project on Python). Here's the code:

[http://www.mediafire.com/file/tg5e9952yu1osxu/DQN.txt/file](http://www.mediafire.com/file/tg5e9952yu1osxu/DQN.txt/file)",reinforcementlearning,Ubermensch001,False,/r/reinforcementlearning/comments/icycez/how_to_make_this_double_deep_q_network_converge/
Practical ways to restrict value function search space?,1597871826,"I want to find a way that forces an RL agent's predicted actions (which is directly affected by the learned value function) to follow a certain property.

For example, in a problem whose state S and action A are both numeric values, I want to force the property that, at a higher S value, A should be smaller than at a lower S value, aka the output action A is a monotonic decreasing function of the state S.

This question was first posted in stable-baselines github page because I met this problem when I was using baselines agents to train my model. You may find a bit more references here: [https://github.com/hill-a/stable-baselines/issues/980](https://github.com/hill-a/stable-baselines/issues/980)",reinforcementlearning,Any_Reality_111,False,/r/reinforcementlearning/comments/icwnb8/practical_ways_to_restrict_value_function_search/
Fast reinforcement learning with generalized policy updates (DeepMind),1597843632,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/icnm64/fast_reinforcement_learning_with_generalized/
Submitting a paper to a conference,1597838334,"Hi, I'm new to how conferences work. And I was wondering if someone could shed a light on it:

1. what is a 'workshop' in a conference? For example,  [https://edl-ai-icpr.labri.fr/](https://edl-ai-icpr.labri.fr/) . If I submit it to there, is that equivalent to submit to to ICPR in terms of recognition? What else does it involve?
2. I'm not sure whether my paper will gather interest, and I was wondering if you think it may be wise to submit it to top conferences to see if it gathers interest? And if so, how do I know which ones to submit to? and how long it would take to do this process of application if there'll be rejections and all? Can I just submit it to all relevant conferences (maybe 5\~) at once and see how it does?
3. This is slightly off-topic, but my main goal is to get into to cambrige next year, and I was wondering - do you think if I get to a top conference I have a higher chance of getting accepted to cambrige than if I submit to, say, ICPR? Or is it more about the quality of the paper in the eye of the cambrige guy that review my application and the refernces I get from colleagues and my grades that matter to increase my chances of getting into cambrige?",reinforcementlearning,kakushka123,False,/r/reinforcementlearning/comments/icm9g8/submitting_a_paper_to_a_conference/
Custom Policy Creation in Stable Baselines,1597833618,"I'm currently attempting to implement some of the features of [this paper](https://xbpeng.github.io/projects/SimToReal/2018_SimToReal.pdf), in particular its network architecture. I'm trying to implement it as a custom policy in Stable Baselines, using the [instructions in the documentation.](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html) However, I am struggling to work out this network structure, in particular how to implement the separate recurrent and feedforward branches, and the different input vectors for the policy and value networks.

&amp;#x200B;

Does anyone have any experience of implementing such custom networks in Stable Baselines?",reinforcementlearning,lijaf,False,/r/reinforcementlearning/comments/icl9ck/custom_policy_creation_in_stable_baselines/
How to get an intuition behind calculating the time complexity of an RL algo ?,1597822418,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/icj7bv/how_to_get_an_intuition_behind_calculating_the/
What are advantages of using RL over NN in what types of problems?,1597815957,"what is the main difference between RL and neural networks? Seems like all of the jobs done by RL can be accomplished through neural network architectures. For instance, actions can be made by multiple softmax layers, and value function approximation with state inputs are just working the same as a normal neural network. I clearly don't get the advantages of using RL. What is difference i.e. what problems should be using RL instead of NN and why is using RL better than NN?",reinforcementlearning,g6ssgs,False,/r/reinforcementlearning/comments/ichzao/what_are_advantages_of_using_rl_over_nn_in_what/
[Question]Constrained MDP and related algorithm,1597808896,"Is there anybody who is famaliar with Constrained MDP and related algorithm like CPO. I'm new in this area and want get some help. The main questions are

1.Why most CMDP algorithm is model-free?Sometions we do have the dynamics(models) of the environment and in this case how do we solve the CMDP?

2.Why CMDP considers the constraints of cumulative cost? For exmaple, in control area like MPC, people always considers a state constraint (e.g., h(s\_t)&lt;=0, t=0,1,2,.....). Is there any research about how to sovle RL with state constraint like that. I don't think state constraint is the same as cumulative cost constraint.

If you are also interested in CMDP, I hope we can take more discussion, too.",reinforcementlearning,pby19,False,/r/reinforcementlearning/comments/icgeft/questionconstrained_mdp_and_related_algorithm/
"""Super-Human Performance in Gran Turismo Sport Using Deep Reinforcement Learning"", Fuchs et al 2020 {Sony}",1597803677,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/icf3nc/superhuman_performance_in_gran_turismo_sport/
"Intuition about ""good event"" selection in UCB regret proof",1597789191,"Hi Reddit,

This is with respect to the proof for Theorem 8.1 for the regret of the UCB algorithm given that the arms are 1-subgaussian.

The book pdf is available here for free on the author's website:

[https://tor-lattimore.com/downloads/book/book.pdf](https://tor-lattimore.com/downloads/book/book.pdf)

I'm very confused about where the ""good event"" comes from. I can agree with everything in the proof and reconstruct it from remembering a little (the high level idea of the ""good event"") and letting my intuition guide me after that, but I'm having trouble seeing where this good event comes from. The authors go through the proof in great detail to allow readers to easily reconstruct it, but why the good event is selected to be what it is alludes me. Some help on this would be appreciated. Thanks!",reinforcementlearning,OriginalMoment,False,/r/reinforcementlearning/comments/icb3ww/intuition_about_good_event_selection_in_ucb/
Taylor Killian on the latest in RL for Health : TalkRL,1597782009,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/ic8uzm/taylor_killian_on_the_latest_in_rl_for_health/
DRL using AWS,1597772377,"Does anyone have any experience using aws for drl projects? I am training a custom gym environment with stable-baselines and have some AWS credits I'd like to use. However, all of my tests are running at the same computation speed as my laptop. I see high cpu and low gpu usage.   


I have been looking at this issue with GPU usage with PPO2 (stable-baselines) \[[https://github.com/hill-a/stable-baselines/issues/308](https://github.com/hill-a/stable-baselines/issues/308)\], but the suggestions aren't helping. The environment is not super simple, and my networks aren't small either.   


If anyone has some insight, I would really appreciate it. Thanks!",reinforcementlearning,SupMathematician,False,/r/reinforcementlearning/comments/ic5ota/drl_using_aws/
n step SARSA on FrozenLake,1597762929,"I am coded up the n-step SARSA alg from sutton's textbook on the frozenlakes env, but the performance seems to be very random. I know the env itself is stochastic, but the other algs seem to work fine. here is the code.

def nstep\_sarsa(env, n, MAX\_ITER=int(1e4), alpha=0.6, epilson=0.4, gamma=0.98):

qvalue = defaultdict(np.random.rand)

for \_ in range(MAX\_ITER):

state = env.reset()

action = epilsongreedy(env, qvalue, state, epilson)

nstates, naction, nreward = \[state\], \[action\], \[0\]

t, T = 0, int(1e5)

while True:

if t &lt; T:

next\_state, reward, done, \_ = env.step(action)

nstates.append(next\_state)

nreward.append(reward)

if done:

T = t+1

else:

action = epilsongreedy(env, qvalue, next\_state, epilson)

naction.append(action)

tau = t-n+1

if tau&gt;=0:

G = 0

for i in range(tau + 1, min(tau + n, T) + 1):

G += (gamma\*\*(i - tau - 1)) \* nreward\[i-1\]

if tau+n&lt;T:

G += (gamma\*\*(n)) \* qvalue\[nstates\[tau+n\], naction\[tau+n\]\]

qvalue\[nstates\[tau\],naction\[tau\]\] += alpha\*(G-qvalue\[nstates\[tau\],naction\[tau\]\])

t+=1

if tau==T-1:

break

return qvalue",reinforcementlearning,domhuh4,False,/r/reinforcementlearning/comments/ic2mnq/n_step_sarsa_on_frozenlake/
Implementing code from a paper: Which kind of loss function would be used for giving the reward function according to this equation?,1597755666,"https://ibb.co/1Mp8L3Q

I'm trying to implement code from a paper in Pytorch. 
The reward function given in the picture depends on the loss.

However, they only give this formula without explanation, so I'm not sure which loss function to use.

Also, I haven't found resources online where the loss functions were written in this way with norms.

Could anyone point me in the right direction?",reinforcementlearning,noootrooo,False,/r/reinforcementlearning/comments/ic0jv2/implementing_code_from_a_paper_which_kind_of_loss/
What happens when exceeding the state action predefined box limits?,1597744574,"Nothing happens, but what really happens when you exceed the boundaries? And is there anything computational related with these boundaries?

Functions 

Spaces.discrete and spaces.box from Gym.",reinforcementlearning,Additional-Teach5180,False,/r/reinforcementlearning/comments/iby30v/what_happens_when_exceeding_the_state_action/
How to save Tensorflow Agent trained policy,1597711714,"Hello Guys, 

I am learning to use Tensorflow Agents. However, the documentation is vague.

Can anyone guide me on how to save a trained agent (i.e PPO Agent) 

And if you have any better resource to learn TF Agents, please share with me. 

Thank you!",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/ibqve9/how_to_save_tensorflow_agent_trained_policy/
Fetch Environments from OpenAI gym,1597711515,I was going through the HER paper and I was wondering if that was the State of the Art for solving the Fetch Environments? Are there algorithms that are better suited to solving such environments in continuous state and action spaces?,reinforcementlearning,TheNush07,False,/r/reinforcementlearning/comments/ibqtl6/fetch_environments_from_openai_gym/
How to land a job in DRL with Embedded system background,1597703078,"Hello Folks,

I need your help. I have been doing Machine/Deep learning for the last 3 years on and off. I know Tensorflow and I have worked on a few projects on supervised learning using deep learning. Lately, I have started learning reinforcement learning. For that, I am currently doing a Coursera course and David Silver youtube course. Are there any suggestions you guys want to provide to land a job in the field of AI. 

I have a master's in Computer engineering with Embedded system specialization from US university. Currently, I am in San Francisco working as a Firmware/Embedded software engineer.

&amp;#x200B;

THanks",reinforcementlearning,chitrang6,False,/r/reinforcementlearning/comments/ibof8j/how_to_land_a_job_in_drl_with_embedded_system/
[R] A Simulation Suite for Tackling Applied Reinforcement Learning Challenges,1597693558,"Researchers identify and discuss nine different challenges that hinder the application of current RL algorithms to applied systems. We then follow up this work with an empirical investigation in which we simulated versions of these challenges on state-of-the-art RL algorithms and benchmark the effects of each. We have open-sourced these simulated challenges in the [Real-World RL](https://github.com/google-research/realworldrl_suite) (RWRL) task suite to help draw attention to these important issues, as well as accelerate research toward solving them.

 [https://arxiv.org/abs/1904.12901](https://arxiv.org/abs/1904.12901) 

 [https://ai.googleblog.com/2020/08/a-simulation-suite-for-tackling-applied.html](https://ai.googleblog.com/2020/08/a-simulation-suite-for-tackling-applied.html)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/iblcsk/r_a_simulation_suite_for_tackling_applied/
"[Philosophy &amp; RL] An Existentialist Interpretation of Free Will, Agency, and Moral Attributability Through the Lens of Model-Based Reinforcement Learning",1597692013,"I wrote a piece drawing insights from RL and philosophy to better understand the human condition. A little different from the usual content on this sub, but I think some might enjoy it.

I'd love to hear your thoughts.

[An Existentialist Interpretation of Free Will, Agency, and Moral Attributability Through the Lens of Model-Based Reinforcement Learning](https://medium.com/@robpmcadam/an-existentialist-interpretation-of-free-will-agency-and-moral-attributability-through-the-lens-a5a39efc633d)",reinforcementlearning,bigrob929,False,/r/reinforcementlearning/comments/ibkunk/philosophy_rl_an_existentialist_interpretation_of/
Using Reinforcement Learning to Design Resilient Spacecraft Trajectories,1597679451,,reinforcementlearning,Gereshes,False,/r/reinforcementlearning/comments/ibgnx6/using_reinforcement_learning_to_design_resilient/
Neural net that plays tic tac toe,1597673322,"[https://github.com/cpita/TicTacToeAI](https://github.com/cpita/TicTacToeAI)

I coded up a neural net from scratch that plays tic tac toe, along with a theoretical explanation. What do you guys think? I'm also thinking about including it in my CV when applying for internships.",reinforcementlearning,cpitaa,False,/r/reinforcementlearning/comments/ibet4q/neural_net_that_plays_tic_tac_toe/
"What has the biggest contribution to the final ""good"" policy? It is about exploration and exploitation.",1597664939,"For reinforcement learning, ""exploration and exploitation"" is a research heat for DRL. 

Exploration is to choose actions that are not suggested by the current policy. It encourages the agent to explore unknown states.  This could potentially break the local optimal. 

Exploitation is a kind of extracting or learning knowledge from current data. For DRL, I think exploitation should be the learning part that learns from the previous data.

My question: what has the biggest contribution to the final good policy?  More straightforward, who ""finds"" the ""good"" policy? Exploration, exploitation, or both.",reinforcementlearning,alreadybetoken,False,/r/reinforcementlearning/comments/ibcmqg/what_has_the_biggest_contribution_to_the_final/
LangLfP: Grounding Language in Play | Paper Explained,1597644965,,reinforcementlearning,bitsofdpl,False,/r/reinforcementlearning/comments/ib8sf2/langlfp_grounding_language_in_play_paper_explained/
[Question] Derivation of the Bellman Equation for the value function,1597641897,"Hi all, I am going through the Rich Sutton book along with the UofAlberta Coursera course, and also referencing the DeepMind RL lectures for reference. 

I don't understand how we arrive at the final result for the value function using the Bellman equation.

Is this a property of Expectations or is it something else?

Rich Sutton Book -  [https://imgur.com/a/rZG9ZfP](https://imgur.com/a/rZG9ZfP) 

DeepMind Lecture :=  [https://www.youtube.com/watch?v=hMbxmRyDw5M&amp;list=PLqYmG7hTraZBKeNJ-JE\_eyJHZ7XgBoAyb&amp;index=4&amp;t=2668s](https://www.youtube.com/watch?v=hMbxmRyDw5M&amp;list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb&amp;index=4&amp;t=2668s)  at 22:04",reinforcementlearning,anweshb10,False,/r/reinforcementlearning/comments/ib863e/question_derivation_of_the_bellman_equation_for/
Huber loss vs mse loss in DQN,1597597601,"I am Implementing DQN on Space invaders environment. I have tried using Huber loss, MSE loss and MAE loss (Pytorch). 
I get maximum average reward (627) using MSE loss but the average loss is 48.61 at the end. 
Whereas, MAE and Huber loss gave the average reward around 500 but average loss was 1.22. 

Most of the resources suggest that Huber loss is used only with reward clipping and I have not clipped my rewards. 

Am I going wrong somewhere or the results given by the 3 loss functions are correct (specially the average loss value of 48.61)?",reinforcementlearning,mishti__doi,False,/r/reinforcementlearning/comments/iaw1gw/huber_loss_vs_mse_loss_in_dqn/
Summary and Commentary of 5 Recent Reinforcement Learning Papers,1597597008,"We will be looking at 5 reinforcement learning research papers published in relatively recent years and attempting to interpret what the papers’ contributions may mean in the grand scheme of artificial intelligence and control systems. I will be commentating on each papers and presenting my opinion on them and their possible ramifications on the field of deep reinforcement learning and its future.

&amp;#x200B;

The following papers are featured:

Bergamin Kevin, Clavet Simon, Holden Daniel, Forbes James Richard “**DReCon: Data-Driven Responsive Control of Physics-Based Characters**”. ACM Trans. Graph., 2019.  

Dewangan, Parijat. Multi-task Reinforcement Learning for shared action spaces in Robotic Systems. December, 2018 (Thesis)  Eysenbach Benjamin, Gupta Abhishek, Ibarz Julian, Levine Sergey. “**Diversity is All You Need: Learning Skills without a Reward Function**”. ICLR, 2019.  

Sharma Archit, Gu Shixiang, Levine Sergey, Kumar Vikash, Hausman Karol. “**Dynamics Aware Unsupervised Discovery of Skills**”. ICLR, 2020.  

Gupta Abhishek, Eysenbach Benjamin, Finn Chelsea, Levine Sergey. “**Unsupervised Meta-Learning for Reinforcement Learning**”. ArXiv Preprint, 2020. 

[https://youtu.be/uvCItgXHWsc](https://youtu.be/uvCItgXHWsc)

&amp;#x200B;

In addition, I put my own take on the current state of reinforcement learning in the last chapter. I honestly want to hear your thoughts on it.

Cheers!",reinforcementlearning,johnlime3301,False,/r/reinforcementlearning/comments/iavuo9/summary_and_commentary_of_5_recent_reinforcement/
What are the best features for Pong in atari?,1597592812,"Are the position of the ball and two boards sufficient? I have tried this to do the DRL and used fully connected neural networks to train a DQN agent. The results show that 

* the policy are unstable (sometimes, the policy can get 21, but in the next round it could get -21) 
* slower convergence than double DQN
* very sensitive to the hyperparameters, such as the learning rate, the number of neurons, the depth of the neural network, and others.  

The basic DQN use CNN to get the features, apparently, we don't know the exact meaning of each output of the final convolution layer.

If we think like a human, the most important features in the pong game are the position of the three objects. If we use these features as our input, we should get much more fast convergence and a more stable policy.  

My question: does this result show that rich features are good for stabilizing DRL training?",reinforcementlearning,alreadybetoken,False,/r/reinforcementlearning/comments/iaukes/what_are_the_best_features_for_pong_in_atari/
Changing the behaviour,1597591937,"I am currently working on an environment which require the agent to do some actions until a certain point and then change the actions to be able to gain a extra reward. however, the agent only learn one type of actions. If i change the discount factor it learn the actions of the second task without the first. Anybody have an idea of how to learn both actions?",reinforcementlearning,Hazem2037,False,/r/reinforcementlearning/comments/iaubao/changing_the_behaviour/
"""Meta-Learning through Hebbian Plasticity in Random Networks"", Najarro &amp; Risi 2020",1597590788,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/iatzx3/metalearning_through_hebbian_plasticity_in_random/
More gradient updates in DDPG/SAC,1597581479,"Hey everyone,

From what I've seen, it is very standard to do one update step of the networks for every environment step in DDPG and SAC. What is the reason for that?  I've played around with more updates per environment step (5-20) and it seems to perform similar with faster learning. Can anyone redirect me to resources that investigate this topic ?",reinforcementlearning,durotan97,False,/r/reinforcementlearning/comments/iarneq/more_gradient_updates_in_ddpgsac/
Batch Training TD3 agent on multiple environments in stable-baselines,1597571236,"Hello,

I want to train my agent on a batch of images for several epochs. I wrote my environment following the gym API guidelines and I'm trying to use the stable-baselines to train my model.

An environment is made of a single image in which multiple steps can be taken (maximum 10) to achieve the goal. Once the goal is achieved or the maximum number of steps are taken the episode ends and the agent has to move on to the next image/environment.

How can I get the model to train on all the images/environments for several epochs?

I have in mind to try the following:

    for epoch in number_of_epochs: for image in images:         env = make_env(image) #here the model should train on the given image for one epoch only

I'm not sure how I can do this however, since you have to specify the env upon model creation, if I do this inside the loop after creating the env that would be a new model starting from scratch every time.

Also, there is no way to specify the number of epochs in the learn  
 method.

I also tried the DummyVecEnv but when I tried it I got an error saying TD3 only accepts single environments.",reinforcementlearning,Jelmazmo96,False,/r/reinforcementlearning/comments/iapp5d/batch_training_td3_agent_on_multiple_environments/
Resources for learning to write good reward functions,1597558057,"I realize how critical reward functions are in RL. While I have found a lot of material in RL algorithms but not as much on writing good reward functions.  

Could anyone point me good resources on the topic?",reinforcementlearning,sindreu,False,/r/reinforcementlearning/comments/ianfvk/resources_for_learning_to_write_good_reward/
[Question] Excercise 2.2 from Sutton and Barto 2nd edition,1597556210,"I am going through the book while also doing the UofAlberta course on Coursera, I am stuck on exercise 2.2.

Although [this link](https://github.com/diegoalejogm/Reinforcement-Learning/tree/master/2.%20Multi-Armed%20Bandits) explains the solution in detail, I can't understand how the tuple values are calculated and how we arrive at the reasoning behind the solution. 

Can someone help me out?",reinforcementlearning,anweshb10,False,/r/reinforcementlearning/comments/ian36r/question_excercise_22_from_sutton_and_barto_2nd/
Which deepmind rl course to take?,1597520316,"There are two reinforcement learning courses on deepmind

[2015](https://www.youtube.com/playlist?list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB)

[2018](https://www.youtube.com/playlist?list=PLqYmG7hTraZBKeNJ-JE_eyJHZ7XgBoAyb)

I have heard David silver's course is good.",reinforcementlearning,28stab-wounds,False,/r/reinforcementlearning/comments/iadx39/which_deepmind_rl_course_to_take/
What are some reasons that an agent (within DQNs) gathers significantly more negative rewards than positive ones ?,1597511365,"And what are some ways that can be implemented in order to solve that? Can large rewards be one of the culprits?

Thanks !",reinforcementlearning,Ubermensch001,False,/r/reinforcementlearning/comments/iab4ze/what_are_some_reasons_that_an_agent_within_dqns/
How to modify my custom environment for using CPO?,1597476176,"I have a custom environment in which I have some safety constraints that I must satisfy. Now I am using PPO and SAC and incorporate these constraints as a penalty in the reward. I would like to try CPO, but from the implementations that I have found it is not clear what modifications I have to do in my environment in order to be able to train it with CPO. Can anyone provide any examples?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/ia35sz/how_to_modify_my_custom_environment_for_using_cpo/
"Interview with Matt Botvinick (Neuroscience, Psychology, and AI at DeepMind), Lex Fridman, 3 July 2020",1597435493,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i9t6xo/interview_with_matt_botvinick_neuroscience/
StableBaselines3 - Custom baseline heuristic as benchmark in custom environment,1597421123,"Hi, I want to compare my RL agent with simple baseline heuristics in a custom environment. What is the best way to integrate simple baselines with stable\_baselines3? The heuristics should make greedy decision with respect to the current environment, thus needs access to it. I want to integrate the heuristics with stable\_baselines3 so that already implemented logging functionalities (custom monitor) are automatically provided. My best guess so far is a long the lines of  

* Inheriting from the class OnPolicyAlgorithm
* Leave the train() function empty (heuristic does not require training)
* Implement the action selection (maybe via the predict() method of the BasePolicy?)

What would be the best way to achieve this integration? Thanks for your help!",reinforcementlearning,overly-curious-duck,False,/r/reinforcementlearning/comments/i9opgi/stablebaselines3_custom_baseline_heuristic_as/
Current RL libraries lack one or another thing in different aspects. Can i expect a team from this subreddit to build a versatile RL library from scratch?,1597417993,"Kindly forgive me if this post doesn't belong here.

I have worked with several RL libraries including tf-agents, PTAN, keras-rl but encountered many issues and sometimes made me do minor changes into their code eventually turning into whole mess.

As RL is relatively a new emerging field and have no base versatile library. I want to build a new RL library along with a team to enable fast and easy experimentation.

&amp;#x200B;

Q: Now why do i expect a team?

A: Building a strong baseline with a single mind will narrow down the impact, ending up being biased as the current scenario of RL libraries. whereas a team can broaden the impact.

&amp;#x200B;

If this seems a nice idea, you can comment if you are in or can DM me if don't wanna comment :). If the number of the volunteers increases 10, we are doing the project.

Thanks.",reinforcementlearning,spenceowen,False,/r/reinforcementlearning/comments/i9nqpo/current_rl_libraries_lack_one_or_another_thing_in/
IQN and Extensions,1597397080,"Hey, 

I created a repository where I implemented Implicit Quantile Networks (IQN) and combined it with several extensions of DQN.

[https://github.com/BY571/IQN-and-Extensions](https://github.com/BY571/IQN-and-Extensions)

Maybe it helps someone, feel free to give some feedback.",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/i9ito7/iqn_and_extensions/
Pytorch implementation for Procgen benchmark,1597394697,"# Hello everyone,

Recently released ProcGen environment is a wonderful benchmark to measure the sample-efficiency and generalization ability for RL algorithms. However, it was not easy for me to try several ideas on official repository since it is implemented with highly modularized tensorflow code.

I tried to reproduce the result in pytorch trying to match the code from [OpenAI baseline](https://github.com/openai/baselines) as close as possible while following the coding style from famous pytorch rl implementation from [ikostrikov](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail).

Please have a look at it [here](https://github.com/joonleesky/train-procgen-pytorch)! Hope it helps someone!",reinforcementlearning,joonleesky,False,/r/reinforcementlearning/comments/i9ieir/pytorch_implementation_for_procgen_benchmark/
Proximal policy optimisation with only terminal rewards,1597392448,"I have a custom environment which gives a positive reward when the agent, embodied with a Dubins car model, reaches the time-limit of the episode, otherwise the episode terminates without any reward if the agent goes out of bounds. The magnitude of this reward is based on how good the agent's trajectory was. The goodness of the trajectory is, in my case, how much the sensor readings along the trajectory have reduced the uncertainty of the world. Due to technical limitations, this reward is only accessible at the end of the episode.

Let's say the time-limit corresponds to 200 steps. Should the number of steps per policy update `n_steps` then be 200? Undoubtedly early episodes will not garner any rewards, as the agent will frequently go out of bounds in the early stages of learning. How can I ameliorate this issue?

I thought about adding a small reward at each time-step to encourage the agent to reach the time-limit, thus staying in-bounds and obtaining the terminal reward. But, how will the policy updates behave in this case? It is the sequence of actions that affect the terminal reward, and I need the agent to learn this.",reinforcementlearning,NomadicAstronaut,False,/r/reinforcementlearning/comments/i9i0bf/proximal_policy_optimisation_with_only_terminal/
Game theory tutorial for multi-agent reinforcement learning,1597390713,"DeepMind is focusing on applying and combining Game Theory (GT) in MARL, so for beginners, are there any useful materials to get started in GT?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/i9hp1q/game_theory_tutorial_for_multiagent_reinforcement/
How to get a tensorboard class to work with a simple DQN algorithm.,1597390525,"For RL I have read that tensorboard isn't ideal since it gives the input of per episode and/or step. Since in RL there are thousands of steps, it doesn't give us an overview of the content. I saw this modified tensorboard class here: https://pythonprogramming.net/deep-q-learning-dqn-reinforcement-learning-python-tutorial/ 

the class:  

    class ModifiedTensorBoard(TensorBoard):
        # Overriding init to set initial step and writer (we want one log file for all .fit() calls)
        def __init__(self, name, **kwargs):
            super().__init__(**kwargs)
            self.step = 1
            self.writer = tf.summary.create_file_writer(self.log_dir)
            self._log_write_dir = os.path.join(self.log_dir, name)

        # Overriding this method to stop creating default log writer
        def set_model(self, model):
            pass

        # Overrided, saves logs with our step number
        # (otherwise every .fit() will start writing from 0th step)
        def on_epoch_end(self, epoch, logs=None):
            self.update_stats(**logs)

        # Overrided
        # We train for one batch only, no need to save anything at epoch end
        def on_batch_end(self, batch, logs=None):
            pass

        # Overrided, so won't close writer
        def on_train_end(self, _):
            pass

        def on_train_batch_end(self, batch, logs=None):
            pass

        # Custom method for saving own metrics
        # Creates writer, writes custom metrics and closes writer
        def update_stats(self, **stats):
            self._write_logs(stats, self.step)

        def _write_logs(self, logs, index):
            with self.writer.as_default():
                for name, value in logs.items():
                    tf.summary.scalar(name, value, step=index)
                    self.step += 1
                    self.writer.flush()


and I would like to make it work with this layer:

    n_actions = env.action_space.n
    input_dim = env.observation_space.n
    model = tf.keras.Sequential()
    model.add(tf.keras.layers.Dense(20, input_dim = input_dim , activation = 'relu'))#32
    model.add(tf.keras.layers.Dense(10, activation = 'relu'))#10
    model.add(tf.keras.layers.Dense(n_actions, activation = 'linear'))
    model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'mse')
    model.fit(x = np.array(X).reshape(-1, *env.ENVIRONMENT_SHAPE),
                y = np.array(y),
                batch_size = MINIBATCH_SIZE, verbose = 0,
                shuffle=False, callbacks=[self.tensorboard] if terminal_state else None)


But I have yet to get it to work. Anyone who has worked with tensorboard before, do you know how to setup this up? Any insight is greatly appreciated.",reinforcementlearning,math7878,False,/r/reinforcementlearning/comments/i9hnql/how_to_get_a_tensorboard_class_to_work_with_a/
"""A multi agent perspective to AI,"" by Anuj Mahajan of University of Oxford",1597384189,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/i9ggb7/a_multi_agent_perspective_to_ai_by_anuj_mahajan/
Latent State Recovery in Reinforcement Learning - John Langford,1597383997,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/i9gexl/latent_state_recovery_in_reinforcement_learning/
"how can ""deterministic policy gradient"" algorithm able to work in the control field?",1597331222,"in the control field, actions and states distribute in continuous space, rather than discrete action space or state space in general reinforcement learning problem like in atrial games. I have spotted a few research papers in the control field mentioned the usage of deterministic policy gradient in continuous action space, which solve the control problem regardless of the system model. I want to solve some problems in continuous space, but I just start learning RL, hoping suggestions for the entry point of more info about how DPG and more algorithm function in continuous space problems.",reinforcementlearning,Benderpan,False,/r/reinforcementlearning/comments/i91jsd/how_can_deterministic_policy_gradient_algorithm/
Best RL algorithm that performs the best for Gym Retro OVERALL?,1597328307,,reinforcementlearning,hlsafin,False,/r/reinforcementlearning/comments/i90psh/best_rl_algorithm_that_performs_the_best_for_gym/
Self play confusion,1597325748,"So I have this very dumb question about self play in RL. As we know most of the problems solved today in RL do so by training a policy which plays against itself for a large amount of steps. So my question basically is when people implement self play, do they train 2 networks separately competing against each other. Or is it the same policy being trained based on the experiences from each side.",reinforcementlearning,alikhan97,False,/r/reinforcementlearning/comments/i900p3/self_play_confusion/
Connecting a custom OpenAI Gym ENV from Pygame using Stable-Baselines.,1597321633,"I wanted to create a simple way to hook up some custom Pygame environments to test out different stable algorithms. I found the quickest way was to use StableBaselines custom ENV setup. The GitHub is below I'll give a little breakdown for some extra help.

&amp;#x200B;

* SpaceEnv contains all the code for the ENV and would be where you tweak reward functions and that sort of thing.
* Train-DQN / Train-PPO will show you how to actually hook up and start training your agents. 
* Run\_env\_DQN / Run\_env\_PPO are examples of running your trained models.
* Run\_env\_random is a file to run your ENV with totally random actions to set a baseline before training models.
* SRC contains the classes and such for the game itself. 

&amp;#x200B;

[Github!](https://github.com/ClarityCoders/SpaceShooter-Gym)",reinforcementlearning,claritycoders,False,/r/reinforcementlearning/comments/i8yzp8/connecting_a_custom_openai_gym_env_from_pygame/
Young children would rather explore than get rewards,1597319657,,reinforcementlearning,DollyNorman,False,/r/reinforcementlearning/comments/i8yj8h/young_children_would_rather_explore_than_get/
DRL: Policy gradients,1597317210,"Hi!

I have some experience in DL, but almost none in DRL, and have a question about policy gradients.

As a toy project, I am trying to implement simple trading bot for stock prices (single stock). I know that a profit is possible, because I managed to find a profitable policy with neuro-evolution. Now I want to use same architecture but optimize it with policy gradients (goal is to compare the two, overfit is not an issue right now). 

The issue is, I have trouble applying reward to the policy gradients, I am not sure that I understand the concept correctly. To the problem:

\- Minute stock data for 28 days, which means around 40K instances.

\- I apply forward pass and save policy probabilities in a holder, at the end I calculate profit, lets say r=1.05.

\- Here am i lost hot to calculate gradients. do I:

a) Multiply whole policy holder with r (while rewarding just the taken action) and calculate ""tf.math.reduce\_mean()"" or sample from policy holder and do the same? What is the best practice?

b) Do I mask the policy prob. of non-taken action to the 0?

c) Do I transform the reward to something like (1.05 -1) \* 100 to emphasize reward?

d) Should I take into the account discount factor in some way?

I would appreciate any insight, thanks!",reinforcementlearning,mlord99,False,/r/reinforcementlearning/comments/i8y0na/drl_policy_gradients/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1597296975,,reinforcementlearning,BitsOfDeepL,False,/r/reinforcementlearning/comments/i8u85j/simple_learning_to_play_atari_with_only_2_hours/
Connections of DRL and traditional Q-learning,1597286686,"Hi,

It always seems to me how different two streams of RL research direction are: one is more traditional and focuses on the time complexity with respect to |S|, |A|, etc; another is the more recent DRL, using DQN, PPO, etc. Do the two streams have some connections nowadays? Or they are becoming more and more different?

Thanks.",reinforcementlearning,Cohencohen789,False,/r/reinforcementlearning/comments/i8rwa0/connections_of_drl_and_traditional_qlearning/
Reset Function RL,1597265785,"Hi everyone,

Just a quick question about the Reset Function on a basic RL environment: what should be in it and what should not?

I explain: I have an environment simulating my system and an agent (my controller). My controller doesnt not excite the same code if this is the first iteration or not. So my idea was to generate the first set of control input and set by the reset function. However, I don’t like the idea of having the first step made by the reset function when the step function is here for it.

Any recommendations are welcome,

Thank you!",reinforcementlearning,Doubydoubs,False,/r/reinforcementlearning/comments/i8m2a5/reset_function_rl/
Companies/Labs to do a RL-related master thesis,1597259485,"Hi! I'm doing a masters in Machine Learning and I'm very interested in Reinforcement Learning (took a couple of courses and did some research &amp; experiments of my own).

My university allows me to do the master's final thesis wherever I want and I would like to join some lab or company and work on a RL research project.

Do you guys know of any place which takes students to work on a masters thesis? (Ideally research-oriented) Not sure if that is a very common thing....",reinforcementlearning,OleguerCanal,False,/r/reinforcementlearning/comments/i8k17j/companieslabs_to_do_a_rlrelated_master_thesis/
Counterfactual regret minimisation to solve cribbage?,1597253654,"Is this the right strategy?

To give more context, if you reach an end game situation in cribbage (the goal is 121 points), then the game becomes simpler, as suits become irrelevant. 

The structure is as follows:

* players dealt 6 cards each, discard 2, face down
* card is cut, if it's a Jack 2 points awarded to dealer, otherwise nothing. If either player reaches 121 points, game is over immediately.
* players play their cards in turn, starting with non-dealer, aiming to score points. Points scored for pairs (2 points), 3oak (6 points), 4oak (12 points), runs of 3 or more (3/4/5/6/7/8 points), reaching a count of 15, where A = 1, TJQK = 10 (2 points), reaching count of 31 (2 points), and playing last card either before 31 points or last card overall (1 point)

Since non-dealer plays first, dealer always scores a minimum of 1 point, and hence if dealer is 120 points, the game will always end during this process. This simplifies the game.

For example, let's say the score is 119-120. Non-dealer is dealt 34456K, and dealer is dealt A278JQ.

Let's say non-dealer holds 3456, and dealer holds A278.  Then a 3 is cut. Now non-dealer knows about the whereabouts of 2 3s, and 2 4s. So either lead is possible since the only score possible is by a pair, which is less likely. A 5 would be a bad lead, because if dealer holds a 5TJQ or K, he scores 2 points and wins. It's to be assumed that the 6 is a bad lead because a 6 can be paired or reach 15 (with a 9 played), but we'd want to prove this, and also perhaps examine whether the ideal play is exploitable, e.g., if there is a very strong bias to hold cards A234. 

Anyway, in general in cribbage the players play in turn to reach a count of 31, or if they don't reach exactly 31 then the last player to play a card plays again until he can't play any more without exceeding 31. After this, the count restarts from 0. Having to lead from 0 is generally bad because the first card has no way to score.

In general:

* there are 18!/6!/12! ways to choose 6 cards from 13 ranks, ignoring suits and allowing repetitions. A small number of these won't be possible (5 or more of the same suit). This is roughly 18,564 possible 6 card hands, per player
* There are 6!/4!/2! = 15 ways for each player to discard his cards, although some of these will work out the same if we ignore suits. E.g., if we have JJJQQQ then there are in fact only 3 possible hands.
* If we consider non-dealer has 4 cards to choose from, then dealer has 4 cards, then non-dealer has 3 etc., that gives 4!*4! = 576 ways to play the pegging game. Although some of these will be illegal. For example if dealer holds J53Q, and the play has gone KJ6, then dealer cannot play his Q next - he can only legally play his 5 or 3. So the actual number of ways to play may be less than 576.

Clearly in general, non-dealer knows from his 6 cards, which cards remain, so for example, if he has 4 5s, there can't be any left in the deck. And once discarding to 4 cards, both players know about the 2 cards they have discarded and the cut. And here, there's no logical distinction between me being dealt 2346JK, discarding the JK and then the 7 being cut, vs. me holding 23467J, discarding the 7J, and then the K  being cut. Both positions are equivalent. The cards in the other player's hand are unknown, but as they play their cards, then the makeup of their hand will become obvious, since for example if non-dealer leads a 6 at a dealer score of 120, and the dealer replies with a K, then dealer can't possibly have a 6 or a 9, because otherwise he would have played them and won the game. 

I've included some code here:

            var dealerHands = GetCombinationsWithRepetitions&lt;byte&gt;(ranks, 6);
            var playerHands = GetCombinationsWithRepetitions&lt;byte&gt;(ranks, 6);
            long totalCombos = 0;
            long validCombos = 0;
            foreach (var dealerHand in dealerHands)
            {
                // convert to array because if we leave it as an IEnumerable it creates a big performance overhead. Maybe this can be refactored later.
                var dh = dealerHand.ToArray();
                foreach (var playerHand in playerHands)
                {
                    var ph = playerHand.ToArray();
                    foreach (var cut in ranks)
                    {
                        bool dealerWins;
                        totalCombos++;
                        long weight = this.CountWeight(dh, ph, cut);
                        if (weight &gt; 0)
                        {
                            validCombos++;
                            if (cut == RankJack)
                            {
                                // we're using dealer score of 120, so dealer wins any time a Jack is cut
                                dealerWins = true;
                            }
                            else
                            {
                                bool illegal;
                                //select 4 cards from 6 = 15
                                var dh4c = GetCombinations&lt;byte&gt;(dh, 4);
                                var ph4c = GetCombinations&lt;byte&gt;(ph, 4);
                                // information that we have
                                // each player has 4 cards, which are a prime number and can be hashed
                                // each player has discarded two cards (see above), and there is a cut. These three cards are also a hash
                                // for the first three cards pone plays, that modifies dealer's information set to make his choices
                                // for the first two cards that dealer plays, pone's information and choices are modified 
                                // in total for any given 4 card hand 
                                foreach (var dh4 in dh4c)
                                {
                                    // 4! card choices each
                                    var dh4P = Cribbage.Permutate&lt;byte&gt;(dh4.ToArray()).ToArray();
                                    foreach (var ph4 in ph4c)
                                    {
                                        var ph4P = Cribbage.Permutate&lt;byte&gt;(ph4.ToArray()).ToArray();
                                        foreach (var d4 in dh4P)
                                        {
                                            foreach (var p4 in ph4P)
                                            {
                                                // next line does a full round of pegging for an ordered list of 2 sets of 4 cards. If either player has won, it will return true (for dealer) or false (for pone) immediately
                                                dealerWins = this.DoPegging(p4.ToArray&lt;byte&gt;(), d4.ToArray&lt;byte&gt;(), 118, 120, out illegal);
                                                if (illegal) continue;
                                            }
                                        }
                                    }
                                }

                            }

                        }

                    }
                }
            }


I've not yet made any attempt to consolidate the code so that it treats identical positions as the same. 

And it's not yet doing any learning, it just iterates through 44,663,023,641,600 positions. But many of these are duplicate.

I'm wondering if I'm going about this the right way, in permutating over the 576 ways to each play 4 cards, and treating each arrangement as a unique decision, or if I need to instead iterate through choices for card 1, card 2, etc.",reinforcementlearning,thelawnet,False,/r/reinforcementlearning/comments/i8i52a/counterfactual_regret_minimisation_to_solve/
Should I just give up on RL?,1597252504,"**tldr: I'm looking for a job in RL. After \~1y of unemployment I'm starting to loose motivation.**

Let's start with some r/reinforcementlearning quotes:

&gt;""RL jobs barely exist for people *with* PhDs at the moment."" ([Jobs in RL without a Phd](https://www.reddit.com/r/reinforcementlearning/comments/avi7yu/jobs_in_rl_without_a_phd/ehg1eyb/))  
&gt;  
&gt;""we are not there yet on using it to actually create value for society."" ([Undergraduates and RL](https://www.reddit.com/r/reinforcementlearning/comments/i8922u/undergraduates_and_reinforcement_learning/g179loc/))

**Are there any** ***junior*** **job in RL? Will the market change  even a bit in the coming years?** 

I finished a Master's in AI in last year with a focus in RL in France. Since then, I've applied to Nokia, Big Cloud, Siemens, Livermore National Laboratory, Microsoft, Thales, Dassault, InstaDeep, Parrot, Harnham, Center for Human Compatible AI and Causa Lens.

The thing is, I was **directly** rejected for all my job applications, except for one where I got 3 interviews &amp; code challenge but still rejected and two still didn't answer.

Things I've done apart from my Master's degree:  
\- interned in RL at a top university (Oxford)--didn't get much results &amp; didn't publish.  
\- reproduced all of the plots from Sutton &amp; Barto + did all the exercises.  
\- reproduced two experiments from a DeepMind paper.  
\- read in depth \~6 papers from spinning up + reimplemented REINFORCE (doing DQN attm). Plan is to read one a day and re-implement spinning up algorithms for the coming month.  
\- worked on the NeurIPS Procgen competition, reaching  \~top 10 so far.

Yet, RL job postings ask for crazy credientals like: 1) PhD 2) top conf. publication 3) 1-3y of experience.

Most importantly, my Master's degree isn't from a top school and my grades are so bad that I don't even put my GPA on my CV, which seems to be the main reason why I don't pass the CV screening.

So, right now, **I invested a lot of time in understanding RL,** but there are just so few jobs outside academia and so much competition (people finishing their PhDs in RL, experienced people, people from top school, people with papers from top conference, etc.) that maybe **I should just give up, find a random ML or Python Job, and wait a few years?** 

However, **if you** ***do*** **have job for me in RL,** or if you have any experience/private feedback to share, **do shoot me an email** at [**finding\_a\_job\_in\_rl@protonmail.com**](mailto:finding_a_job_in_rl@protonmail.com)**.**

Otherwise, feel free to share your experiences / advices below!",reinforcementlearning,MuskFeynman,False,/r/reinforcementlearning/comments/i8hrt9/should_i_just_give_up_on_rl/
Negative actor loss PPO algorithm | Agent unable to learn,1597245970,"Hi everyone, I have implemented a PPO algorithm where both the actor and critic losses are decreased over time, but the agent seems to be acting randomly and learn nothing. Also, as you can see from the attached image, the actor loss goes below zero, which I try to explain logically but I still dont get it. 

Thank you in advance!",reinforcementlearning,k_ili,False,/r/reinforcementlearning/comments/i8fq2n/negative_actor_loss_ppo_algorithm_agent_unable_to/
Question with the DDPG algorithm on a specific problem,1597244666,"Hi there.  I was using the DDPG(Deep Deterministic Policy Gradient) algorithm and tried to solve a robot navigation problem. The training process was done in the simulator. A major challenge is that the simulator was built in a different programming language(C++) so I saved the model in the python script and loaded it in C++, which means in one episode, the model won't be updated. Learning will be executed in the python script when one episode was over and the \[pre\_state, action, reward, state\] have been written into the file.  The issue is that it doesn't really optimize it. Instead, as learning went, the robot tends to stay in the original position. I set the reward to be the negative distance between current and goal position. Here is the training curve, x as the episode, y as the average reward. The expected result is that it was converged to somewhere like -30. Could you guys give me some advice in solving these issues? either from model adjustment or hyper-parameter tunning or different Deep RL method. I will appreciate any help from you so much! Thanks.

&amp;#x200B;

https://preview.redd.it/kn0r52yg6lg51.png?width=392&amp;format=png&amp;auto=webp&amp;s=bb9a0439428cbcaa9222beeeaf1d7b1db729d460",reinforcementlearning,runorz,False,/r/reinforcementlearning/comments/i8fc12/question_with_the_ddpg_algorithm_on_a_specific/
"Informal article about ""communicative autostimulation for the emergence of better autocurricula""",1597242844,,reinforcementlearning,drcopus,False,/r/reinforcementlearning/comments/i8et3p/informal_article_about_communicative/
[R] Deep RL for Tactile Robotics: Learning to Type on a Braille Keyboard,1597236575,"Abstract:  In this paper, researchers propose a new environment and set of tasks to encourage the development of tactile reinforcement learning: learning to type on a braille keyboard. 

Four tasks are proposed, progressing in difficulty from arrow to alphabet keys and from discrete to continuous actions. A simulated counterpart is also constructed by sampling tactile data from the physical environment. Using state-of-the-art deep RL algorithms, they show that all of these tasks can be successfully learned in simulation, and 3 out of 4 tasks can be learned on the real robot. A lack of sample efficiency currently makes the continuous alphabet task impractical on the robot. 

According to the research, this work presents the first demonstration of successfully training deep RL agents in the real world using observations that exclusively consist of tactile images. To aid future research utilizing this environment, the code for this project has been released along with designs of the braille keycaps for 3D printing and a guide for recreating the experiments. 

Paper link:  [https://arxiv.org/abs/2008.02646v1](https://arxiv.org/abs/2008.02646v1) 

A brief video summary:  [https://www.youtube.com/watch?v=eNylCA2uE\_E&amp;feature=youtu.be](https://www.youtube.com/watch?v=eNylCA2uE_E&amp;feature=youtu.be)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/i8d570/r_deep_rl_for_tactile_robotics_learning_to_type/
Undergraduates and Reinforcement Learning,1597215427,Do companies and research labs working on reinforcement learning hire undergraduates as interns or employees?,reinforcementlearning,TheNush07,False,/r/reinforcementlearning/comments/i8922u/undergraduates_and_reinforcement_learning/
Help about Deep Meta-RL,1597166786,"Hey folks. 

I'm currently working on a project but I need some help because I find the papers a little to general about the way to implement it.

The goal of my project is to make agents share radio frequencies without interfering each other but also the primary users. Each frequency is assigned to a primary user and he got a probability to be active on it. The problem for now is close to a MAB.

From now I've implemented a DQN with a LSTM, because the problem is discrete and it is what I know best. But first of all I was wondering about implementing other algorithms, if it's worth or not. I was thinking about SAC and A2C.

But my main problem is about Meta-RL, because I try to make my agents learn on a distribution of MDPs, which is here a distribution of different probabilities of activity for the primary users.

I know it has to work because it is basically what they tested in the papers, but I'm unsure about the way to implement it.

\- Does having a end flag is relevant ? Because episode doesn't make sense in my environment

\- The input is made of the state at t and action and reward at t-1, does stacking this into a vector is ok to feed the LSTM ? Is it the same for feeding the Q target (next\_state, action and reward at t)

\- Is it supposed to converge fast or more slowy ? I only trained for max 600k steps (with trials length 100, so 6000 trials)

I'm looking for any help towards the implementation, I'm really unsure about certain things. I'm sorry if it appears to be dumb, but I lack of intuition here.",reinforcementlearning,Krokodeale,False,/r/reinforcementlearning/comments/i7vyqe/help_about_deep_metarl/
Compute action log probability for surrogate loss in PPO,1597162845,"Hello guys, hope you have a good day.

After reviewing PPO implementations on Github, I see the action log prob is calculated differently by others.  Somes calculate directly from 2 probability outputted by old and recent policy (compare 2 vectors). Somes just calculate the log prob w.r.t the action taken in the past (compare 2 scalars).

So which is the best way to evaluate the action log probability used for updating PPO?

Thank you in advanced",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/i7uq6m/compute_action_log_probability_for_surrogate_loss/
Old policy and new policy in PPO,1597159759,"Hello guys, hope you have a good day.

I try to reimplement PPO from scratch and get confused by the actor loss. To update the actor using the surrogate loss we need the ""old policy action probability"" and ""new policy action probability"". 

According to some github implementations (i.e. [https://github.com/keiohta/tf2rl](https://github.com/keiohta/tf2rl)) I see no target network, so how could I get the ""old policy action probability"" in the right way?

Thank you in advance!",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/i7ts98/old_policy_and_new_policy_in_ppo/
Increased average episode length in SAC,1597154396,"Hi,

I'm running a SAC agent and on some runs the average episode lengths increases dramatically towards the end of the run. There is also a decrease in the critic loss, increase in actor loss and a jump in the alpha loss values at about the same time step. I've attached a link to an imgur post with graphs for these values. 

I would like to better understand how the change in the losses affects the average episode lengths. From what I understand, the average episode length is determined by the policy, which is in turn determined by the actor network. A higher value in the actor loss would indicate a worse optimization of this network, and consequently a worse policy. However, I cannot understand why there is a decrease in the critic loss value, or a large jump in the alpha loss value.

Any ideas how these loss values are related and how I can better explain the increase in average episode length based on them?

Thank you!",reinforcementlearning,andreivadan,False,/r/reinforcementlearning/comments/i7sbob/increased_average_episode_length_in_sac/
Worst chess player,1597134037,"Hi guys!
Today in the morning I've been thinking about worst possible chess agent. The strategy would be simple, postivie rewards for losing a game as quickly as possible, and negative rewards for winning a game. Does anybody have a clue how might the agent's strategy look like?

Thanks :)",reinforcementlearning,Bearnardd,False,/r/reinforcementlearning/comments/i7nxqo/worst_chess_player/
What Is Better? Experience Replay vs Parametric Dynamic Model – Reinforcement Learning,1597126996,,reinforcementlearning,BitsOfDeepL,False,/r/reinforcementlearning/comments/i7mlti/what_is_better_experience_replay_vs_parametric/
Count-Based Exploration for Tabular Setting,1597122238,"Hi, does anyone know any papers about Count-Based Exploration for Tabular Setting?

Thanks",reinforcementlearning,AiLearnerXYF,False,/r/reinforcementlearning/comments/i7llmf/countbased_exploration_for_tabular_setting/
How Many Random Seeds?,1597094908,"Training a PPO agent and when I test the trained agent, I get a much different reward than when I test it (typically much less). I noticed by tuning the seeds and allowing for non-deterministic action selection, I get closer to the reward I expect. How can you tell how many seeds to use so that I can get the max training reward found to match the test reward?",reinforcementlearning,big-waves-r-us,False,/r/reinforcementlearning/comments/i7e2nn/how_many_random_seeds/
Implementation of Hierarchical Proximal Policy Optimization (HiPPO)?,1597061288,"I've been digging around trying to find an implementation of this algorithm on GitHub.  No luck.  Anyone know where I could find one?  I don't need it in any particular language, library, or toolkit.",reinforcementlearning,icebrgr,False,/r/reinforcementlearning/comments/i73m6c/implementation_of_hierarchical_proximal_policy/
High variance in episode return with SAC,1597052694,"Hey everyone,

&amp;#x200B;

I have implemented Soft Actor-Critic in Pytorch and it generally seems to work, but it also has some stability issues. Is there something wrong about my code or is this a known problem in SAC?

&amp;#x200B;

For reference, I am testing it on InvertedPendulumBulletEnv-0 and the episode return curves are bouncing around quite a bit, even though quite a lot of episodes return the max reward. 

&amp;#x200B;

Here are the curves for 5 different seeds : [https://imgur.com/a/IpmgHMH](https://imgur.com/a/IpmgHMH) 

Code for reference: [https://github.com/Tibert97/SAC](https://github.com/Tibert97/SAC)

&amp;#x200B;

Thanks a lot,",reinforcementlearning,durotan97,False,/r/reinforcementlearning/comments/i71uns/high_variance_in_episode_return_with_sac/
"""HAL: Language as an Abstraction for Hierarchical Deep Reinforcement Learning"", Jiang et al 2019 {G}",1597029035,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i6x2y3/hal_language_as_an_abstraction_for_hierarchical/
BC or IL as policy pretraining,1597010843,"Hello,

I pretrain a policy using BC with expert trajectories. From there I want to continue training my pretrained policy using PPO. The problem is when I start learning from the environment, after a couple timesteps my policy gets destroyed even if the value function is pretrained as well. Do you know any papers that could help me with that? Like expert trajectory distillation or something?  


Thank you in advance",reinforcementlearning,olivierp9,False,/r/reinforcementlearning/comments/i6s96j/bc_or_il_as_policy_pretraining/
Confusion when reading some papers,1597008156,"Hi,

I found some papers more practical while some papers are more theoretical with tons of theorems and proofs. I'm just wondering how they think of these theorems, how they know what to propose (these theorems really seem too many for me). Is there some set of common procedures to begin with? Thanks!",reinforcementlearning,Cohencohen789,False,/r/reinforcementlearning/comments/i6rglm/confusion_when_reading_some_papers/
BADGER: Learning to (Learn [Learning Algorithms] through Multi-Agent Communication),1597007112,,reinforcementlearning,lninreal,False,/r/reinforcementlearning/comments/i6r5yc/badger_learning_to_learn_learning_algorithms/
What are some Hierarchical RL algorithms?,1596999917,"I've found papers talking about MAXQ, PHAMs, and HAMs, but it's been difficult to pinpoint which are considered hierarchical algorithms.  There are many other algorithms such as MADQN and MADDPG which are multi-agent but I do not believe are hierarchical.  What are the common algorithms implemented for hierarchical reinforcement learning?",reinforcementlearning,icebrgr,False,/r/reinforcementlearning/comments/i6p04i/what_are_some_hierarchical_rl_algorithms/
How to make on-policy work for flappy bird?,1596985865,"Hi everyone. My Env is https://github.com/floodsung/Gym-Flappy-Bir d. The  action are do nothing and jump. Rewards are 1 for passing through, 0.1 for doing nothing and -1 for crashing. 

I want on policy with buffer and only one worker. I'm using ray library and tried the default config but with one worker. I used impala, appo and soft critic. None of these improved from start. PPO and dqn worked quite well. Any idea how to tune impala or any of the others for this env?",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/i6kwnl/how_to_make_onpolicy_work_for_flappy_bird/
How can I use on policy for flappy bird?,1596985008,"Hi everyone. My is Env https://github.com/floodsung/Gym-Flappy-Bird. The  action are do nothing and jump. Rewards are 1 for passing through, 0.1 for doing nothing and -1 for crashing. 

I want on policy with buffer and only one worker. I'm using ray library and tried the default config but with one worker. I used impala, appo and soft critic. None of these improved from start. PPO and dqn worked quite well. Any idea how to tune impala and the others for this env?",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/i6ko85/how_can_i_use_on_policy_for_flappy_bird/
"I need some book recommendations for Someone new to the RL and Deep RL community, and for beginners",1596960469,,reinforcementlearning,chukwudi23,False,/r/reinforcementlearning/comments/i6fsq1/i_need_some_book_recommendations_for_someone_new/
Question about the inverse model in ICM,1596950055,"Hi, I am reading the paper ""Curiosity-driven Exploration by Self-supervised Prediction"". I noticed  we only train the inverse model but dont use it for generating intrinsic reward. It seems redundant to me. My guess about the  inverse model is that it is used for producing a better forward model that understands the environment and lead to generate more appropriate intrinsic rewards. Does anyone have a deeper understanding and better idea about using the inverse model?

Thanks.",reinforcementlearning,AiLearnerXyf1,False,/r/reinforcementlearning/comments/i6dxjs/question_about_the_inverse_model_in_icm/
PettingZoo- a Gym-like library of multi-agent environments- just released version 1.0,1596942143,,reinforcementlearning,justinkterry,False,/r/reinforcementlearning/comments/i6c8fe/pettingzoo_a_gymlike_library_of_multiagent/
[sim2real] Quantifying the Reality Gap in Robotic Manipulation Tasks,1596929118,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/i693e9/sim2real_quantifying_the_reality_gap_in_robotic/
[sim2real] Traversing the Reality Gap via Simulator Tuning,1596929028,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/i692lm/sim2real_traversing_the_reality_gap_via_simulator/
Complexity for Rainbow,1596912664,"Hello,

Some algorithms focus on dealing with sample efficiency and beating the human score with the Rainbow framework currently being SOTA. (please correct me if I'm wrong)

While the Rainbow framework does perform better (for comparison, Rainbow DQN vs Vanilla DQN), I wonder if it can be used for time critical tasks. Is it computationally very expensive? Is there a graph or some other metric I could refer to for its space/time complexity?

In a time-critical, high-consequence, learning-on-the-go scenario, wouldn't a delay of a second caused by Rainbow (or distributional RL) render it dangerous or make it less preferred?

Are there existing methods that are more suited to such a scenario? Any active, ongoing research?

Would love to know more about this. Thanks.",reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/i64i4x/complexity_for_rainbow/
Q loss question (SAC),1596904006,"Hello. I am trying to train an agent using SAC algorithm, with double Q function to stabilize and speed up training. My question rises when loss graph for those two functions is drasticly different when trained in non parallel maner. Altho, when trained using Ray, where 1 thread is used for updating network, and others are just memory data collectora, loss graph/values for both Q networks is identical. Learning algorithm is the same for both implementations.
What could possibly lead to such behavior?",reinforcementlearning,Dexter_fixxor,False,/r/reinforcementlearning/comments/i61y06/q_loss_question_sac/
"""Neural Architecture Search', Lilian Weng 202 review",1596900212,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i60vpe/neural_architecture_search_lilian_weng_202_review/
Actions overfitting,1596875977,"Hi everyone.

I'm training a DQN model with an action space of 6 actions. My problem is somehow, every train runs the model prioritizes learning a group of actions (usually a group of 2-3) while neglecting the rest.  
Here's an example:  
You can see it focus on the accuracy of action #0 and #5

https://preview.redd.it/bfi2gvvaqqf51.png?width=1356&amp;format=png&amp;auto=webp&amp;s=6abc0da588e9b3e3942cb7d0b19cbcbdf768ae1f

So my question is how can I deal with this issue?",reinforcementlearning,Takatomi_Fubuki,False,/r/reinforcementlearning/comments/i5vvfg/actions_overfitting/
TD3 not converging for Bipedal Hardcore?,1596850185,"Hey there,

So recently I implemented TD3 for BipedalWalker in openAI gym and the model converged in less than 500 episodes ([https://github.com/QasimWani/policy-value-methods](https://github.com/QasimWani/policy-value-methods), see ReadMe). However, when training on the hardcore version, it doesn't seem anywhere close to convergence. I trained it for over 2K episodes (over 16 hours GPU) and it's no where close convergence.

Here's one of the early policy it learned (see gif below):

[TD3 BipedalWalker easy model weights transferred to hardcore version](https://i.redd.it/3hn1hmjglof51.gif)

&amp;#x200B;

However, the latest policies involve the agent just staying still and doing nothing. this is because it figured that actually moving and falling costs more than just staying still and doing nothing. I want to make this as model free as possible. Is TD3 not going to work for it? If so, how can I fix it? If not, why?

I haven't found many solutions that actually work on the hardcore version. Would appreciate any help/resources for training the hardcore version.

I've scoured through github and other blogs but couldn't find much that worked.

Thanks for the help in advance.

Cheers!",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/i5qhh2/td3_not_converging_for_bipedal_hardcore/
Why doesn't TD3 converge for BipedalWalkerHardcore-v3?,1596849623,,reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/i5qcjv/why_doesnt_td3_converge_for/
Library for making games for RL,1596824343,"Hi, so my problem is that I want to make the games that I want to apply RL on. For example, I want to try making the dinosaur game from Chrome and then use RL for that. Can you suggest some good options? Is Pygame the only option I got in python? Any help will be appreciated.",reinforcementlearning,puneet_saini,False,/r/reinforcementlearning/comments/i5izzq/library_for_making_games_for_rl/
Open Sourced Policy Value Methods,1596818641,"Hey all,

I've been working over the past months on some common state of the art policy value methods such as PPO, A3C, DDPG, TD3, REINFORCE, ARS, and genetic algorithms such as Evolutionary Strategies.

I've implemented them in single file jupyter notebooks as well as in simple out of the box implementation using terminal based on python implementation. 

I'd really appreciate it if you could check out the [repository](https://github.com/QasimWani/policy-value-methods) and give it a star on github. It really means a lot to me. 

If you find any errors, do raise issues in the repos, will promptly fix them :D

Cheers!",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/i5h89y/open_sourced_policy_value_methods/
Simulation Environments,1596816756,"Hi all,

Question for those who have experience working with simulation environments in RL. 

What do you guys feel are some of the biggest challenges in starting a new project using some open-sourced simulator (e.g. ThreeDWorld, CARLA, etc)?

Thanks!",reinforcementlearning,rl_student_555,False,/r/reinforcementlearning/comments/i5gnre/simulation_environments/
"Why isn't this research making it into the real world? Self driving cars, robots arms, agricultural tasks.",1596804857,"I see many great demos from research labs. I also see lots of startups trying to apply RL to tasks like cleaning, picking strawberries, picking cherry tomatoes, sorting, walking, driving. But I see little evidence of commercial success over the last few years.

Why is that? Or am I wrong?",reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/i5d9qt/why_isnt_this_research_making_it_into_the_real/
Parallelized MARL Environment API,1596797161,"Hey all, is anyone aware of a parallelized MARL environment API? Looking for something similar to a [SubProcEnv](https://stable-baselines.readthedocs.io/en/master/_modules/stable_baselines/common/vec_env/subproc_vec_env.html#SubprocVecEnv) but for n\_agents&gt;1. Having trouble finding something. Thanks!",reinforcementlearning,big-waves-r-us,False,/r/reinforcementlearning/comments/i5blxq/parallelized_marl_environment_api/
What reward should I give a live agent between rewards?,1596790492,"I can control how many action it does in a second. This leads to a lot of action that does nothing and doesn't collect rewards. If agent get reward every 10 or 20 actions, what reward should I give between those. In this case 0 doesn't make sense because it will ruin discount function. 

&amp;#x200B;

    Lets say the rewards are [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1], the discounted rewards using 0.99 will be [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1]. What is a good way to fix this. Can I give 0.001 rewards or -0.001?",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/i5ae3f/what_reward_should_i_give_a_live_agent_between/
Questions about RL,1596762638,"Hi all,

I am a beginner in RL and been following an online course to learn over the past few weeks and reading some research papers. I have a few questions that I was hoping to get answers for.

&amp;#x200B;

1. Is continuous state/action space RL more challenging discrete state/action space? I believe it is because of the larger exploration space, but I was wondering if there's a better answer.
2. I'm from a robotics background. I'm wondering what makes learning-based approaches better than traditional robotics methods for things like task planning or navigation? 
3. In a sparse reward setting, I found that people typically use curiosity driven exploration and Hindsight Experience Replay. Can I use both; do they synergize well with each other?

Thanks! Hopefully these questions make sense. :)",reinforcementlearning,rl_student_555,False,/r/reinforcementlearning/comments/i54esw/questions_about_rl/
Is there any works about integrating GNN with NAS?,1596755868,"I am trying to lookup works that are integrating GNN into NAS.

Essentially, NAS is finding some structure, which I think can be described by a graph.

Then I suppose there will be some work leverage that graph and to use GNN to improve or to do more advanced applications.

Can somebody point me to the related topics?",reinforcementlearning,FelixKao,False,/r/reinforcementlearning/comments/i52m8p/is_there_any_works_about_integrating_gnn_with_nas/
Can anyone give the proof of the off-policy TD learning algorithm?,1596743164,"I was reading the book ""Introduction to Reinforcement Learning"" by Richard Sutton In section 7.3 he write the formula for n-step off-policy TD as:. 

&amp;#x200B;

[off-policy n-step TD](https://preview.redd.it/trq4muq5rff51.png?width=538&amp;format=png&amp;auto=webp&amp;s=303ee0604e699ab9d2e3282c693a81ffc383643e)

Here, P = importance sampling ratio.

&amp;#x200B;

[importance sampling ratio](https://preview.redd.it/gciia2rbrff51.png?width=240&amp;format=png&amp;auto=webp&amp;s=1d7d5f98141e043fd1728fe49648913e892c401f)

Can anyone give the proof of this formula, please?",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/i4yqq6/can_anyone_give_the_proof_of_the_offpolicy_td/
Which is a better way to learn,1596728720,"I'm practicing for a test that requires a lot of critical thinking, logical reasoning, and analytical reasoning, and I've been thinking about what the best way to practice for it is... 

is it better to keep practicing something at an advanced level in order to gain experience on that level

OR

is it better to start at a basic level and build your way to the most advanced level?

and why do you think that?",reinforcementlearning,Eshtabel3asal,False,/r/reinforcementlearning/comments/i4u4z2/which_is_a_better_way_to_learn/
PyTorch Multi-Agent Algorithms,1596724092,"My question is about this GitHub repository of multi-agent reinforcement learning algorithms or use with PyTorch.  The documentation says the repo includes ""includes PyTorch implementations of various Deep Reinforcement Learning algorithms for both single agent and multi-agent"" and then lists several algorithms.  Here's the link: [https://github.com/ChenglongChen/pytorch-MADRL](https://github.com/ChenglongChen/pytorch-MADRL).

I'm wondering if this means that for each of those algorithms, a multi-agent and single-agent version is included?  Or if some are single-agent, while others are multi-agent?  Can all of those even be implemented for multi-agent?",reinforcementlearning,icebrgr,False,/r/reinforcementlearning/comments/i4srxg/pytorch_multiagent_algorithms/
Reinforcement learning library recommendations,1596721250,"I'm looking for recommendations of reinforcement learning libraries. I know that a quick Google search would've given me what I'm looking for, but I'd like to hear back from you on the libraries you've tried. 
I've tried TF-Agents before but it was too buggy and the error messages weren't that helpful. Also, their support when I submitted an issue was inadequate and too slow. Afterwards, I started using Tensorforce and I've been very satisfied with it. I especially love its modularity and ease of use, moreover the support I received whenever I had a question or request has been honestly phenomenal. But, with that said, I've now run into a wall where I'm looking for better performing agents (Tensorforce doesn't yet implement agents like Rainbow or R2D2) which seem necessary for my use case. The best performance I got so far was with Dueling-DQN which wasn't all that great. So I'm now looking for libraries or agent implementations that you've had a good experience with. Ideally, it should implement relatively new and better-performing off-policy algorithms like Rainbow, R2D2 or NGU.",reinforcementlearning,HeisenbergsMyth,False,/r/reinforcementlearning/comments/i4s09x/reinforcement_learning_library_recommendations/
Current state of industry and research DL framework adoption?,1596715682,"I know that OpenAI has adopted Pytorch and Deepmind is invested in Tensor flow.

Other than the two giants above, is there an industry leaning or consensus for a deep learning framework for RL?  I'm about to start getting my hands dirty on a project that's likely to be a building block for more future work.

This industry isn't 'future proofable', but I'd like to start the project with the most probably stable framework so that when I hand it off it's not going to break in a week.",reinforcementlearning,gdpoc,False,/r/reinforcementlearning/comments/i4qmx9/current_state_of_industry_and_research_dl/
How to decide whether to formulate a specific problem/application (eg learning to rank / game / recommender sysym) as a MDP or POMDP .,1596695935,,reinforcementlearning,honolulu22,False,/r/reinforcementlearning/comments/i4mqgn/how_to_decide_whether_to_formulate_a_specific/
Solving Fetch Reach using DDPG,1596674165,"I wanted to gain experience with implementing the DDPG algorithm and so I decided to try it out on the Fetch-Reach environment from Gym and wanted to further build up from there. 

I know vanilla DDPG isn't the best algorithm to apply here, and something like Hindsight Experience Replay would be more suited to this problem, but I wanted to implement DDPG properly before getting my hands dirty with HER. 

In any case, the algorithm does converge for problems like the Continuous Mountain Car environment. But when I apply it to the Fetch-Reach environment, it doesn't show any improvement in the performance with each passing epoch.

Does anyone have any ideas why this might be? 

Also, when I print out the losses, both the actor and critic loss converge to 1.00!",reinforcementlearning,TheNush07,False,/r/reinforcementlearning/comments/i4hj25/solving_fetch_reach_using_ddpg/
Question about priority replay buffer update,1596659003,"As I read about priority replay buffer, I know that it's usually implemented by sum tree structure, which is efficient in sampling. My question is, as the network is being trained, the time difference error has changed also, so do we need to update for those in the buffer? updating all the TD error is time-consuming, or is it feasible we just update for those sampled batch?",reinforcementlearning,Cohencohen789,False,/r/reinforcementlearning/comments/i4d2d2/question_about_priority_replay_buffer_update/
How to represent a game's sequential actions in a pytorch tensor?,1596647317,"I have dataset that looks like this (extracted from recording an agent play a game):

    INDEX        VALUES                        LABELS   
    0            (0, 1), UpDownUpUp            Up
    1            (2, 3), UpUpUpDownDownDown    Up
    2            (0, 2), DownUp                Down
    3            (0, Undefined), DownUp        Up

where 

* the **values** represent the initial random chance events (in the tuple) and the previous actions at this point in the game.
* the **labels** represent the next optimal action.


How can I represent the **values** into a PyTorch tensor? My issues come mainly from the fact that the values are of different sizes (DownUp is two actions, UpDownUp is three, etc.), and the second element in the tuple is sometimes undefined.",reinforcementlearning,pythonistaaaaaaa,False,/r/reinforcementlearning/comments/i49adc/how_to_represent_a_games_sequential_actions_in_a/
Why are plots so small in research papers?,1596644299,"It seems like every year, the plots in deep RL research papers are getting smaller. I understand that for an arxiv preprint you can just keep zooming in until it's big enough. Is this to maximize the experiments per page based on the page limits for popular conferences?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/i48bek/why_are_plots_so_small_in_research_papers/
Role of External Memory in the Deep Neural Network to solve Computer Vision problems,1596643826,"What is the role/advantage of using external memory in the deep neural networks to solve computer vision tasks. I gone through few works like [anomaly detection](https://openaccess.thecvf.com/content_CVPR_2020/papers/Park_Learning_Memory-Guided_Normality_for_Anomaly_Detection_CVPR_2020_paper.pdf), [object segmentation](https://openaccess.thecvf.com/content_ICCV_2019/papers/Oh_Video_Object_Segmentation_Using_Space-Time_Memory_Networks_ICCV_2019_paper.pdf), [temporal summarization](https://openaccess.thecvf.com/content_cvpr_2018/papers/Lee_A_Memory_Network_CVPR_2018_paper.pdf), but what I am lacking is

1. What is the role/advantage of using external memory
2. What information to store in the memory
3. When to read/write to/from the memory
4. Can you intutively explain how the external memory is impoving the overall accuracy",reinforcementlearning,s927,False,/r/reinforcementlearning/comments/i4862m/role_of_external_memory_in_the_deep_neural/
"[D][P] Paper discussion | Project - Learning Dexterous In-Hand Manipulation, OpenAI 2019",1596618469,"While working on a project I came across this paper of the OpenAI team.

Their objective was to create an agent, which is able to handle a square with letters on each side. The agent has to rearrange the square sequentially given a goal. They call it a dexterous in-hand manipulation policy.

The agent is learning only in a simulation, where various physical properties (friction coefficients, object dimensions, object masses, ...) were randomly drawn at the beginning of each episode from uniform distributions with certain ranges.

The idea behind it is to be able to generalize to different conditions. Within the first few interactions of the agent with the environment, he should be able to realize which setting he is encountering at the moment.

According to the paper, a key factor to the agent's success to transfer his control policy from simulation to reality is:

&gt;memory augmented control polices which admit the possibility to learn adaptive behaviour and implicit system identification on the fly 

&amp;#x200B;

But what does that mean? Do they just use an Lstm Policy? From my experience, Lstm Policies take a lot longer to converge in an RL setting.

What are your thoughts on this?",reinforcementlearning,invadrvranjes,False,/r/reinforcementlearning/comments/i41yzm/dp_paper_discussion_project_learning_dexterous/
How can I study RL effectively,1596618157,"My approach  is to study   an entire textbook like sutton RL book, but I discovered it will be slow and it will take time. I'm planning to study  textbook and research papers together. I need an advice.",reinforcementlearning,Osarenomawise,False,/r/reinforcementlearning/comments/i41wxt/how_can_i_study_rl_effectively/
DRL with BNN,1596610422,"I am looking for resources on DRL solutions that utilize BNN.

This far I could find only two -

* [https://towardsdatascience.com/bayesian-neural-networks-with-random-inputs-for-model-based-reinforcement-learning-36606a9399b4](https://towardsdatascience.com/bayesian-neural-networks-with-random-inputs-for-model-based-reinforcement-learning-36606a9399b4)
* [https://pdfs.semanticscholar.org/b039/0ea8f9e55a9ac01c901bb62b508f227f3e51.pdf](https://pdfs.semanticscholar.org/b039/0ea8f9e55a9ac01c901bb62b508f227f3e51.pdf)

Preferably with some reference code.

Please share in comments if you have seen anything like that, besides those I already mentioned.

Thanks!",reinforcementlearning,maxvol75,False,/r/reinforcementlearning/comments/i40ihc/drl_with_bnn/
"[P] RLcycle: RL agents framework based on PyTorch, Ray, and Hydra",1596601813,"Hi! I'd like to introduce an RLcycle, an RL agents framework based on PyTorch, Ray (for parallelization) and Hydra (for configuring experiments). 

Link: [https://github.com/cyoon1729/RLcycle](https://github.com/cyoon1729/RLcycle)

Currently, RLcycle includes:

* DQN + enhancements, Distributional: C51, Quantile Regression, Rainbow-DQN.
* Noisy Networks for parameter space noise
* A2C (data parallel) and A3C (gradient parallel).
* DDPG, both Lillicrap et al. (2015) and Fujimoto et al., (2018) versions.
* Soft Actor Critic with automatic entropy coefficient tuning.
* Prioritized Experience Replay and n-step updates for all off-policy algorithms.

RLcycle uses:

* [PyTorch](https://github.com/pytorch/pytorch) for computations and building and optimizing models.
* [Hydra](https://github.com/facebookresearch/hydra) for configuring and building agents.
* [Ray](https://github.com/ray-project/ray) for parallelizing learning.
* [WandB](https://www.wandb.com/) (Weight &amp; Biases) for logging training and testing.

The implementations have been tested on Pong (Rainbow, C51, and Noisy DDQN all achieve 20+ in less than 300 episodes), and PyBullet Reacher (Fujimoto DDPG, SAC, and DDPG all perform as expected). 

I do plan on carrying out more rigorous testing on different environments, as well as implementing more SOTA algorithms and distributed architectures. 

I hope this can be interesting/helpful for some. 

Thank you so much!",reinforcementlearning,cyoon1729,False,/r/reinforcementlearning/comments/i3ysn6/p_rlcycle_rl_agents_framework_based_on_pytorch/
How to optimize more than one policy at the same time?,1596594533,"Hi, I am searching for how to optimize more than one policy at the same time.

In our case, the agent model returns two different policies, and the trainer optimizes both policies at the same time.

I am thinking of optimizing the summed of the two policies or the averaged the two policies, or training each policy sequentially.

I am unsure how to update two different policies.

Is any recommended paper or implementation available in this problem?

Thanks.",reinforcementlearning,epitk,False,/r/reinforcementlearning/comments/i3x2rn/how_to_optimize_more_than_one_policy_at_the_same/
What is distribution drift in context of reinforcement learning?,1596567355,I understand the general idea of distribution drift being a phenomenon in which distribution of a variable slowly changes to something else but would highly appreciate of somebody can provide intuition for this and may be clarify what distribution is referred to as drifting when the term is used in context of RL (in something like behaviour cloning for example).,reinforcementlearning,namuradAulad,False,/r/reinforcementlearning/comments/i3paz1/what_is_distribution_drift_in_context_of/
Help with a little project in C++,1596560485,"Hi everyone, I'm a PhD student trying to get to walk the 2D-Walker model from the dm\_control suite in C++. I would like to go through the code and discuss the problem (possibly via Skype) with someone that already have experience with RL and MuJoCo environments. 

DM me if interested :)

Cheers!",reinforcementlearning,vittorione94,False,/r/reinforcementlearning/comments/i3n3sq/help_with_a_little_project_in_c/
Any implementation for Meta-Learning shared Hierarchies in Pytorch,1596559223,Does anyone work on an implementation for the paper of meta-learning shared hierarchies in Pytorch.... or anyone interested we can try to implement it as group?,reinforcementlearning,Hazem2037,False,/r/reinforcementlearning/comments/i3mp8c/any_implementation_for_metalearning_shared/
PPO - to high entropy coefficient makes agent unlearn stuff he already learned,1596542966,"As mentioned in the title if the entropy coefficient is too high (in my case &gt;0.01) the agent unlearns stuff he learned before. At the moment I am using the ppo2 implementation from stable baselines.

I attached a plot which shows, how the agent learns to accomplish the task and the average reward getting higher and higher, but at a certain point he unlearns everything. Shouldn't ppo prevent something like this to happen? Shouldn't it stay either the same average reward, or getting better?

&amp;#x200B;

https://preview.redd.it/2wlpr9qa8ze51.png?width=864&amp;format=png&amp;auto=webp&amp;s=416c85c142d8698e94407149e3e3be5b8a600e66

Can someone explain this to me? Hopefully I provided enough information :) Thanks for your help!",reinforcementlearning,OP_Hidan,False,/r/reinforcementlearning/comments/i3i5qa/ppo_to_high_entropy_coefficient_makes_agent/
PPO - too high entropy loss makes algorithm unlearn stuff,1596542892,"As mentioned in the title if the entropy coefficient is too high (in my case &gt;0.01) the agent unlearns stuff he learned before. At the moment I am using the ppo2 implementation from stable baselines.

I attached a plot which shows, how the agent learns to accomplish the task and the average reward getting higher and higher, but at a certain point he unlearns everything. Shouldn't ppo prevent something like this to happen? Shouldn't it stay either the same average reward, or getting better?

&amp;#x200B;

[average reward over time with dif ent\_coef](https://preview.redd.it/1v0gx3gy7ze51.png?width=864&amp;format=png&amp;auto=webp&amp;s=0b26ee4486db391860943b46d12cc59d0261bfa4)

Can someone explain this to me? Hopefully I provided enough information :) Thanks for your help!",reinforcementlearning,OP_Hidan,False,/r/reinforcementlearning/comments/i3i53y/ppo_too_high_entropy_loss_makes_algorithm_unlearn/
A2C loss function for custom action model with two sets of actions,1596539374,"For a project, I need to create a model that possess two sets of actions. An action set A and an action set B (in the following example, the action set A will be the first two elements of the output tensor, while the action set B will be the third and fourth element of the tensor).

I want to be able to input a state, and output all the sets of actions. Only thereafter, using vanilla python and numpy (and not TF) process the action set that interest me the most. Therafter, use this last to fit the network.

I therefore : create a model that output a tensor with a linear activation. I'll then choose thereafter to do the softmax on ouput\[0,1\] or ouput\[2,3\].

I am doing A2C, I therefore will train with the log likelihood and the advantage (here delta). However I need to mask depending on the action set that I need to use. And it starts to be hairy \^\^'

I think everything seems to work, except one thing, since I output linear value, but do the loss on probabilities, I guess that my network will learn really slowly (or not at all). 

My question is therefore, how do I need to convert my log likelyhood/change my loss function for it to be able to train my linear layer.

I am not sure I was very clear, and I may edit the post \^\^'

The full code is avaliable below:

&amp;#x200B;

`import numpy as np`

`from scipy.special import softmax`

&amp;#x200B;

`import tensorflow as tf`

`from tensorflow.keras import backend as K`

`from tensorflow.keras.optimizers import Adam`

`from tensorflow.keras.layers import Dense, Softmax`

`from tensorflow.keras import Input, Model`

&amp;#x200B;

`main_input = Input(shape=(10,))`

`delta = tf.Variable([[0.]], trainable=False)`

`mask = tf.Variable([[1.0, 1.0, 0.0, 0.0]], trainable=False)`

`output = Dense(4, activation='linear')(main_input)`

&amp;#x200B;

`def custom_loss(delta, mask):`

`def loss(y_true, y_pred):`

`dummy_scores = tf.ones_like(y_pred) * -99999.0` 

`y_pred_masked = tf.where(tf.cast(mask, dtype='bool'), y_pred, dummy_scores)`

`y_pred_masked_softmax = Softmax()(y_pred_masked)`

`y_pred_masked_softmax_clipped = K.clip(y_pred_masked_softmax, 1e-8, 1 - 1e-8)`

`log_likelihood = y_true * K.log(y_pred_masked_softmax_clipped)`

`return tf.multiply(K.sum(-log_likelihood * delta), mask)`

`return loss`

&amp;#x200B;

`model = Model(inputs=[main_input], outputs=output)`

`model.compile(optimizer=Adam(lr=0.001), loss=custom_loss(delta, mask))`

&amp;#x200B;

`print(model.predict(np.ones((1,10))))`

`print(softmax(model.predict(np.ones((1,10)))))`

`delta.assign([[2.0]])`

`mask.assign([[1.0, 1.0, 0.0, 0.0]])`

[`model.fit`](https://model.fit)`(np.ones((10000,10), dtype='float'),np.asarray(10000*[[0.7, 0.3, 0.00, 0.00]], dtype='float'))`

`print(model.predict(np.ones((1,10))))`

`print(softmax(model.predict(np.ones((1,10)))))`

&amp;#x200B;

If the initial condition is close the the wanted result, it works really well :

 \[\[ 1.540099   1.2330251 -0.6329754  1.7009711\]\] 

\[0.5761709  0.42382914\] 

Train on 1000 samples 1000/1000 \[==============================\] - 0s 333us/sample - loss: 19.7297 

\[\[ 1.8134781   0.95964587 -0.6329754   1.7009711 \]\] 

\[0.7013705  0.29862958\] 

&amp;#x200B;

But if the initial condition is far from the expected, it doesn't work anymore:

\[\[-2.3998928   0.69341695  1.6462358  -1.3345295 \]\] 

\[0.04338406 0.9566159 \] 

Train on 1000 samples 

1000/1000 \[==============================\] - 0s 325us/sample - loss: 63.5080 

\[\[-2.050389   0.3439127  1.6462358 -1.3345295\]\] 

\[0.08360824 0.91639173\]",reinforcementlearning,thomashirtz,False,/r/reinforcementlearning/comments/i3he5k/a2c_loss_function_for_custom_action_model_with/
Upgrade a PC for reinforcement learning,1596533658,"Maybe someone can help me with this issue.  


I have been playing with a DQN (rainbow) and NES games in and old computer (I5 4670, 8 GB and a 960 GTX 2 GB) because it's the only with a graphic card. I don't expect so much, but at least is faster than my Apple computer for ML task.   


Lately I've been thinking to make an upgrade and I try to find what to upgrade first: CPU, GPU, RAM. So I saw my CPU is 100% while my GPU is around 50-60%. I made some research and I found that the DQN use mainly CPU, because the only part of the code that use GPU is the NN, and because is small it's not going to benefits for a better GPU.  


While the replay memory is filling the CPU is around 50%, but when it starts to send batch to the NN it jumps to 100%. Also, the FPS down from 180 to 30  


So if I buy a CPU (I've thinking a 3800X with 16 GB) I am going to see some improvement, but in the future when I upgrade the GPU is going to be the same. I mean, I can spend money but at the end the improvement is not going to be so great.  


I am missing something?. At first, I thought that maybe the problem was lack of RAM because SO has to paginate, but with a smaller replay memory the same thing happens. Another thing that I thought it could be is the VRAM, but at least TensorFlow should not use RAM (The code is in PyTorch, but I think is kind of the same).  


I wonder if with a better PC part I will see a significant improvement (various magnitudes) because there are something (like RAM or VRAM) that made the CPU work harder than it should. Or the CPU is going to be always the really bottleneck and if I don't but a Threadripper I am not going to see a real improvement.

&amp;#x200B;

The code is this: [https://github.com/medipixel/rl\_algorithms](https://github.com/medipixel/rl_algorithms)",reinforcementlearning,pacoflaco,False,/r/reinforcementlearning/comments/i3g9rm/upgrade_a_pc_for_reinforcement_learning/
blendtorch v0.2: OpenAI gym support added - train PyTorch agents in Blender environments through OpenAI.,1596528061,,reinforcementlearning,chrisheind,False,/r/reinforcementlearning/comments/i3f9up/blendtorch_v02_openai_gym_support_added_train/
Just try a different random seed.,1596520850,,reinforcementlearning,spenceowen,False,/r/reinforcementlearning/comments/i3dwoz/just_try_a_different_random_seed/
"PhD in Reinforcement Learning/Blockchains, worth it ?",1596510135,"Hi there ! 

I have been given an opportunity to do a 3 years PhD in a research organization in Europe.

But I’m starting to have some bad feelings...
I don’t know if it will be worth it doing a PhD in this area especially since I’m not interested in academia but more about doing research for the industry.

I’m from electronics and control background then I fear that I’ll be not good enough to be able to pursue such a PhD. I did some programming but not much algorithmic.

The only reason for me doing this PhD is for the fun of exploring a new topic and getting more skills for later (because my final internship was cancelled due to COVID and I think starting a PhD now to skip this uncertain days for a freshly graduated student might be a good thing instead of having a hard time finding a job or taking a job I don’t want).

Thanks in advance for your advices.",reinforcementlearning,RegunHD,False,/r/reinforcementlearning/comments/i3bird/phd_in_reinforcement_learningblockchains_worth_it/
Need help with Q Learning!,1596500328,I am trying to develop a project which uses Q Learning approach and I am badly stuck and want help regarding some implementation stuff. I've the algorithm ready on paper just confused how to properly implement it. Please help! 😥,reinforcementlearning,MikeyXcalibur,False,/r/reinforcementlearning/comments/i38zrk/need_help_with_q_learning/
"""AlphaNPI-X: Learning Compositional Neural Programs for Continuous Control"", Pierrot et al 2020",1596499247,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i38p43/alphanpix_learning_compositional_neural_programs/
Teaching a set of N NPCs each with their unique strengths and weaknesses to play against each other optimally?,1596495875,"Is there a name for this type of reinforcement learning?

Basically, you're simulating N different agents each with their unique strengths and weaknesses.  To understand a simplified version of this problem, think of there being N different characters in like an RPG game each with their unique stats.  Your job is to train each character to battle at its best.  When you're starting how, none of your N characters know anything about the best way to battle.  You train N different RL agents by pitting them against each other in 1 on 1 battles (yes, this takes O(N^2) time) over and over again until each agent improves.

Is there a name for this type of reinforcement learning?",reinforcementlearning,ragnarkar,False,/r/reinforcementlearning/comments/i37rpt/teaching_a_set_of_n_npcs_each_with_their_unique/
RL for turn-based AI,1596481272,"For a side project I'm trying to build a (simplified) AI for Heroes Of Might and Magic, using (as a starting point) deep Q-learning. But I'm having trouble to understand how the ""state space"" is supposed to be represented. 

Roughly speaking:

* there are 2 opposing armies
* each armies has troops
* each troop occupies an hex and contains a number of soldiers
* the more soldiers, the more HP and attack a ""troop"" has
* each troop attacks in turns, and during its turn it can either move, or move and attack an adjacent enemy troop
* the army that manages to kill all enemy troops win

I've simplified this such that I reduced the board size and am only considering 1 troop per army. However I am not sure how I should ""formalize"" the problem into a RL problem, especially concerning the ""state"" of the game. For state, I can either:

* have a state that exposes the Hit Point, position, attack, etc of the enemy and my own
* have a state that is a NxMxP tensor that contains all the info of the board. Ie the tensor at position (0,0) where there is nothing would contain a 0-vector, but the tensor at position (1,2) where an enemy is would contain a vector of all the enemy statistics

It seems to me that the first solution would be simpler but complex to generalize to a variable number of enemy units. Second solution has all the infos but the state is getting big and very sparse. Also I'm not sure it's the best to input the state in terms of absolute positions, rather than relative position to the unit that has to take an action.

Likewise for 'actions' ,  I don't really know if the actions should be relative to the unit (like ""go up"") or absolute (like, ""go at this location"")

What do you think ? Are there any resources to understand better how such a problem should be transformed into a RL problem?

**TL;DR : how  do you represent the state in a generic board game?**",reinforcementlearning,lezebulan,False,/r/reinforcementlearning/comments/i3367m/rl_for_turnbased_ai/
Learning vs Unlearning vs Re-Learning?,1596480178,,reinforcementlearning,steinbergerscott,False,/r/reinforcementlearning/comments/i32t9g/learning_vs_unlearning_vs_relearning/
Comparison between RL and A* for indoor navigation,1596469199,"What are the advantages of using DDPG,TD3 over A\* algorithms in long range indoor navigation .",reinforcementlearning,ajithvallabai,False,/r/reinforcementlearning/comments/i2z9e0/comparison_between_rl_and_a_for_indoor_navigation/
Evaluation of RL experiments,1596415691,"I'm looking for metrics and evaluation possibilities for RL agents, especially to compare different agents. 
I already found some(like: plotting rewards per episode, how many steps the agent needs to take to achieve the desired result)
However, I'd like to find more(and more interesting evaluation possibilties, maybe you could point me to some resources?
Thanks!",reinforcementlearning,noootrooo,False,/r/reinforcementlearning/comments/i2nf3m/evaluation_of_rl_experiments/
Questions,1596407792,"I have some questions i trying to find an answer to it. 

1) suppose i have a walking robot which learned to run and learned to climb stairs in another policy. How to combine these two policies ? 

2) what does it means end to end in DRL?  

3) Does any one have an idea how to add obstacle avoidance mechanism in Mujoco?",reinforcementlearning,Hazem2037,False,/r/reinforcementlearning/comments/i2lanv/questions/
Rewards fluctuate when learning using SAC.,1596407772,"I am trying to control a robot using Soft Actor Critic algorithm.

I tried to do it by changing various variables, but as a result, there is a problem that the reward does not continuously increase but fluctuates.

Please let me know if you know a solution to this problem or if you have any suggestions.

https://preview.redd.it/f2x05to82oe51.png?width=1376&amp;format=png&amp;auto=webp&amp;s=c4e5b0106fe446bec8d0dfa2beaeea9f12dadbc3",reinforcementlearning,Hansol_Onesoul_Kang,False,/r/reinforcementlearning/comments/i2lag4/rewards_fluctuate_when_learning_using_sac/
Help installing gym with Roboschool,1596391084,"Hi, as I'm not a student or a professional I don't have access to a Mujoco key for a physics engine, I've looked into other opensource ones (Roboschool), but installation instructions are scant and I don't have great backend knowledge. Would anyone be willing to make detailed instructions on how to install gym with Roboschool? I would really appreciate it. Thank you!",reinforcementlearning,Cabbarge,False,/r/reinforcementlearning/comments/i2gfcf/help_installing_gym_with_roboschool/
"Open RL Benchmark @ 0.3.0 (benchmark.cleanrl.dev, 7+ algorithm and 34+ games)",1596373361,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/i2bp97/open_rl_benchmark_030_benchmarkcleanrldev_7/
Recommend an RL framework friendly to minimax/tree search,1596371694,"Currently I'm looking at taking a project that I previously developed using my own implementations of DQN, and porting it to run under an RL framework such as Stable Baselines or something similar, I haven't yet really decided on which one to use, but I want something that will let me experiment with a few state of the art RL methods without having to implement everything from the ground up.

In this case the environment is actually a two-player game, and previously I trained it on a kind of simplified 1-player version of the game; but I want to now incorporate minimax search methods for the real 2-player scenario.  Is Stable Baselines a good choice, or is there some RL framework that is particularly apt for combining with minimax scenarios?

Thanks in advance.",reinforcementlearning,radarsat1,False,/r/reinforcementlearning/comments/i2bccm/recommend_an_rl_framework_friendly_to_minimaxtree/
Why the hell are we researching about first-person-shooters like Doom? (and making it open-source),1596370232,"I honestly don't understand why there's decent academic interest in solving a game like Doom, where the intention is to shoot other human-shaped beings, and from a first-person viewpoint.

I don't want to come across as someone who's saying AGI is going to take over humanity with killer robots and stuff, but this line of research, if successful, would have direct military application and interest. Yes, making the agent learn to play Doom is not the same as putting it onto a robot and making it do the same thing. Yes, this technology doesn't work right now, but it is actively trying to solve one part of this jigsaw. Yes, this may take multiple years/decades to solve. But, the technology doesn't work until it suddenly works (look at the fantastic work of Boston Dynamics).

In addition, this technology is all open-sourced. Anyone with a decent desktop can implement this with very basic coding knowledge / desire to implement such a system. This technology should not be open-sourced and rather, it should be ""Export Controlled"" (i.e. controlled very very strictly, much like any military or dual-use technology is managed these days already).

Thoughts?",reinforcementlearning,LearnAgentLearn,False,/r/reinforcementlearning/comments/i2b1vt/why_the_hell_are_we_researching_about/
"Sample Factory, a new training framework for Reinforcement Learning slashes the level of compute required for state-of-the-art results",1596367522,,reinforcementlearning,dipanshunagar,False,/r/reinforcementlearning/comments/i2ajr6/sample_factory_a_new_training_framework_for/
Automating hyperparameter tuning with own DQN,1596355274,"I've been reading up on RLlib, Ray Tune and other packages that supposedly find optimal hyperparameters for algorithms but I still don't see how it could work with my example. Does anyone know how I can setup RLlib or something similar to find optimal parameters of this simple DQN algorithm?

    import gym
    import numpy as np
    import matplotlib.pyplot as plt 
    import tensorflow as tf
    import pennylane as qml

    env = gym.make(""FrozenLake-v0"")

    n_actions = env.action_space.n
    input_dim = env.observation_space.n
    model = tf.keras.Sequential() 
    model.add(tf.keras.layers.Dense(32, input_dim = input_dim , activation = 'relu'))
    model.add(tf.keras.layers.Dense(16, activation = 'relu'))
    model.add(tf.keras.layers.Dense(n_actions, activation = 'linear'))
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss = 'mse')

    def replay(replay_memory, minibatch_size=32):
        minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)
        s_l =      np.array(list(map(lambda x: x['s'], minibatch)))
        a_l =      np.array(list(map(lambda x: x['a'], minibatch)))
        r_l =      np.array(list(map(lambda x: x['r'], minibatch)))
        sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))
        done_l   = np.array(list(map(lambda x: x['done'], minibatch)))
        qvals_sprime_l = model.predict(sprime_l)
        target_f = model.predict(s_l) 
        for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): 
            if not done:  target = r + gamma * np.max(qvals_sprime)
            else:         target = r
            target_f[i][a] = target
        model.fit(s_l,target_f, epochs=1, verbose=0)
        return model

        n_episodes = 1000
        gamma = 0.99
        epsilon = 0.99
        minibatch_size = 64
        r_sums = []  
        replay_memory = []
        mem_max_size = 10000

        for n in range(n_episodes): 
            ss = env.reset()
            states_total = 16
            data = [[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]]
            def encode(data, states_total):
                targets = np.array(data).reshape(-1)
                return np.eye(states_total)[targets]
            m = encode(data,states_total)
            s = m[ss]
            #print(s)
            #print(len(s))
            done=False
            r_sum = 0
            while not done: 
                #env.render()
                qvals_s = model.predict(s.reshape(1,-1))
                if np.random.random() &lt; epsilon:  a = env.action_space.sample()
                else:                             a = np.argmax(qvals_s); 
                sprime, r, done, info = env.step(a)
                r_sum += r
                q = encode(data,states_total)
                sprime = q[sprime]
                if len(replay_memory) &gt; mem_max_size:
                    replay_memory.pop(0)
                replay_memory.append({""s"":s,""a"":a,""r"":r,""sprime"":sprime,""done"":done})
                #s = n[sprime]
                s=sprime
                model=replay(replay_memory, minibatch_size = minibatch_size)
            if epsilon &gt; 0.001:      epsilon -= 0.001
            r_sums.append(r_sum)
            print(r_sum)
            print(epsilon)
            if n % 100 == 0: print(n)",reinforcementlearning,math7878,False,/r/reinforcementlearning/comments/i28jdp/automating_hyperparameter_tuning_with_own_dqn/
How to use a custom RL algorithm for unitys ML agents,1596350630,"So I'm trying to use reinforcement learning with unity MLAgents to help solve a game I made but I would like to use an algorithm I coded from scratch . Is there any way to do so with unity as opposed to using the PPO algorithm that comes prewritten from unity? Any resources, advice and help ect would be appreciated. Thanks in advanced.",reinforcementlearning,tinytina113,False,/r/reinforcementlearning/comments/i27rgv/how_to_use_a_custom_rl_algorithm_for_unitys_ml/
Deep RL Book Choice,1596341514,"Hi, I need a book reccomendation. I was going to start off with Sutton and Barto, but I heard it doesn't cover Deep Reinforcement Learning. Right now, I am more concerned with implementation and applications than theory since I will be having a fairly rigorous RL class later on. 

I have shortlisted three books:

a. [https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998/](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-optimization/dp/1838826998/) 

Deep RL Hands-On by Maxim Lapan.

 Seems interesting, highly rated, but some Reddit threads mentioned that it was sloppy in some respects. I don't know if this is quibbling or not.

b. [https://www.amazon.com/Deep-Reinforcement-Learning-Action-Alexander/dp/1617295434/](https://www.amazon.com/Deep-Reinforcement-Learning-Action-Alexander/dp/1617295434/)

Deep RL in Action by Zai and Brown 

This seems interesting too, and was recommended in another Reddit thread, but some reviews mentioned that the code in the book is wrong and the Github code doesn't correlate with the text.

c. [https://www.amazon.in/Foundations-Deep-Reinforcement-Learning-Addison-Wesley-ebook/](https://www.amazon.in/Foundations-Deep-Reinforcement-Learning-Addison-Wesley-ebook/) 

Foundations of Deep RL by Graesser and Keng 

Again, interesting, but less used. 

&amp;#x200B;

Which book would you suggest?",reinforcementlearning,pakodanomics,False,/r/reinforcementlearning/comments/i260cr/deep_rl_book_choice/
Summarizing things I've learned about maximizing throughput for Deep Q learning,1596315113,,reinforcementlearning,ihexx,False,/r/reinforcementlearning/comments/i1zlfr/summarizing_things_ive_learned_about_maximizing/
Deep Reinforcement Learning for Early Text Classification,1596312739,"hi!

I've been looking for implementations of DRL for Early Text Classification. The basic idea is to classify text before read all the text using Deep Reinforcement Learning. I've read this Paper [https://www.aclweb.org/anthology/N19-1163.pdf](https://www.aclweb.org/anthology/N19-1163.pdf), and i tried to follow their implementation([https://github.com/DeepBrainAI/ERD](https://github.com/DeepBrainAI/ERD)), but it didn't work. Do you know where i could find how to implement something similar? I'm using Pytorch, but i could try another framework like keras or TF.

I've tried first to make a LSTM with Glove embedding to classify, but i don't know how to join with a DQN or some other DRL algorithm.

Thanks",reinforcementlearning,Reverse_Martell,False,/r/reinforcementlearning/comments/i1yt07/deep_reinforcement_learning_for_early_text/
Deep Reinforcement Learning for Early Text Classification,1596312729,"hi!

I've been looking for implementations of DRL for Early Text Classification. The basic idea is to classify text before read all the text using Deep Reinforcement Learning. I've read this Paper [https://www.aclweb.org/anthology/N19-1163.pdf](https://www.aclweb.org/anthology/N19-1163.pdf), and i tried to follow their implementation([https://github.com/DeepBrainAI/ERD](https://github.com/DeepBrainAI/ERD)), but it didn't work. Do you know where i could find how to implement something similar? I'm using Pytorch, but i could try another framework like keras or TF.

I've tried first to make a LSTM with Glove embedding to classify, but i don't know how to join with a DQN or some other DRL algorithm.

Thanks",reinforcementlearning,Reverse_Martell,False,/r/reinforcementlearning/comments/i1ystw/deep_reinforcement_learning_for_early_text/
Recommended models for sample efficient learning ?,1596304013,"Hi,

I'm currently trying to solve an environnement that has the following characteristics :

\-around 10-20 steps long max (often less when the agent fails).

\-computing the final rewards involves solving a Linear Program (or even a non-linear convex program), making the simulator relatively slow, and making parrallel environnements barely interesting (the linear/convex solvers being already parrallel and largely dominating the simulation time by over 99%)

Thus I'm wondering which RL algorithm to choose. I already had good results using A2C on small versions of my environnement. However, since the solving time for the final reward grows relatively quickly with respect to the size of my environnement, A2C begins to require long training time for medium environnement sizes (in the realm of the few days).

I believe there certainly are ways to reduce this time-complexity by leveraging more sample efficient algorithms.   
Could you please point me to the current go-to models that have good sample efficiency ? Ideally that wouldn't require a high number of parrallel environnements.

I've read a lot of good things on SAC and Rainbow in that regard but I'm not sure I'm up to date.

Maybe I should turn to model-based RL too ? If so, suggestions of must-read papers would be greatly appreciated as I'm a lot less aware of what happens in that area.

Thanks in advance !",reinforcementlearning,Artichoke-Lower,False,/r/reinforcementlearning/comments/i1wakv/recommended_models_for_sample_efficient_learning/
Soft Actor Critic not working great,1596295196,"Hey everyone,

&amp;#x200B;

I am currently implementing a standard soft actor critic agent. It works ""ok"" on something trivial like pendulum or cartpole, but does badly on anything harder like walking tasks. I noticed that one problem seems to be that the agent outputs really high values for the mean and std which are then used for the normal distribution. Can anybody help me find my errors?

 

class Agent(nn.Module):  
 def \_\_init\_\_(self,d\_state,d\_action,action\_limit):  
 super(Agent,self).\_\_init\_\_()  
 self.lin1 = nn.Linear(d\_state,300)  
        nn.init.uniform\_(self.lin1.weight,-0.001,0.001)  
        nn.init.uniform\_(self.lin1.bias,-0.001,0.001)  
 self.norm1 = nn.LayerNorm(300)  
 self.lin2 = nn.Linear(300,300)  
        nn.init.uniform\_(self.lin2.weight,-0.001,0.001)  
        nn.init.uniform\_(self.lin2.bias,-0.001,0.001)  
 self.norm2 = nn.LayerNorm(300)  


self.out\_mean = nn.Linear(300,d\_action)  
        nn.init.uniform\_(self.out\_mean.weight,-0.001,0.001)  
        nn.init.uniform\_(self.out\_mean.bias,-0.001,0.001)  
 self.out\_logstd = nn.Linear(300,d\_action)  
        nn.init.uniform\_(self.out\_logstd.weight,-0.001,0.001)  
        nn.init.uniform\_(self.out\_logstd.bias,-0.001,0.001)  
   
 self.action\_limit = action\_limit  
 def forward(self,x):  
        x = self.lin1(x)  
        x = self.norm1(x)  
        x = F.relu(x)  
   
        x = self.lin2(x)  
        x = self.norm2(x)  
        x = F.relu(x)  
   
        mean = self.out\_mean(x)  
        log\_std = self.out\_logstd(x)  
        log\_std = torch.clamp(log\_std, -20, 2)  
        std = torch.exp(log\_std)  
        normal = Normal(mean, std)  
        sampled\_action = normal.rsample()  
        log\_prob = normal.log\_prob(sampled\_action)  
   
   
   
 return torch.tanh(sampled\_action)\*self.action\_limit,log\_prob,mean  
class A\_Critic(nn.Module):  
 def \_\_init\_\_(self,d\_state,d\_action):  
 super(A\_Critic,self).\_\_init\_\_()  
 self.lin1 = nn.Linear(d\_state+d\_action,300)  
 self.norm1 = nn.LayerNorm(300)  
        nn.init.uniform\_(self.lin1.weight,-0.001,0.001)  
        nn.init.uniform\_(self.lin1.bias,-0.001,0.001)  
 self.lin2 = nn.Linear(300,300)  
 self.norm2 = nn.LayerNorm(300)  
        nn.init.uniform\_(self.lin2.weight,-0.001,0.001)  
        nn.init.uniform\_(self.lin2.bias,-0.001,0.001)  
 self.lin3 = nn.Linear(300,300)  
 self.norm3 = nn.LayerNorm(300)  
        nn.init.uniform\_(self.lin3.weight,-0.001,0.001)  
        nn.init.uniform\_(self.lin3.bias,-0.001,0.001)  


self.out = nn.Linear(300,1)  
        nn.init.uniform\_(self.out.weight,-0.001,0.001)  
        nn.init.uniform\_(self.out.bias,-0.001,0.001)  
   
 def forward(self,state,action):  
        x = torch.cat((state,action),dim=1)  
        x = self.lin1(x)  
        x = self.norm1(x)  
        x = F.relu(x)  
   
        x = self.lin2(x)  
        x = self.norm2(x)  
        x = F.relu(x)  
   
 return (self.out(x))

&amp;#x200B;

 

def model\_training(epochs):  
  gen.train()  
  crit.train()  
  gan\_training(epochs)  
  gen.eval()  
  crit.eval()

&amp;#x200B;

 

def environment\_step(environment,obs):    
 with torch.no\_grad():  
    agent\_input = torch.Tensor(obs).view(1,-1).to(device)  
    agent.eval()  
    action,\_,\_ = agent(agent\_input)  
    agent.train()  
    action = action\[0\].cpu().numpy()  
    observation, reward, done, info = env.step(action)   
    env.render()  
 if done:  
      alive = 0  
 else:  
      alive = 1  
    replay\_buffer.add\_sample(np.concatenate((obs\[:d\_state\],action,observation\[:d\_state\],\[alive\],\[reward\])))  
 return observation,reward,done

&amp;#x200B;

 

def compute\_q\_targets(samples):  
  s = samples\[:,:d\_state\]  
  a = samples\[:,d\_state:d\_state+d\_action\]  
  s\_new = samples\[:,d\_state+d\_action:d\_state+d\_action+d\_state\]  
  d = samples\[:,d\_state+d\_action+d\_state\]  
  r = samples\[:,d\_state+d\_action+d\_state+1\]  
  policy,log\_prob,\_ = agent(s\_new)  
  critic\_1\_output = a\_critic\_target\_1(s\_new,policy).squeeze()  
  critic\_2\_output = a\_critic\_target\_2(s\_new,policy).squeeze()  
  minimum\_outputs = torch.min(critic\_1\_output,critic\_2\_output)  
 try:  
    targets = r + GAMMA \* d \* (minimum\_outputs-alpha\_entropy\*(torch.sum(log\_prob.squeeze(),dim=1)))  
 except:  
    targets = r + GAMMA \* d \* (minimum\_outputs-alpha\_entropy\*(log\_prob.squeeze()))  
 return targets  
def update\_q\_network(targets,samples):  
  s = samples\[:,:d\_state\]  
  a = samples\[:,d\_state:d\_state+d\_action\]  
  loss\_fn = torch.nn.MSELoss()  
  q\_a\_optimizer\_1.zero\_grad()  
  predictions\_1 = a\_critic\_1(s,a).squeeze()  
  loss\_1 = loss\_fn(predictions\_1,targets.detach())  
  loss\_1.backward()  
  q\_a\_optimizer\_1.step()  
  q\_a\_optimizer\_2.zero\_grad()  
  predictions\_2 = a\_critic\_2(s,a).squeeze()  
  loss\_2 = loss\_fn(predictions\_2,targets.detach())  
  loss\_2.backward()  
  q\_a\_optimizer\_2.step()  
  q\_a\_losses.append(loss\_1.item())  


def update\_agent\_network(samples):  
  s = samples\[:,:d\_state\]  
  a\_optimizer.zero\_grad()  
  actions, log\_prob,\_ = agent(s)  
  q\_1 = a\_critic\_1(s, actions)  
  q\_2 = a\_critic\_2(s, actions)  
  q\_min = torch.min(q\_1,q\_2)  
  loss = torch.mean(-(q\_min-alpha\_entropy\*log\_prob.squeeze()))  
  a\_losses.append(loss.item())  
  loss.backward()  
  a\_optimizer.step()  
def update\_target\_network(model,target\_model,tau):  
 for target\_param, param in zip(target\_model.parameters(), model.parameters()):  
        target\_param.data.copy\_(  
            target\_param.data \* tau + param.data \* (1.0 - tau)  
        )  


def agent\_training(replay\_buffer, epochs):  
 for epoch in range(epochs):  
        sample= replay\_buffer.get\_sample(agent\_batches).to(device)  
        q\_targets = compute\_q\_targets(sample)  
        update\_q\_network(q\_targets,sample)  
        update\_agent\_network(sample)    
        update\_target\_network(a\_critic\_1,a\_critic\_target\_1,tau)  
        update\_target\_network(a\_critic\_2,a\_critic\_target\_2,tau)

&amp;#x200B;

 

a\_lr = 1e-3  
q\_a\_lr = 1e-3

 

alpha\_entropy = 0.2  
tau = 0.995  
agent\_batches = 64

 

environment\_name = 'AntBulletEnv-v0'  
env = gym.make(environment\_name)  
d\_state = env.observation\_space.shape\[0\]  
d\_action = env.action\_space.shape\[0\]  
action\_limit\_max = env.action\_space.high\[0\]  
action\_limit\_min = env.action\_space.low\[0\]

&amp;#x200B;

 

agent = Agent(d\_state,d\_action,action\_limit\_max).to(device)  
a\_critic\_1 = A\_Critic(d\_state,d\_action).to(device)  
a\_critic\_target\_1 = copy.deepcopy(a\_critic\_1)  
a\_critic\_2 = A\_Critic(d\_state,d\_action).to(device)  
a\_critic\_target\_2 = copy.deepcopy(a\_critic\_2)

 

replay\_buffer = ReplayBuffer(1000000)

&amp;#x200B;

 

\#initialise the buffer  
env = gym.make(environment\_name)  
d\_state = env.observation\_space.shape\[0\]  
print(d\_state)  
d\_action = env.action\_space.shape\[0\]  
print(d\_action)  
for t in range(10):  
  start = env.reset()  
  observation = start  
 while True:  
 \#  time.sleep(1./60.)  
    action = env.action\_space.sample()  
    obs = observation  
    observation, reward, done, info = env.step(action)  
 if done:  
      alive = 0  
 else:  
      alive = 1  
    replay\_buffer.add\_sample(np.concatenate((obs\[:d\_state\],action,observation\[:d\_state\],\[alive\],\[reward\])))  
 if done:   
 break  
print(replay\_buffer.\_\_len\_\_())  
env.close()

&amp;#x200B;

 

env = (gym.make(environment\_name))  
obs = env.reset()  
cum\_reward = 0  
episode\_counter = 1  
for i in range(int(1e6)):  
    obs,reward,done = environment\_step(env,obs)  
    cum\_reward += reward  
    agent\_training(replay\_buffer, epochs = 1)  
 if done:  
 print('Episode number',episode\_counter)  
 print('Total reward of episode:', cum\_reward)  
      cum\_reward = 0  
      env = (gym.make(environment\_name))  
      obs = env.reset()  
      env.render()",reinforcementlearning,durotan97,False,/r/reinforcementlearning/comments/i1tv2o/soft_actor_critic_not_working_great/
Does changing a neural network void all of the previous successful hyperparameters?,1596294014,"In RL, there are a lot of parameters one can fiddle with to where it becomes a guessing game what combination in the end achieves an optimal policy. E.g. with the epsilon greedy function alone one can alter so much: the type of function (linear, exponential, etc.), what starting and ending values and the decay rate. So if I hypothetically have a DQN algorithm that learns well and if I wanted to try out a new neural network, but keep all the hyperparameters the same (especially the learning rate), would I usually need to start the guessing game again or is it expected that the hyperparameters would also work well without adjustment for the new neural network?",reinforcementlearning,math7878,False,/r/reinforcementlearning/comments/i1tjpj/does_changing_a_neural_network_void_all_of_the/
Procedural Generation of Environment with PyTorch,1596290998,"Hello everybody,

I am currently  trying to create procedurally generated environments to train agents on MiniGrid Envs (MiniGrid Link: [https://github.com/maximecb/gym-minigrid](https://github.com/maximecb/gym-minigrid))

A useful resource that I found on the topic is the following:

[https://github.com/facebookresearch/impact-driven-exploration](https://github.com/facebookresearch/impact-driven-exploration)

Any help on the Topic would be highly appreciated.",reinforcementlearning,datascguy,False,/r/reinforcementlearning/comments/i1ssi2/procedural_generation_of_environment_with_pytorch/
Can Q-learning/policy gradient methods be used for contextual bandit problems?,1596236144,"I was under the impression that various algorithms like q-learning, actor critic, etc. could also be applied to contextual bandit problems. Is this true and if so, why would you use these over UCB or thompson sampling?",reinforcementlearning,shrekbehindu,False,/r/reinforcementlearning/comments/i1hgdr/can_qlearningpolicy_gradient_methods_be_used_for/
Role of discount factor depending on rewards sparsity,1596233440,"I am wondering about the role of the discount factor in Q-learning  kind of algorithms. I usually see 0.99 as value adopted for discounting. To me the choice of such hyperparameter would require a little bit of tuning, but it can in general depends on the sparsity of the reward function.

For instance, in an episodic environment with sparse rewards you would delay the effect of your action and adopt a high discount value, while in a continuing environment where the rewards is not sparse you would choose a much lower value. Does this make sense?  I am working on financial trading applications and I realized that values lower than 0.7 work better than higher values, so I am trying to make sense out of it. In my environment in fact the reward is not sparse at all.",reinforcementlearning,alebrini,False,/r/reinforcementlearning/comments/i1gpin/role_of_discount_factor_depending_on_rewards/
"""HO2: Data-efficient Hindsight Off-policy Option Learning"", Wulfmeier et al 2020 {DM}",1596221703,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i1d8yo/ho2_dataefficient_hindsight_offpolicy_option/
Research in RL: Determining network architectures and other hyper-hyperparameters,1596203721,"When reading papers, often details regarding exact network architectures and hyperparameters used for learning are relegated to tables in the appendix. 

This is fine for determining how researchers got their **results**. However, they very rarely indicate HOW they went about finding their hyperparameters, as well as their *hyper*\-hyperparameters, such as network architectures (number and sizes of layers, activation functions, etc). 

At some level I suspect lots of optimization and experimentation was done for network architectures, since often the values used seem totally arbitrary (numbers like ""90"" or ""102""). I understand if the architectures are copied over directly from reference papers, like ""using the architecture from the SAC paper"". However, this is an issue if this level of optimization is not done equally for baselines that are being compared to. If network architecture etc is optimized for the proposed method, and then that same network architecture is just re-used or slightly modified to accomodate the baseline methods, then those baseline methods were not really afforded the same optimization budget, and the comparison is no longer fair.

Should researchers be reporting their process for choosing network architectures, and explicitly detailing how they made sure comparisons to baselines were fair? 

How do **you** determine the network architecture to use for your experiments?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/i17wry/research_in_rl_determining_network_architectures/
Methods to solve belief-state MDPs and alternative approaches?,1596190063,"What I'm referring to exactly is the [alternate formulation of a POMDP.](https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process#Belief_MDP)


I have this POMDP with a huge observation space 10^25. (Football/soccer matches to be precise), the action space is small (can be as small as 3-20 actions each time step) and the state is unobservable due to the uncertainty in team abilities and obviously the randomness in match outcomes. We can however make some assumptions about the state i.e. the teams abilities (how good/bad they are).


A few people have already successfully set a benchmark for this in [this paper](https://eprints.soton.ac.uk/340382/1/fantasyFootball2012cr.pdf)
.
They used the belief-state formulation and made use of the [Bayesian Q-learning](https://www.aaai.org/Papers/AAAI/1998/AAAI98-108.pdf) algorithm to solve the underlying MDP.

I am looking into alternative approaches but I am struggling for ideas. Mostly this problem is complicated due to the complex state representation. Can anyone think of some alternate directions I could pursue for this?

Many thanks in advance.",reinforcementlearning,gauzah,False,/r/reinforcementlearning/comments/i1501x/methods_to_solve_beliefstate_mdps_and_alternative/
Errors when using a DQN for the FrozenLake openai game,1596185435,"Hey everyone, I am trying to make a DQN algorithm work for the FrozenLake-v0 game but am getting errors. I know that a DQN is probably an overkill but I would really like to get this to work. Here is the code:

    import gym
    import numpy as np
    import tensorflow as tf
    
    env = gym.make(""FrozenLake-v0"")
    
    n_actions = env.action_space.n
    input_dim = env.observation_space.n
    model = tf.keras.Sequential() 
    model.add(tf.keras.layers.Dense(64, input_dim = input_dim , activation = 'relu'))
    model.add(tf.keras.layers.Dense(32, activation = 'relu'))
    model.add(tf.keras.layers.Dense(n_actions, activation = 'linear'))
    model.compile(optimizer=tf.keras.optimizers.Adam(), loss = 'mse')
    
    def replay(replay_memory, minibatch_size=32):
        minibatch = np.random.choice(replay_memory, minibatch_size, replace=True)
        s_l =      np.array(list(map(lambda x: x['s'], minibatch)))
        a_l =      np.array(list(map(lambda x: x['a'], minibatch)))
        r_l =      np.array(list(map(lambda x: x['r'], minibatch)))
        sprime_l = np.array(list(map(lambda x: x['sprime'], minibatch)))
        done_l   = np.array(list(map(lambda x: x['done'], minibatch)))
        qvals_sprime_l = model.predict(sprime_l)
        target_f = model.predict(s_l) 
        for i,(s,a,r,qvals_sprime, done) in enumerate(zip(s_l,a_l,r_l,qvals_sprime_l, done_l)): 
            if not done:  target = r + gamma * np.max(qvals_sprime)
            else:         target = r
            target_f[i][a] = target
        model.fit(s_l,target_f, epochs=1, verbose=0)
        return model
    
    n_episodes = 500
    gamma = 0.99
    epsilon = 0.9
    minibatch_size = 32
    r_sums = []  
    replay_memory = []
    mem_max_size = 100000
    
    for n in range(n_episodes): 
        s = env.reset()
        done=False
        r_sum = 0
        print(s)
        while not done: 
            qvals_s = model.predict(s.reshape(16))
            if np.random.random() &lt; epsilon:  a = env.action_space.sample()
            else:                             a = np.argmax(qvals_s); 
            sprime, r, done, info = env.step(a)
            r_sum += r 
            if len(replay_memory) &gt; mem_max_size:
                replay_memory.pop(0)
            replay_memory.append({""s"":s,""a"":a,""r"":r,""sprime"":sprime,""done"":done})
            s=sprime
            model=replay(replay_memory, minibatch_size = minibatch_size)
        if epsilon &gt; 0.1:      epsilon -= 0.001
        r_sums.append(r_sum)
        if n % 100 == 0: print(n)


And the errors I am getting are:

    Traceback (most recent call last):
      File ""froz_versuch.py"", line 48, in &lt;module&gt;
        qvals_s = model.predict(s.reshape(16))
    ValueError: cannot reshape array of size 1 into shape (16,)

And when I try to then change `qvals_s = model.predict(s.reshape(16))` to `qvals_s = model.predict(s.reshape(1))` I get the error:

    ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 16 but received input with shape [None, 1]


I'd appreciate any help!",reinforcementlearning,math7878,False,/r/reinforcementlearning/comments/i146ai/errors_when_using_a_dqn_for_the_frozenlake_openai/
DQN learn every step or episode?,1596158001,"Hey folks, I just started with Reinforcement Learning and am using DQN for an environment that I designed. It has a natural start and end point (episodic) and discrete actions. I am trying to understand how people ""ususally"" do things with respect to updating the weights of the action network. Specifically, I wonder if it is updated

a) every step?  
b) or every episode?  


Let's say that in my case, every episode has 5-20 steps when picking the most optimal route or picking random routes with high epsilon (&gt;0.9). This increases up to a 100 if you have low epsilon (&lt;0.1) and are stuck taking wrong/poor actions.",reinforcementlearning,puddle-aged,False,/r/reinforcementlearning/comments/i0yc39/dqn_learn_every_step_or_episode/
Making an AI similar to alphastar AI,1596157789,"Oct last year the alphastar ai made by deepmind used multi agent reinforcement learning to achieve competitive levels of gameplay in popular RTS game StarCraft ii. 

When talking about computer beating humans in games created by humans, board games such as chess, go have been the stepping stone, the next step which many ai failed to do was RTS genre of video games. 


Deep mind build an AI from the ground up which beat the best human players in Starcraft ii.

RTS games are different in the sense they require real time decision making with many a tools and variables at hand. The complexity at which the best human player operates at cannot be hard-coded and it was easy in the past to exploit a certain things AI tend to do.

With alphastar it changed, it was build upon its predecessor AlphaGo which beat the best human Go player from south korea who was considered a prodigy on its own.

(https://youtu.be/WXuK6gekU1Y)[link]



My question is can we replicate what's alphastar ai has achieved for broader RTS games such as age of empires 2 de which operates on the similar complexity levels as StarCraft ii.



And how can we build it from ground up using the same model?



https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning",reinforcementlearning,cookieongo,False,/r/reinforcementlearning/comments/i0ya5f/making_an_ai_similar_to_alphastar_ai/
Action range in SAC,1596153679,"How could I set an action range like (0, 1) since SAC uses the squashing function and returns actions in range (-1, 1), in the most implementations that I saw the authors used an action scaler, but I don't know if this works in my case since that it's not a box shaped action space...
Has anyone used action space like that or have any idea of how to do that?",reinforcementlearning,hidden-7,False,/r/reinforcementlearning/comments/i0x80e/action_range_in_sac/
Contraction Mapping Proof,1596117415,"Could anyone help with explaining the following contraction mapping proof ?

&amp;#x200B;

https://preview.redd.it/zxbjr3ax20e51.png?width=867&amp;format=png&amp;auto=webp&amp;s=f1c7d99f6874ea55fc3f8480e220204fb080a54c",reinforcementlearning,promach,False,/r/reinforcementlearning/comments/i0mh7s/contraction_mapping_proof/
How are you parallelising / better utilising your computation?,1596113005,"I've managed to set up my environment and get (tabular) Q-Learning / DQN etc. working well for my problem; however, the code is all single-threaded and mainly CPU intensive. This seems quite wasteful.

I have an 8-core i7, 2080 Ti and 32GB RAM (i.e. single desktop) and I want to utilise this as efficiently as possible, and was thinking to parallelise the code / shift the compute to GPU.

1) How are you splitting your problem across the CPU cores? 

* I was thinking that, given I need to repeat an experiment e.g. 8 times (to get a good understanding of the algorithm's performance), then I can just do one repetition per core.
* Another method could be to simply generate more agent experiences with the environment in a manner similar to Distributed PER (as far as I understand, although they seem to use multiple single-threaded CPUs instead) - is this a better way to ""parallelise"" (or at least more efficiently utilise the hardware)?

2) My state is currently a numpy array (maybe 2000 rows x 1000 columns x 3 channels). Is there much benefit in shifting this to the GPU? Nvidia has done this with Atari ([https://arxiv.org/abs/1907.08467](https://arxiv.org/abs/1907.08467)) and seen significant speed-ups, but I'm not really familiar with this.",reinforcementlearning,LearnAgentLearn,False,/r/reinforcementlearning/comments/i0lds3/how_are_you_parallelising_better_utilising_your/
What exactly is a baseline in RL?,1596107432,"Emma Brunskill explained the idea of a baseline as sort of an unbiased estimator for calculation of the loss function in the policy gradient methods such as PPO. 

However, I'm not fully sure I understand the intuition and general reasoning behind why we use a baseline. 

Would appreciate a response on the understanding &amp; intuition behind a baseline.

Thanls.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/i0k6r3/what_exactly_is_a_baseline_in_rl/
How would you validate value function estimation?,1596104500,"Hi all!
I'm implementing something similar to [MVE](https://arxiv.org/abs/1803.00101) and want to validate my implementation before I delve into hyperparameter search.

Validating the transition model is easy - I just record a horizon long s, a, s' transitions, and compare the s' with the ones predicted by my model.

What I'm not sure about is how to go about validating the value function's pedictions. Essentially I'm bootstrapping to train the value function. On the other hand, for validation I thought of somehow getting a Monte-Carlo estimate of the value function by somehow unrolling trajectories.

Any ideas/thoughts would be very much appreciated!",reinforcementlearning,yardenaz,False,/r/reinforcementlearning/comments/i0jmiw/how_would_you_validate_value_function_estimation/
Autoregressive Discretization in reinforcement learning?,1596100816,"Hi, I watched lecture 2 of the Sergey Levine's Deep Learning course and I learned about Autoregressive Discretization, i.e. a way of discretizing continuous actions into discrete actions avoiding an exponential explosion in the action space size.  


Can you point me to some key papers/works in which this technique is applied in Reinforcement Learning?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/i0iyf5/autoregressive_discretization_in_reinforcement/
Why not explicitly model the probabilities in q-learning updates using your specified exploration strategy?,1596091378,"Hey, potentially dumb question.

Say we're doing q-learning with eps greedy exploration. Recall that the Q-value of (s, a) is the expected return of interacting with the environment given that we are currently in s and do a. Why is it that our updates push the Q-values towards r + max\_a' Q(s', a') where s' is the next state, ie, assuming that we act greedily, instead of explicitly modelling the probabilities induced by epsilon greedy search?",reinforcementlearning,OriginalMoment,False,/r/reinforcementlearning/comments/i0h9nm/why_not_explicitly_model_the_probabilities_in/
PPO (Proximal Policy Optimization) in Pong,1596057233,"hey all,

i recently made a [video](https://www.youtube.com/watch?v=5DI7jpg2mHw) on how i was able to implement PPO to train an agent to outplay the opponent in a game of Pong. I'd really appreciate if you could check it out and give feedback on how I could improve my approach? The learned policy has some flaws (although its chances of winning are well over 99.9%) and would appreciate how I could fix them (I mention the flaws in the video).

Here's the [code](https://github.com/QasimWani/policy-value-methods)mentioned in the video, would appreciate a star haha. have been working hard on this for long.

Once again, i make these videos to help myself understand DRL better. This subreddit has been really helpful and I wanted to sort of share my knowledge and understanding based on what I learned so far.

Appreciate it,

Q.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/i08w30/ppo_proximal_policy_optimization_in_pong/
"""SHIFTT: Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text"", Hill et al 2020 {DM} (plugging BERT in)",1596040307,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i03olv/shiftt_human_instructionfollowing_with_deep/
"""LangLfP: Grounding Language in Play"", Lynch &amp; Sermanet 2020 {G} (plugging self-supervised language models into robots)",1596040153,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i03n1r/langlfp_grounding_language_in_play_lynch_sermanet/
"""WordCraft: An Environment for Benchmarking Commonsense Agents"", Jiang et al 2020",1596039376,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/i03f4x/wordcraft_an_environment_for_benchmarking/
Stable baselines pretraining soft actor-critic model,1596016072,"Hello, 

I am using Stable Baselines to pretrain a SAC model. To my understanding this changes the problem into a supervised learning problem where the actor tries to mimic the expert actions. However, the critic networks (two Q-networks and a value network for this implementation) also change as a result of pretraining. Could anyone help me understand how these are changed based on the set of expert demonstrations?",reinforcementlearning,Zequator,False,/r/reinforcementlearning/comments/hzxw6o/stable_baselines_pretraining_soft_actorcritic/
Knapsacks with Variable Number of Items,1595996319,"Hi everyone!

I’m thinking through how to formulate the knapsack problem and I was hoping you fine people could help.

1. I have a clear idea of what the environment could be for the normal knapsack problem, but are there any agent algorithms that would be suited to a task like this with such a short episode?

More importantly...

2. In the case where the knapsack can have varying lengths, how would you represent that state in a consistent way? I’d like to use NN’s to do this, and I’ll need a consistent input for those. Maybe feed each item into an RNN and come out with the state representation that way?

Would love to hear any thoughts or see any examples that you think might be relevant. Thanks!",reinforcementlearning,MaceGrim,False,/r/reinforcementlearning/comments/hzu073/knapsacks_with_variable_number_of_items/
What is state of the art in off-policy RL?,1595994843,"I'm looking for an algorithm - can be model free or model based, but has to be off-policy (I have expert rollouts).  Any ideas?",reinforcementlearning,ADGEfficiency,False,/r/reinforcementlearning/comments/hztnm8/what_is_state_of_the_art_in_offpolicy_rl/
How many times should I repeat an algorithm to estimate the mean/median reward etc.?,1595973238,"It's well known that many RL algorithms have high variance, e.g. due to methods such as epsilon greedy exploration. Therefore, people recommend to repeat these algorithms many times to get a good estimate of the mean/median rewards etc.

But how many times is enough? Is there a way to estimate the statistical power or something?

I'm also wondering, e.g. if in my problem, an average episode lasts 100 steps. If I use epsilon=0.05, then I expect roughly 5 exploratory moves per episode. Can I use this kind of knowledge to help me determine how many times I need to repeat my algorithms?

Can someone point me in the right direction please? Thanks!",reinforcementlearning,LearnAgentLearn,False,/r/reinforcementlearning/comments/hznxvo/how_many_times_should_i_repeat_an_algorithm_to/
An Introductory Reinforcement Learning Event,1595970689,"Posting for my company...The event is free/online...The speaker is legit (company's co-founder). He's a real scholar in the field and he actually teaches RL courses at Columbia. You might need to get used to his accent tho...

Anyways, thx for letting me post this.

RSVP: [https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-113849695504?aff=r](https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-113849695504?aff=r)",reinforcementlearning,oyolim,False,/r/reinforcementlearning/comments/hzn6a1/an_introductory_reinforcement_learning_event/
RL in Minecraft - are DQNs overkill for this task?,1595958304,"I saw [this](https://www.youtube.com/watch?v=q5iislMQwtA) demo where they use an RL agent in Minecraft to go through a simple lava maze using a DQN and decided to replicate it, but figuring out the trade-off between exploration and exploitation has been a nightmare, but the more I work on it I realised are DQNs overkill for this task? 

Since there are &lt; 70 states or so (each block in the grid x 4 directions), won't a DQN quickly overfit to particular states at the start? Will this always be a problem or it's just due to bad hyperparameters?",reinforcementlearning,guyi567,False,/r/reinforcementlearning/comments/hzj7p1/rl_in_minecraft_are_dqns_overkill_for_this_task/
Resources for implementing MDPs in TensorFlow?,1595955200,"I'm very new to RL, however, it notionally looks like a good fit for my research goals. 

In industry, I worked primarily in logistics; I'd frequently use linear programming, genetic algorithms, etc to optimize shipping/routing. I changed gears in grad school, focusing efforts on NLP and recommender systems, primarily. A professor recently introduced me to some [white papers](https://github.com/OptMLGroup/VRP-RL) that combine NN architectures (such as encoder-decoder RNN with an attention layer) for VRP and TSP (vehicle routing problem and traveling salesman problem, respectively.)  This is something I want to work towards.

I can follow their RNN architecture; where I get lost was understanding how they turned their ""business problem"" into a markov decision process, and learned from it. 

My questions are:

\- Is a framework like Open AI Gym required for any deep RL? 

\- (If not) how can an MDP be implemented in TensorFlow? (Simple if/else control flow or specific design?)

\- (Continued from above) Whether by policy gradients, Q-learning, etc how can an MDP policy be optimized by TF? 

Thanks in advance!",reinforcementlearning,jbuddy_13,False,/r/reinforcementlearning/comments/hzi82q/resources_for_implementing_mdps_in_tensorflow/
Need help for reinforcement learning,1595953105,"hello, I am CS student who knows advanced math (such as functional analysis) and who wants to make research in reinforcement learning, but unfortunately, I am a starter and need help to learn things in this field and in mathematics. Unfortunately, no one in my university is interested in reinforcement learning and so they can't help me. My question is do you know any school(especially free) or university or some other educational center that teaches you how to make research in this field?(if not RL then CV or NLP is okay too) or are there any useful links or resources that can help me to study by myself?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/hzhkta/need_help_for_reinforcement_learning/
Question on Markov chains example calculation,1595937956,"Could anyone help to explain how to obtain the probabilities from days 2 onward ?  


https://preview.redd.it/pktxyxsb9ld51.png?width=878&amp;format=png&amp;auto=webp&amp;s=1bc10631e174a2b3465a173c1bfc36fa2ab3200e",reinforcementlearning,promach,False,/r/reinforcementlearning/comments/hzdh2c/question_on_markov_chains_example_calculation/
Looking ressources for PhD offers,1595926203,"Hello,

I'm currently ending my master's degree and I would like to continue with a PhD, mostly looking for RL. 

I wanted to know if there was websites or listing mails specialized in RL to look for. I've already looked for websites but they are more general. 

Thank you",reinforcementlearning,Krokodeale,False,/r/reinforcementlearning/comments/hzb5md/looking_ressources_for_phd_offers/
How to utilize both Cross-EntropyLoss and BLEU(no-differentialloss) using RL algorithms?,1595904576,"While creating systems like text generation or dialogue seq2seq how can both the Cross-Entropy and RL loss be used for generation? 

At the word/sentence level?",reinforcementlearning,KevinisChang_,False,/r/reinforcementlearning/comments/hz6mir/how_to_utilize_both_crossentropyloss_and/
Practical RL,1595895549,"Close to finishing Udacity's nanodegree on DRL. Where can I use this knowledge &amp; skills to solve real world problems?

So far, my experience with RL has been through Gym (OpenAI) and Unity environments. Would love some practical use-cases of this technology.

Thanks.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hz4a10/practical_rl/
How to decide on size of neural network on DQNs?,1595876055,"I am applying double Q networks on a game. How do you choose your network? (number and types of layers etc). For example my input is like an image, so I start with convolutional layers and then dense layers. But how would you decide how many of them? Thanks!",reinforcementlearning,asrvnon,False,/r/reinforcementlearning/comments/hyycwj/how_to_decide_on_size_of_neural_network_on_dqns/
Variance of a (gaussian) state value function,1595874312,"Hi everyone! 

I've been wondering, if you know of significant papers / implementations where a state-value function was modeled by a neural network with (state-conditional) mean and variance (of a gaussian). 

I know of few, but couldn't find very convincing examples of this outside of distributional RL where the focus seems to be slightly different from what I mean. I am mainly interested in the variance of a state-value as a measure of uncertainty, not the distribution of Q-Values over actions. 

What I know so far are 

\-the GAE paper: where the variance essentially modelled as the MSE and therefore not state conditional

\- STEVE : here the variance was modelled as the variance across ensemble models, which seems to me rather costly computationally

\- Udluft / Hans, Efficient Uncertainty Propagation for Reinforcement Learning with Limited Data: Here the focus seems to be on deriving the Bellman Equations for variances, but I couldn't find good information on how their value function was in fact modelled 

Apart from this I have seen several others with GP, but could not find convincing implementations of NN-Value Estimators. I'd be happy to hear some input and also opinions on this !

&amp;#x200B;

Cheers, anyboby",reinforcementlearning,anyboby,False,/r/reinforcementlearning/comments/hyxsit/variance_of_a_gaussian_state_value_function/
Troubles starting with sonnet/tensorflow,1595862267,"Hi,

I'm quite new to tensorflow/sonnet and was having some issues with implementing a test problem in deepmind's acme framework. I tried asking this query in stack overflow as well but no success. So, thought I'll ask it here. 

While running the agent/environment loop, the network is trained and saved at checkpoints. Acme uses [sonnet](https://github.com/deepmind/sonnet) to create the network. Post training, I tried saving the network by following the method described  [here](https://github.com/deepmind/sonnet#tensorflow-saved-model) , but I cant figure out how to regenerate the trained network with the weights from the loaded model.

In the walk through, they describe the loaded object as not a Sonnet module but a container object that has the specific methods and properties but I'm too inexperienced to make out anything from it.",reinforcementlearning,vishnukmd7,False,/r/reinforcementlearning/comments/hytx2r/troubles_starting_with_sonnettensorflow/
What makes Soft Actor-Critic off-policy?,1595858602,"Why is Soft Actor-Critic off-policy? It seems [in the paper](https://arxiv.org/pdf/1801.01290.pdf) that when they derive the soft actor-critic all the expectations are taken according to the state transition distribution for state expectations and according to a fixed policy pi for actions. However, when they move onto using function approximations the expectations are taken with respect to a replay-buffer -- this does not seem to correspond to how the algorithm was derived as you could be sampling from the replay buffer from a trajectory that was created with some previous iteration of the policy?",reinforcementlearning,DefinitelyNot4Burner,False,/r/reinforcementlearning/comments/hysu0a/what_makes_soft_actorcritic_offpolicy/
Model-based RL for Atari,1595850870,,reinforcementlearning,_djab_,False,/r/reinforcementlearning/comments/hyqvcf/modelbased_rl_for_atari/
Difference between Bayes-Adaptive MDP and Belief-MDP?,1595840298,"Hi guys,

I have been reading a few papers in this area recently and I keep coming across these two terms. As far as I'm aware Belief-MDPs are when you cast a POMDP as a regular MDP with a continous state space where the state is a belief (distribution) with some unknown parameters.

How is the Bayes-adaptive MDP (BA-MDP) different to this? 

Thanks",reinforcementlearning,gauzah,False,/r/reinforcementlearning/comments/hyoqf5/difference_between_bayesadaptive_mdp_and_beliefmdp/
how to find the best seed value for your libraries?,1595826160,"So, when using gym, torch, and np, how do you go about finding the best seed value? do you just perform grid-search with some set of seed values, or some other form of tuning? Or are there a set of seed values that are known to give good results?

I'm saying this because I've been training the LunarLander agent using REINFORCE algorithm, and nearly 4000 episodes later, the average reward is still \~10 (it keeps fluctuating, so ik my model isn't performing). Besides the seed value, what else should I optimize for/tune?

Would really appreciate if you could look at my [code](https://github.com/QasimWani/policy-value-methods/blob/master/REINFORCE/REINFORCE.ipynb) and help me out as to how I can make it better. 

Thanks.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hylz8v/how_to_find_the_best_seed_value_for_your_libraries/
New RL library for PyTorch,1595813568,,reinforcementlearning,iffiX-,False,/r/reinforcementlearning/comments/hyj2np/new_rl_library_for_pytorch/
Why does impala stuck doing one set of choices?,1595806108,"I'm using RLib with only one worker. The default setting are here [https://docs.ray.io/en/master/rllib-algorithms.html#impala](https://docs.ray.io/en/master/rllib-algorithms.html#impala). After around 5000 steps my agent stop doing anything new and repeat itself. DQN has succeed at getting good scores. I try adding buffer and also increasing  entropy\_coeff. While  increasing  entropy\_coeff  does delay it, it eventually falls back to the repeated behavior. Any suggestion is appreciated.",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/hyh678/why_does_impala_stuck_doing_one_set_of_choices/
"""The Overfitted Brain: Dreams evolved to assist generalization"", Hoel 2020",1595775711,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hy8dpr/the_overfitted_brain_dreams_evolved_to_assist/
How are you implementing hyperparameter (grid) search?,1595770495,"I currently want to essentially do a gridsearch / sweep over a bunch of hyperparameters (so I get a good feel of how hyperparameters are affecting my problem domain / inspect how it affects the agent's learning).

Clearly, one way I can do this is through just using lots of for loops, but it makes the code really messy when I have a lot of hyperparameters. How are you all implementing this type of search? I was thinking of having a .csv/.xls to read in the different settings I want, but not sure if there's a better way.",reinforcementlearning,LearnAgentLearn,False,/r/reinforcementlearning/comments/hy70kx/how_are_you_implementing_hyperparameter_grid/
Setting up RL environment on Windows,1595768339,"Hi. Does anyone here know a guide on setting up everything on Windows. I know Linux would be ideal, but I would like to try with Windows first.",reinforcementlearning,themad95,False,/r/reinforcementlearning/comments/hy6ij5/setting_up_rl_environment_on_windows/
[AI application] Python implementation of Proximal Policy Optimization (PPO) algorithm for Super Mario Bros. 29/32 levels have been conquered,1595766727,,reinforcementlearning,1991viet,False,/r/reinforcementlearning/comments/hy65gb/ai_application_python_implementation_of_proximal/
Cliff Walking Q-Learning,1595763399, [https://github.com/gokseltokur/Cliff-Walking-Q-Learning](https://github.com/gokseltokur/Cliff-Walking-Q-Learning),reinforcementlearning,goekue,False,/r/reinforcementlearning/comments/hy5hnh/cliff_walking_qlearning/
Any RL framework suggestions? Which RL framework do you use?,1595741059,I used tf-agents but that kinda sucks.,reinforcementlearning,spenceowen,False,/r/reinforcementlearning/comments/hy1cwy/any_rl_framework_suggestions_which_rl_framework/
Which algorithm to use?,1595715017,"Hi guys, I am looking for a bit of insight and would like help figuring out which algorithm to use. It seems like almost everyday I run into a new algorithm I haven't heard before that surpasses all previous deep-RL algorithms, and it doesn't seem clear to me as to which one I should choose. I have implemented DQN, Double-Q and Dueling Networks and I know there should be much better algorithms that have come out, but I can't find a simple list that compares all of them. I would like to have an algorithm that can run on a single machine and can learn to play games.",reinforcementlearning,undercover_intern,False,/r/reinforcementlearning/comments/hxv9we/which_algorithm_to_use/
Having a hard time using RLglue,1595694593,"Total beginner to Python and RL 
Any tips to ease the learning curve???",reinforcementlearning,tusharkulkarni95,False,/r/reinforcementlearning/comments/hxpjsk/having_a_hard_time_using_rlglue/
PETRL - People for the Ethical Treatment of Reinforcement Learners,1595633857,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/hxciz0/petrl_people_for_the_ethical_treatment_of/
[R] Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?,1595626457,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/hxakx0/r_is_a_good_representation_sufficient_for_sample/
How can I use DDPG to predict two actions?,1595613892,"I'm trying to train an agent to move in a 2D environment. At each step the agent needs to decide what direction to move in and how far.

I'm considering DDPG since these are continuous action spaces. Can it be done and how?

Edit: RL beginner here",reinforcementlearning,Jelmazmo96,False,/r/reinforcementlearning/comments/hx6pwo/how_can_i_use_ddpg_to_predict_two_actions/
Model-based Reinforcement Learning: A Survey,1595611575,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/hx5z8a/modelbased_reinforcement_learning_a_survey/
Activation Functions in Deep RL (PPO),1595607435,"Hey everybody, I have been playing with a PPO agent with an actor-critic model lately on custom environments and noticing the significance of using different activation functions. I have been flipping through ReLU, Tanh, and Sigmoid. On all training sessions, Sigmoid performs the best... and to be frank, I'm not sure why. Also, when I let this Sigmoid-activated agent train for longer times, I end up seeing it converge to an 'optimal' policy and then drop out to give lowest reward along with NaN losses. Is there a method in selecting activation functions for custom environments? And, what could be the reason for these immediate fallouts in rewards in longer training sessions?",reinforcementlearning,big-waves-r-us,False,/r/reinforcementlearning/comments/hx4obj/activation_functions_in_deep_rl_ppo/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595571738,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hwwpil/simple_learning_to_play_atari_with_only_2_hours/
Advice on rainbow dqn parameters,1595567665,"I'm using Ray RLlib for reinforcment. My environment is infinite, the agent take coins so the reward is +1 and once it misses it get -1 and reset. Agent scores 0 when it does neither. The the episode score can go beyond 100. My issue is with there parameters  v\_min, v\_max, num\_atoms, n\_step. What ranges should I try. Every time I try a value it never converges compared to normal dqn. 

I tried

v\_min=-1,  v\_max=1, num\_atoms=51, n\_step=3, noisy=True",reinforcementlearning,blue20whale,False,/r/reinforcementlearning/comments/hwvu8e/advice_on_rainbow_dqn_parameters/
Prioritized Experience Replay - the paper!,1595553946,"Hey,  
I recently made a 2-part [video](https://www.youtube.com/watch?v=yXGkAqAwUMQ&amp;list=PLGSbYGhGpgY47-t5rLcl13mCC8fhSoeTx&amp;index=4) on Prioritized Experience Replay. I make these videos to help myself understand these ideas better. I discussed this using a paper on this approach in the second part of the [video](https://www.youtube.com/watch?v=cQtGJDS-Rfs&amp;list=PLGSbYGhGpgY47-t5rLcl13mCC8fhSoeTx&amp;index=5). I'd really appreciate if you guys have any feedback and if my explanation makes sense overall.

If you have any ideas on what video I should make next, please lmk. Really appreciate if you could subscribe to my YT channel, just trying to explain cool stuff haha.

Cheers.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hwsizi/prioritized_experience_replay_the_paper/
The problem with Q-learning | Double Q-Learning,1595536163,"Hey,  
I recently made a [video](https://youtu.be/T8Q0EVIcX2s) on Double Q learning. I make these videos to help myself understand these ideas better. I discussed this using a paper on this algorithm. I'd really appreciate if you guys have any feedback and if my explanation makes sense overall.

If you have any ideas on what video I should make next, please lmk. Really appreciate if you could subscribe to my YT channel, just trying to explain cool stuff haha.

Cheers.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hwng6p/the_problem_with_qlearning_double_qlearning/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595521874,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hwisrr/simple_learning_to_play_atari_with_only_2_hours/
Memory usage of Training DQN/Rainbow on Atari,1595505226,"Hi, I want to train DQN and Rainbow for Atari, the experience replay size is 1 million, how much RAM are these algorithms need?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/hweb38/memory_usage_of_training_dqnrainbow_on_atari/
Can someone recommend me a good set of research papers that tackle terrain-based path planning using DRL?,1595503639,,reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/hwdzg0/can_someone_recommend_me_a_good_set_of_research/
POMDP to test recurrent polices,1595503378,"Hi everyone,

while implementing different approaches for a recurrent policy based on PPO, I was wondering which environments are suitable for testing the implementation.

So far I'm using cartpole with masked velocity and [Minigrid-Hallway](https://github.com/maximecb/gym-minigrid#memory-environment). What would be a more sophisticated environment as a next step, because the aforementioned environments seem to be a little too easy to be solved. Well, I fear that a faulty recurrent policy implementation can still solve these.",reinforcementlearning,LilHairdy,False,/r/reinforcementlearning/comments/hwdxm6/pomdp_to_test_recurrent_polices/
"Help Debugging using TD3, overwhelming warning/error outputs",1595474209,"Summary: I am attempting to implement RL using soft robotic hardware, skipping the modeling and simulation for various reasons. I have a pneumatic syringe hooked into a Gcode interpreter (hence the x/y notation through my code. I am using ROS as a messaging middle ware between my components.

I have tried to implement some TD3 functionality to get my robot to learn to just move away from its starting point. I will start up (with tons of output), run one ""step()"" from my custom Gym environment, and then spit out an error that I understand at face value, but am having a lot of trouble tracking down.

I have tested my entire system and It seems I have all functionality correct outside of implementing the RL tidbits. I can control the robot, read its sensors/state, and have a ground truth system giving feedback on the robot position for reward calculation. All of this data is being piped corretly into my script and stored in global variables to be used in different scopes throughout my gym and RL environment.

My background is mechanical/electrical engineering. I am new to RL as well as software development on the whole. It looks like this all has a lot to do with the libraries, and I am scared to go and change anything in fear of breaking the whole library or something like that. 

I am unclear where to even start looking for all of these feedbacks, warnings, and errors. any guidance would be much appreciated.

Please critique my post and help me present my issue better if it is not up to par with this sub's standards.

&amp;#x200B;

==========My RL script==========

    #!/usr/bin/env python3
    #you need this above to get ROS to execute the script - points to the python executable
    #use python 3 here due to the RL libraries being written in python 3
    from gym import spaces
    import gym
    import numpy as np
    import time
    import random
    from math import sqrt
    
    # from stable_baselines.td3.policies import MlpPolicy as Td3MlpPolicy
    # # from stable_baselines import TD3
    # from stable_baselines.ddpg.noise import OrnsteinUhlenbeckActionNoise
    # from stable_baselines.common.vec_env import DummyVecEnv 
    
    
    #import necessary stable baselines TD3 libraries for learning purposes
    from stable_baselines import TD3
    from stable_baselines.td3.policies import MlpPolicy as Td3MlpPolicy
    from stable_baselines.ddpg.noise import OrnsteinUhlenbeckActionNoise
    from stable_baselines.common.vec_env import DummyVecEnv
    #import ROS specific libraries and custom message types
    import rospy
    from std_msgs.msg import String
    from std_msgs.msg import Bool
    from std_msgs.msg import Empty
    from soft_robot_learning.msg import sensor_processing
    from soft_robot_learning.msg import gcode_packager
    from soft_robot_learning.msg import apriltag_data
    
    
    #global variables
    testString = """"
    
    xCommand = 0.
    yCommand = 0.
    
    xState = 0.
    yState = 0.
    
    xPosition = 0.
    yPosition = 0.
    
    xZero = 0. #zero values for each episode
    yZero = 0.
    
    action_done = False
    
    
    
    #define constants
    NUM_STEPS = 10
    TIME_PER_STEP = 1 #this could be variable depending on hardware
    
    # define ROS publisher nodes
    cmd_pub = rospy.Publisher('/actuator_commands', gcode_packager, queue_size = 30)
    direct_cmd_publisher = rospy.Publisher('/grbl_commands', String, queue_size = 30)
    grbl_reset_pub = rospy.Publisher('/system_cmd', Empty, queue_size=1)
    #test pub to update the subscribed node. for testing
    testpub = rospy.Publisher('/testData', String, queue_size = 20)
    
    #define ROS subscriber callback methods
    def robot_state_callback(data):
    	#print(""Updating State"")
    	global xState
    	global yState
    	xState = data.xSensor
    	yState = data.ySensor
    	#print(""x sensor: {}"", format(xState))
    	#print(""y sensor: {}"", format(yState))	
    
    def gnd_truth_callback(data):
    	#print(""updating Ground Truth Data"")
    	global xPosition, yPosition
    	xPosition = data.x_pos_gnd
    	yPosition = data.y_pos_gnd
    	#print(""x position: {}"", format(xPosition))
    	#print(""y position: {}"", format(yPosition))
    
    def action_done_callback(data):
    	global action_done
    	action_done = data.data
    
    def testData_callback(data):
    	#print(""test data recieved"")
    	global testString
    	testString = data.data + '\n'
    	
    
    
    #Define ROS subscriber nodes
    def RL_subscribers():
        rospy.init_node('soft_robot_learning', anonymous=True)
        rospy.Subscriber(""/robot_state"", sensor_processing, robot_state_callback)
        rospy.Subscriber(""/gnd_pos_truth"", apriltag_data, gnd_truth_callback)
        rospy.Subscriber(""/action_done"", Bool, action_done_callback)
        rospy.Subscriber(""/testData"", String, testData_callback)
    
        # skipping spin to see if the subscriber works without spinnign whle the thin is running
        #rospy.spin()	
    
    def distanceFromOrigin(x,y,xZero,yZero):
    	xDist = x - xZero
    	yDist = y - yZero
    	total_distance = sqrt(xDist*xDist+yDist*yDist)
    	return total_distance, xDist, yDist
    
    def wait_for_action():
    	global action_done
    	count = 0
    	while not action_done:
    		count = count + 1
    		#print(action_done)
    		
    	print(""Action Complete..."")
    	time.sleep(1)
    
    
    
    def homeGrblController():
    	print(""Homing System...."")
    	direct_cmd_publisher.publish('$H')
    	wait_for_action()
    	direct_cmd_publisher.publish('G92 X0 Y0')
    	print(""Homing Complete"")
    
    def initGrblController():
    	print(""Initializing Grbl System..."")
    	direct_cmd_publisher.publish('$X')
    	homeGrblController()
    
    def resetGrblController():
    	print(""Resetting grbl controller..."")
    	msg = Empty()
    	grbl_reset_pub.publish(msg)
    	time.sleep(4) #wait for reset to take plae
    	initGrblController()
    	print(""grbl controller reset!"")
    
    def homeRobot():
    	print(""Sending Robot Home"")
    	direct_cmd_publisher.publish('G0 X0 Y0')
    	wait_for_action()
    	print(""Robot Home"")
    	
    
    
    class soft_learner():
    	def __init__(self):
    		#init position variables
    		self.testS = """" #test string for testing data passing
    		self.x = 0
    		self.y = 0
    		#init steps and dt (time per step)
    		self.n_steps = 0
    		self.dt = TIME_PER_STEP #1=1second - play with this variable
    		#initialize proper spaces and metadata 
    		#both the action and state space are bounded by 0-100 for %of actuation
    		#mapping these to real world actiona dnnand sensors are handeled in another script
    		self.observation_space = spaces.Box(low=np.array([0.,0.]), high=np.array([100.,100.])) #obs space = continuous, 
    		self.action_space = spaces.Box(low=np.array([0.,0.]), high=np.array([100.,100.]))
    		self.metadata = 0
    
    		#send initial commands to grbl (home, set 0 all that)
    
    	def reset(self):
    		global xZero, yZero, xPosition, yPosition
    		self.x = 0
    		self.y = 0
    		self.n_steps = 0
    		
    		
    		#re calibrate and set x and y zero values for each new run for april tags data
    		xZero = xPosition
    		yZero = yPosition
    
    		#reset grbl controller
    		resetGrblController()
    
    		#run calibration function here
    		return self.x, self.y
    
    	def step(self, generated_cmd_array):
    		global xCommand, yCommand, xPosition, yPosition, xZero, yZero, xState, yState
    		
    		# print(generated_cmd_array)
    		xCommand = generated_cmd_array[0]
    		yCommand = generated_cmd_array[1]
    
    		#generate an action
    		# xCommand = random.randint(0,100)
    		# yCommand = random.randint(0,100)
    		print(""command generated x {} y {}"", format(xCommand), format(yCommand))
    		#v = np.clip(v, self.action_space.low, self.action_space.high)
    		#above line in example not sure why it is used
    
    		#publish action
    		cmd_message = gcode_packager()
    		cmd_message.x_percentage = xCommand
    		cmd_message.y_percentage = yCommand
    		cmd_pub.publish(cmd_message)
    
    		#wait for hardware to complete action
    		wait_for_action()
    
    		#subscribe/read state
    		state = {'x': xState, 'y': yState}
    		print(""xState: {}"", format(xState))
    		print(""yState: {}"", format(yState))
    
    		#compute reward
    		reward, x_calibrated, y_calibrated = distanceFromOrigin(xPosition,yPosition, xZero,yZero)
    		#print(""x position: {}"", format(xPosition))
    		#print(""y position: {}"", format(yPosition))
    		print(""x calibrated: {}"", format(x_calibrated))
    		print(""y calibrated: {}"", format(y_calibrated))
    		print(""reward: {}"", format(reward))
    
    		#increment and finish step
    		self.n_steps += 1
    		done = self.n_steps &gt; NUM_STEPS
    
    		return state, reward, done, {}
    
    
    if __name__ == '__main__':
    	
    	#run suscriber nodes
    	RL_subscribers()
    	print(""Starting..."")
    
    	time.sleep(3) #give ros time to set up
    
    	#init environmnet
    	env = soft_learner()
    	env.reset()
    
    	#run and testing
    
    	a_dim = env.action_space.shape[0]
    	td3_noise = OrnsteinUhlenbeckActionNoise(np.zeros(a_dim), 0.003*np.ones(a_dim)) 
    	td3_env = DummyVecEnv([lambda: env])
    	td3_model = TD3(Td3MlpPolicy, td3_env, verbose=0, action_noise=td3_noise, tensorboard_log='~/tensorboard')
    	td3_model.learn(total_timesteps=100)
    	td3_model.save(""~/td3_model"")
    	print('Complete training TD3')
    	x = td3_env.reset()
    #     for i in range(100): 
    #         x, r, _, _ = td3_env.step(td3_model.predict(x)) 
    #     print(x) 
    #     print(r) 
    	
    
    
    
    
    	
    	# #rospy.spin()
    	# print(""running sensor mount testing"")
    	# count = 1
    	# cmd_message = gcode_packager()
    	# while True:
    	# 	cmd_message.x_percentage = 95
    	# 	cmd_message.y_percentage = 0
    	# 	cmd_pub.publish(cmd_message)
    	# 	wait_for_action()
    	# 	print(""fully inflated..."")
    	# 	cmd_message.x_percentage = 0
    	# 	cmd_message.y_percentage = 0
    	# 	cmd_pub.publish(cmd_message)
    	# 	wait_for_action()
    	# 	print(""Not inflated..."")
    	# 	count =count+1
    	# 	print (count)
    	# 	if (count%10 == 0):
    	# 		resetGrblController()
    
    	# #for i in range(10):
    	# #	time.sleep(1)
    	# #	env.step()

&amp;#x200B;

===========Terminal Feedback==============

    earning/src$ rosrun soft_robot_learning learning.py P-Pavilion-Gaming-Laptop-15-cx0xxx:~/rl/home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      np_resource = np.dtype([(""resource"", np.ubyte, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
    /home/robertslab/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
      np_resource = np.dtype([(""resource"", np.ubyte, 1)])
    Starting...
    /home/robertslab/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: WARN: Box bound precision lowered by casting to float32
      warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
    Resetting grbl controller...
    Initializing Grbl System...
    Homing System....
    Action Complete...
    Homing Complete
    grbl controller reset!
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/td3.py:132: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/policies.py:125: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use keras.layers.flatten instead.
    WARNING:tensorflow:Entity &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d840cbe0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d840cbe0&gt;&gt;: AttributeError: module 'gast' has no attribute 'Num'
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use keras.layers.dense instead.
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
    Instructions for updating:
    Call initializer instance with the dtype argument instead of passing it to the constructor
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff93a7a9c88&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff93a7a9c88&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff93988e4e0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff93988e4e0&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff9183ce748&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff9183ce748&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff93988e4e0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff93988e4e0&gt;&gt;: AttributeError: module 'gast' has no attribute 'Num'
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d840c9b0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d840c9b0&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d840c9b0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d840c9b0&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff9398c6ac8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff9398c6ac8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8477cc0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8477cc0&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8477cc0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8477cc0&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff9398c6ac8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff9398c6ac8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d84b7cc0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d84b7cc0&gt;&gt;: AttributeError: module 'gast' has no attribute 'Num'
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f0b8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f0b8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f0b8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f0b8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d847f550&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d847f550&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d83d0dd8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d83d0dd8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d83d0dd8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d83d0dd8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d83d0940&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d83d0940&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d813f0b8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d813f0b8&gt;&gt;: AttributeError: module 'gast' has no attribute 'Num'
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f048&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f048&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f048&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f048&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f0b8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d813f0b8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/td3.py:165: The name tf.random_normal is deprecated. Please use tf.random.normal instead.
    
    WARNING:tensorflow:Entity &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d813bf98&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Flatten.call of &lt;tensorflow.python.layers.core.Flatten object at 0x7ff8d813bf98&gt;&gt;: AttributeError: module 'gast' has no attribute 'Num'
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d80d87b8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d80d87b8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d80d87b8&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d80d87b8&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d80d8438&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d80d8438&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8110b38&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8110b38&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8110b38&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d8110b38&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:Entity &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d81106a0&gt;&gt; could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting &lt;bound method Dense.call of &lt;tensorflow.python.layers.core.Dense object at 0x7ff8d81106a0&gt;&gt;: AssertionError: Bad argument number for Name: 3, expecting 4
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/td3.py:195: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.&lt;locals&gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
    Instructions for updating:
    Use tf.where in 2.0, which has the same broadcast rule as np.where
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/td3.py:227: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/td3.py:241: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.
    
    WARNING:tensorflow:From /home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1143: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.
    
    Resetting grbl controller...
    Initializing Grbl System...
    Homing System....
    Action Complete...
    Homing Complete
    grbl controller reset!
    command generated x {} y {} 58.89289093017578 14.919524192810059
    Action Complete...
    xState: {} 0.0
    yState: {} 0.0
    x calibrated: {} -0.00041447579860687256
    y calibrated: {} -0.0005988478660583496
    reward: {} 0.0007282918057437168
    Traceback (most recent call last):
      File ""/home/robertslab/rl_workspace_0/src/soft_robot_learning/src/learning.py"", line 244, in &lt;module&gt;
        td3_model.learn(total_timesteps=100)
      File ""/home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/td3/td3.py"", line 331, in learn
        new_obs, reward, done, info = self.env.step(unscaled_action)
      File ""/home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/vec_env/base_vec_env.py"", line 150, in step
        return self.step_wait()
      File ""/home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/base_class.py"", line 1084, in step_wait
        obs, rewards, dones, information = self.venv.step_wait()
      File ""/home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py"", line 47, in step_wait
        self._save_obs(env_idx, obs)
      File ""/home/robertslab/.local/lib/python3.6/site-packages/stable_baselines/common/vec_env/dummy_vec_env.py"", line 90, in _save_obs
        self.buf_obs[key][env_idx] = obs
    TypeError: float() argument must be a string or a number, not 'dict'",reinforcementlearning,csullivan107,False,/r/reinforcementlearning/comments/hw88ve/help_debugging_using_td3_overwhelming/
Deep RL &amp; Industry - Is it a match?,1595456147,"High sample complexity, the need for exploration and the black-box nature of deep neural networks are just some of the high barriers reinforcement learning has to overcome in order to be practical.

Do you think deep RL is mature enough to be used in industry and robotics? 

If not, what is required therefore and how fast can we get there?",reinforcementlearning,ml_keychain,False,/r/reinforcementlearning/comments/hw3fj2/deep_rl_industry_is_it_a_match/
[D] What do you think is missing from RL libraries?,1595437990,,reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/hvxqnc/d_what_do_you_think_is_missing_from_rl_libraries/
"""LPG: Discovering Reinforcement Learning Algorithms"", Oh et al 2020 {DM}",1595436143,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hvx5e9/lpg_discovering_reinforcement_learning_algorithms/
Discount factor for early termination in Dreamer,1595435738,"Hey everyone,

&amp;#x200B;

I am currently trying to implement a modified version of the Dreamer paper ([https://arxiv.org/abs/1912.01603](https://arxiv.org/abs/1912.01603)) with a different model, but the same way to train the policy and value networks.

When using my trained models in environments which run for a set number of steps, e.g. Pendulum, everything works fine, so my overall training process seems to be correct. However, I have massive problems in environments that terminate early if some conditions happen, e.g. Cartpole.

&amp;#x200B;

The paper mentions that ""In tasks with early termination, the world model also predicts the discount factor from each latent state to weigh the time steps in Equations 7 and 8 by the cumulative product of the predicted discount factors, so terms are weighted down based on how likely the imagined trajectory would have ended."", so my model outputs a reward value and a value predicting how likely the episode ended at that point ( lets call it p\_end. I am however unsure how to incorporate those into the value-estimates. The way I understand it, the total value-estimate, which is computed as normal, is just multiplied by the product of all p\_end along the states in that value-estimate. However, that doesnt seem to work at all for me. Other things I have tried, which all were not great, is to always use the product of the reward\*p\_end as the ""real"" reward, and to multiply the estimates by the product of all p\_end BEFORE the state.

&amp;#x200B;

What is the correct way to do this? Can anybody help me here ?",reinforcementlearning,durotan97,False,/r/reinforcementlearning/comments/hvx0tz/discount_factor_for_early_termination_in_dreamer/
AutoML-Zero,1595433037,"[https://towardsdatascience.com/automl-zero-b2e065170941](https://towardsdatascience.com/automl-zero-b2e065170941)

This article discusses  AutoML-Zero, a new variant of AutoML which aims to design everything from scratch by using basic mathematical operations rather than using predefined fancy operations like the ReLU layer,  Convolution layer,  Batch Normalization layer.

Assume that there's no Tensorflow, no PyTorch, just using NumPy to evolve the concepts of Machine Learning.",reinforcementlearning,spenceowen,False,/r/reinforcementlearning/comments/hvw6pj/automlzero/
Unable to use GPU with TF-Agents,1595422909,"Running this simple code takes same time on GPU and CPU, here is the link to colab [https://gist.github.com/saahiluppal/9c9501c92cc2c3e66ba957793b122d8c](https://gist.github.com/saahiluppal/9c9501c92cc2c3e66ba957793b122d8c)  


Am i doing something wrong? Because as far as i know, tensorflow automatically runs on gpu if available.

 Can anyone take a look?",reinforcementlearning,spenceowen,False,/r/reinforcementlearning/comments/hvtdf7/unable_to_use_gpu_with_tfagents/
A new gym RL environment for robotic pick &amp; place,1595404952,"Hey guys,

[https://github.com/PaulDanielML/MuJoCo\_RL\_UR5](https://github.com/PaulDanielML/MuJoCo_RL_UR5)

Thought I would share this gym environment I have been working on for the last few months.

It allows RL agents to interact with a simulated (Mujoco) environment in order to learn pixel-wise predictions of grasp success chances. 

Some main differences to currently available Mujoco gym environments are the more complex observation space (RGB-D images) and the action space (pixels), as well as the fact that a real robot model (UR5) is used. 

I am still working on new iterations and features, but right now you can already get some nice results with it :) 

Would love to see how policy-based methods perform here, as the next iteration will significantly increase the size of the action space. 

Glad about any feedback, cheers",reinforcementlearning,Pau_D,False,/r/reinforcementlearning/comments/hvpqvt/a_new_gym_rl_environment_for_robotic_pick_place/
RL+NLP. Code Implementation flow?,1595389404,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hvmi27/rlnlp_code_implementation_flow/
RL vs Random Search,1595382198,Here is my [2 min article](https://medium.com/@ankurnet1996/how-good-is-random-search-97182fcfc180) showing random search can lead to very good solutions for small problems like Gym CartPole.,reinforcementlearning,dekankur,False,/r/reinforcementlearning/comments/hvknmv/rl_vs_random_search/
Solving LunarLander with linear function approximation possible?,1595366238,"I have implemented semi gradient sarsa with tile coding and linear function approximation. I got really good results in gym's MountainCar and CartPole environments but it seems it's not enough for LunarLander. Is LunarLander even possible with linear approx. or is neural networks or other non-linear means required to solve it? 

I've solved it before with vanilla DQN.",reinforcementlearning,Sroidi,False,/r/reinforcementlearning/comments/hvg35k/solving_lunarlander_with_linear_function/
Is this a good way to think about tensors?,1595350820,,reinforcementlearning,a_ghould,False,/r/reinforcementlearning/comments/hvb5mh/is_this_a_good_way_to_think_about_tensors/
How does the Target Network in Double DQNs find the max Q* value for each action?,1595342449,"Hello,  
I understand the fact that the neural network is used to take the states as inputs, and then it outputs the Q-value for the various state-action pairs. However, in order to compute this Q-value and update the neural network's weights and biases we need to calculate the Max Q\*-value for the next state s'. In order to get that in the DDQN case we input that next state s' in the Target Network: what I'm not clear on is how do we train this second Target Network itself that will help us train the first NN? What is its Cost Function?

Thanks.",reinforcementlearning,Ubermensch001,False,/r/reinforcementlearning/comments/hv8hpr/how_does_the_target_network_in_double_dqns_find/
Machines That Don’t Kill: How Reinforcement Learning Can Solve Moral Uncertainties,1595336344,[https://analyticsindiamag.com/reinforcemenet-learning-moral-dilemma-ethics/](https://analyticsindiamag.com/reinforcemenet-learning-moral-dilemma-ethics/),reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/hv6wgh/machines_that_dont_kill_how_reinforcement/
Advise on how to improve performance and scale up easily,1595329591,"Hi, 
I have been implementing multi agent a2c for the simple spread environment (multiagent particle environment by openai).
I was successful and scaling the model with 3 agents but with a shared network between the actor and critic. However when I moved towards 4 agent case, the number of episodes required for training increased by a lot. I didn't expect this to happen. 

Further, I tried to have two separate networks for the actor and critic to solve the environment and see if it scales well. As the networks are similar to the shared network and there is no change in the hyper parameters (have tried out other hyper parameters but the one that worked for shared layer works better), the environment seems to unsolvable for a single agent as well. The reward function plateaus and there is no improvement in performance whatsoever. This has happened with different set of hyper parameters as well. 

I am wondering if there is a way to scale up the number of agents? 
Also is there anyway to transition from a shared later to a separate nets for both actor and critic?

Any help, suggestion, advise, recommendation? 

Thanks :D",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/hv5gcx/advise_on_how_to_improve_performance_and_scale_up/
How Reinforcement Learning Can Help In Data Valuation,1595325192,,reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/hv4mv0/how_reinforcement_learning_can_help_in_data/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595314018,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hv2m41/simple_learning_to_play_atari_with_only_2_hours/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595313053,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hv2fb7/simple_learning_to_play_atari_with_only_2_hours/
Pytorch starter Code for UC Berkeley's Deep RL Course,1595298896,"UC Berkeley's Deep RL course (available for free online here: http://rail.eecs.berkeley.edu/deeprlcourse/) is a fantastic way to learn deep RL. Their assignments, however, are in tensorflow 1, which generally does not seem to be the most common (or imo intuitive/easy to work with) DL library in the field

In lieu of this I have put together a pytorch version of thier starter code for others that wish to complete the course in pytorch: https://github.com/mdeib/berkeley-deep-RL-pytorch-starter (My solutions are also available in another repository). It is not perfect but it is something I would have found immensely helpful going into this so I am posting it here.",reinforcementlearning,mdeib,False,/r/reinforcementlearning/comments/huz94a/pytorch_starter_code_for_uc_berkeleys_deep_rl/
"""Sharing Pixelopolis, a self-driving car demo from Google I/O built with TF-Lite"", de Andrés-Clavera",1595274621,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/husbm1/sharing_pixelopolis_a_selfdriving_car_demo_from/
What is a good policy that works for one agent?,1595258907,"For my problem there is only one agent, what policy should I try and what policies should I avoid? I'm working with live playing which means the observation I get takes a bit of time to retrieve. Is using off policy in this case better? So far I tried A3C and dqn with prioritized\_replay. A3C seem to be better.",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/hun8yu/what_is_a_good_policy_that_works_for_one_agent/
Model-Based Reinforcement Learning for Atari | Paper Explained,1595240170,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/huitpb/modelbased_reinforcement_learning_for_atari_paper/
Learning to run a power network in a sustainable world - NeurIPS 2020 competition,1595238906,[removed],reinforcementlearning,marota91,False,/r/reinforcementlearning/comments/huilh5/learning_to_run_a_power_network_in_a_sustainable/
SimPLe: Learning to play Atari with only 2 hours of gameplay,1595234253,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/huhsu7/simple_learning_to_play_atari_with_only_2_hours/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595234032,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/huhrez/simple_learning_to_play_atari_with_only_2_hours/
"Observation spaces from Competitive Environments in ""Emergent Complexity via Multi-Agent Competition"" for porting them to pybullet",1595233840,"I'm trying to bring the competitive Environments from [https://arxiv.org/pdf/1710.03748.pdf](https://arxiv.org/pdf/1710.03748.pdf) / [https://sites.google.com/view/multi-agent-competition](https://sites.google.com/view/multi-agent-competition) from the annoyingly stupid to use and expensive MuJoCo to PyBullet.   
Right now I'm struggling with building the respective observations for the agents; I'm not sure what they used in the original environments. Any insights into the issues and/or ideas on what to include (and what not to, to keep observation spaces small) would be appreciated!  
Right now I care primarily about ""Walk to Goal"" and ""Sumo"" - and first of all Ants because it's simpler than humanoid, but again, any insights would be much appreciated!",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/huhq68/observation_spaces_from_competitive_environments/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595229251,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hugxaz/simple_learning_to_play_atari_with_only_2_hours/
SimPLe: Learning to play Atari with only 2 hours of gameplay | Paper Explained,1595226992,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hugi13/simple_learning_to_play_atari_with_only_2_hours/
Practical RL in signal processing,1595204017,"Hey there,

Apart from the application to DRL in robotics and open world games, is there a specific example(s) related towards signal processing where DRL algorithms come in handy? 

I'm saying this because I'm interested in EEG/ECG devices and was wondering if we could learn a policy function that could estimate the intended function such as focusing on a task/thinking of X/breathing/etc. using some form of RL algorithm. Or would it be strictly limited to DL and RNN algorithms?

Thanks.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hub8em/practical_rl_in_signal_processing/
"""Can RL From Pixels be as Efficient as RL From State?"", Laskin et al 2020 {BAIR} (on RAD/CURL data augmentation for model-free DRL)",1595182121,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hu4z8e/can_rl_from_pixels_be_as_efficient_as_rl_from/
RL libraries?,1595175864,"Hey, there seems to be a ton of ML sandbox libraries where you can potentially train &amp; test your model in few lines of code. Is there an alternative for DRL as well? Thinking about a project where you implement RL solutions in fewer than 10 lines.

Thanks",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hu36zf/rl_libraries/
megastep: 1 million frames a second on a single GPU,1595160232,,reinforcementlearning,bluecoffee,False,/r/reinforcementlearning/comments/htzf94/megastep_1_million_frames_a_second_on_a_single_gpu/
"DeepMind Introduces TayPO, A PO Framework For RL Algorithm",1595160183,[https://analyticsindiamag.com/deepmind-introduces-taypo-a-policy-optimisation-framework-for-rl-algorithm/](https://analyticsindiamag.com/deepmind-introduces-taypo-a-policy-optimisation-framework-for-rl-algorithm/),reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/htzeww/deepmind_introduces_taypo_a_po_framework_for_rl/
How to normalize the discounted rewards and advantages in A3C?,1595134436,"My rewards are -1,0,1 and with the discounting reward it can reaches very high values in long runs. I noticed that once it become very good and can collect so many rewards it becomes very unstable and have waves of bad scores(alternating between really good score and really bad scores). It might be the discounted rewards extreme ranges. I'm using this blog [https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2).

&amp;#x200B;

This is the code to calculate the advantages and discounted reward.

    self.rewards_plus = np.asarray(rewards.tolist() + [bootstrap_value])    discounted_rewards = discount(self.rewards_plus,gamma)[:-1]   
    self.value_plus = self.value_plus=np.asarray(values.tolist() + [bootstrap_value])    
    advantages = rewards + gamma * self.value_plus[1:] - self.value_plus[:-1]   '
    advantages = discount(advantages,gamma) 

Is it right to normalize by doing this. 

    advantages=(advantages -advantages.mean())/advantages.std()
    
    discounted_rewards=(discounted_rewards -discounted_rewards.mean())/discounted_rewards.std()",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/htuz4k/how_to_normalize_the_discounted_rewards_and/
Construction of good learning rollout in reverse,1595123336,"I was making an AI to play 2048 via RL. By random play, it's pretty hard to get to a good spot, so my agent seems to get stuck at certain level of play.

So I though, why I don't start with a good outcome, and in reverse construct a rollout that would get there. So like playing 2048 by starting with 2048 (and at least one 2) on the board, and play ""backwards"". Until I start with 2 tiles.

So this way I can construct a ""good example"" for my agent to learn from. Then I also sample some random play rollout to get some bad examples.


I am sure someone in RL has thought about this idea where generate good ""rollouts"" via reverse play. Of course, this is only possible for simple environments.

Has there been any paper on this technique?",reinforcementlearning,xiaodaireddit,False,/r/reinforcementlearning/comments/htsi3a/construction_of_good_learning_rollout_in_reverse/
A.I. Learns to Play Tic-Tac-Toe,1595105386,"I created an A.I. that learns to play Tic-Tac-Toe using Q-learning. I explained how I did it on the video. If you enjoyed it a like and subscribe on the youtube video would be much appreciated.

[https://www.youtube.com/watch?v=OHON6-JC-Xs](https://www.youtube.com/watch?v=OHON6-JC-Xs)",reinforcementlearning,MyWorldRules,False,/r/reinforcementlearning/comments/htnwxl/ai_learns_to_play_tictactoe/
Idea about an algorithm: does it exist?,1595091415,"Not ground breaking idea, just curious whether it already exists or not:

If I understood correctly, DDPG is some sort of a value iteration algorithm that alternates between optimizing the optimal Q function (the same way as DQN), then optimize the policy to get close to argmax(Q) by computing the gradient of the Q(pi(s),s) with respect to the parameters of the policy. It is a way to bypass the computation of argmax(Q) in continuous action space, which is complicated.

On the other hand, policy gradient methods such as A2C computes the value function of the current policy, then optimize the log-probability of the policy taking an action by scaling with the advantage of that action. This is known to be quite noisy, due to the log-probability.

Instead of optimizing the log-probability, can't one use the same trick as in DDPG, i.e. optimize the policy by directly computing the gradient of  Q(pi(s),s) w.r.t the parameters of the policy?

I realize it's not super interesting since at that point , one can just use DDPG. But I wonder how this would perform nonetheless, and whether it would outperform A2C which uses the notorioisly noisy gradient using the log-probability.

Thanks!",reinforcementlearning,youpiyaya,False,/r/reinforcementlearning/comments/htjw8t/idea_about_an_algorithm_does_it_exist/
Enquiries on RL for parameter tuning,1595082423,,reinforcementlearning,SSCopter,False,/r/reinforcementlearning/comments/hthgo3/enquiries_on_rl_for_parameter_tuning/
Self Driving AI in 100 lines of code | Behavior Cloning,1595069584,,reinforcementlearning,saraltayal,False,/r/reinforcementlearning/comments/hteqck/self_driving_ai_in_100_lines_of_code_behavior/
Insurance Claim methodoly,1595051267,"Hi, I want to create an RL model that can predict insurance claims rejection/acceptance based on multiple categorical inputs. Side note: Cardinality can be high!
The RL rules learned need to be human readable (eg tree-like policy or propose input for the claim to be accepted).

Methodology ideas, papers, Medium posts/github repo with code, .... are welcome!

Thanks community :)",reinforcementlearning,bmibott,False,/r/reinforcementlearning/comments/htblk0/insurance_claim_methodoly/
Question regarding how to compute policy gradient \delta ln \pi_\theta(A_t|S_t),1595028715,"In the basic Reinforce algorithm, we see that there is a term

$\\delta ln \\pi\_\\theta(A\_t|S\_t)$

&amp;#x200B;

&amp;#x200B;

[How to compute the highlighted term](https://preview.redd.it/2o2exu5q5ib51.png?width=878&amp;format=png&amp;auto=webp&amp;s=666f5705eb823fcf1df2a022341c2c619ccdbd8f)

So how do I compute that exactly? I am trying to write a policy gradient method from scratch.

Say my policy \\pi returns a vector of probabilities, e.g. `[prob of moving left, and prob of moving right]`, say in this particular roll out, I am moving left and the probability from the policy is 0.4. What do I do?

Do I define a new function

    function policy_left(policy, data)
       policy(data) |&gt; first # to get the probability of moving left
    end

Then I differentiate it using some AD system and go

    grad = gradient(policy_left, at = data)

Is the `grad` above the same as the term $\\delta ln \\pi\_\\theta(A\_t|S\_t)$ in the REINFORCE algorithm?",reinforcementlearning,xiaodaireddit,False,/r/reinforcementlearning/comments/ht6fw1/question_regarding_how_to_compute_policy_gradient/
Project Ideas NLP + RL,1595016070,"I'm an MS Student, my professor asked me to come up with 2 project ideas that include NLP and RL. Let's start the discussion.",reinforcementlearning,skandari,False,/r/reinforcementlearning/comments/ht2qy0/project_ideas_nlp_rl/
Differences between SAC and TD3 with learnable noise?,1595005268,"Hello everyone, 

I am a self taught RL hobbyist and have just gone through the OpenAI spinning up course as well as reading the papers on TD3 and SAC. I just have a couple of questions to ask about the fundamental differences between the two methods whether or not SAC (the implementation without the V appropriator) can be seen as a slight extension of TD3 using learnable noise.

The main difference between the two algorithms that I can see is that where TD3 uses a deterministic policy and then add extra noise using target policy smoothing, SAC outputs a mean and deviation for each action, which when coupled with independent noise, becomes a stochastic policy.  The other difference I see is the addition of entropy as a regularisor.

So my main question is: is SAC not just simply TD3 + [noisy networks](https://arxiv.org/pdf/1706.10295.pdf)(but only for the bias in the final layer) + entropy regularisation. 

Sure there are some other smaller tweaks, like if the policies updates are delayed and if they are based on one Q network (TD3) or the min of both (SAC), but these differences are are minor and could be used in either method without changing much.

So let me just explain the noisy_nets part. If the final layer of a TD3 policy network is equipped a bias which is influenced by learnable noise (see above paper) then the output would be:

tanh( original_activation + bias_noise_magnitude * bias_noise )

Is this not just the same as the stochastic policy used in SAC:

tanh( mean + deviation * independent_noise )

In the noisy_nets paper, the magnitude of bias_noise adjusted by the network, which is the exact same as the learnable standard deviation in the SAC policy output.

So if I were to replace the exploration noise, and target policy smoothing noise in TD3 with just a noisy output layer as shown, and then slap on entropy regularisation, would I not have just recreated SAC? And if that is the case, might not TD3 benefit from using some weight parameter noise, as used in the original noisy network paper, or even make all parameters in the policy network noisy. This has been done in [before](https://openai.com/blog/better-exploration-with-parameter-noise/), but the justification here was for exploration. 

Sorry if I am completely missing some massive or if this was just a dumb question in general, as stated before, I am just a hobbyist trying to understand.",reinforcementlearning,Tanavast,False,/r/reinforcementlearning/comments/hsze42/differences_between_sac_and_td3_with_learnable/
"How can I implement a 2 player, simultaneous play Gym environment ?",1594991351,"I have a game where 2 players face off, and they must make their moves at the same time for the environment to advance (think of it like chess but they have to make their moves simultaneously).

Currently, my environment's observation and action space just contain both player's observations and actions, but that doesn't allow to just plug the env into any pre-implemented algorithms for Gym environments.

Is there any way to make this work ?

Thanks !",reinforcementlearning,Mintiti,False,/r/reinforcementlearning/comments/hsve6x/how_can_i_implement_a_2_player_simultaneous_play/
Understanding Gym’s Action and Observation Space (Continuous),1594961418,"Hi all, I’m doing RL on a custom environment that has continuous action and observation spaces. To set up the spaces, I used Gym’s Box method for both. It seems to work fine, but I notice my agent learns differently when I change the “low” and “high” ranges of the Box spaces. Is there a method in determining the range for these spaces? I had seen using low=-1 and high=1 because my actions and states are normalized, but I have elements in my observation and action tensor that lie outside this range (some as great as 30). How have you guys approached Gym’s Box spaces?",reinforcementlearning,big-waves-r-us,False,/r/reinforcementlearning/comments/hspk4u/understanding_gyms_action_and_observation_space/
A.I. Learns to Play Blackjack,1594959399," 

I created an A.I. that learns to play Blackjack. I explain how I did it on the video (beginner-friendly). If you enjoyed it a like and subscribe on the youtube video would be much appreciated.

[https://www.youtube.com/watch?v=VSLcoP5vAMM](https://www.youtube.com/watch?v=VSLcoP5vAMM)",reinforcementlearning,MyWorldRules,False,/r/reinforcementlearning/comments/hsp48b/ai_learns_to_play_blackjack/
An introductory RL event,1594954815,"Posting for my company...Free/online...The speaker is legit (company's co-founder). He's a real scholar in the field and he actually teaches RL courses at Columbia. You might need to get used to his accent...

Anyways, thx for letting me post this.

RSVP:[https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-113849695504?aff=rd](https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-113849695504?aff=rd)",reinforcementlearning,oyolim,False,/r/reinforcementlearning/comments/hso1jd/an_introductory_rl_event/
"""Tutorial on Model-Based Methods in Reinforcement Learning"", Mordatch &amp; Hamrick 2020 {DM} [ICML tutorial talk on MBRL]",1594953559,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hsnqxg/tutorial_on_modelbased_methods_in_reinforcement/
Easy to install 3D AI Gyms?,1594933158,"Hey all,

I'm looking for an AI gym that's very straightforward to install. Most of the ones I've used, like VizDoom, require a bunch of work to set up with compilation and library management. It would be great if there was a gym that I could just `pip install` that had a 3d world, preferably with an interactive environment or enemies to shoot.",reinforcementlearning,Marthinwurer,False,/r/reinforcementlearning/comments/hsi9zj/easy_to_install_3d_ai_gyms/
Instantaneous increase in Reward Graph: Actor-Critic with PER(AC_PER),1594924053,"Hi, 

I am training an agent with off policy( PER) AC. After each epoch, training is done with batch size of 32 and each epoch simulates 100 dialogues(episodes).  In the reward graph (below image), Why there is a sudden increase in reward in AC\_PER? What does it indicate? Also, there is abnormality that AC\_PER is not doing better than AC?  Please comment your view. 

&amp;#x200B;

[Reward Graph](https://preview.redd.it/953vp153i9b51.png?width=394&amp;format=png&amp;auto=webp&amp;s=69341619fa3226298dab91da80412178bcd13cce)

Thank you",reinforcementlearning,virabhi,False,/r/reinforcementlearning/comments/hsf7t7/instantaneous_increase_in_reward_graph/
"""Concept2Robot: Learning Manipulation Concepts from Instructions and Human Demonstrations"", Shao et al 2020",1594918090,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hsd56d/concept2robot_learning_manipulation_concepts_from/
Q-Losses is approaching zero when using target network in eval mode,1594914717,"I am implementing an agent to play an Atari game. I am using DQN as the Reinforcement Learning algorithm. Also, I am using the Batch Normalization and Dropout layers in the NN architecture. When the target network was in train mode the Q-losses curve was sensible.

[Q-losses when target network in train mode](https://preview.redd.it/mokhaxoiq8b51.png?width=432&amp;format=png&amp;auto=webp&amp;s=562d2ef04ae442f83ac02efe4991b3ee58bcfaa6)

Then After research, I found that the Target network shall be in eval mode because we want the results without being affected by the regularization layers. But when I changed the mode of the target network to be eval mode, the Q-losses converged nearly to zero.

&amp;#x200B;

[Q-losses when target network in eval mode](https://preview.redd.it/fxrt0urnq8b51.png?width=432&amp;format=png&amp;auto=webp&amp;s=db8f1b408a557e0c6f97b1c56fe7725e21c44a48)

&amp;#x200B;

Also, In the second case, the performance of the agent is better than the first. So why is this behavior happening just because of only changing the mode of the target layer?",reinforcementlearning,medoaashry,False,/r/reinforcementlearning/comments/hsc1mq/qlosses_is_approaching_zero_when_using_target/
Understanding Adam optimizer on RL problems,1594905688,"Hi,

Adam is an adaptive learning rate optimizer. Does this mean I don't have to worry that much about the lr?

I though this was the case, but then I ran an experiment with three different learning rates on a MARL problem: (A gridworld with different number of agents present, PPO independent learners. The straight line on 6 agent graph is due to agents converging on a policy where all agents stand still).

https://preview.redd.it/zdv4zcl9z7b51.png?width=1008&amp;format=png&amp;auto=webp&amp;s=52db1f2f288931f63ec22586b79660af69300eb9

Any possible explanations as to why this is?",reinforcementlearning,acc1123,False,/r/reinforcementlearning/comments/hs9j5a/understanding_adam_optimizer_on_rl_problems/
Defining States,1594888616,How can i define shooting alien as  state in my own space inviders game ? I want agent to find out that agent gets plus points when the agent fires.,reinforcementlearning,brosuitup,False,/r/reinforcementlearning/comments/hs60pi/defining_states/
Reinforcement learning on regression and classification?,1594875896,"I work in signal processing and machine learning, and I have been longing to get started with reinforcement learning methods and apply them in my work. However, wherever I search for basic examples, they are game-oriented or grid-space search oriented. I understand the reasoning behind this intuitive approach. However, I was wondering if anyone can provide examples that apply RL or deep RL on regression or classification problems. 

Is it possible at all? Say for example, in a filtering application, can we train an agent to learn a policy that can tolerate a range of noise and work better than a regular non-RL approach?",reinforcementlearning,kirtyv,False,/r/reinforcementlearning/comments/hs3n34/reinforcement_learning_on_regression_and/
Resources,1594872334,"Hey guys! Can y'all recommend some resources (conference/blogs/twitter handles) that would keep me updated about the field, understanding papers etc.. 

Cheers",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/hs2v03/resources/
Monte Carlo control method for Cartpole in openAI gym,1594864654,"Hey all,

I've been recently learning about RL and Bellman equations. Few days ago, I built this [RL agent](https://www.youtube.com/watch?v=RrhVLzdmJ34) using Monte Carlo methods with policy greedy method to train the classic cartpole agent in openAI gym. 

I actually made a short [video](https://www.youtube.com/watch?v=RrhVLzdmJ34) about it where I explained my process/approach behind it and I'd appreciate it if you guys could give me some feedback.

Sorry if it sounds like I'm promoting myself but I just wanted to get technical feedback on where I can improve on.

Thanks.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hs107n/monte_carlo_control_method_for_cartpole_in_openai/
Question: Intuition behind Policy ImprovementTheorem,1594833203,"&amp;#x200B;

[Policy improvement theorem](https://preview.redd.it/k11mqpz7v1b51.png?width=2375&amp;format=png&amp;auto=webp&amp;s=259b72da16f1585132270e0f10168d55256d7a72)

Hello,

I've been going through this theorem and I can't seem to understand a few things:  


1. Why is it that on Line 1 V(s) is less or equal to the action-value function given that we act greedily to a certain state? Why isn't possible that an action is picked that gives a long term discounted reward that is significantly worse than the original policy? Is it because taking the highest reward action will by definition always give a higher long-term value (unless it's already an optimal policy) ?
2. Should I think of each line with an inequality as an iteration of policy improvement?
3. What does the very last line mean? I know it means the value of state s under policy π' but what does it mean intuitively why is there a strict equality does that mean that **V****^(π)****(s) = V****^(π')****(s)** ?  


Thanks.",reinforcementlearning,Ubermensch001,False,/r/reinforcementlearning/comments/hrrl97/question_intuition_behind_policy/
"""Monte-Carlo tree search as regularized policy optimization"", Grill et al 2020 {DM} (AlphaZero/MuZero)",1594825756,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hrpab4/montecarlo_tree_search_as_regularized_policy/
Problem with knowledge about the future,1594811497,"Suppose you have some knowledge about the future if you choose a certain action. How would you use this in order to help your model learn better?
An obvious way is to add this as features. However, I feel that we should be able to do better as we can simulate the possible future scenarios for each action. For example, if the action space was discrete we would be able to do a brute force search and pick the best action. I am interested if there are efficient ways of doing something similar when both state and action spaces are continuous?",reinforcementlearning,Light991,False,/r/reinforcementlearning/comments/hrlo2f/problem_with_knowledge_about_the_future/
[question] Research about what matters in off-policy RL,1594806327,"Recently I studied these two very nice papers about what actually matters in on policy RL and I found them really useful for the practical implementation perspective:

 [https://arxiv.org/abs/2006.05990](https://arxiv.org/abs/2006.05990) 

 [https://arxiv.org/abs/2005.12729](https://arxiv.org/abs/2005.12729) 

Is there any similar work for off-policy methods, like SAC?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/hrkos9/question_research_about_what_matters_in_offpolicy/
"""Maximum Entropy Gain Exploration for Long Horizon Multi-goal Reinforcement Learning"". Pitis, Chan, et al. ICML 2020 {Toronto}",1594778672,,reinforcementlearning,MartianTomato,False,/r/reinforcementlearning/comments/hreyia/maximum_entropy_gain_exploration_for_long_horizon/
"""Revisiting Fundamentals of Experience Replay"", Fedus et al 2020 {GB}",1594753517,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hr7o4z/revisiting_fundamentals_of_experience_replay/
Sergey Levine: Robotics and Machine Learning | AI Podcast #108 with Lex Fridman,1594746757,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/hr5h56/sergey_levine_robotics_and_machine_learning_ai/
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones,1594735829,,reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/hr27fs/longterm_planning_with_deep_reinforcement/
Error for prioritizing experience reply in Actor critic Algorithm,1594724252,"Which error should be used as a priority in AC? As there are two losses 1. Actor loss (TD \* log\_prob) and 2. Critic loss ( TD\^2).Also  please tell you intuition for so.

&amp;#x200B;

Thanks

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

[View Poll](https://www.reddit.com/poll/hqzem7)",reinforcementlearning,virabhi,False,/r/reinforcementlearning/comments/hqzem7/error_for_prioritizing_experience_reply_in_actor/
RO-1.0X: Introduction to Planning &amp; Decision Making for Autonomous Vehicles and Robots,1594713718,,reinforcementlearning,shani_786,False,/r/reinforcementlearning/comments/hqxgyg/ro10x_introduction_to_planning_decision_making/
Never Give Up: Learning Exploration strategies in RL | Paper Explained,1594709219,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/hqwmin/never_give_up_learning_exploration_strategies_in/
"""Deep Reinforcement Learning and Its Neuroscientific Implications"". Botvinick et al 2020 {DM} (review)",1594694789,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hqtexz/deep_reinforcement_learning_and_its/
[R] Data-Efficient Reinforcement Learning with Momentum Predictive Representations (new SoTA on Atari in 100K steps),1594693086,,reinforcementlearning,ankeshanand,False,/r/reinforcementlearning/comments/hqszvc/r_dataefficient_reinforcement_learning_with/
Recent algorithms to tackle POMDPs?,1594682248,"Hi guys, I am working on my Master's thesis at the moment and I have this problem formulated as a Partially Observable MDP. 

At a high level it works by maintaining a belief state of underlying characteristics of the environment and in the past people have tried basic Q-learning and Bayesian Q-learning trying to tackle the problem. Within both algorithms they simulate outcomes of the next state and also update beliefs (additional steps to the traditional algorithms). 

I am pretty new to RL an I was wondering if anyone had any ideas for algos to try with this problem. One I've come across is Double Q-learning.",reinforcementlearning,gauzah,False,/r/reinforcementlearning/comments/hqq5p8/recent_algorithms_to_tackle_pomdps/
[D] Should I do unsupervised-pre-training if I have a perfect simulator?,1594630804,"
Working towards a production system,  we have decent results with DRL and  massive on-policy exploration roll-outs in the simulator

Should we still consider insupervised pre-learning?  

It is a discrete combinatorial fully observable domain. We can perfectly simulate and rollout and rollback actions. 
The main challenge is sparse rewards with 1000s of actions and 100s of steps.

I am looking at things like BERT/Alberta/GPT. And I wonder if doing this with our near infinite data from real world experiences can give us a better representation maybe.",reinforcementlearning,yazriel0,False,/r/reinforcementlearning/comments/hqbwx5/d_should_i_do_unsupervisedpretraining_if_i_have_a/
Griddly - A Fast and customizable RL Environment,1594628989,,reinforcementlearning,NeuralBamford,False,/r/reinforcementlearning/comments/hqbldj/griddly_a_fast_and_customizable_rl_environment/
Control more than one variable,1594605791,"Hey, guys. If in my environment I want to control two variables and both works in continuous action spaces, how can I do that? Is there an algorithm that works better for this purpose?",reinforcementlearning,hidden-7,False,/r/reinforcementlearning/comments/hq6qyg/control_more_than_one_variable/
Has anyone solved Pong-ram-v0 with Policy Gradient?,1594592687,"I've been looking for code that solves OpenAI gym's Pong-ram-v0 environment with a policy gradient method. I figured the ram environment would be easier to learn than the image input. Does anyone have links to papers/code that solve Pong-ram?

I've written some A2C code for the environment but the agent simply learns to move the paddle to the corner of the screen and stop moving. The same thing happens with any other Pong-v0 code I find online and adapt to run Pong-ram-v0. Any ideas on what could be going on?",reinforcementlearning,rb31415,False,/r/reinforcementlearning/comments/hq3eix/has_anyone_solved_pongramv0_with_policy_gradient/
[question] Constrained Policy Optimization Implementation,1594579257,Are there any recommendations for handy CPO implementations?,reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/hpzi0p/question_constrained_policy_optimization/
"""SUNRISE: A Simple Unified Framework for Ensemble Learning in Deep Reinforcement Learning"", Lee et al 2020 (uncertainty-weighted bootstrap ensemble w/UCB exploration for sample-efficiency)",1594579156,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hpzgxi/sunrise_a_simple_unified_framework_for_ensemble/
One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control,1594571986,,reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/hpxdz2/one_policy_to_control_them_all_shared_modular/
How much of research is actually just brainstorming?,1594563106,"I am a fresh grad (graduating in \~1 month) and I have been a part of an RL research team (of 3 + PhD mentor) where we had to formulate a problem, create an environment, and test different learning algorithms. (i.e. carry a problem from abstraction to formulation and code).  


I would say that &gt;50% of the effort done by any of us was coming up with ideas and critically analyzing each other's ideas, eventually reaching something that we believed was feasible/plausible. Both pre-experiments (formulation) and post-experiments (analysis). This makes me wonder: How much of RL research is actually just discussions with your teammates?   


TL;DR I spent most of my time during research in team discussions. Is this to be expected in RL research?",reinforcementlearning,abs_waleedm,False,/r/reinforcementlearning/comments/hpuztv/how_much_of_research_is_actually_just/
[D] Stanford CS330 coding support group!,1594549674,"I have noticed that there is substantial amount of people who have decided to tackle the Meta-learning and Multi-task course by Chelsea Finn. This includes e.g.: [https://www.reddit.com/r/reinforcementlearning/comments/fvy68z/asking\_for\_help\_for\_the\_cs330\_stanford\_course/ftlmxz0/?context=3](https://www.reddit.com/r/reinforcementlearning/comments/fvy68z/asking_for_help_for_the_cs330_stanford_course/ftlmxz0/?context=3)  
I am on my way to complete the course, however I find some parts of the homework quite challenging. This is why I am looking for people who would like to team-up to work together on the assignments.

I have spent quite some time solving homework 1. You can take a look at the results here:

[https://github.com/dtransposed/CS330\_Deep-Multi-Task\_and\_Meta\_Learning/tree/master/hw1](https://github.com/dtransposed/CS330_Deep-Multi-Task_and_Meta_Learning/tree/master/hw1)

Right now I'm working on homework 2. I have started with implementing prototypical networks. However, I fail to get it running. You can take a look:  
[https://github.com/dtransposed/CS330\_Deep-Multi-Task\_and\_Meta\_Learning/blob/master/hw2/prototypical\_net.ipynb](https://github.com/dtransposed/CS330_Deep-Multi-Task_and_Meta_Learning/blob/master/hw2/prototypical_net.ipynb)",reinforcementlearning,dtransposed,False,/r/reinforcementlearning/comments/hpsagx/d_stanford_cs330_coding_support_group/
Help needed brainstorming a custom environment; representing spatial elements without images,1594541462,"Hi, I’m looking to develop an environment in which an agent has to traverse a path (think a route for navigation, except the path is singular, doesn’t diverge). The reason for including this ‘path’ is that it’s important the agent ‘knows’ whether it’s on the curved portion or a straight line at different points in the path. Additionally the agent can be in one of three ‘energy’ states; these depend on which energy level the agent selects. 

Does anyone have any thoughts or has seen related work that can assist me +/ give me some leads ? 

Thanks!",reinforcementlearning,rower22,False,/r/reinforcementlearning/comments/hpqv7j/help_needed_brainstorming_a_custom_environment/
Why don't people release learning curves?,1594535343,"We can all agree, without a single shred of doubt, that RL is expensive. So my question is, why do most people avoid releasing their learning curves? We could save hundreds of compute hours but for whatever reason, very few people actually supplement them.",reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/hppt6a/why_dont_people_release_learning_curves/
Experiments with DQN,1594521652,"I did some experimentation with DQN on the CartPole-v1 environment to see how the performance of DQN varies when the hyperparameters related to the target network are varied.

Experiments

* **Experiment 1: No seperate target network:** Observe DQN performance when there is no seperate target network.
* **Experiment 2. Seperate target network but not updated:** Observe DQN performance when there is a seperate target network but it is never updated.
* **Experiment 3. Hard update the target network:** Observe DQN performance when we hard update the target network at different target intervals.
* **Experiment 4. Soft update target network:** Observe DQN performance when we soft update the target network with varying TAU.
* **Experiment 5: Increase the number of hidden layers:** Observe DQN performance when the number of hidden layers of the Q networks are varied.

You can read about these experiments here: [https://kaustabpal.github.io/dqn](https://kaustabpal.github.io/dqn)",reinforcementlearning,retro-rabbit,False,/r/reinforcementlearning/comments/hpmzjn/experiments_with_dqn/
"""Decentralized Reinforcement Learning: Global Decision-Making via Local Economic Transactions"", Chang et al 2020 {BAIR}",1594509037,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hpju91/decentralized_reinforcement_learning_global/
[Python] HELP Offline RL Contextual Bandit,1594500793,"Hi, I am new to the field of ML and RL. I am trying to solve a contextual bandit problem with static data. Most resources I have found so far do not address the use the of static data, and are usually on- or off-policy. Can any one recommend some good jumping off point or resources?

Thanks!",reinforcementlearning,iGoatyou,False,/r/reinforcementlearning/comments/hphix7/python_help_offline_rl_contextual_bandit/
Nan Jiang on TalkRL Podcast,1594493681,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/hpfgat/nan_jiang_on_talkrl_podcast/
[R] One Policy to Control Them All: Shared Modular Policies for Agent-Agnostic Control (Link in Comments),1594490799,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hpemio/r_one_policy_to_control_them_all_shared_modular/
PPO: Why does the loss of the value function is added negatively in the total loss?,1594487829,"In the PPO paper ([https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347)), the total loss is given at the 9th formula as:

    TOTAL_LOSS = L_CLIPPED_POLICY - C1 * LOSS_VALUE_FNCT + C2 * ENTROPY

But why minus the loss value ? This makes no sense to me to deteriorate the critic  
And why adding the entropy has a positive value? Isn't the goal of the entropy coefficient to allow more exploration by reducing the step size when the entropy is big? At least it is my understanding.

Also it's in contradiction with the loss formula of A2C ([https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)) given in the **Algorithm S3** which as a positive factor for the loss\_value and a negative one for the entropy",reinforcementlearning,ingambe,False,/r/reinforcementlearning/comments/hpdqt0/ppo_why_does_the_loss_of_the_value_function_is/
Batch RL benchmarks besides google's?,1594471164,"Hi everyone,
I am looking for benchmarks alternative to Google's https://github.com/google-research/batch_rl 

These benchmarks are 3.5x larger than imagenet, i.e. .5 TB, so I am looking for something more... sensible.",reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/hp9bc3/batch_rl_benchmarks_besides_googles/
Multi discrete action spaces for DQN,1594470339,"I am currently struggling with DQN in the case of multi discrete action spaces. I know that the output layer of the Deep Q Net should have the same dimensionality of the discrete action space. What if I have several discrete action spaces because I want my Q agent to perform simultaneously different actions? How does the architecture of the net change?

I figured out that the output layer should have the dimesion of all possible combinations of the discrete action spaces, but in that case the dimensionality of the output layer would grow fast. Moreover, in doing that each neuron in the output layer would be associated with N values instead just one, where N is the number of simultaneous actions that can perform.

Is there a more efficient and easier way to deal with it?",reinforcementlearning,alebrini,False,/r/reinforcementlearning/comments/hp95c6/multi_discrete_action_spaces_for_dqn/
"green bot under DQN (Pytorch),1500 epoch, but progress is too slow. Bots can strike by laser and have health limit. action space=5. state space use coordinates of bots, velocity and etc dim=12 . Think that main problem in reward function or need to use more complex algorithm?",1594460386,,reinforcementlearning,DigitCell,False,/r/reinforcementlearning/comments/hp7c9r/green_bot_under_dqn_pytorch1500_epoch_but/
Blackjack AI &amp; Reinforcement Learning: Is it actually working?,1594446111,"First of all, I'm not an expert in AI &amp; ML.  
I made some reads and watched some stuff, the field is so interesting that I decided to have fun running this experimental project.  
So please bear with the technical mistakes.

I've coded a blackjack game able to play by itself and learn over time.  
In a nutshell:

* 52 cards deck
* Just Hit and Stand action
* Decision making: if the status X is unknown, the agent will decide randomly; if there is previous experience in Status X, it'll choose according to the past experience.
* KPI: to determine the success of the method I use the **loss %**

Here's come the doubt. 

I check the loss % every 1k game played.  
This is cumulative: the first datapoint is from game 1 to 1000, the second from 1 to 2000, and so on.

  
With this method, after 500k games, I can clearly see the loss % drop down until it stabilizes itself around 47,79%  (first datapoint around 54%, then rapid drop down and then super slow improvement)

Running the agent in ""random mode"" brings to a 64% in loss % after 100k/500k games.

Can I consider the agent successful?  
There might be a way to reduce the loss % even further?",reinforcementlearning,vba-automation,False,/r/reinforcementlearning/comments/hp4toq/blackjack_ai_reinforcement_learning_is_it/
"[P] mrl: modular RL (modular pytorch implementations of HER, MEGA, CODA, DDPG, TD3, SAC, and more)",1594420872,"Link to code: [https://github.com/spitis/mrl](https://github.com/spitis/mrl)

A large part of the code that I use for research is now public, to go along with our ICML 2020 paper, Maximum Entropy Gain Exploration for Long Horizon Multi-Goal Reinforcement Learning (MEGA) ([Arxiv](https://arxiv.org/abs/2007.02832), [ICML link with presentation](https://icml.cc/virtual/2020/poster/6622)). 

It's called `mrl` and contains performant implementations of:

- DDPG, TD3, SAC (better performance / modularity / wall clock time than Spinning Up implementations)
- [HER](https://arxiv.org/abs/1707.01495) (SOTA implementation---cleaner and more flexible than baselines implementation, compatible with all off-policy algos, does online relabeling rather than static like stable baselines implementation, and beats most follow-ups to HER by a wide margin on the usual Gym benchmarks)
- [MEGA](https://arxiv.org/abs/2007.02832) (sets intrinsic goals to maximize entropy in goal space and achieves SOTA performance on long-horizon multi-goal tasks)
- [CoDA](https://arxiv.org/abs/2007.02863) (leverages local causality to generalize HER to object-oriented / partial state relabeling and greatly improves sample efficiency in disentangled state spaces) 
- A few other random things including basic DQN

The implementations have good performance: [see benchmarks](https://github.com/spitis/mrl/blob/master/experiments/benchmarks/readme.md). 

As compared to the many other frameworks out there, the main advantage is modularity. It has a tiny bit of a learning curve, but once you get how the lifecycle hooks &amp; global namespace works, I think it's a real pleasure to prototype new ideas in. 

This code has only been used by a handful of people, so any feedback / suggestions you have are welcome!  I can't promise significant support, but will do my best to answer any questions.",reinforcementlearning,MartianTomato,False,/r/reinforcementlearning/comments/hoyv53/p_mrl_modular_rl_modular_pytorch_implementations/
PBT Explore implementation question,1594419750," Question about PBT Explore ( I think I got my answer from looking at Ray's implementation but wanted clarification ),

1. Can PBT be used to optimize Lambda and Gamma values as well? Not just hyperparameter values to train a Neural Network (excluding layer depth/width).
2. Does PBT Explore randomize one hyperparameter at a time?
3. Does PBT Explore randomize a value from min to max or is there some logic applied to limit the range of randomness after copying over the better model's hyperparameters and weights?

Paper:  [Population-Based Training](https://arxiv.org/pdf/1711.09846.pdf)",reinforcementlearning,Heartomics,False,/r/reinforcementlearning/comments/hoyjun/pbt_explore_implementation_question/
"Anybody have good, cheap robots to practice sim-to-real?",1594411923,"An example of what I’m looking for is a robot like [Kraby ](https://kraby.readthedocs.io/). One that is easily bought and could be easily built and not too expensive—maybe a robotic arm or something with hopefully an accompanying gym env (if not I’d just do that too). 

Thought someone in here would have seen something. Thanks fam",reinforcementlearning,memelord_5517,False,/r/reinforcementlearning/comments/how8kd/anybody_have_good_cheap_robots_to_practice/
Dexterous Gym - Challenging extensions to OpenAI Gym's Hand Manipulation Environments,1594396805,"I just wanted to share a project that I've been working recently which modifies some of the OpenAI Gym hand manipulation environments in order to create some significantly more challenging tasks. These include trying to learn how to spin a pen as well as two hands playing catch with different (or multiple) objects, amongst others. The link to the project is here: [https://github.com/henrycharlesworth/dexterous-gym](https://github.com/henrycharlesworth/dexterous-gym)

I have found that all of these environments are difficult for current RL algorithms (I plan to add some detailed experiments demonstrating this soon), so they make for interesting and challenging domains that could be used to test new algorithms. I have included a few gifs that show some example trajectories that I've been able to generate for some of the tasks using a trajectory optimisation algorithm that I've been working on (so no RL involved), and which will (hopefully) be published soon. This is the only method I've tried so far which has been able to produce good results for a number of the tasks, and so I'd be interested to (a) find out if anyone is able to produce good results with any other existing techniques and (b) if not, pose these tasks as challenges for RL researchers to try and achieve good results on!",reinforcementlearning,henrythepaw,False,/r/reinforcementlearning/comments/horiy6/dexterous_gym_challenging_extensions_to_openai/
Atari preprocessing OpenAI,1594393464,"Hi guys, I am trying to implement DQN on some atari games from OpenAI gym, but when I am using ataripreprocessing from OpenAI I got low performance comparing with atari preprocessing, which is in Phil Tabor git repo. ( [https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code](https://github.com/philtabor/Deep-Q-Learning-Paper-To-Code) )  Atari preprocessing meaning like 84x84, grayscale etc. Is first atari preprocessing good to use ? or do I have to make it from scratch ??",reinforcementlearning,edoudo,False,/r/reinforcementlearning/comments/hoqil5/atari_preprocessing_openai/
Model training time - Jupyter Notebook vs IDE,1594375830,"Is it true that IDE takes comparatively lower time to train a model over a Jupyter Notebook? 
If yes, then why do people in academic  community still prefer to use Jupyter Notebook?",reinforcementlearning,Careful_Manager,False,/r/reinforcementlearning/comments/homdm7/model_training_time_jupyter_notebook_vs_ide/
DDPG vs PPO vs SAC: when to use?,1594371081,What do you look at when deciding which of these algorithms to use? In which situations do they perform better?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/holioy/ddpg_vs_ppo_vs_sac_when_to_use/
Deep-learning on Steam Games,1594370553,"How would I go about capturing the screen on steam games to use in a DQN? For example, training an agent on the PogoStuck game",reinforcementlearning,undercover_intern,False,/r/reinforcementlearning/comments/holfdu/deeplearning_on_steam_games/
Duality: A New Approach to Reinforcement Learning,1594366705,[https://analyticsindiamag.com/duality-theorem-new-paradigm-reinforcement-learning/](https://analyticsindiamag.com/duality-theorem-new-paradigm-reinforcement-learning/),reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/hokqym/duality_a_new_approach_to_reinforcement_learning/
Actor Critic learns well and then dies,1594330639,"I am training a standard actor critic using advantage where I estimate Q value using the instantaneous reward and the next state V value. It performs pretty well and solves, for example continuous cart pole, and suddenly just converges to a policy that dies as soon as possible.

Is there any literature on this?

If you have any idea how might I tackle this problem?

&amp;#x200B;

Edit: This does not happens for all choice of sigma (in the Gaussian policy) decay. However, It happens for all the environments I tried.

&amp;#x200B;

https://preview.redd.it/ngg3811njw951.png?width=397&amp;format=png&amp;auto=webp&amp;s=513051050a58da91bc3e165e021a87565ff1b044",reinforcementlearning,Light991,False,/r/reinforcementlearning/comments/hobx8l/actor_critic_learns_well_and_then_dies/
Explore the environment by learning a random neural network - RND,1594313688,,reinforcementlearning,BitsOfDL,False,/r/reinforcementlearning/comments/ho6neu/explore_the_environment_by_learning_a_random/
[R] Self-Supervised Policy Adaptation during Deployment,1594305490,,reinforcementlearning,joepadde,False,/r/reinforcementlearning/comments/ho4764/r_selfsupervised_policy_adaptation_during/
"Converting matlab's pre-defined ""rlQAgent"" into a 'minimax-q' agent",1594304167,"Hello everyone. This request might be out of content so please warn me if so. For my dissertation project, I need a 'minimax-q' agent, but my supervisor told me to convert ""rlQAgent"" (which is predefined in matlab) into a 'minimax-q' agent. I post the ""rlQAgent"" code in the following code block. I can see that I need to change a few lines like; `function Action = getActionImpl(this,Observation)` this part as it gives a max value, but I don't know how. For instance, should I introduce a boolean to understand whether this agent is a max or min? What parts would you suggest me to change and how?

Thanks in advance!!

    classdef rlQAgent &lt; rl.agent.AbstractAgent
        % rlQAgent: Creates a Q-learning agent.
        %
        %   agent = rlQAgent(CRITIC) creates a Q-learning agent with default
        %   options and the specified critic representation.
        %
        %   agent = rlQAgent(CRITIC,OPTIONS) creates a Q-learning agent with
        %   the specified options. To create OPTIONS, use rlQAgentOptions.
        %
        %   See also: rlQAgentOptions, rlSARSAAgent, rlDDPGAgent, rlPGAgent, rlACAgent, rlDQNAgent
        
        % Copyright 2017-2019 The MathWorks Inc.   
    
        properties (Dependent)
            % Options to configure RL agent
            AgentOptions
        end
        
        properties (Access = private)
            % Private options to configure RL agent
            AgentOptions_ = [];
        end
        
        properties (Access = private)
            % Critic Representation
            Critic
            
            % Epsilon-Greedy Exploration parameters
            ExplorationModel
        end
        
        properties (Constant, Access = private)
            % version indicator for backward compatibility
            Version = 2
        end
        
        methods
            function this = rlQAgent(Critic, Option)
                % Constructor
                
                this = this@rl.agent.AbstractAgent();
                
                % extract observation and action info
                this.ActionInfo = Critic.ActionInfo;
                this.ObservationInfo = Critic.ObservationInfo;
                
                % set agent option
                this.AgentOptions = Option;
                
                % REVISIT: not support agent has state
                if hasState(Critic)
                    error(message('rl:agent:errAgentHasStateNotSupport'))
                end
                
                % set representation
                this = setCritic(this,Critic);
                this.HasCritic = true;
      
                % Construct exploration model
                this.ExplorationModel = this.AgentOptions.EpsilonGreedyExploration;
            end
            
            function Action = getActionWithExploration(this,Observation)
                % Return an action with exploration
                
                if rand &lt; this.ExplorationModel.Epsilon
                    Action = usample(this.ActionInfo);
                else
                    Action = getAction(this,Observation);
                end
                
                if iscell(Action) &amp;&amp; numel(Action) == 1
                    % unwrap cell to produce same behavior with previous
                    % release
                    Action = Action{1};
                end
               
                if strcmp(getStepMode(this),""sim-with-exploration"")
                    % (for parallel training) update the noise model on workers
                    this.ExplorationModel = update(this.ExplorationModel);
                end
            end
            
            %==================================================================
            % Get/set
            %==================================================================
            function Critic = getCritic(this)
                % getCritic: Return the critic representation, CRITIC, for the
                % specified reinforcement learning agent, AGENT.
                %
                %   CRITIC = getCritic(AGENT)
                
                Critic = this.Critic;
            end
            
            function this = setCritic(this,Critic)
                % setCritic: Set the critic of the reinforcement learning agent
                % using the specified representation, CRITIC, which must be
                % consistent with the observations and actions of the agent.
                %
                %   AGENT = setCritic(AGENT,CRITIC)
                %
                            
                % validate critic is a Q representation
                validateattributes(Critic, {'rl.representation.rlQValueRepresentation'}, {'scalar', 'nonempty'}, '', 'Critic');
                
                % validate action ans observation infos are same
                if ~isCompatible(this.ActionInfo,Critic.ActionInfo)
                    error(message('rl:agent:errIncompatibleActionInfo'))
                end
                if ~isCompatible(this.ObservationInfo,Critic.ObservationInfo)
                    error(message('rl:agent:errIncompatibleObservationInfo'))
                end
                
                % set loss
                Critic = setLoss(Critic,""rl.loss.rlmse"");
                
                % DQN does not support critic with continuous action data spec
                if isa(Critic.ActionInfo, 'rl.util.rlNumericSpec')
                    error(message('rl:agent:errDQNContinuousCritic'))
                end
                
                % Set critic network
                this.Critic = Critic;
                
                reset(this);
            end
            
            function set.AgentOptions(this,NewOptions)
                validateattributes(NewOptions,{'rl.option.rlQAgentOptions'},{'scalar'},'','AgentOptions');
                % validate experience buffer has sufficient length
              
                this.AgentOptions_ = NewOptions;
                this.SampleTime = NewOptions.SampleTime;
                this.ExplorationModel = NewOptions.EpsilonGreedyExploration;
            end
            
            function Options = get.AgentOptions(this)
                Options = this.AgentOptions_;
            end
        end
        
        %======================================================================
        % Implementation of abstract methods
        %======================================================================
        methods (Access = protected)
            function [rep,argStruct] = generateProcessFunctions_(this,argStruct)
                % Q agent code gen
                rep = this.Critic;
                if strcmpi(getQType(this.Critic), 'multiOutput')
                    argStruct = rl.codegen.generateDiscreteCriticMultiOutQFcn(argStruct,this.ActionInfo);
                else
                    argStruct = rl.codegen.generateDiscreteCriticSingleOutQFcn(argStruct,this.ActionInfo);
                end
            end
            
            function q0 = evaluateQ0(this,exp)
                % overload for agents that implement critics
                observation = exp{1};
                q0 = getMaxQValue(this.Critic,observation);
                if isa(q0,'dlarray')
                    q0 = extractdata(q0);
                end
            end
            
            % set/get tunable parameters
            function setLearnableParametersImpl(this,p)
                this.Critic = setLearnableParameters(this.Critic,p.Critic);
            end
            function p = getLearnableParametersImpl(this)
                p.Critic = getLearnableParameters(this.Critic);
            end
            
            function Action = getActionImpl(this,Observation)
                % Given the current state of the system, return an action.
                [~,ActionIdx] = getMaxQValue(this.Critic, Observation);
                Action = getElementValue(this.ActionInfo,ActionIdx);
            end
            
            function resetImpl(this)
                % Revert exploration model to original parameters
                this.ExplorationModel = this.AgentOptions.EpsilonGreedyExploration;
            end
            
            function action = learn(this,exp)
                % learn from current experiences, return action with exploration
                % exp = {state,action,reward,nextstate,isdone}
                
                Observation     = exp{1};
                Action          = exp{2};
                Reward          = exp{3};
                NextObservation = exp{4};
                Done            = exp{5};
                
                if Done == 1
                    % for final step, just use the immediate reward, since
                    % there is no more a next state
                    QTargetEstimate = Reward;
                else
                    TargetQValue = getMaxQValue(this.Critic, NextObservation);
                    QTargetEstimate = Reward + this.AgentOptions.DiscountFactor * TargetQValue;
                end
                
                % update the critic with computed target
                CriticGradient = gradient(this.Critic,'loss-parameters',...
                        [Observation,Action], QTargetEstimate);
                this.Critic = optimize(this.Critic, CriticGradient);
                
                % update exploration model
                this.ExplorationModel = update(this.ExplorationModel);
                
                % compute action from the current policy
                action = getActionWithExploration(this, NextObservation);
            end
            
            function trainingOptions = validateAgentTrainingCompatibilityImpl(~,trainingOptions)
                % Validate Q agent training options compatibility
                
                if ~strcmpi(trainingOptions.Parallelization,'none')
                    % currently do not support parallel training
                    error(message('rl:general:errParallelTrainNotSupport'));
                end
            end
            
            function HasState = hasStateImpl(this)
                HasState = hasState(this.Critic);
            end
        end
        
        methods (Hidden)
            function actor = getActor(~)
                % getActor: Q agent does not have actor. Therefore, it
                % returns empty.            
                
                actor = [];
            end
        end
        
        methods (Static)
            function obj = loadobj(s)
                if isstruct(s)
                    if ~isfield(s,'Version')
                        % version 1 does not have Version field
                        % In version 2,
                        %   - Critic changes from rlRepresentation to rlQValueRepresentation
                        criticModel = s.Critic.getModel;
                        if isa(criticModel,'DAGNetwork') || isa(criticModel,'nnet.cnn.LayerGraph')
                            s.Critic = rlQValueRepresentation(criticModel,...
                                s.ObservationInfo, s.ActionInfo, ...
                                'Observation', s.Critic.ObservationNames, ...
                                'Action', s.Critic.ActionNames, ...
                                s.Critic.Options);
                        else
                            s.Critic = rlQValueRepresentation(criticModel,...
                                s.ObservationInfo, s.ActionInfo, ...
                                s.Critic.Options);
                        end
                    end
                    obj = rl.agent.rlQAgent(s.Critic,s.AgentOptions_);
                    % REVISIT: how to retain the state of optimizer
                    obj.ExplorationModel = s.ExplorationModel;
                else
                    obj = s;
                end
            end
        end
    end",reinforcementlearning,mynameiscba,False,/r/reinforcementlearning/comments/ho3tmv/converting_matlabs_predefined_rlqagent_into_a/
Actor Critic Negative Loss?,1594293976,"I was just implementing an Advantage Actor Critic. Although it performs reasonably well, there is this thing that bothers me. 

An example of the advantage function could be TD error:

&amp;#x200B;

*Processing img rt15h6vhgt951...*

But this thing might as well be negative. In that case the loss function will be negative and optimizer will try to make it even more negative. Could this possibly be an issue that causes instabilities in the training?  If no, why? If yes, how can that be corrected while staying with policy gradient convergence conditions?",reinforcementlearning,Light991,False,/r/reinforcementlearning/comments/ho1fih/actor_critic_negative_loss/
Where to submit your paper on ?( Reinforcement Learning especially),1594292061,,reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/ho120h/where_to_submit_your_paper_on_reinforcement/
Difference in importance weighting,1594291638,"Hey guys, I wanted to ask, if someone can explain to me the difference it has on the learning effect and maybe in what cases which one to chose of the given formulas. Note that rho\_t is an truncated importance weight. Generally speaking one is weighting the TD difference, whereas the other is weighting the TD target, but I am unable to understand the intuition of the difference.

[See IMPALA paper](https://preview.redd.it/x9s20h2m9t951.png?width=199&amp;format=png&amp;auto=webp&amp;s=8c1b27475b9864cb5c5d1d2542e9f1ea447fd7ac)

&amp;#x200B;

[See DAVID SILVER lecture RL Lecture 5](https://preview.redd.it/ec1283li9t951.png?width=621&amp;format=png&amp;auto=webp&amp;s=78f692fc7d7a3ddf1a99dad40d0211773c70616d)",reinforcementlearning,Lunnaris001,False,/r/reinforcementlearning/comments/ho0z4n/difference_in_importance_weighting/
Tuning hyperparameters or changing algorithms?,1594276088,"Hi, we all know that the RL algorithm is very sensitive to hyperparameters. So we spend a lot of time to tune hyperparameters. I wonder how long should we test hyperparameters. If the performance (e.g. reward curve) is poor, then we keep tune hyperparameters. Particularly, in the customized environment, we can design a reward function such as changing the function or more shaping rewards. On the other hand, we can change the agent algorithm from DQN to DDPG, for instance. 

Obviously, DQN cannot naturally solve a multi-agent control problem. However, we can get some good results after finding perfect hyperparameters and the right reward functions, and this case is very rare or impossible. After all, the poor performance comes from wrong algorithm choice, not enough hyperparameter tuning, incorrect reward function, or all of them. 

It would be hard to verify the reason, but normally which part should we take first? When having poor performances, should we change the algorithm or take more hyperparameter tuning (and test different reward functions)?",reinforcementlearning,TK-SZ,False,/r/reinforcementlearning/comments/hny5io/tuning_hyperparameters_or_changing_algorithms/
Should the target network in DQN be in train\eval mode?,1594238897,"I am using DQN to train an agent to play a game. And I am using BatchNorm and Dropout layers. So when I want to get the values of the next states should the target network be in which mode eval\\train?  
As eval disable the functionality of the BatchNorm and dropout layers and train do the opposite?",reinforcementlearning,medoaashry,False,/r/reinforcementlearning/comments/hnogl5/should_the_target_network_in_dqn_be_in_traineval/
Doubt about epsilon-greedy methods from Reinforcement Learning: Andrew Barto and Richard S. Sutton,1594227936,"In chapter 2 pg 28, there's a statement about epsilon-greedy methods.  ""An advantage of these methods is that, in the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the Qt(a) converge to q\*(a).  This of course implies that the probability of selecting the optimal action converges to greater than 1 - epsilon, that is, to near certainty.""

Why is it the case that the probability will converge to be greater than 1 - epsilon?

Thanks!",reinforcementlearning,PleaseKillMeNowOkay,False,/r/reinforcementlearning/comments/hnkw1b/doubt_about_epsilongreedy_methods_from/
Question about AGZ self-play,1594225868,"I'm implementing AGZ for another game and I'm trying to understand how instances of self-play differ sufficiently within a single batch (that is, using the same set of weights).

My current understanding of the process is as follows: for a given root state, we get a policy from the move priors generated by the network + Dirichlet noise. This will clearly be different across multiple games. However, it seems that once we start simulating moves underneath a given child of the root we would get a deterministic sequence, leading to similar distributions from which to draw the next move of the game. (This is particularly concerning to me because the width of the tree in my application is significantly smaller than that of a game of Go.)

So my questions are:

1. Is my understanding correct, or is there something I've missed that makes this a non-issue?
2. Otherwise, should I be looking to add more noise into the process somehow, perhaps moreso in  the early stages of training?",reinforcementlearning,parallelparkerlewis,False,/r/reinforcementlearning/comments/hnk8qs/question_about_agz_selfplay/
Bimodal and Multimodal distributions for action selection,1594205490,"Hey, 

I'm looking for some resources in terms of learning multimodal distributions for the action selection in a policy search. Usually, most algorithms simply learn a unimodal distribution alias gaussian. However, most paper claim that multimodal distributions might lead to some improvements. 

Has someone worked on it already or knows some resources about it? Being applied or general theory. I would be interested in what you think about it as well.

&amp;#x200B;

Thanks in advance!",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/hnex6v/bimodal_and_multimodal_distributions_for_action/
DQN not learning in tensorflow 2.x,1594203300,"Hello everyone, I am trying two code DQN in tensorflow 2.x. But the network does not even learn a bit.

These are three colab notebooks, 

1) These two  does not learn a thing with or without a target network

[https://colab.research.google.com/drive/1wi\_ceQEYQVFEE2n14cAG6xRx8fgJISuJ?usp=sharing](https://colab.research.google.com/drive/1wi_ceQEYQVFEE2n14cAG6xRx8fgJISuJ?usp=sharing)

[https://colab.research.google.com/drive/1RzY6xVjAnPO1xyPZ04vWAaeYVyxaNiZM?usp=sharing](https://colab.research.google.com/drive/1RzY6xVjAnPO1xyPZ04vWAaeYVyxaNiZM?usp=sharing)

2) This one learns very fast without the target network.

[https://colab.research.google.com/drive/1RzY6xVjAnPO1xyPZ04vWAaeYVyxaNiZM?usp=sharing](https://colab.research.google.com/drive/1RzY6xVjAnPO1xyPZ04vWAaeYVyxaNiZM?usp=sharing)

Why the above Deep Q learning agents do not learn. Any help would be appreciated, 

Thank you.",reinforcementlearning,abhisheksuran,False,/r/reinforcementlearning/comments/hnehy6/dqn_not_learning_in_tensorflow_2x/
I need suggestions on good RL courses,1594202327,"Hello, as a start I'm well experienced with programming and python, conducted many deep learning and general purpose projects, and generally I never find difficulties implementing pretty much anything in python. I'm encountering a real difficulty in finding an RL course with good and understandable programming exercises. I tried so far 3 different courses and I'm very upset with programming exercises that either expect you to be an RL expert already attending their course for fun or need lots of time in understanding their exercise logic rather than focusing on RL concepts to learn. Another problem is that I cannot read books on the topic because they are ultra-cryptic, full of mathematical notations that for you to understand them, you need to be at least a PHD level in mathematics and you will still struggle through translating the mathematical aspect to some useful code. Here are the courses I tried

1. [RL specialization](https://www.coursera.org/specializations/reinforcement-learning) on coursera, it's understandable in terms of lecture videos but when it comes to programming exercises, you'll be spending 90% of the time trying to unlock their very complicated program structure(you'll worry about the details that you shouldn't worry about) besides exercises are extremely boring and the main exercise goal is usually ambiguous, you might spend several hours to output some ordered list of items that contains certain values in a certain order so most of the exercises are pretty pointless(do not focus on any practical aspect) and hard to understand.

2. [David Silver's youtube course](https://www.youtube.com/watch?v=2pWv7GOvuf0) which is pretty comprehensive but the downsides are the lecture videos are too long(2 hours each) and the exercises are complicated and it's also unclear to me the main purpose of the exersises

&amp;#x200B;

3. [Practical reinforcement learning](https://www.coursera.org/learn/practical-rl) pros are the course is very short and to the point(no several hours of videos) cons are pretty much the same thing about other courses: the awful programming exercises that either assume you are already an expert or need to spend days studying the code in order to be able to use their exercises

&amp;#x200B;

It's been more than a month I've been struggling to understand however I'm failing and I need to know what should I do / what sources should I be looking at because I really got bored and thinking of ignoring the whole learning RL idea.",reinforcementlearning,emadboctor,False,/r/reinforcementlearning/comments/hnebb8/i_need_suggestions_on_good_rl_courses/
I need suggestion on good RL courses with good programming exercises.,1594201460,"Hello, I'm encountering a real difficulty in finding an RL course with good and understandable programming exercises. I tried so far 3 different courses and I'm very upset with programming exercises that either expect you to be an RL expert already attending their course for fun or need lots of time in understanding their exercise logic rather than focusing on RL concepts to learn.

I tried [RL specialization](https://www.coursera.org/specializations/reinforcement-learning) on coursera, it's understandable in terms of lecture videos but when it comes to programming exercises, you'll be spending 90% of the time trying to unlock their very complicated program structure(you'll worry about the details that you shouldn't worry about) besides exercises are extremely boring and the main exercise goal is usually ambiguous, you might spend several hours to output some ordered list of items that contains certain values in a certain order so most of the exercises are pretty pointless and hard to understand.

&amp;#x200B;

I also tried",reinforcementlearning,emadboctor,False,/r/reinforcementlearning/comments/hne5pf/i_need_suggestion_on_good_rl_courses_with_good/
Bellman Equation Video review,1594173904,"Hey guys,

I recently made a [video](https://youtu.be/71mPOqQpXng) on Bellman Expectation equations and I'd really love your feedback on how correct my understanding and derivation is.

I made this because I wanted to really understand this to its core. I'm not 100% confident I did tho, but making the video definitely helped me understand it better than just glossing over a textbook.

I'd really appreciate if you could pinpoint my mistakes/recommend other videos to further help me understand this topic.

Thanks bunch!",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hn89rc/bellman_equation_video_review/
Interesting problems in Applied RL,1594166972,I want to focus on some applied RL research. I wonder if there are some interesting problems in various domains where RL could possibly be explored. Any suggestions?,reinforcementlearning,-Ulkurz-,False,/r/reinforcementlearning/comments/hn6ev3/interesting_problems_in_applied_rl/
Rollouts in Model Free Reinforcement Learning (specifically MOTO),1594162738,"Trying to understand the approach given in this paper: [Model Free Trajectory Optimization for Reinforcement Learning](http://proceedings.mlr.press/v48/akrour16.pdf), but I'm stuck.

They give the algorithm block 

    Input: Initial policy pi^0, number of trajectories per iteration M, step-size epsilon and entropy reduction rate beta_0 
    Output: Policy pi^N
    
for i = 0 to N − 1 do
        Sample M trajectories from pi^i
        for t=T to 1 do (should be 1 to T?)
            Estimate state distribution (Sec. 4.3) 
            Compute IW for all (s,a,s') for all i in D (Sec. 4.2)
            Estimate the Q-Function Q_predicted at (s,a) (Sec. 4.1)
            Optimize: g^t_i(eta, omega)  (Sec. 3.3)
            Update pi^{i+1} using eta, omega, p_obs, Q_predicted  (Sec. 3.2)

but I don't understand how they are sampling trajectories with a model-free agent and in a continuous action space. Do they mean that the state estimate distribution is done T times based purely on a probabilistic model they are creating (p)? 

The notation is also tripping me up a little but from what I understand each time they go to update the policy they have to generate a probabilistic trajectory using observed transitions and in fact they do it for N experiences that come up in an experience replay. 

If anyone has experience with MOTO I'd love to talk more. First time I'm trying to implement RL in a controller with continuous state space.",reinforcementlearning,little_grey_mare,False,/r/reinforcementlearning/comments/hn56go/rollouts_in_model_free_reinforcement_learning/
What are absorbing states?,1594152466,"Hi, I was reading the [MuZero paper](https://arxiv.org/pdf/1911.08265.pdf) and one of the key differences from AlphaZero they mention is that there aren't any explicit terminal nodes, and that they treat terminal states as absorbing states during training. 

&gt;AlphaZero stopped the search at tree nodes representing terminal states and used the terminal value provided by the simulator instead of the value produced by the network. MuZero does not give special treatment to terminal nodes and always uses the value predicted by the network. Inside the tree, the search can proceed past a terminal node - in this case the network is expected to always predict the same value. This is achieved by treating terminal states as absorbing states during training.

What exactly does this mean? How is training at terminal states different than normal states?",reinforcementlearning,1yian,False,/r/reinforcementlearning/comments/hn20tq/what_are_absorbing_states/
Framework for DQN to complement with C/C++,1594136374,"Hi all,
I am interested in implementing a DQN agent for my project. The end goal would be to connect the DQN agent to an environment that is coded in C/C++. Which python-based framework and library would be easier to connect to C/C++?
I came across various libraries like RL-Glue, opengym, Google acme, rllib, etc.
Any help would be really appreciated.",reinforcementlearning,splurgein,False,/r/reinforcementlearning/comments/hmwv30/framework_for_dqn_to_complement_with_cc/
Actor-Critic Vs DQN,1594126417,"Hi,

I am working on discrete action space (Dialogue Manager ) and after doing experiments, I found DQN Agent is doing better than AC Agent(Off Policy) in terms of success rate, Reward, and Dialogue length.  Few people are claiming ACM should learn faster but here nothing like that? When i am running ACM with Paramaetre as optimal hyperparameter of DQN then ACM is leading.  Is there any Generalize hypothesis that ACM does better than DQN? Please comment your views. 

&amp;#x200B;

 

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

[View Poll](https://www.reddit.com/poll/hmtzqp)",reinforcementlearning,virabhi,False,/r/reinforcementlearning/comments/hmtzqp/actorcritic_vs_dqn/
"ValueError: not enough values to unpack (expected 3, got 1)",1594122310,"Hi RL fanatics,
I have an issue on an RL program where the ‘time_step’ and ‘next_time_step’ do not match. I believe this is what is causing the error mentioned in the title when I try to run the program. When comparing my program to the example DQN tutorial provided by TensorFlow, it looks like the time_step and next_time_step need to provide the exact same structure (observation, reward and action). 

Can anyone help me in debugging this issue?

I have less than a rudimentary understanding of the RL code structure so let me know if further information needs to be provided xx",reinforcementlearning,masochistr,False,/r/reinforcementlearning/comments/hmt0ix/valueerror_not_enough_values_to_unpack_expected_3/
NN forward passes for MCTS too slow. Advice?,1594062789,"Background: I'm trying to apply the NN-directed MCTS from AlphaGo Zero to a different board game. My network consists of two inputs (the board, which runs through a small residual tower, and some metadata which goes through a series of dense layers), many policy outputs, and \~10M trainable parameters. I'm currently running on a GTX 980 Ti and an i7-5930K @ 3.5GHz, using keras 2.3.1 and tensorflow 2.1.0.

Right now, generating training samples is far too slow. Via CPU, a single forward pass takes \~5ms. Maybe surprisingly, it takes \~6ms via GPU. Given that a single move requires hundreds of MCTS simulations, I think this is currently unusable. I have the following questions:

1. Is the CPU speedup expected? Could it be attributed to the overhead of going to the GPU given that the batch size is always 1? If not, how might I go about fixing this? (I have verified via \[set\_log\_device\_placement\] that the code runs where I think it runs.)
2. How can I tell whether these latencies are expected for this hardware / network size?
3. Other than parallelizing self-play games / reducing the network size / convnet tricks, are there other avenues that I can explore to make this better?

One quick aside: my hardware is fairly old, but I tried spinning up a Google C2 instance and weirdly got a 5x slowdown (their advertised clock speeds are similar to mine so I don't know what's going on there). Are there better hardware resources that I should be looking into?",reinforcementlearning,parallelparkerlewis,False,/r/reinforcementlearning/comments/hme983/nn_forward_passes_for_mcts_too_slow_advice/
[Project] RLRunner - a simple framework for Reinforcement Learning,1594061997,"https://github.com/PriestTheBeast/RLRunner#readme

RLRunner is an easy to use and expand framework for Reinforcement Learning experimentation and run simulation.

I made this to be as whatever you might need as possible.

You can install it as a python library and quickly have a system for comparing some agents and experiment with RL, or even take the package from here, slam it in your project and redesign anything you want from it, providing a good foundation for extension.

I hope this can be useful to people :)",reinforcementlearning,CreativeUsername1000,False,/r/reinforcementlearning/comments/hmdu0c/project_rlrunner_a_simple_framework_for/
[D] Has anyone received the NeurIPS desk rejects yet.,1594060954,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/hmdb6l/d_has_anyone_received_the_neurips_desk_rejects_yet/
Batch RL: neural fitted Q iteration and training process,1594043224,"The original Neural fitted Q iteration algorithm makes use of the Rprop algorithm for fast supervised learning. Here is a quote from the paper:

""The training of the pattern set is repeated for several epochs \[...\], until the pattern set is learned successfully""

However, I don't really understand how they can assess that the network is successfully learning. What does it mean ""until the pattern set is learned successfully"" ? What is the stopping criterion for learning the next Q\_k approximation ? Is it just based on the number of epochs ?

Paper: [http://ml.informatik.uni-freiburg.de/former/\_media/publications/rieecml05.pdf](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf)",reinforcementlearning,loicsacre,False,/r/reinforcementlearning/comments/hm7xz4/batch_rl_neural_fitted_q_iteration_and_training/
How to choose Network Architecture and parameters like number of hidden layers number of nodes in Deep Network for Deep Reinforcement Learning?,1594015351,,reinforcementlearning,shivangg27,False,/r/reinforcementlearning/comments/hm24la/how_to_choose_network_architecture_and_parameters/
PP02 C# implementation,1593988571,"Does anyone know of any C# implementations of PPO2? 

I have only found pytorch implementations and some tensorflow implementations, also python.

I am aware that this algorithm is still relatively new, but I would get my own implementation running in C# and would love to have some references :)",reinforcementlearning,Flonaldo,False,/r/reinforcementlearning/comments/hlvrin/pp02_c_implementation/
Where to go after finishing an intro to RL course?,1593979826,"I have finished Ben Eaters RL course, working through most of the content in Sutton and Barto's Introduction to Reinforcement Learning as well. I was wondering where I should go from here. What would be an appropriate next course or book to build upon my knowledge? I have been implementing SOTA RL algorithms, however I would prefer some sort of guided learning instead. My end goal is to get a research position at my university in their RL lab starting in the fall (I am an undergrad).",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/hlt9vl/where_to_go_after_finishing_an_intro_to_rl_course/
How long to learn DRL coming from DL?,1593976322,"Hey there,
I recently finished Andrew Ng's specialization on Deep Learning (5 course specialization by deep learning.ai).
How long do you think it'd take me to become proficient/understand and implement the basics in DRL having the knowledge (math intensive) of ML and DL?
Just a note: I'm confident in linear algebra, multivariate calculus, and probability+stats.

Do you think I could take Emma Brunskill's class on DRL (CS 234) in a week or 2? I can give 60 hours a week (I'm a sophomore undergrad, hence the free time lol). Any other resources you recommend? 

Thanks and appreciate the help.",reinforcementlearning,bci-hacker,False,/r/reinforcementlearning/comments/hls8yj/how_long_to_learn_drl_coming_from_dl/
Top Evaluation Metrics For Reinforcement Learning,1593962833,,reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/hloez8/top_evaluation_metrics_for_reinforcement_learning/
PPO vs DQN Output Layer Activation Function,1593952845,"So I've been experimenting with both DQN (And it's dueling DQN variant) and PPO agents lately for my discrete action space environment. Thing is, I've struggled a bit with understanding how the output layers of both methods work. It is my understanding that DQN uses a linear output layer, while PPO uses a fully connected one with softmax activation. For a while, I thought my PPO agent didn't function as well as DQN because I didn't frame the problem and reward function well enough. Today though, while reading online, I discovered that I've been using a linear output layer with PPO like I've been doing with DQN. Upon switching to a fully connected softmax output layer, PPO suddenly came alive and started learning much better. I'm struggling to understand why that is. 
Why does DQN require a linear output layer to learn well while PPO requires a fully connected softmax one? What determines which layer and activation function should be used in the output layer in RL?",reinforcementlearning,HeisenbergsMyth,False,/r/reinforcementlearning/comments/hllzwr/ppo_vs_dqn_output_layer_activation_function/
Emergence of complex strategies through multi-agent competition,1593922806,Complex strategies can naturally emerge through multi-agent competition. Take a look at our [video](https://youtu.be/ltHgKYc0F-E) showing guards and attackers competing against each other while training with reinforcement learning. I believe you'll find it interesting.,reinforcementlearning,dekankur,False,/r/reinforcementlearning/comments/hlgm2c/emergence_of_complex_strategies_through/
Advantage calculation in A2C,1593920705,"Hi guys, I'm writing an Advantage actor-critic implementation in pytorch, but my algorithm is having trouble solving CartPole-v0

Looking at a variety of sources online, it seems like there are a few variations of advantage and loss functions. Which one of these should I be using?

For advantage calculation: 

1) advantage = R + (discount\_factor)\*V(nxt\_state) - V(state)

2) advantage = R - V

&amp;#x200B;

For Critic loss calcuation:

1) critic\_loss = MSE(R, V)

2) critic\_loss = advantage\^2

&amp;#x200B;

If anyone has links to working A2C code that would be helpful!",reinforcementlearning,rb31415,False,/r/reinforcementlearning/comments/hlg6xa/advantage_calculation_in_a2c/
Multi-agent Reinforcement Learning Workshop by Marc Lanctot,1593900586,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/hlbpxp/multiagent_reinforcement_learning_workshop_by/
"Reinforcement learning for continuous Temporal data, controlling Timeout value of printer hardware.",1593883746,"hello i'm currently doing my dissertation which aims to apply different types of timeout policies to continuous temporal data of print jobs for a whole year. the general aim is to balance the amount of state changes from idle to sleep, and the total sum of time spend in the sleep state by controlling the machine's timeout value. I have modelled a static, adaptive and predictive policy, but i want to include a model that utilises reinforcement learning, would anyone know how this could be done?",reinforcementlearning,toluu,False,/r/reinforcementlearning/comments/hl73wv/reinforcement_learning_for_continuous_temporal/
Any resource on problems of distribution of multiple agents?,1593864565,"Exactly like the title, I have been looking into distribution of agents, so that multiple agents go to different locations on the map, usually in path planning/target finding type of situations. 

Aim is to focus solely on making agents go separate ways, and not swarm one location. 

So I would be really glad, if someone knows good papers/blogs or any other insights on this. 

Thank you.",reinforcementlearning,rikt789,False,/r/reinforcementlearning/comments/hl2bo6/any_resource_on_problems_of_distribution_of/
AI learns to play the chrome dino game using Q learning &amp; Conv Nets,1593857586,,reinforcementlearning,saraltayal,False,/r/reinforcementlearning/comments/hl12d4/ai_learns_to_play_the_chrome_dino_game_using_q/
"""Improving GAN Training with Probability Ratio Clipping and Sample Reweighting"", Wu et al 2020 (PPO for stabilizing WGANs)",1593820209,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hktlkm/improving_gan_training_with_probability_ratio/
"""Model-based Reinforcement Learning: A Survey"", Moerland et al 2020",1593819937,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hktj76/modelbased_reinforcement_learning_a_survey/
How to visualize DQN results?,1593803872,,reinforcementlearning,n-smj,False,/r/reinforcementlearning/comments/hkp4jk/how_to_visualize_dqn_results/
RL Weekly 42: Special Issue on NeurIPS 2020 Competitions,1593790419,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/hkl4rm/rl_weekly_42_special_issue_on_neurips_2020/
Question about Soft Actor-Critic,1593783932,"In [Soft Actor-Critic Algorithms and Applications](https://arxiv.org/pdf/1812.05905.pdf#algorithm.1), the author says that the policy is represented as a single Gaussian. While in Sergey Levine's talk, he use the following examples. (Fig 1 as the standard RL, and Fig 2 as the MaxEnt RL) In that example, the policy seems to catch the multimodal distribution.

[Fig 1](https://i.imgur.com/0pThbxP.png)
[Fig 2](https://i.imgur.com/s6CEOU9.png)

Q: How can a Gaussian policy model a multimodal distribution? If not, isn't the policy just a high variance distribution centering at the optimal action/",reinforcementlearning,zbqv,False,/r/reinforcementlearning/comments/hkjb9r/question_about_soft_actorcritic/
Playing Space Invaders using a Deep Q Network.,1593776065,,reinforcementlearning,white_noise212,False,/r/reinforcementlearning/comments/hkhht2/playing_space_invaders_using_a_deep_q_network/
Reinforcement learning as a career.,1593762936,Hey guys I am really interested in focusing my career towards reinforcement learning and as of right now I am a second year college student pursuing his [B.tech](https://B.tech)  in computer science. What are the necessary steps that I should take that would help me in achieving this goal? What should I focus on from this very moment...like the pre-requisites? I have been learning a lot of probability recently as when I directly first jumped into reinforcement learning  I was not exactly comfortable by the probability theory involved. But apart from that could someone tell me what should I focus on and what do I pursue in the future for me to get into the field of reinforcement learning? It would really help me a lot!!!,reinforcementlearning,pinkman8144,False,/r/reinforcementlearning/comments/hkf26w/reinforcement_learning_as_a_career/
Mystery game for RL from Udacity RL,1593760204,,reinforcementlearning,RafazZ,False,/r/reinforcementlearning/comments/hkekog/mystery_game_for_rl_from_udacity_rl/
A survey for developing of reinforcement learning environments,1593742452,"Hello, I'm making various RL environments such as

* RosettaStone -  Hearthstone, and Hearthstone: Battlegrounds simulator using C++ with some reinforcement learning  ( [https://github.com/utilForever/RosettaStone](https://github.com/utilForever/RosettaStone) )
* baba-is-auto -  Baba Is You simulator using C++ with some reinforcement learning ( [https://github.com/utilForever/baba-is-auto](https://github.com/utilForever/baba-is-auto) )
* PokeMaster -  Pokemon Battle simulator using C++ with some reinforcement learning ( [https://github.com/utilForever/PokeMaster](https://github.com/utilForever/PokeMaster) )
* conquer-the-spire - Slay the Spire simulator using C++ with some reinforcement learning ( [https://github.com/utilForever/conquer-the-spire](https://github.com/utilForever/conquer-the-spire) )
* AlphaTFT - Teamfight Tactics simulator using C++ with some reinforcement learning ( [https://github.com/utilForever/AlphaTFT](https://github.com/utilForever/AlphaTFT) ) 
* Runeterra - Legends of Runeterra simulator using C++ with some reinforcement learning ( [https://github.com/utilForever/Runeterra](https://github.com/utilForever/Runeterra) )
* perfect-mario-maker -  Super Mario Maker 2 simulator using C++ with some reinforcement learning  ( [https://github.com/utilForever/perfect-mario-maker](https://github.com/utilForever/perfect-mario-maker) )

Several projects are currently actively being developed, and some are now in their early stages.If you have any games you would like to make into an environment for RL, please feel free to tell me and I will refer them to development. Thank you.",reinforcementlearning,utilForever,False,/r/reinforcementlearning/comments/hkapnj/a_survey_for_developing_of_reinforcement_learning/
Looking for cloud resource recommendations,1593717325,,reinforcementlearning,parallelparkerlewis,False,/r/reinforcementlearning/comments/hk3ci0/looking_for_cloud_resource_recommendations/
NAF: normalized advantage function,1593701461,"Hey,  


since yesterday I'm working on an implementation of [NAF](https://arxiv.org/pdf/1603.00748) but somehow it's not converging. 

I'm not sure whats the problem. Would be great if someone could have a look: [Code](https://github.com/BY571/NAF/blob/master/NAF.ipynb)",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/hjyepu/naf_normalized_advantage_function/
Papers about code-level optimizations in Policy Gradient?,1593679152,Could you point me to some papers that describe the most important optimizations and implementation details that make Policy Gradient algorithms work better?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/hjtdgv/papers_about_codelevel_optimizations_in_policy/
Learning what action to take and to anticipate when to make a new decision can simplify learned policies and increase the learning speed,1593673666,,reinforcementlearning,Science_Squid,False,/r/reinforcementlearning/comments/hjsc39/learning_what_action_to_take_and_to_anticipate/
Submissions now open for the MineRL 2020 Competition: Sample Efficient RL in Minecraft,1593648610,"# [http://minerl.io/competition](http://minerl.io/competition)

Starting July 1st, we are holding a NeurIPS 2020 competition on sample-efficient reinforcement learning using human priors.

![gif](lfhp7cao5c851)

**The competition**

Standard methods require months to years of game time to attain human performance in complex games such as Go and StarCraft. We want to catalyze research on reinforcement learning algorithms that don't require hundreds of years of samples a day to solve complex tasks, lowering the computational barrier to entry.

In our competition, participants develop a system to obtain a diamond in Minecraft using only four days of training time.

![video](lo3hyjtc5c851)

&amp;#x200B;

To enable environment-sample efficiency we have created one of the largest imitation learning datasets \*MineRL-v0\* with over 60 million frames of recorded human player data. Our dataset includes a set of tasks which highlights sparse rewards and hierarchical policies.

![video](e88uuwje5c851)

To improve the experience for competition participants, we have developed our own Minecraft Gym environment \*MineRLEnv\* on top of Malmo to support many new features, including synchronous ticking, pausing, and extremely fast stepping (1000 FPS with head!)

![gif](q68pat3h5c851)

This isn't your traditional RL competition; to ensure real progress is made on sample-efficiency, \***we train and evaluate your models from scratch**.\* Here's how it works

&amp;#x200B;

![img](2ft5r5bk5c851)

We prevent over-engineering to the domain by obfuscating the action and observation spaces using randomized auto-encoders!

RL competitions should promote general, domain agnostic solutions.

![img](1l0b960m5c851)

Among those teams who make it the furthest, the top 3 will be awarded GPUs from NVIDIA and more prizes from additional sponsors to come!

The contest will run from July 1st to December 15th. Here's a full schedule!

&amp;#x200B;

![img](8tg72flr5c851)

Would love to hear some feedback from the r/reinforcementlearning community on what they'd like to see in this and future MineRL competition! Here's the sign-up link: [http://minerl.io/competition](https://t.co/p6hrmnLh3o)",reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/hjmgo3/submissions_now_open_for_the_minerl_2020/
Counterfactual Regret Minimisation,1593643247,"hey guys, im currently trying to wrap my head around the CFR algorithm, but it's pretty hard to do all by myself. I've done vanilla cfr &amp; chance sampling cfr on Kuhn but now trying to add one layer of complexity by playing Leduc and Im stuck. 

So i was wondering if anyone knows a discord or  something like that to chat about this algorithm in particular ? it'd be great to have a community for this topic so we can help each other decipher those papers &amp; exchange ideas. 

thanks",reinforcementlearning,pouuuuuuuplo,False,/r/reinforcementlearning/comments/hjkvz7/counterfactual_regret_minimisation/
Advice for creating a Machine-Learning-Sandbox Game using Unity3D,1593634248,"Hello!

I am currently working out a topic for my bachelor thesis in Game Design and was planning on creating some sort of machine learning sandbox, in which the user can stitch together agents with different sensors (see, hear...) and behaviours (move, jump, shoot...). The player should then be able to create a level (environment), place the agents and define its rewards/penalties and then watch it train, using RL.

The goal is to allow players to quickly test out certain agents and environments, and get interesting results. Most importantly **it should spark an interest in machine learning**.

I wanted to get this to work with Unity3D, since I have lots of experience with the engine. Unity's ML-Agents package seemed promissing, though it only allows training inside the Editor itself, not in builds, due to the Python dependencies.

Having a build (a single executable) would significantly lower the entry barrier for players, since nothing but the executable would be needed (no unity engine, no python...).

Does anyone know if it would be possible to train the agents right in the build? I assume this requires me to somehow bundle python within the build, but I am not sure if that is even possible. Another option would be to use Tensorflow.NET and build my own neural network implementation right inside Unity using C#, completly taking ML-Agents and Python out of the loop (or maybe it's possible to make ML-Agents use Tensorflow.NET instead of Python?).

As you can see, I am pretty unsure as to how and if my plans are achievable. Mostly because I wasn't able to find any meaningful resources for my problem - most machine learning applications seem to run using python, and ML-Agents is structly.

I would really appreciate any kind of advice, ideas or pointers to good resources!   
**Thank you!**

\---  
On another note, I was thinking that I could could simulate the same environment maybe 10 or 20 times next to each other, let the player expolore all environments while the agents are training. This would speed up the training time, though I am not sure if this won't kill my performance...",reinforcementlearning,Flonaldo,False,/r/reinforcementlearning/comments/hji0gc/advice_for_creating_a_machinelearningsandbox_game/
A Closer Look at Invalid Action Masking in Policy Gradient Algorithms,1593631857,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/hjh8wb/a_closer_look_at_invalid_action_masking_in_policy/
Bold lines/Shaded Areas meaning in RL plots?,1593628577,"I see lots of very appealing plots in RL papers such as

[\[Andrychowicz, 2019\] \(https:\/\/arxiv.org\/pdf\/1808.00177.pdf\)](https://preview.redd.it/t3cnb1jbia851.png?width=1596&amp;format=png&amp;auto=webp&amp;s=bdaf5fca85a66273789b58da7da9ce8624d6ccef)

&amp;#x200B;

Especially for average returns:

[\[Mahmood et al. 2018\] \(https:\/\/arxiv.org\/pdf\/1809.07731.pdf\)](https://preview.redd.it/psqmexw3ia851.png?width=1818&amp;format=png&amp;auto=webp&amp;s=d5a99cbccb64daf29bfcb23bd73d8a6f027e927d)

The bold line is mean/average of the values, what are the shaded areas representing? Are they representing max/min or standard deviation in one index in x-axis?

Thanks in advance.",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/hjg4v7/bold_linesshaded_areas_meaning_in_rl_plots/
Coverage Algorithm,1593628289,"I [posted earlier](https://www.reddit.com/r/reinforcementlearning/comments/hfrm04/divergence_before_convergence/) for help with a specific issue, but we haven't been able to come up with a solution.

Do you have any advice for how to make an agent learn how to fully cover a 'grid-world' type environment (i.e visit every coordinate)? We have tried a lot of methods, mostly just consisting of a tabular q-learning algorithm with rewards positive if it finds a new coordinate and negative if it finds a coordinate that its been at already. This only works for grids up to about 4x4 in size and anything greater than that the agent never finds a solution.

We think that maybe introducing the visited nodes history as part of the observation space and adding neural network to handle the increased size might work, but that sounds really hard lol.

Have any of you figured out how to do this or have any advice?",reinforcementlearning,a_ghould,False,/r/reinforcementlearning/comments/hjg1eb/coverage_algorithm/
Parameters for Reward plot?,1593628270,"I see lots of very appealing reward plots in RL papers such as 

&amp;#x200B;

[\[Andrychowicz et al. 2019\] https:\/\/arxiv.org\/pdf\/1808.00177.pdf](https://preview.redd.it/lgrxnq8wga851.png?width=1596&amp;format=png&amp;auto=webp&amp;s=84871727c27ab94ac71c812b0c75744ebfd6fc89)

&amp;#x200B;

[\[Mahmood et al. 2018\] https:\/\/arxiv.org\/pdf\/1809.07731.pdf](https://preview.redd.it/lglzq7p3ha851.png?width=682&amp;format=png&amp;auto=webp&amp;s=4fcd3cbb441d168768f6e5001a4281709e37448e)

The bold line is mean/average of the values, what are the shaded areas representing? Are they representing max/min or standard deviation in one index in x-axis?

Thanks in advance.",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/hjg152/parameters_for_reward_plot/
How to reward the agent that uses limit orders as actions in a trading environment?,1593624521,"Hi Everyone,

Recently, I went through some academic works on training a RL agent to execute buy/sells on historical data, mostly using technical indicators as features including state variables such as inventory. There are also some work done on using limit order book data as extracted features from CNN architectures, but they all use market orders. 

Let's say in your trading environment, it can store orderbook depths and process incoming limit orders (agent's action) at t=1. But the observations (limit orderbook data) does not change t=20, which will trigger the limit order submitted from t=1. I think this is related to Semi-MDP, Eligibility Traces, but not sure how to address this. 

**My question is, how to reward the agent in this case, and what are some existing baseline environments that behave similarly?**",reinforcementlearning,NumberChiffre,False,/r/reinforcementlearning/comments/hjernn/how_to_reward_the_agent_that_uses_limit_orders_as/
What are best SOTA reinforcement learning algorithms?,1593614544,"What are the current best state of the art RL algorithms, both in off- and on-policy case?

I've read in another post, that PPO has been outperformed by newer algorithms, but none were actually mentioned and I was going through this reddit community but suprisingly could not see any recent post listing best SOTA algorithms.

Thank you for the hints!",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/hjbimf/what_are_best_sota_reinforcement_learning/
Best SOTA reinforcement learning algorithms,1593614251,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hjbfa1/best_sota_reinforcement_learning_algorithms/
Is my Flappy Bird agent learning anything?,1593599801,"First of all, it's my first time doing any ML project. I've never taken any courses so every concept is very new to me. Sorry if I sound too dumb.

I'm trying to train a Flappy Bird agent using a DQN. I've searched for different DQN codes and tried to apply them to my game. There are 3 states: y position of bird, bird's vertical and horizontal distances to pipes. And 2 actions - jump or do nothing. My DQN code is based on this work: [https://github.com/BAbhiraman/Q-learning-flappy-bird/blob/master/q%20learning%20bumble%20b%20blog.py](https://github.com/BAbhiraman/Q-learning-flappy-bird/blob/master/q%20learning%20bumble%20b%20blog.py)

I have no idea about how long the training process is supposed to take. I've heard that since I'm using pixel data as input, it'd take longer. I trained it for about 4 hours and this is how it turned out: 

![img](stekq5j448851)

4 hours might not be a long time but it looks like it isn't learning anything. Is it because there is something wrong with my code or am I supposed to run this for 30k+ generations to get better results? Any help is much appreciated.",reinforcementlearning,n-smj,False,/r/reinforcementlearning/comments/hj7so7/is_my_flappy_bird_agent_learning_anything/
"What could cause an agent to continue to explore a ""bad solution"" despite continuously given negative reward? (A2C)",1593597556,,reinforcementlearning,Spencer4678,False,/r/reinforcementlearning/comments/hj7cin/what_could_cause_an_agent_to_continue_to_explore/
Advice for personal RL project,1593586739,"Been working on a personal project with RL for the last couple of days. Basically the idea is to input a target RGB image and a blank canvas. The agent then has an action that modifies the canvas and gets rewarded based on how much closer it got to the target so the canvas is updated and given to the agent again up to some limit of actions (end of episode). The goal is to get some kind of shitty sketch bot.

&amp;#x200B;

So far I am concatenating and feeding in two 50x50 images so 100x50x3  input and using the ""nature\_cnn"" + continuous PPO with gym interface from stable-baselines 3. The action I am giving the agent at each iteration is to choose two points and a color, a line of preset width is then drawn between those points on the canvas so, size 7 action space. The reward I am using is dif\_before-dif\_after where dif is average matrix entry distance and dif\_before is initialized as the difference from the white canvas. In order to not overfit to a small set of images, I am currently randomizing between \~5k resized logos. Also I was initially using an action limit of 10 but I was mostly getting large lines with a blend of the colors right through the middle, which sort of makes sense. My hope is that with larger action limits it would be forced to actually start matching the form and color of the target. I tried bumping it up to 30 and ran it for \~5 mil iterations but getting very slow progress.

&amp;#x200B;

I hope this wasn't too unclear. Any advice about things to consider/change is much appreciated!

[For 30 action limit](https://preview.redd.it/4yb6ptv0z6851.png?width=1536&amp;format=png&amp;auto=webp&amp;s=0c90ac05438cf80f6f90bb6a08ed7dee3e563d3f)

&amp;#x200B;

[After the above training](https://preview.redd.it/li7ys75907851.png?width=640&amp;format=png&amp;auto=webp&amp;s=1c5808b9a3d770ce6393273984ee23317d4e6666)

&amp;#x200B;

&amp;#x200B;

[500k steps with 10 act limit](https://preview.redd.it/v3i5iesjz6851.png?width=1798&amp;format=png&amp;auto=webp&amp;s=6fc1e0b41411f52f8bf9d1a64ecee8cf5d24a3bc)

&amp;#x200B;

[After the above training, gets colors fairly consistently](https://preview.redd.it/jebmyrdr07851.png?width=640&amp;format=png&amp;auto=webp&amp;s=07cbded00a0673c740ecd484342b6ddafb62f4e8)",reinforcementlearning,samme013,False,/r/reinforcementlearning/comments/hj5bul/advice_for_personal_rl_project/
[D] What are some active research topics in RL?,1593585869,I want to try to do independent research and try to publish a paper so I'll be accepted into grad school but I wanted to hear some of the community's input on what a good topic might be,reinforcementlearning,Awill1aB,False,/r/reinforcementlearning/comments/hj564c/d_what_are_some_active_research_topics_in_rl/
Policy gradient,1593581458,"I am studying the policy gradient technique and there are some points that are a little bit dark for me.

I understand that we can start with our state, put it in NN and then collect the output . At this point we can compose the entire trajectory re-inserting the new state after the first action in the NN and so on. Once finished the first trajectory we can construct a batch of them following the same principle.

 At this point we can apply SGD and update the weights and then re -calculate the trajectories to improve the learning of the system

 What I dont understand is why in the first phase in which we built the trajectories we can apply train\_on\_batch without having a training phase before. What kind of unctionality is applied by the NN when we use it to produce an action from a state, without a training phase before? 

Many thanks",reinforcementlearning,Antonyellow,False,/r/reinforcementlearning/comments/hj46x9/policy_gradient/
"Introducing High School Students to A.I., M.L. &amp; R.L.",1593574719,"Hello,

I am interacting with high school students through a guest lecture/webinar and will be giving them an introduction to A.I. 

My aim is to excite them rather than intimidate them.

Would be immensely helpful if you could suggest interesting topics (from a student's perspective) that can be covered, without going deeper into the math. These students haven't been exposed to calculus. 

Also any cool resources/videos that can be demonstrated to make it more exciting?

Any constructive advice is welcome.

Thanks.",reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/hj2np7/introducing_high_school_students_to_ai_ml_rl/
Positive vs Negative reward,1593570116,"I have realise that for my application if I use a mix of positive and negative reward, my model does not give optimal solution and if I shift the reward eg subtract by a constant till all the reward is negative, it   gives me the optimal solution.

What is the theory behind this? Any paper to recommend?

I am using dqn and the action space is only 2 moves

I feel that this is also the reason why mountain car uses - 1 for each time step and 0.5 at the goal.",reinforcementlearning,hujingrui,False,/r/reinforcementlearning/comments/hj1j7x/positive_vs_negative_reward/
Spellcaster Control Agent in StarCraft II Using Deep Reinforcement Learning,1593550963,Training method using the A3C algorithm!,reinforcementlearning,ReststrahlenEffect,False,/r/reinforcementlearning/comments/hiw8ix/spellcaster_control_agent_in_starcraft_ii_using/
How Humans and Artificial Intelligence Exploit Loopholes,1593536641,,reinforcementlearning,Wise-Software,False,/r/reinforcementlearning/comments/hird2q/how_humans_and_artificial_intelligence_exploit/
Resources on applied Reinforcement Learning outside games and robotics,1593523648,"I am lookin for resources in the form of research papers, company stories or patents that apply reinforcement techniques in domains such as personalization, recommendations, advertisement or dialogue systems.  I managed to find contextual bandits mostly from Microsoft research in personalization and dialogue, but I suspect there might be more. Do you know of anything else?",reinforcementlearning,dkajtoch,False,/r/reinforcementlearning/comments/hinc23/resources_on_applied_reinforcement_learning/
Is it a popular mistakes to compute the gradient of the next state in the TD-Update ?,1593522854,"When I look at popular TD updates of the value/Q/critic network, in the loss, the target is computed with gradient attached to him, but isn't it a mistake? Shouldn't the gradient be only computed with respect to our current prediction?

To be clear, I will give a (PyTorch) pseudo code here:

**What I often see:**

    state_value =  q_value(state)
prediction = q_value(next_state)
target = reward + gamma * prediction
loss = MSE(state_value, target)
loss.backward()

**But shouldn't it be like this:**

&gt;state\_value =  q\_value(state)  
**with torch.no\_grad():**  
prediction = q\_value(next\_state)  
target = reward + gamma \* prediction  
loss = MSE(state\_value, target)  
loss.backward()",reinforcementlearning,ingambe,False,/r/reinforcementlearning/comments/hin4c4/is_it_a_popular_mistakes_to_compute_the_gradient/
Constrainting Open AI custom environment,1593480826,"Hi Does open AI provide an option to constraint its state space, I mean apart from upper limit and lower limit.",reinforcementlearning,Saty18,False,/r/reinforcementlearning/comments/hidjjh/constrainting_open_ai_custom_environment/
Way to easily play web games,1593471066,"Hi, I found some Chrome developer program that let you easily extract data from web games and interact with them a while ago, but I can’t remember what it was. Any ideas?",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/hiatst/way_to_easily_play_web_games/
Best Resource for Reinforcement learning,1593463733,"Hi everyone,   
I have taken a grad level course on Machine learning. I'm doing my masters right now and have summer break till September beginning. I really fascinated by the idea of reinforcement learning. I want to be able to make an AI that can play a game of my choice. (Does not have to be super complicated but yeah).  


What would be a good place to start learning RI? I don't want it to just be theory. Maybe where I can build projects and learn as I go? Any suggestion? Thanks in advance!",reinforcementlearning,vr1126,False,/r/reinforcementlearning/comments/hi8ggd/best_resource_for_reinforcement_learning/
Equivalence of Contextual Bandit formulations,1593449149,"There exists mainly two different types of Contextual Bandit problem formulations in the literature:

Definition 1: ([https://hunch.net/\~jl/projects/interactive/sidebandits/bandit.pdf](https://hunch.net/~jl/projects/interactive/sidebandits/bandit.pdf)) In a contextual bandits problem, there is a distribution 𝑃 over (𝑥,𝑟\_1,...,𝑟𝑘), where x is context, 𝑎∈{1,...,𝑘} is one of the k arms to be pulled, and 𝑟𝑎∈\[0,1\] is the reward for arm 𝑎. The problem is a repeated game: on each round, a sample (𝑥,𝑟1,...,𝑟𝑘) is drawn from 𝑃 , the context 𝑥 is announced, and then for precisely one arm a chosen by the player, its reward ra is revealed.

Definition 2: ([http://rob.schapire.net/papers/www10.pdf](http://rob.schapire.net/papers/www10.pdf)) The algorithm observes the current user 𝑢\_𝑡 and a set 𝐴𝑡 of arms or actions together with their feature vectors 𝑥𝑡,𝑎 for 𝑎∈𝐴𝑡. The vector 𝑥𝑡,𝑎 summarizes information of both the user ut and arm 𝑎, and will be referred to as the context. Based on observed payoffs in previous trials, A chooses an arm 𝑎𝑡∈𝐴𝑡, and receives payoff 𝑟𝑡,𝑎𝑡 whose expectation depends on both the user 𝑢𝑡 and the arm 𝑎𝑡.

The fact that when stating definition number 2, the authors of [http://rob.schapire.net/papers/www10.pdf](http://rob.schapire.net/papers/www10.pdf) cite the paper from the first definition is very confusing to me. In particular, they say ""Following previous work \[([https://hunch.net/\~jl/projects/interactive/sidebandits/bandit.pdf](https://hunch.net/~jl/projects/interactive/sidebandits/bandit.pdf))\], we call it a contextual bandit.1"".

In particular, in Definition 1 assumes that only one context is revealed to the learner. In the second formulation, you observe ""contexts"" or better features for all the arms. I was thus wondering if there is any equivalence between the two formulations or a way to relate them.

Thanks for your help.",reinforcementlearning,Apprentice12358,False,/r/reinforcementlearning/comments/hi3e9u/equivalence_of_contextual_bandit_formulations/
Scaling actor output in DDPG,1593447452,"I am trying to implement DDPG for solving a financial trading enviroment. The continuous action range may be very large (for instance +-150k) with respect to standard RL testbed where that range is in \[-1,1\]. I noticed from seminal paper and  [openai spinning up implementation](https://github.com/openai/spinningup) that the output of tanh is usually rescaled to the desidered range.

&amp;#x200B;

My concern is when to rescale exactly. Some implementation let the actor network output directly the action in the large range, instead of in \[-1,1\]. Is it not preferable to rescale the action after computing gradients and updating parmaeters? Don't big actions produce larger gradients and make the learning process more difficult?

&amp;#x200B;

I am struggling with this problem and I don't find many information about it. I suppose that rescaling before computing gradients is a bad idea since my actor network seems to saturate and choose actions at the extremity of the interval.",reinforcementlearning,alebrini,False,/r/reinforcementlearning/comments/hi2u53/scaling_actor_output_in_ddpg/
OA 2018 Form 990/DM 2018 Full Accounts: further growth (OA budget of $51m; $31m cloud expenses) (DM budget: $700m),1593443431,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hi1l94/oa_2018_form_990dm_2018_full_accounts_further/
[D] Paper Summary - What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study.,1593435226,,reinforcementlearning,dtransposed,False,/r/reinforcementlearning/comments/hhzbgp/d_paper_summary_what_matters_in_onpolicy/
"Reinforcement Learning in Real World Resources(Books,Papers,People to Follow etc)",1593429850,"Since there are a lot of works done in this field recently it became really hard to keep track of current work. I decided to make a big list of compilations of work done. Papers,books,conference talks, work done in specific areas other than directly RL (meta-learning,imitation learning etc) , researchers and companies to follow for their research.

I'm up to any PR and contributions and If anyone who worked on topics and have a project just let me know so I can add your project for more people to see &amp; contribute etc.

[https://github.com/ugurkanates/awesome-real-world-rl](https://github.com/ugurkanates/awesome-real-world-rl)",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/hhy3wh/reinforcement_learning_in_real_world/
Spot Micro Pybullet Simulation &amp; OpenAI Gym Env!,1593406610,,reinforcementlearning,Mauri97,False,/r/reinforcementlearning/comments/hhtng4/spot_micro_pybullet_simulation_openai_gym_env/
[2006.13823] Reducing Overestimation Bias by Increasing Representation Dissimilarity in Ensemble Based Deep Q-Learning,1593403543,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/hhsz5a/200613823_reducing_overestimation_bias_by/
A set of CartPole modified environments [Research],1593401870,,reinforcementlearning,modanesh,False,/r/reinforcementlearning/comments/hhslpw/a_set_of_cartpole_modified_environments_research/
Good simple envs for learning?,1593375439,"Hi, I’m going through DRL: Hands On atm and looking for some problems to practice what I’m learning on. I’m currently on the DQN section— are there any ai-gym (or other) envs other than Cartpole that DQNs work well on? Is ai-gym in general a good place for basic practice?",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/hhlkoz/good_simple_envs_for_learning/
Reinforcement learning with constraints,1593365602,Any idea how to impose constraints on a 100 length vector. I am trying to impose it on the objective function but learning does not seem gud enof,reinforcementlearning,Saty18,False,/r/reinforcementlearning/comments/hhijdj/reinforcement_learning_with_constraints/
Continuous DDPG with constraints,1593355769,"Hi,

I am working on a reinforcement learning with continous action space of vector size 100. I have a few constraints on the system but I am not sure how to impose them. If I try to do them via rewards, most often than not, the action gets choosen that exacerbate the reward and finally my agent only learns the best of the worst. Any ideas are helpful

Thanks",reinforcementlearning,Saty18,False,/r/reinforcementlearning/comments/hhfn15/continuous_ddpg_with_constraints/
I trained a Falcon 9 Rocket with PPO/SAC/D4PG,1593352707,"Hello , I had little free time last week so I went and trained 3 agents on RocketLander environment made by one of our Redditors ( ***EmbersArc)*** 

This environment is based on LunarLander with some changes here and there. It definitively felt more harder to me.

I included a detailed blog post about process &amp; included all code with notebooks and local .py files.

You can check videos and more on github &amp; blog post.

Feel free to ask me anything about it. Code is also MIT licenced you can easily take &amp; modifiy do whatever you want. I also included Google Colab notebooks for those interested.

I trained agents with PTan library so some knowledge needed for it.

[https://medium.com/@paypaytr/spacex-falcon-9-landing-with-rl-7dde2374eb71](https://medium.com/@paypaytr/spacex-falcon-9-landing-with-rl-7dde2374eb71)

[https://github.com/ugurkanates/SpaceXReinforcementLearning](https://github.com/ugurkanates/SpaceXReinforcementLearning)

\[Imgur\]([https://i.imgur.com/A4W5HRM.gifv](https://i.imgur.com/A4W5HRM.gifv))",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/hhesz1/i_trained_a_falcon_9_rocket_with_pposacd4pg/
OpenAI gym: System identification for the cartpole environment,1593340495,"In the OpenAI gym simulator there are many control problems available. One of them is an inverted pendulum called CartPole-v0. It is not recommended to control the system directly by the observation set which contains of 4 variables. Instead, a prediction model helps to anticipate future states of the pendulum.

We have to predict the future of the observation set:

- cartpos+= cartvel/50
- cartvel: if action==1: cartvel+= 0.2, elif action==0: cartvel += -0.2
- polevel+= -(futurecartvel-cartvel)
- angle: unclear

It seems that the angle variable is harder to predict than the other variables. Predicting cartvel and cartpos is easy going, because they are depended from the action input signal. The variation of the polevelocity and the angle are some sort of differential equations with an unknown formula.

Question: how to predict the future angle of the cartpole domain?",reinforcementlearning,ManuelRodriguez331,False,/r/reinforcementlearning/comments/hhc95q/openai_gym_system_identification_for_the_cartpole/
Reimagining the Dog,1593335148,,reinforcementlearning,two-hump-dromedary,False,/r/reinforcementlearning/comments/hhbb0q/reimagining_the_dog/
"Sutton &amp; Barto: Tabular RL Illustrated Examples (Racetrack, Gridworld)",1593312457,,reinforcementlearning,laxatives,False,/r/reinforcementlearning/comments/hh6xkp/sutton_barto_tabular_rl_illustrated_examples/
Favourite RL Paper/Book of yours!,1593298793,"TL;DR: Suggest Reinforcement Learning and Deep Reinforcement Learning papers and books that you find absolutely interesting! 

I am a junior University student with much love and ambition for RL. I’ve taken every offered class on the topic so far but everything else FUN is in grad school or later. 

I wanted to try other options but due to COVID-19, I couldn’t get a proper internship and summer lab practice. Thus, I am focusing on self-study right now to improve significantly and then conduct a research/project by myself. I am planning on reading 2-3 papers a week this summer ranging on all levels of difficulty. 

I am especially interested in Deep Reinforcement Learning as I see it gaining more and more uses real time.

That being said, I’d be happy if you suggested any interesting paper or book for my studies. Any other type of resources or suggestions are welcome as well!!!",reinforcementlearning,AGI-Wolf,False,/r/reinforcementlearning/comments/hh3j14/favourite_rl_paperbook_of_yours/
2 out of 7 Observations Defined in MATLAB DDPG Reinforcement Learning Environment. Are the rest given random values?,1593292693,"After reading up one Deep Deterministic Policy Gradient, I found this example on MATLAB:

[https://www.mathworks.com/help/reinforcement-learning/ug/train-agent-to-control-flying-robot.html#TrainDDPGAgentToControlFlyingRobotExample-4](https://www.mathworks.com/help/reinforcement-learning/ug/train-agent-to-control-flying-robot.html#TrainDDPGAgentToControlFlyingRobotExample-4)

My question is the following: In DDPG, we plug in the Observation to our Actor to get our actions. The observations in the MATLAB environment are 7: x, y, dx, dy, sin, cos, dtheta. However, only x and y are assigned in the beginning. Does that mean that the rest are given random values before placed in the Critic Network? If my understanding is wrong, could someone please explain to me what is occurring in this model? Thank You",reinforcementlearning,HuzaifahS10,False,/r/reinforcementlearning/comments/hh1q26/2_out_of_7_observations_defined_in_matlab_ddpg/
What frustrates you the most when implementing RL algorithms/papers?,1593273847,I am trying to get a better understanding of what researchers/practitioners in RL spend most of their time struggling with and how this could be made more simple. I personally have found that spotting bugs in training can be really hard and that copying &amp; pasting code from others works very badly when implementing RL. What have you found difficult?,reinforcementlearning,joshsny,False,/r/reinforcementlearning/comments/hgw47v/what_frustrates_you_the_most_when_implementing_rl/
"""Sample Factory: Egocentric 3D Control from Pixels at 100000 FPS with Asynchronous Reinforcement Learning"", Petrenko et al 2020 {Intel}",1593267575,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hgubbl/sample_factory_egocentric_3d_control_from_pixels/
Intel's New AI System Can Optimise RL Training On A Single System,1593258785,[https://analyticsindiamag.com/intels-new-ai-system-can-optimise-reinforcement-learning-training-on-a-single-system/](https://analyticsindiamag.com/intels-new-ai-system-can-optimise-reinforcement-learning-training-on-a-single-system/),reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/hgs92u/intels_new_ai_system_can_optimise_rl_training_on/
No fixed amount of steps between observations,1593250786,"I need to apply RL to an asynchronous environment (Microsoft's [Project Malmo](https://github.com/microsoft/malmo)). This means that I receive observations that do not have the same amount of steps between them. For example, I can receive observations at time-steps `t= 0, 4, 7, 11, 15,` etc. 

How can I perform RL in such an environment? Can I use the standard RL algorithm? Or do they suffer from this?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/hgqo5e/no_fixed_amount_of_steps_between_observations/
Basic doubt on Rewards,1593239611,"I have an agent which gets +10 on reaching goal state and -1 for all other states

But after the training, the results show the agent is achieving more than 10 reward

 How is that possible?

I used DDPG

&amp;#x200B;

&amp;#x200B;

![img](zj8ljl0ude751)",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/hgokh2/basic_doubt_on_rewards/
Help need in accomplishing a task in the gym fetch env,1593217427,"So, I created this custom fetch env by modifying the fetch reach environment. The environment has a hemispherical composite (deformable) object on the ""table"", the task is to use the robot's grippers end to paint the entire surface of the hemisphere while applying a perpendicular force within a set threshold, I used mjContact to retrieve the perpendicular force at the contact point. The episode length has been increased to 200 from the default 50.

&amp;#x200B;

The approach I tried uses a list that stores the contact coordinates and gives a reward whenever a new coordinate is encountered, a very small positive reward when an old coordinate is reached and a bonus reward when the first condition and the force threshold is reached, this is done by a chain of if statements (which looks wrong to me). 

&amp;#x200B;

The problem with the agent is that it doesn't move the gripper anymore after moving it from the top to the bottom in a straight line, it doesn't seem to ""get"" that moving it to new coordinate over the hemi will get it a much higher reward.

&amp;#x200B;

I tried using DDPG, SAC and TD3 and all three produce the same result, all were trained for atleast 30k episodes with 200 steps. I need some help designing a proper reward function.

&amp;#x200B;

[Custom fetch environment](https://preview.redd.it/6194wpjnjc751.png?width=1920&amp;format=png&amp;auto=webp&amp;s=0cdde5e17aebc010016156b56663bc68c0699f37)",reinforcementlearning,Snapdragon_625,False,/r/reinforcementlearning/comments/hgjh7n/help_need_in_accomplishing_a_task_in_the_gym/
"""Building AI Trading Systems"", Denny Britz [brief lessons learned from RL for financial market trading]",1593208843,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/hgh2i1/building_ai_trading_systems_denny_britz_brief/
[2006.13888] RL Unplugged: Benchmarks for Offline Reinforcement Learning,1593201726,,reinforcementlearning,frostbytedragon,False,/r/reinforcementlearning/comments/hgeutq/200613888_rl_unplugged_benchmarks_for_offline/
RL Unplugged: Benchmarks for Offline Reinforcement Learning,1593201526,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hgesi4/rl_unplugged_benchmarks_for_offline_reinforcement/
How do you implement multiprocessing ?,1593176514,"I have implemented a sequential version of A2C, but now I want to distribute the actors on different cores to have an efficient algorithm.

I have tried to use Ray, but it seems like that if I don't compute the gradient inside the actor and then average the gradient over the actors, it doesn't work. I don't want to do that, I want to use the actor to get the experience and then compute the loss of a GPU, this way I can maximize the usage of my GPU with big batch and compute the policy really fast on cpu with autograd turned off.

How do you do that ? Which library do you use in general for your algorithm ?",reinforcementlearning,ingambe,False,/r/reinforcementlearning/comments/hg75je/how_do_you_implement_multiprocessing/
Looking for research opportunities,1593126419,"Hi all, I recently lost my research internship due to COVID-19 and have been looking for research opportunities in RL for a while. If anyone here knows of any such interesting opportunities or positions to apply for, please let me know. I am an Indian who finished my undergraduate in CS in 2020  and willing to relocate. Thanks",reinforcementlearning,boynextdoor9,False,/r/reinforcementlearning/comments/hfwa14/looking_for_research_opportunities/
[2006.10742] Learning Invariant Representations for Reinforcement Learning without Reconstruction,1593113525,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/hfshpr/200610742_learning_invariant_representations_for/
Divergence before convergence,1593110607,I am training an agent to do a very simple task (grid-world coverage). Often times when I run my very simple Q-learning approach the amount of iterations it takes to cover starts at around three times the amount of iterations it should take optimally. This then quickly explodes to around two or three hundred times the amount of iterations it should take. Is this normal? Has anybody else experienced similar results?,reinforcementlearning,a_ghould,False,/r/reinforcementlearning/comments/hfrm04/divergence_before_convergence/
Doubt regarding TD control methods.,1593097880,"I was studying the TD on-policy control method (Sarsa). I understand the algorithm and how does it work. But I'm not able to connect the algorithm pseudocode and the concept of the algorithm.

Here is the pseudocode of Sarsa as of Sutton and Barto book:

&amp;#x200B;

[Sarsa Pseudocode](https://preview.redd.it/wsioojiml2751.png?width=631&amp;format=png&amp;auto=webp&amp;s=ca437c12f73cf29c6f8e1ea5a38147d81e5ce814)

Here, I understand that in line 8, we are updating the action-value for a state-action pair based on the target and this is where we are evaluating the policy. However, what about updating the policy, I am not able to see code that updates the policy. Do we update the policy outside of the loop?

If that is the case, how is the learning online, isn't that like MC control, because we are updating after the end of the episode in this case?",reinforcementlearning,FyreFlei,False,/r/reinforcementlearning/comments/hfnrfy/doubt_regarding_td_control_methods/
Reading recommendation on safe RL and constrained MDP.,1593091936,"I am starting a project in the space of safe RL and constrained MDPs. Is there a tutorial/reading list that you can recommend for this topic? If not individual paper recommendations are also welcomed. 

I am in particular interested in approaches to determine the (safety) constraints. Is it always based on domain knowledge or are there any alternative methods?",reinforcementlearning,namuradAulad,False,/r/reinforcementlearning/comments/hfm5d2/reading_recommendation_on_safe_rl_and_constrained/
A new Gym environment for real-time strategy PvP mobile game,1593091633,"Hi, everyone! My team and I [opensourced](https://github.com/Nordeus/heroic-rl) RL environment for [Heroic - Magic Duel](https://www.heroicgame.com/), which is a real-time, strategy, 1 v 1 player-versus-player mobile game. We previously published a paper on this called [Deep RL Agent for a Real-Time Action Strategy Game](https://arxiv.org/abs/2002.06290).

We use PPO to train an agent which achieves human-level performance and, in some cases, outperforms existing in-game AI.

You can read more in this [blogpost](https://engineering.nordeus.com/heroic-magic-duel-deep-rl/). 

Questions and comments are welcome!

Self-promotion note: I work at a company that made this game.",reinforcementlearning,dimitrijer89,False,/r/reinforcementlearning/comments/hfm2eg/a_new_gym_environment_for_realtime_strategy_pvp/
Why does OpenAI‘s Cartpole use Image data?,1593029477,"So I’m building my own environment for a simple rectangle placement process.
It got me wondering, since I am not passing in a whole Image to the network, why does Cartpole Environment do this? 
Wouldn’t it be more effective to just add the slider coordinate, the top of the pole and the angle as a tensor?

Why include the whole image?",reinforcementlearning,tarrandaviddurham,False,/r/reinforcementlearning/comments/hf8fx4/why_does_openais_cartpole_use_image_data/
What are some major applications of RL in computer vision other than game playing?,1593025020,,reinforcementlearning,s927,False,/r/reinforcementlearning/comments/hf6zvv/what_are_some_major_applications_of_rl_in/
"Without any doubt, gradient descent methods are fundamental when training a neural networks or even Bayesian networks. Here is an attempt on an animated lecture which demystifies this topic. Enjoy !!",1593023383,,reinforcementlearning,bazziapps,False,/r/reinforcementlearning/comments/hf6gdj/without_any_doubt_gradient_descent_methods_are/
DQN flappybird wont learn anything.,1593015788,"&amp;#x200B;

Hi, im trying to implement DQN to play flappybird.  Action space contains two actions. Flap and do nothing.  Ive been playing with the hyperparameters, but still i cant make it learn except going up.  Input for the network is four stacked  80x80 images.  I cant find bug from the code, but i guess there is atleast one.  Network  does not predict NaN/Inf values. Could someone help me ? Thank you for your time.

My implementation [https://hastebin.com/tayelehehi.py](https://hastebin.com/tayelehehi.py)",reinforcementlearning,kraskovski,False,/r/reinforcementlearning/comments/hf40z7/dqn_flappybird_wont_learn_anything/
Discount factor in proof of policy gradient theorem,1593007008,"When proving the policy gradient theorem, most proofs just use the un-discounted return R instead of the discounted return G. When I include the discount factor gamma and use G instead of R, I find that the expression for the policy gradient is actually different.

Here is a screencap from my writeup:

https://preview.redd.it/aat3jxc26v651.png?width=663&amp;format=png&amp;auto=webp&amp;s=0808d076747bbda87e4f4624e8edcfe6b1fdd1cc

(the hyperlink links here: [https://spinningup.openai.com/en/latest/spinningup/extra\_pg\_proof1.html](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html))

&amp;#x200B;

Is there something wrong with my math or have I been under a misconception this entire time?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/hf1g9h/discount_factor_in_proof_of_policy_gradient/
[R] Probing Emergent Semantics in Predictive Agents via Question Answering,1592997626,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/hez9rs/r_probing_emergent_semantics_in_predictive_agents/
[R] Mutual Information Based Knowledge Transfer Under State-Action Dimension Mismatch -- Transfer learning in RL when expert and learner have different state- and action-spaces.,1592961661,,reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/hergbb/r_mutual_information_based_knowledge_transfer/
[R] Mutual Information Based Knowledge Transfer Under State-Action Dimension Mismatch (https://arxiv.org/abs/2006.07041),1592961309,Transfer learning in RL when expert and learner have different state- and action-spaces.,reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/herd6h/r_mutual_information_based_knowledge_transfer/
How should I optimise training this RL agent given its variable action space and conflicting goals?,1592957738,"I am investigating how to optimise a series of decisions that are currently made by humans, by assessing the performance of an agent trained via reinforcement learning in a custom environment designed to simulate the real environment in which these decisions are being made. My question is on what I can do to optimise the training of this agent, and indeed what algorithm/s would be the most appropriate to try, given the agent's full knowledge of the environment's state at each time step, a variable action space, a variable number of actions taken at each time step, and the agent's (partially) conflicting goals.

In the environment the agent has to make a decision at each time step about whether or not to *open* new *processes* and/or *close* currently running processes. So for example let's say there are currently two processes running at time *t*, then the agent can decide to do any combination of the following:

* Close process 1
* Close process 2
* Open one or more new processes
* Do nothing

These processes are completely independent of each other and can only be started by the agent, although they can either be closed manually by the agent or close themselves (thus *failing* in the latter case). The processes have known probability distributions dictating the probability of them failing or completing successfully after a certain amount of time. These probabilities are the same for each process and are fixed in the simulation. So for example any process opened at time *t* has a probability *p\_t'* of completing successfully at time *t + t'* and a probability *q\_t'* of failing at time *t + t'*.  The simulation terminates once *N* processes have successfully completed, where *N* is a fixed number decided at the beginning of training.

In short, the agent has to try and complete *N* processes that run independently of each other and of the agent, and at each time step the agent can decide whether to carry on waiting for the running processes to either succeed or fail, to open new processes (hedging its bets, so to speak), and/or to give up on any of the running processes, or to just do nothing.

I've attached a sequence diagram (undoubtedly unconventional, forgive my ignorance) that I hope illustrates the sort of scenario I'm talking about, where in this case *N =* 2; I've not put in any time markings but you can assume that the time steps are discrete and running down the diagram.

https://preview.redd.it/s3uwe7x62r651.png?width=783&amp;format=png&amp;auto=webp&amp;s=3646276d3b8223ce3946721d5b56da2d0b06b99b

The agent has two goals which apparently conflict with each other:

1. Minimise the total time steps of the simulation until *N* processes have successfully completed
2. Minimise the total number of processes opened during the simulation

These goals conflict to some extent since a fairly trivial solution to achieve the first goal would be to open a huge number *X&gt;&gt;N* of processes at *t=0* to overcome any low probability of each process succeeding after a short amount of time. I understand therefore that it would be difficult to train any agent to optimise for both goals unless some weighting is applied to each one in the form of scaled rewards.

The action space is variable because at each time step the agent can open any number of new processes but also close any of the currently open ones (i.e. those that haven't ""failed"" and are thus removed from the environment).

As I mentioned at the beginning, the agent has full knowledge of the environment's state. Encoding this has been a bit tricky so to reduce the complexity of the model I've had to reduce the observation space to the current age of the simulation, the number of failed processes, the number of open processes, and the age of each open process. Given the independence of the processes I don't think it matters that this simplification eliminates information on the age of failed processes, but there may be something else more useful that I'm unintentionally removing.

Up until now I've worked with just a basic DQN with decaying epsilon-greedy exploration, on a simplified version of the problem, where for the action space I've replaced the variable-length vector of True/False actions (corresponding to whether or not to close each of the current open processes) with a simple integer *x* where the corresponding action is ""close the *x* oldest open processes"". This hasn't worked brilliantly though, and I believe it's ignoring the fairly complex probability distributions for each process completing/failing. Any advice on a more suitable algorithm for this particular scenario, or any other tips that might help with exploring the space more (e.g. perhaps training from *t != 0* with random scenarios so it's not just getting a cold start each time?), would be greatly appreciated!",reinforcementlearning,dhatch75,False,/r/reinforcementlearning/comments/heqfp1/how_should_i_optimise_training_this_rl_agent/
DQN model always takes the same action regardless of what the state is?,1592928575,"I am implementing a DQN model to play Tetris.After training it for over 10000 games and then testing the model it would take the same action no matter what the state was.I have changed the hyperparameters like Batch Size, synchronization between the training net and the target net, Learning rate, and epsilon decay rate.

Also, I am adding dropout layers to regularize the network and prevent it from overfitting.

I am implementing grouped actions that means it would decide the column and orientation of the piece. 

So it would be in total 40 action if the width is 10. 

And the state space is column height and the current piece. 

the network architecture is simply consisted of fully connected layers (No cnn or lstm)

But it is the same result, the network always favors an action no matter what.Could anyone help me.",reinforcementlearning,medoaashry,False,/r/reinforcementlearning/comments/hehfkc/dqn_model_always_takes_the_same_action_regardless/
Google's New Algorithm Increases Deployment Efficiency With Low Costs,1592909746,,reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/hechi4/googles_new_algorithm_increases_deployment/
RL implementation - Site Selection (Wind Turbine),1592902197," 

Hi,

I am currently writing my thesis in which I try to figure out how to implement a reinforcement learning algorithm ( or/and MDP) on a static environment (e.g. site selection for wind turbines). Can you think of any possible way to handle this? I mean, after the clustering and exclusion of specific areas, the ""most valuable"" site(s) can be already seen and there is no need to send an agent into the 'known' environment. However, I would appreciate and welcome all your ideas.",reinforcementlearning,crypt0gh0ul,False,/r/reinforcementlearning/comments/heb1ul/rl_implementation_site_selection_wind_turbine/
Top Exploration Strategies Used In Reinforcement Learning,1592901519,,reinforcementlearning,analyticsindiam,False,/r/reinforcementlearning/comments/heaxrc/top_exploration_strategies_used_in_reinforcement/
How would you use a LSTM with a Discrete Actor-Critic (Not A2C),1592898264,"I am a mechanical engineer working on my final year project in which i need to create a Actor-Critic network and am pretty novice when i comes to implementing neural networks and the like. I would like to use a lstm in my Actor-critic network which currently uses  simple fully-connected networks

https://preview.redd.it/u80teoci5m651.png?width=708&amp;format=png&amp;auto=webp&amp;s=e54e231b03cb14c46ba37367fcd47fab6941a9e4

However im not sure how i would need the code to look for the lstm as the Actor-critic im using is a train per step setup 

&amp;#x200B;

https://preview.redd.it/iivjm0186m651.png?width=841&amp;format=png&amp;auto=webp&amp;s=0d80fcebf7d25a1c06de7135941035776c942b90

and the only implementation of lstms i could find are ones which take in batches to update futheremore im unsure how the hidden state and cell state are setup and trained

Any advice or links to sites/code that would be helpfull will be greatly appreciated.",reinforcementlearning,AFKv3,False,/r/reinforcementlearning/comments/heac3d/how_would_you_use_a_lstm_with_a_discrete/
Summary: GameGAN,1592874759,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/he50nx/summary_gamegan/
[Beginner] What are some good online platforms where i can train and implement RL algorithms fast?,1592856910,"I started learning RL some time ago and have encountered a problem in training and implementing RL algorithm. They require large computation resources and it takes a lot of time on my local machine. I have tried google Golab's GPU and paperspace's PS5000 but they still take a very long time.
Are there any other platforms designed specifically to solve RL problems?",reinforcementlearning,comeonusernametaken,False,/r/reinforcementlearning/comments/hdzvrn/beginner_what_are_some_good_online_platforms/
Help with Reinforcement Learning in Reconchess,1592856124,"Hi, I've been working on this competition lately called Reconnaissance Blind Chess and I've been wondering if anyone has any ideas about how to apply reinforcement learning to this variation of chess? Reconchess is basically just normal chess except for the fact that you have no idea about how your opponent moves and you have to sense where they are.  The imperfect information makes it much more difficult to solve and because the python library itself doesn't offer a typical RL environment like openai gym I have no clue how to approach this. The only thing that I can find that may be somewhat useful is a game history class that lets you query the game after it's finished so I don't know how useful that will be in helping to train an agent. If someone can give me some advice that would be great. Thanks in advance.

&amp;#x200B;

Competition:  [https://rbc.jhuapl.edu/](https://rbc.jhuapl.edu/) 

API for reconchess (game history):    [https://reconchess.readthedocs.io/en/latest/reconchess.html#gamehistory](https://reconchess.readthedocs.io/en/latest/reconchess.html#gamehistory)",reinforcementlearning,A01u,False,/r/reinforcementlearning/comments/hdzmv3/help_with_reinforcement_learning_in_reconchess/
"RL Weekly 41: Adversarial Policies, Image Augmentation, and Self-Supervised Exploration with World Models",1592854675,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/hdz6it/rl_weekly_41_adversarial_policies_image/
Why does IMPALA rollout include last episode from the previous rollout?,1592807720,Why do the rollouts of IMPALA start with the last episode of the previous rollout? Shouldn’t it only add new episodes?,reinforcementlearning,urw7rs,False,/r/reinforcementlearning/comments/hdmyn4/why_does_impala_rollout_include_last_episode_from/
I printed a second Xbox arm controller and decided to have an air hockey AI battle . I used unity to make the game and unity ml-agent to handle all the reinforcement learning thing . It is sim to real which I am quite happy to have achieved even if there is so much that could be improved .,1592772822,,reinforcementlearning,Little_french_kev,False,/r/reinforcementlearning/comments/hdebeu/i_printed_a_second_xbox_arm_controller_and/
[Beginner] Which Algorithm,1592772701,"Hi,

I have an environment with a set with 4 states and a set with 4 actions do you guys recommend I use a method that uses a neural network as a function approximator like DQN? Another question is, which algorithm works well with continuous state values?",reinforcementlearning,hidden-7,False,/r/reinforcementlearning/comments/hdea6t/beginner_which_algorithm/
Confused about difference between model and environment in context of planning vs learning,1592759741,"As title says I understand a model is a MDP showing state-action transitions with likelihoods of occurrence. Learning has to do with sampling actions and seeing how agent performs against environment.

&amp;#x200B;

But I'm not sure how the two are different, I'm going through Sutton's book and they look the same.",reinforcementlearning,balloonboom,False,/r/reinforcementlearning/comments/hdakij/confused_about_difference_between_model_and/
Learning Dynamic Belief Graphs to Generalize on Text-Based Games,1592756058,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/hd9ikv/learning_dynamic_belief_graphs_to_generalize_on/
on-policy vs. off-policy actor-critic,1592726907,"Hi everyone, 

Does this figure correctly represent the overall general idea about actor critic methods for on-policy and off-policy case. I am a bit confused about off-policy (right figure). Could someone share the thoughts?

Thanks",reinforcementlearning,Asad_Shahid,False,/r/reinforcementlearning/comments/hd35zt/onpolicy_vs_offpolicy_actorcritic/
Steplimit/terminal indication in RL,1592689174,"So I am still looking for a concrete answer whether a steplimit/terminal indication in the state features is really needed. When you are talking about making an environment more markov, i’d say Yes, yet I see a lot of offical researches or just others examples without this indication. 

When you don’t indicate whether a state is going to reach the terminal state, then the episode just ends, without any further notice. So far as my theory goes this would make most environments non markov, usually, especially when you have a steplimit. 

I am looking for information sources which discuss this topic.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/hcubfh/steplimitterminal_indication_in_rl/
Making a Reinforcement Learning in Unity using ML-Agents toolkit!,1592650865,,reinforcementlearning,Mingoooose,False,/r/reinforcementlearning/comments/hckdi7/making_a_reinforcement_learning_in_unity_using/
Modelling self in environment,1592584354,"Hey,

I came across this problem popular in AI safety field concerning RL. It's the idea of Embedded Agency (https://arxiv.org/abs/1902.09469). The problem arises due to RL agents usually considering themselves as removed from the environment.


Have there been answers to this problem from the RL side of things?",reinforcementlearning,Maplernothaxor,False,/r/reinforcementlearning/comments/hc3v5y/modelling_self_in_environment/
"MARL for competitive games, where to start and some ‘simple’ questions",1592548593,"Hey all, been getting into RL recently with the hopes to do some work in MARL (going from gym-like environments to my own to play with) later on. I saw a post earlier concerning cooperative play, but am curious about competitive play. I’m not quite sure where to look and how to get hands on with this stuff... does anyone have an idea as to what’s a good path to go down?

Also got a pretty simple question... from a programming standpoint is MARL simply duplicating an agent from SARL? So I’d be putting N agents in a competitive environment as opposed to just 1 agent?",reinforcementlearning,big-waves-r-us,False,/r/reinforcementlearning/comments/hbvpkl/marl_for_competitive_games_where_to_start_and/
Using opponent agent states for training?,1592515632,"I was wondering if there was any literature regarding adding states from an opponent's viewpoint to that of an agent's training? 

I got the idea from thinking about fighting games, as I personally when I see someone doing a cool combo or I'm getting hit by cool combo I want to learn it too. 

Thanks in advance,",reinforcementlearning,1nate146,False,/r/reinforcementlearning/comments/hbnel3/using_opponent_agent_states_for_training/
The Quest for AGI,1592515555,,reinforcementlearning,uncountably-infinite,False,/r/reinforcementlearning/comments/hbnds5/the_quest_for_agi/
Julia Reinforcement Learning Implementations,1592508352,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/hbl4uh/julia_reinforcement_learning_implementations/
Relative Entropy Inverse Reinforcement Learning,1592494542,"Hello there,

I'm currently trying to understand the IRL method proposed in \[1\], called Relative Entropy IRL.

However, I do not quite understand the optimization function. What exactly should the baseline policy be and how should it be obtained?

Has anyone studied the paper and could help me?

Many thanks in advance and greetings

&amp;#x200B;

\[1\] [http://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf](http://proceedings.mlr.press/v15/boularias11a/boularias11a.pdf)",reinforcementlearning,Random_Agent_TUD,False,/r/reinforcementlearning/comments/hbgws3/relative_entropy_inverse_reinforcement_learning/
Model fluctuates between best and worst possible reward. What might be the cause?,1592488460,"Hi, I've been trying to figure this out for a week now, I am stuck and I think I need help. I can't figure this out with my current knowledge, and I am also searching but couldn't find a similar case.

I am trying to implement a trading algorithm with a custom env. My issue is model picks a side (usually at start, sometimes later) and learns either best or worst possible outcome. Even it is negative it learns the perfect thing to do in my small test window. I tried DQN, A2C, PPO and all of them have the same issue. I checked my custom env many times, tried many reward functions like giving the entire portfolio value, differences, log return and tried 0 when it is negative, it just doesn't care. I also tried 2 more custom envs with their default dataset from other people, but with out a luck, same outcome.

I'd appreciate any help. I am really out of ideas, and maybe someone has experience with something like this. From what I understand this only happens with time series. It is just like flipping. Model decides to do the exact opposite after five minutes for example. All the model implementations above that I mentioned are working fine with other problems like breakout, lunarlander etc.  Thanks.",reinforcementlearning,sequence_9,False,/r/reinforcementlearning/comments/hbf0hu/model_fluctuates_between_best_and_worst_possible/
What's the right way of doing hyperparameter tuning when solving problems using deep RL?,1592481992,"I'm trying to solve a problem that can be modeled as a POMDP using Deep RL. Because I started with basically 0 experience in the field (and after reading about how notoriously hard it can be to get DRL algorithms working properly) I decided to use an already existing implementation - so I settled for TF-agents. The constraints of my problem (continuous observations/actions) pushed me towards using Soft Actor-Critic.  
After experimenting a bit with my observations/rewards I got to a point where the agent is doing a satisfactory job, but there is definitely room for improvement. Until now I have been using mostly the default hyperparameters for SAC (the ones used by the creators in their [minitaur tutorial](https://www.tensorflow.org/agents/tutorials/7_SAC_minitaur_tutorial)), but now I'd like to start tuning these hyperparameters to my problem. The issue is that my performance varies a lot from one training run to another - so much so that the difference between my best and my worst runs can be up to 15% on the main metric I'm interested in (I assume this problem is caused by sampling ""better"" or ""worse"" experience). From the little I have read about this domain it seems that it's ""normal"" to have such a large variance in your results ([\[1\]](https://www.alexirpan.com/2018/02/14/rl-hard.html), [\[2](http://amid.fish/reproducing-deep-rl)[\]](http://amid.fish/reproducing-deep-rl)).  
Because my performance varies so much just based on the random seed in the model I don't understand how I could actually tell if some improvements in performance come from changing a hyperparameter or just from a lucky random seed.  
A suggestion I've heard was using a fixed random seed for all experiments and only changing hyperparameters. I understand why this solution would work in supervised lerning (where your dataset is fixed from the beginning and will be the same for all training runs), but since in RL any hyperparameter change can slightly affect the policy, and that in turn will affect my sampled experience, wouldn't this get me into the same situation as if I didn't use a fixed random seed at all? 

How is this problem usually handled in RL research? Do you recommend any papers I could read on that, or do you have any advice from experience?",reinforcementlearning,NectarineCorrect1231,False,/r/reinforcementlearning/comments/hbdcs1/whats_the_right_way_of_doing_hyperparameter/
How is Policy Gradient variance implemented in continuous actions?,1592472984,"I'm reading about basic Policy Gradient and how to implement it and I didn't completely understand how the gradient of log pi is implemented. I understood that in order to efficiently compute the gradient we create a computational graph such that its gradient is the gradient of the objective function

&amp;#x200B;

[The objective function](https://preview.redd.it/nb08obd20n551.png?width=501&amp;format=png&amp;auto=webp&amp;s=eec514a54622cfea9f347f38718cd189cd5d00f5)

Now, my neural network will output the mean of, for example, a Gaussian distribution, and my actions will be taken by sampling from that distribution. The loss will be a Squared Error loss between the predictions given the states and the actual actions taken in those states, multiplied by the **rewards to go** `Q` such that the gradient of the network is the gradient of the objective. [From Lecture 5 of Sergey Levine DeepRL course](http://rail.eecs.berkeley.edu/deeprlcourse/): 

&amp;#x200B;

[Computational graph to implement gradient of RL objective](https://preview.redd.it/whp7m2el1n551.png?width=1436&amp;format=png&amp;auto=webp&amp;s=0bad13b0905fee2728aeef34b7b58c1f47781248)

However, this is not taking the **variance** of the action distribution into account. How do we decide and adjust the variance of the action distribution? And shouldn't we take it into account when computing the loss?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/hbbhbd/how_is_policy_gradient_variance_implemented_in/
[R] Generalized State-Dependent Exploration for Deep Reinforcement Learning in Robotics,1592471085,,reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/hbb4df/r_generalized_statedependent_exploration_for_deep/
Can you suggest best research papers for reinforcement learning????,1592465884,,reinforcementlearning,aliza211,False,/r/reinforcementlearning/comments/hba54a/can_you_suggest_best_research_papers_for/
What are some 'must-read' paper concerning reinforcement learning and Stock trading?,1592457635,"

I'm planing writing my Master Thesis about AlgoTrading using Reinforment learning and NLP. What are some 'must-read' papers? I'm also interested in reading some papers you found interessting or usefull. Tips and tricks are much appreciated.",reinforcementlearning,Candpolit,False,/r/reinforcementlearning/comments/hb8aw9/what_are_some_mustread_paper_concerning/
Open Problems in RL/IRL,1592447768,"What are some interesting open problems in Reinforcement Learning, specifically relating to safety and control? Maybe even covering Inverse Reinforcement Learning/Imitation Learning?

I've been reading up about this recently - most papers I've seen are recent (usually after 2016), I still can't quite gauge what the current, state-of-the-art research is focusing on. For reference, I've seen DeepMind's Safety Gridworlds paper, read through a bit about the Safety Gym and OpenAI's work, gone through approaches around constrained policy optimization, and read quite a bit about Imitation Learning.
I am planning to spend the next few months researching this, maybe even taking inspiration from its intersection with Control Theory, to port something to the RL domain. I'd be thankful if you could direct me towards some open problems that are hopefully approachable yet exciting.",reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/hb5yyi/open_problems_in_rlirl/
Clarification on sampling actions with Soft Actor Critic,1592446321,"I understand that in SAC, the policy network outputs the mean and variance of the action space probability distribution. I am confused, however, on how this is used to sample an action in a continuous action space. Given mean mu and variance sigma, how does one find action a?",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/hb5llk/clarification_on_sampling_actions_with_soft_actor/
How would you envisage use of a transformer for a RL based problem,1592425359,Let the discussions begin,reinforcementlearning,athenos2910,False,/r/reinforcementlearning/comments/hazbt2/how_would_you_envisage_use_of_a_transformer_for_a/
Does anyone here worked on real world robotic grasping systems?,1592416360,"I'm trying to build a robotic visual based grasping system that tries to grasp objects as part of a project. There are so many works regarding to this so it would be more helpful if there are any one that actually did work on this , their experiences etc

Since collecting real world data that would be needed for RL is very hard for regular researchers (if you are not at Google or something) , most obvious way to go is training on simulation and hoping with domain generalization and randomization to get it done in real life?  or at least with really minimal real world data on top of it.

I believe because of this on policy systems are no go for most robotic systems/real world systems. (correct me If i'm wrong) . One of the successful paper is QT-Opt one which they introduce a new algorithm for these kind of tasks

I have confused with some of concepts though , since all works  I read either just justify their experiments on simulation with trained policy on simulation. Or they are just using ways to increase Sim2Real efficiency. I have yet to see a paper / work that both tries to grasp unseen objects and while doing this from simulation data + little maybe adaptation real data.

I seen offline RL used in these work does anyone have idea about it ? it seems it doesnt actually help to adaptation. Meta learning maybe? 

Work i looked that might be useful

[https://pearl-insertion.github.io/](https://pearl-insertion.github.io/)

[https://awacrl.github.io/](https://awacrl.github.io/)

[https://github.com/google-research/google-research/tree/master/dql\_grasping](https://github.com/google-research/google-research/tree/master/dql_grasping)

[https://sites.google.com/view/efficient-ft/home](https://sites.google.com/view/efficient-ft/home)

[https://sites.google.com/view/thinkingwhilemoving/home](https://sites.google.com/view/thinkingwhilemoving/home)

&amp;#x200B;

But just like i said it would nice if there was someone who worked on topics like this to explain a bit more. What would best way to go if you have simulation and want to train a policy that would work on real systems and unseen objects.",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/hawbu8/does_anyone_here_worked_on_real_world_robotic/
Question about which Reinforcement Learning algorithm I may use,1592397881,"I have a set of 512x512 binary images with starting patterns that would represent my state space, for every image I want to add or remove squares of 16x16 pixels or do nothing. every image is evaluated by a software that returns the area of the error  ( number of pixels that are not well set or unset ).

I need an agent that can modify the starting image by adding or removing squares so that I can minimize the area of the error given by the external software.

Is Reinforcement Learning a good approach for this problem? I took a look into PixelRL ( [https://arxiv.org/pdf/1912.07190.pdf](https://arxiv.org/pdf/1912.07190.pdf) ) they have an agent per pixel and they denoise an image, but I think it is not the same problem I have. Do you have any recommendation where I may look ? 

Thank you :)",reinforcementlearning,marcogma,False,/r/reinforcementlearning/comments/haqen3/question_about_which_reinforcement_learning/
"HI , is there a difference between PPO with parallel environments and a multi agent PPO . What do they mean and how do they differ can anyone explain with example .Thanks",1592376266,,reinforcementlearning,honolulu22,False,/r/reinforcementlearning/comments/halzoy/hi_is_there_a_difference_between_ppo_with/
Amount of updates to do per step with SAC?,1592370588,"Hey guys. I've been experimenting with SAC a lot lately and I have a question. Currently most implementations I've seen and my implementation as well do only 1 update per step. Which means that at the end of each timestep they do an update using only 1 sampled batch. 


I have two questions about this:
- Would it be a good idea to experiment with more updates per step? If so is there a rule of thumb as to how many should I do?
- Would it make sense to do an update at the end of each episode and not at the end of each timestep? So for example I finish an episode and then do N updates where N is the amount of timesteps that were in that episode.


Thanks in advance for any tips on the matter that you might have.",reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/hakmde/amount_of_updates_to_do_per_step_with_sac/
Why do RL frameworks hate Rainbow DQN?,1592354682,"1. **Why do these frameworks hate Rainbow DQN?**
2. **Are non-parallel training environments with Discrete Actions a solved problem or something?** 
3. **Do you use or recommend a framework that contains Rainbow DQN? \[Flexible enough to support RAM versions of Atari\].**

&amp;#x200B;

**Rant due to not knowing what I don't know:**

&gt;Most frameworks seem to not have Rainbow Implemented.  
&gt;  
&gt;If I do find one then I could never get the framework to run on the RAM version of Atari Games.  
&gt;  
&gt;Most article with example code will solve Cartpole (forgiving/easy) but the same codebase with adjusted hyperparameters will fail at Lunar Lander (and pong, breakout, space invaders, boxing, etc.)   
&gt;  
&gt;I'm asking because I trust peer-reviewed code that others have done than my own. 

&amp;#x200B;

**Example:**

*My Multistep Noisy Dueling Double DQN on Cartpole ( no per )*

 Episode: 48  Steps: 8744 Mean: 202.02325581395348 

*Lunar Lander*

Episode: 306  Score: 283.2007001100344  Steps: 171503 Mean: 190.15124556992873 

&amp;#x200B;

*Stable Baselines DQN on Cartpole*

 episodes                | 900      | | mean 100 episode reward | 164      | | steps                   | 79802  | 

 episodes                | 4200    | | mean 100 episode reward | 142      | |  steps                  | 473776   |

 episodes                | 4900    | | mean 100 episode reward | 99.5     | |  steps                  | 552664   | 

&amp;#x200B;

Getting a Custom MLP Policy to run required some digging. Documents were updated but DQN was not changed ( ignored maybe? ).

`from stable_baselines.common.atari_wrappers import make_atari`  
`from stable_baselines import DQN`  
`from stable_baselines.deepq.policies import MlpPolicy, CnnPolicy, FeedForwardPolicy, register_policy`  
`from stable_baselines.common.vec_env import DummyVecEnv`  
`import gym`  
`# Custom MLP policy of two layers of size 128 each`  
`class CustomPolicy(FeedForwardPolicy):`  
 `def __init__(self, *args, **kwargs):`  
        `super(CustomPolicy, self).__init__(*args, **kwargs,`  
                                           `layers=[128,128],`  
                                           `feature_extraction=""mlp"")`  
   
`register_policy('DCustomPolicy', CustomPolicy)`  
`env = gym.make('CartPole-v0')`  
`model = DQN('DCustomPolicy', env, verbose=1, learning_rate=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.001).learn(total_timesteps=int(1e7))`",reinforcementlearning,Heartomics,False,/r/reinforcementlearning/comments/hagmyt/why_do_rl_frameworks_hate_rainbow_dqn/
amount of data for Inverse RL (IRL) ?,1592346820,"I trying to guesstimate how much human data I would need for an IRL project. Some specs:

1. continuous state space (but could be discretized)
2. continuous action space.

I am not looking for answers, but for pointers on how to estimate it.",reinforcementlearning,victorialena,False,/r/reinforcementlearning/comments/haeglh/amount_of_data_for_inverse_rl_irl/
Help with training an agent to play MsPacman,1592337399," I'm new to RL and I'm attempting to train an RL agent to play MsPacman in PyTorch. I've adapted the code from this [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) on the PyTorch page for my problem. The DQN has the following architecture: 

    DQN(
      (conv1): Conv2d(1, 32, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (linear1): Linear(in_features=7040, out_features=512, bias=True)
      (linear2): Linear(in_features=512, out_features=9, bias=True)
    )

I'm using the actor (policy) and critic (target) method with replay memory which has the following settings:

* **Replay Buffer**: 100,000
* **Target Update**: Every 10,000 steps
* **Bath Size**: 128
* **Discount Rate**: 0.999

For the exploration trade-off I'm using epsilon-greedy with the following curve:

https://preview.redd.it/k8w2xa1nub551.png?width=388&amp;format=png&amp;auto=webp&amp;s=40b5b529c9e29d0b5bd4da9407e22517fd5e35a2

where the x-axis is the step number (in million) and y the probability of selecting a random action.

The update of the policy network looks like this:

    # next_state_values = Q-values precited by the target network
    # GAMMA = discount rate (0.999)
    # reward_batch = rewards for the states
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

 Loss calculation: 

    # state_action_values = actions taken by the policy agent
    # expected_state_action_values - this is calculated above
    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))

 Updating the policy (gradients are clamped): 

    optimiser.zero_grad()
    loss.backward()
    for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)
    optimiser.step()

 While training the agent I plot the duration of each episode shown below (the orange line shows the average of the previous 100 episodes): 

https://preview.redd.it/ogkas3tsub551.png?width=733&amp;format=png&amp;auto=webp&amp;s=2e1b81bbfad2cbaa993ae7d7fbbea2ffcca937d0

 After 4,000 episodes the agent isn't really progressing and gets stuck in corners like below: 

&amp;#x200B;

https://preview.redd.it/dwnw6msuub551.png?width=284&amp;format=png&amp;auto=webp&amp;s=55d3c8e2084e961f53c8aafc3d3149e501f02278

I know this is a lengthy post and thanks for taking the time to read it. I would be grateful for any help with my problem. Thanks!",reinforcementlearning,harpalss,False,/r/reinforcementlearning/comments/habeut/help_with_training_an_agent_to_play_mspacman/
"""Q*BERT: How to Avoid Being Eaten by a Grue: Structured Exploration Strategies for Textual Worlds"", Ammanabrolu et al 2020",1592335558,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/haat3j/qbert_how_to_avoid_being_eaten_by_a_grue/
Deep Q network,1592334110,Has anyone had this problem of the agent always choosing the same action?,reinforcementlearning,hidden-7,False,/r/reinforcementlearning/comments/haabnr/deep_q_network/
Benchmarking Multi-Agent Reinforcement Learning Algorithms,1592333698,"Check out our recent NeurIPS submission about the evaluation of Multi-Agent RL algorithms.

[https://arxiv.org/abs/2006.07869](https://arxiv.org/abs/2006.07869)

We open-source two multi-agent environments that we developed as part of this work

Level-based foraging: [https://github.com/uoe-agents/lb-foraging](https://github.com/uoe-agents/lb-foraging)

Multi-Robot Warehouse: [https://github.com/uoe-agents/robotic-warehouse](https://github.com/uoe-agents/robotic-warehouse)",reinforcementlearning,gpap93,False,/r/reinforcementlearning/comments/haa6n2/benchmarking_multiagent_reinforcement_learning/
RL for portfolio management,1592326869,"Hey guys, newbie question. I am thinking on financial portfolio management as the use case. I find papers using both armed bandits approach but even deep q learning and even actor critic approaches. Any suggestions on best approach?",reinforcementlearning,vvella74,False,/r/reinforcementlearning/comments/ha7xog/rl_for_portfolio_management/
Tutorial on Multi-Agent RL using TF-Agents,1592323956,,reinforcementlearning,drcopus,False,/r/reinforcementlearning/comments/ha6zs8/tutorial_on_multiagent_rl_using_tfagents/
"Hi , is there any literature or content availaible on Multi agent proximal policy optimaztion methods ? How can we formulate them ?",1592321025,,reinforcementlearning,honolulu22,False,/r/reinforcementlearning/comments/ha62vz/hi_is_there_any_literature_or_content_availaible/
"Learning curves: Is an algorithm ""better"" if it learns faster but converges to the same final performance?",1592320706,"Publications often state that their algorithm is ""better"" than some baseline because their learning curve is steeper earlier, despite both their algorithm and the baseline converging to the same final performance. Does this really mean that this algorithm is ""better""?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/ha5z9q/learning_curves_is_an_algorithm_better_if_it/
Pendulum-v0 learned in 5 trials [Explanation in comments],1592317539,,reinforcementlearning,Plane-Mix,False,/r/reinforcementlearning/comments/ha502e/pendulumv0_learned_in_5_trials_explanation_in/
Help with Multi-Agent Reinforcement Learning,1592302205,"Hi all. So, I was wondering if there any good resources for Multi-Agent RL? My problem statement requires a co-operative framework with a continuous state and continuous action space.",reinforcementlearning,vikram39,False,/r/reinforcementlearning/comments/ha15m4/help_with_multiagent_reinforcement_learning/
What's the best algorithm to use when the environment is extremely slow and costly?,1592282716,"Hey guys. Say the environment is a physical component, and an episode takes around 10 minutes, with 30 timesteps in it. What's the best algorithm that I can use to expect convergence in a few hundreds maybe a few thousands of episodes?


The two main ones I've heard of so far are SAC and PPO. But PPO is on-policy which seems like a major disadvantage for sample efficiency.",reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/h9wto0/whats_the_best_algorithm_to_use_when_the/
Coursera Study Group UofA Reinforcement Specialization?,1592273342,"Hey all! So I've decided to work my way through the Reinforcement Learning Specialization on Coursera and I was wondering if anybody her was doing the same and wanted to create a study group!

If you know of any study groups up right now I would be floored to know about them, so let me know!

I'm, based out of Canada on the west coast and am available to meet most days in the afternoons minus the weekends.

Cheers!",reinforcementlearning,Outside_Inspector,False,/r/reinforcementlearning/comments/h9ubd8/coursera_study_group_uofa_reinforcement/
Need help understanding this...,1592242504,"Hey all,

I have just started learning Reinforcement Learning and right now I am studying Monte Carlo methods for Sutton and Bartos book on RL. Recently I am studying Monte Carlo Policy Improvement and I am not able to understand that how is the policy able to converge to the optimal policy even after using the returns from the older sub-optimal policy. I came across these lines in the book explaining the same but I am a bit dumb so I can not understand them. Here are the lines I am talking about:

https://preview.redd.it/2cblxg1q04551.png?width=712&amp;format=png&amp;auto=webp&amp;s=3376bee25592574cbb7b433777ff9d727399d208

Can anyone please explain to me what does that mean. Thank you :)",reinforcementlearning,FyreFlei,False,/r/reinforcementlearning/comments/h9kv0p/need_help_understanding_this/
Preview your agents,1592239953,"I apologize if this is not the right place but I feel you can definitely benefit from it.

I often want to preview videos of how my reinforcement learning (generally multi agent RL) perform. It is a tedious process to open and play multiple videos one by one. Hence I created this [tool](https://github.com/Ankur-Deka/Video-preview-tool) that can play all my videos at once. I hope you find this useful and do let me know if there are other tools available for this.",reinforcementlearning,dekankur,False,/r/reinforcementlearning/comments/h9k263/preview_your_agents/
MDP vs POMDP,1592231847,"Hi all, can any kind soul explain to me the following:

1. Difference between MDP (Marlon Desicion Process) and an POMDP (Partially Observable MDP)

2. When to use one or the other?

Thank you very much in advance!",reinforcementlearning,IamKun2,False,/r/reinforcementlearning/comments/h9hl4l/mdp_vs_pomdp/
Breaking in to the Top 10 of AWS Deepracer Competition - May 2020,1592226793,,reinforcementlearning,EarlGreyNRice,False,/r/reinforcementlearning/comments/h9g3nt/breaking_in_to_the_top_10_of_aws_deepracer/
"Which Reinforcement learning-RL algorithm to use where, when and in what scenario? The what? why? when? and which? of Reinforcement learning algorithms and quick facts about existing reinforcement learning algorithms.",1592223541,,reinforcementlearning,athenos2910,False,/r/reinforcementlearning/comments/h9f7u2/which_reinforcement_learningrl_algorithm_to_use/
Best Algorithm for Multi agent problems,1592223309,"Hi everyone, I have been working in multi-agent problems from some time, but have been wondering is PPO a sota multi agent algorithm or not? 
If not what is currently the best DRL techniques for controlling atleast 10 agents. Also a good cooperation strategy (apart from reward sharing and global reward system) would be an added bonus. Looking forward to some answers 🙂",reinforcementlearning,athenos2910,False,/r/reinforcementlearning/comments/h9f5p8/best_algorithm_for_multi_agent_problems/
What is the best way to learn about Reinforcement Learning?,1592184805,,reinforcementlearning,camlinke,False,/r/reinforcementlearning/comments/h96buu/what_is_the_best_way_to_learn_about_reinforcement/
Is there any literature or post availaible on Multi agent systems in recommender systems specifically ? Any idea how to formulate multiple agents,1592149431,,reinforcementlearning,honolulu22,False,/r/reinforcementlearning/comments/h8w3rp/is_there_any_literature_or_post_availaible_on/
Vehicle Routing Problem using Deep RL,1592146679,"Hi everyone, recently I along with two of my colleagues, gave an online talk (link below) at AI festival on how we can use DeepRL to solve combinatorial optimization problems such as capacitated vehicle routing. Give it a watch if you got some time and let me know your thoughts and suggestions. 
 [VRP using DeepRL](https://festival.aiacceleratorinstitute.com/talks/route-optimization-with-deep-reinforcement-learning/)",reinforcementlearning,learner_version0,False,/r/reinforcementlearning/comments/h8vafi/vehicle_routing_problem_using_deep_rl/
How to get reproducible results for comparison,1592134785,"I have come up with a DRL policy and I want to test it against a simple heuristic policy in real-time. I have currently two scripts, one which loads the DRL policy and runs it to the environment and the same for the heuristic policy. The problem is that some of the observations are based on some probability distributions and at each run I am getting different results. What can I do to be able to compare them based on the same scenario?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/h8sbfc/how_to_get_reproducible_results_for_comparison/
Deep RL with Own Game,1592121704,"Interested in putting in an RL agent in my own game I want to create. I’ve seen plenty of examples of algorithm implementations (e.g.) through OpenAI Gym, but not much on non-OpenAI-related games. Unsure on where to start and how an algorithm would ‘plug into’ my own game/environment. Anyone got any advice/ideas on where to start?",reinforcementlearning,justcallmedyl56,False,/r/reinforcementlearning/comments/h8pon2/deep_rl_with_own_game/
"""SBR: Learning to Play No-Press Diplomacy with Best Response Policy Iteration"", Anthony et al 2020 {DM}",1592099208,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/h8kq9d/sbr_learning_to_play_nopress_diplomacy_with_best/
How to interpret the parameter sharing in multi agent RL modeled as Dec-POMDP,1592094607,"In Dec-POMDP, each agent can only obtain local observations. In the paper ""*Learning to Communicate with Deep Multi-Agent Reinforcement Learning*"", the parameter sharing is allowed to train the network of each agent even when each of them only contribute its local information to the network.

How to do parameter sharing even in such a completely decentralized system? What is the theory/principle behind to support the soundness of parameter sharing?

Intuitively, the *parameter sharing* scheme is mostly used when all the agents have the same global state information.",reinforcementlearning,kechang,False,/r/reinforcementlearning/comments/h8jgj9/how_to_interpret_the_parameter_sharing_in_multi/
How to interpret the modification to basic DRQN by introducing the past action as an input?,1592093655,"In basic DRQN, the input is the observation only. In the well known paper ""Learning to Communicate with Deep Multi-Agent Reinforcement Learning"", the previous action is introduced as one of the input. 

The question is what is theory behind to motivate such a change? In addition to the intuitive explanation that this way we can approximate the observation-action history, instead of the observation history only in DRQN, what kind of theories can give a reason for such a change?",reinforcementlearning,kechang,False,/r/reinforcementlearning/comments/h8j77o/how_to_interpret_the_modification_to_basic_drqn/
Is it possible to use DRL for such an application?,1592055201,"I am curious if I can use DRL for scheduling in an EV charging station. I am thinking to have an action for each charging port e.g the action space will be n dimensional (where n is the number of charging ports) and each action will be from 0 to the charging rate of each port. The EVs arrive based on a distribution and ask for some energy based also on a distribution. In the literature, there are papers that schedule the aggregated demand or the actions decide which EV to charge, but not how much. I cannot get how the DRL will be able to understand to which charging port will give how much of energy. 

I appreciate any kind of opinion or help.",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/h87md5/is_it_possible_to_use_drl_for_such_an_application/
Any good source on how to create a good environment?,1592049627,"Hello all! I'm an RL newbie and am currently creating a new robot RL environment in PyBullet. The task would have a sparse reward. I keep asking myself questions such as should the observations be normalized? Should rewards be somehow normalized? If there is some early stopping condition such as robot self-collision, how do I penalize? Should I remove the -1 rewards at each step? I tried to run spinning up SAC and DDPG implementations on it and it seems to not learn anything. 

It seems most online sources are inconsistent. Could you give me a good source/tutorial on how to design environments if you can? Any feedback is appreciated.",reinforcementlearning,dennisushi,False,/r/reinforcementlearning/comments/h8688d/any_good_source_on_how_to_create_a_good/
How to make interesting RL contributions without the resources of Google or OpenAI?,1592039636,"I just bought a good GPU laptop and I'd like to start some RL projects in the near future. What are some active areas of RL research in which you can achieve some interesting results, even without the resources of tech giants? Any suggestion is much appreciated.",reinforcementlearning,Le2vo,False,/r/reinforcementlearning/comments/h845le/how_to_make_interesting_rl_contributions_without/
RL/IRL/Imitation Learning project,1592037143,"Hello,

I am a graduate student and I am interning remotely this summer.
As part of the internship, while I'll be having access to resources (GPUs etc) and mentorship, I've been told to figure out myself, a research/problem statement to work on the next 2-3 months.

I want to do something in the RL/IRL/Imitation Learning space and I'm looking for project ideas. I have experience coding in Python and C++. I have taken courses related to ML &amp; RL and have implemented common algorithms, such as Policy Iteration, TD learning etc. I would love to work with Imitation Learning approaches such as DAgger etc. but I'm not sure how to go about collecting data and deciding the scope of the project.

I would love suggestions and any advice is welcome. Even if that requires picking up new skills, like maybe coding in CUDA etc, I look forward to something that could give me a good learning experience as well as a presentable output.

Thank you.",reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/h83nsw/rlirlimitation_learning_project/
No real life NeurIPS this year,1592007564,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/h7wmr3/no_real_life_neurips_this_year/
GAE - how are gamma and lambda different?,1591988779,"The final expression for the generalized advantage estimate given in [https://arxiv.org/pdf/1506.02438.pdf](https://arxiv.org/pdf/1506.02438.pdf) is

\\sum\_{l=0}\^\\infty (\\gamma \\lambda)\^l \\delta\_{t+l}\^{\\pi\_\\theta}

&amp;#x200B;

All of the derivations in the paper make sense, but in the final form I don't see why you can't just swap gamma for lambda and lambda for gamma. Why do these two terms have different interpretations and ""meanings"" if they are mathematically interchangeable?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/h7qyby/gae_how_are_gamma_and_lambda_different/
[R] What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study,1591957468,[removed],reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/h7i6g3/r_what_matters_in_onpolicy_reinforcement_learning/
Baseline with reward to go in Policy Gradient,1591955586,"I'm following Lecture 5 of the [Berkeley Deep Learning Course](http://rail.eecs.berkeley.edu/deeprlcourse/).

I learned about the reward to go trick and how it solves the causality issue in Policy Gradient techniques. Now I'm reading about using the mean reward baseline and I have a problem understanding how the two techniques can be used together. 

If the mean reward is computed from t=1 to T when I subtract it from the reward to go (the total reward from t to T) I will likely get a negative number and the gradient will make the trajectory less likely. Should I compute the *mean reward to go* instead?

&amp;#x200B;

What I'm saying is, can I employ this trick

[The reward to go](https://preview.redd.it/vge0g1rdbg451.png?width=620&amp;format=png&amp;auto=webp&amp;s=bcc958d95d2c11f4ec8f5bb68e42e74c429b67d4)

Together with this? In this picture r(tau) is the total reward from t=1 to T

[Using the mean reward as a baseline](https://preview.redd.it/muy3t6ugbg451.png?width=416&amp;format=png&amp;auto=webp&amp;s=4ac0fd58f53581f06aef627409f0a95c0cdcc6dd)",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/h7hsx5/baseline_with_reward_to_go_in_policy_gradient/
Facebook Nethack with Acme/Dopamine/rlpyt?,1591941847,"Hello  
I trying to use the Facebook Nethack as an environment in one of the above frameworks. Acme and rlpyt both use an OpenAI gym wrapper and I am able to load the environment but when I attempt to start a DQN agent I get errors about the env shape:  


From Acme:  
  File ""/opt/conda/lib/python3.8/site-packages/sonnet/src/reshape.py"", line 120, in \_initialize|  
if inputs.shape.rank &lt; self.\_preserve\_dims:  
AttributeError: 'dict' object has no attribute 'shape'  


From rlpyt:  
  File ""/workspace/rlpyt/rlpyt/agents/base.py"", line 82, in initialize  
self.env\_model\_kwargs = self.make\_env\_to\_model\_kwargs(env\_spaces)  
File ""/workspace/rlpyt/rlpyt/agents/qpg/sac\_agent.py"", line 93, in make\_env\_to\_model\_kwargs  
assert len(env\_spaces.action.shape) == 1  
AssertionError

  
Wondering if anyone has managed to use Nethack as a gym environment in one of these frameworks.  Also looking for user forums for any of the above.

thanks for you help  
Maurice",reinforcementlearning,momanning,False,/r/reinforcementlearning/comments/h7f752/facebook_nethack_with_acmedopaminerlpyt/
The 32 Implementation Details of Proximal Policy Optimization (PPO) Algorithm,1591915665,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/h78h2v/the_32_implementation_details_of_proximal_policy/
Which Framework is better for a beginner in Reinforcement Learning,1591914661,"Hi,I am a master student in computer science and I have background in ML. I would like to program a reinforcement environment, I have enough python knowledge for it and the problem that I am going to solve with RL is a simple 2D simulation therefore I assume that I can do it in python. However there is not enough example environments for it or I couldn't find.

On the other hand, I have plenty experience with Unity and there are really awesome examples in ML-agents package of unity. I can write in C# well but I am not sure that if Unity is overkill or not for just 2D simulation.

Is there any recommendation for this ?

Edit: The 2D simulation that I would like to implement looks like [this](https://cs.stanford.edu/people/karpathy/reinforcejs/waterworld.html). Basically dots will avoid each other and  at some point they need the fulfill their needs. So they need to visit a place kind of marketplace to restore their needs. I wonder if  they are going to be able to learn to enter in and out one by one to that place?",reinforcementlearning,WiseStrider,False,/r/reinforcementlearning/comments/h78620/which_framework_is_better_for_a_beginner_in/
[R] Learning to Reach Goals via Iterated Supervised Learning,1591912279,,reinforcementlearning,pytorchisbae,False,/r/reinforcementlearning/comments/h77e1x/r_learning_to_reach_goals_via_iterated_supervised/
Options as responses: Grounding behavioural hierarchies in multi-agent RL (Accepted to ICML 2020),1591904080,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/h169r4/options_as_responses_grounding_behavioural/
Value function estimators and Advantage Estimation for A2C,1591902004,"I'm implementing A2C by extending a working implementation of REINFORCE that I got working on CartPole and CartPoleSwingUp. 

I want to estimate the advantage by subtracting a value estimate V(s\_t) from the return G\_t for each time step t in a rollout.

However, when fitting the Value function network, it seems it will always have a relatively high loss for CartPole. If the agent learns to balance the pole, the ""upright"" position will have high returns at the beginning of an episode and low returns at the end. The Value function network then tries to predict both these high and low returns. As a result, when subtracting the Value estimate from the empirical returns to estimate the advantage A\_t, the result doesn't necessarily make any sense.

Is doing A\_t = G\_t - V(s\_t) not reasonable for CartPole and similar environments then?",reinforcementlearning,avandekleut,False,/r/reinforcementlearning/comments/h15kod/value_function_estimators_and_advantage/
[Help] PPO with Self-Attention implemented with PyTorch,1591897547,"Hello,

I tried to make PPO + Self-Attention implementation 2 months ago, I was fighting with few things but at the end of the day I managed to solve a simple Pong environment with this setup.  
Problem was that it didn't work for Breakout, only for env mentioned earlier.

My implementation is kind of suspicious as it doesn't work with suggested hyperparams mentioned in the original PPO paper [https://arxiv.org/abs/1707.06347](https://arxiv.org/abs/1707.06347).  
I was suspicious about wrappers as my observation space ranged went from 0.0 - 1.0 to 0 - 255 and again from 0.0 - 1.0. I fixed that but it is still not working.  
I tried to investigate the training loop but I didn't found a bug there.  
Regarding plots - entropy and value loss are disproportionately large compared to policy loss.

I came back to the project as someone got interested in it and now I am curious about what went wrong.  
If you guys have any experience in implementing RL, let me know.  


REPOSITORY  
[https://github.com/RvuvuzelaM/self-attention-ppo-pytorch](https://github.com/RvuvuzelaM/self-attention-ppo-pytorch)",reinforcementlearning,RvuvuzelaM,False,/r/reinforcementlearning/comments/h143oa/help_ppo_with_selfattention_implemented_with/
Why don't these agents learn to play tic tac toe?,1591892946,"I am trying to train two DQN agents (QRNNs) to play tic tac toe against one another, however, as we can see from [this diagram](https://dylancope.github.io/Multi-Agent-RL-with-TF/output_24_0.png) the loss scores are not converging. I understand that convergence is difficult, but tic tac toe is a very simple game.

The environment that I've adapted from [TF-Agents' single-agent example](https://github.com/tensorflow/agents/blob/655c9ea27e19b16e374dd217dce9adc460eac54d/tf_agents/environments/examples/tic_tac_toe_environment.py) gives a reward of +1 to the winning player, -1 to the losing player, and -2 to any player that ends the game by making an illegal move (i.e. trying to make a play on a tile that is already filled). The -2 was changed from a -0.01 because I figured if an agent could see that it was about to lose it would take the smaller negative penalty over letting the other player win.

The full notebook is [here](https://github.com/DylanCope/Multi-Agent-RL-with-TF).

I will also show an example play from after ~6000 training iterations:
```
Start:

      |   |  
    - + - + -
      |   |  
    - + - + -
      |   |  
    
Player: Player1, Action: 4, Reward: 0.0

      |   |  
    - + - + -
      | X |  
    - + - + -
      |   |  
    
Player: Player2, Action: 3, Reward: 0.0

      |   |  
    - + - + -
    O | X |  
    - + - + -
      |   |  
    
Player: Player1, Action: 4, Reward: -2.0

      |   |  
    - + - + -
    O | X |  
    - + - + -
      |   |  
```
As we can see, player 1 hasn't learned to avoid the -2 for the illegal move.",reinforcementlearning,drcopus,False,/r/reinforcementlearning/comments/h12ltk/why_dont_these_agents_learn_to_play_tic_tac_toe/
What is the current state of the art for Multi-agent DRL?,1591845871,I am looking specifically for discrete state-action  MDPs. Any help would be highly appreciated.,reinforcementlearning,zarrokx,False,/r/reinforcementlearning/comments/h0r3cf/what_is_the_current_state_of_the_art_for/
What is the Current State of the Art in Mukti-Agent DRL?,1591845476,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/h0qzt9/what_is_the_current_state_of_the_art_in/
"""What Matters In On-Policy Reinforcement Learning? A Large-Scale Empirical Study"", Andrychowicz et al 2020 {GB} [training 250k PG agents like PPO to ablate implementation details]",1591842320,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/h0q61z/what_matters_in_onpolicy_reinforcement_learning_a/
Has anyone attempted this Reinforced Imitation paper?,1591819773,"Hi, I am trying to implement the codes based on this paper ""Reinforced Imitation: Sample Efficient Deep Reinforcement Learning for Map-less Navigation by Leveraging Prior Demonstrations"" ([https://arxiv.org/abs/1805.07095](https://arxiv.org/abs/1805.07095)) by Pfeiffer et al, but there are several issues I faced while doing so.

Wondering if anyone has tried the codes and run them successfully, any input will be greatly appreciated!",reinforcementlearning,sixfeetzero,False,/r/reinforcementlearning/comments/h0j1kc/has_anyone_attempted_this_reinforced_imitation/
[P] Yet Another Agents Framework - An RL research-oriented framework for agent prototyping and evaluation,1591809393,,reinforcementlearning,root_at_debian,False,/r/reinforcementlearning/comments/h0fbpo/p_yet_another_agents_framework_an_rl/
YAAF - Yet Another Agents Framework,1591801713,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/h0cuqm/yaaf_yet_another_agents_framework/
True Off-Policy Policy-Gradient algorithm,1591797001,"As I understand it, the Policy Gradient Theorem restricts the applications to on-policy based methods because we divide by the policy pi (leading to the grad log) to adjust for oversampling from our policy. Intuitively, I would guess that an off-policy PG would lead to a gradient grad pi / b, b being the behavior policy; however, I have doubts it would be so easy. A [paper I tried to study](https://arxiv.org/abs/1811.09013) uses Empathic Weightings but I don't see how you would extract these in a Deep setting.

As I understand, DDPG, SAC and such are considered off-policy because they can use a replay buffer, but I fail to see the theoretical ground for the claim that they are in fact off-policy, in the sense that they wouldn't break if confronted with a drastically different policy from their own. 

Is there some theory I'm missing that ensures SAC and DDPG are off-policy? Or even if it does not apply to DDPG and SAC, is there something like an off-policy policy-gradient algorithm that exists, outside linear function approximations?",reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/h0bd1d/true_offpolicy_policygradient_algorithm/
DQN perfect in training but horrible in test (Pong),1591758259,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/h02inq/dqn_perfect_in_training_but_horrible_in_test_pong/
Some questions regarding 'An Investigation of Model-Free Planning',1591737423,"Hey, I am reading the paper ['An Investigation of Model-Free Planning'](https://arxiv.org/abs/1901.03559) where the authors devise a function approximator based on stacked ConvLSTM with 3 repeats. Experimentally, they show that 

* having recurrent structure in a neural network helps to learn a planning function (ie, possibly implicit look-ahead)
* if a trained network is given more computation time (and no additional data), then its prediction improves (by 5%)

So, I struggle to understand these two findings. Why does recurrence help? I understand that may be due to the memory, but there is more than memory I think. I also don't understand why for each time step in the environment they have the algorithm do 3 internal time-ticks inside of the ConvLSTM (again, recurrence...)  


Also, what does no-op mean in their case. Is it similar to Atari game environment, where we just don't do anything after a network predicts an action? Then, why does it help with the predictions if executed on the trained network during the test?   


If you could shed a bit of light on these questions, that would be aweeeesome. Thanks",reinforcementlearning,denis56,False,/r/reinforcementlearning/comments/gzwfkn/some_questions_regarding_an_investigation_of/
Ideal Reward function in reinforcement learning,1591720170,"In general, there are two kinds of rewards in RL, Dense and Sparse reward. Each one has its own advantage and disadvantage.

Dense rewards help the agent train fast by providing a learning signal at every step but dense rewards are hard to formulate, requires domain expertise and can not be generalised over a range of tasks. 

A sparse reward is easy to formulate and can be used with every other task, but the learning is extremely slow as it is difficult for the agent to master a skill with just a binary reward and no prior experience.

So, considering these factors, which among these two is an ideal reward function, which would further progress the RL research?",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/gzqijo/ideal_reward_function_in_reinforcement_learning/
NIPS 2020: Procgen &amp; MineRL competitions announced {AIC/OA/DM/CMU/MS/PN},1591719392,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gzq94l/nips_2020_procgen_minerl_competitions_announced/
Meta-Learning and Curriculum Learning,1591718755,"In meta-learning, a policy is learnt on a given set of training tasks and then this policy is adapted to a new set of test task. The main aim of meta-learning is to learn a policy such that it can adapt to a new task fast.

In curriculum learning, a policy is trained on a simple task and then over time as the simple task are mastered the complexity of the task is increased, this approach helps the agent to master complex task fast.

So is there any difference between these two concepts?",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/gzq1n3/metalearning_and_curriculum_learning/
"""Exploration Strategies in Deep Reinforcement Learning"", Lilian Weng",1591715836,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gzp3k1/exploration_strategies_in_deep_reinforcement/
PyTorch baselines,1591705420,"Hello! I would like to work on top of existing algorithms -- to begin, DQN, but later, others. 

I know of [OpenAI](https://github.com/openai/baselines) and [stable](https://github.com/hill-a/stable-baselines) baselines, but as far as I know, these are all in TensorFlow, and I don't know any similar work on PyTorch. There's [stable-baselines3](https://github.com/DLR-RM/stable-baselines3) but they are still in beta version and DQN isn't finished yet.

I was wondering if you knew of a good repo, similar to OpenAI baselines, but in PyTorch?

Thanks in advance for any help!",reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/gzm3kf/pytorch_baselines/
Help with differentiating wrt output instead of weights,1591605304,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/gyvitx/help_with_differentiating_wrt_output_instead_of/
"Question about nodes and layers in the game ""Evolution"" by Keiwan",1591583687,"I'm someone who really loves learning about and keeping up with learning AI, but I still have a lot of trouble understanding it and had a question while I was revisiting this game.

I was wondering what the difference between adding nodes to a layer, and adding more layers would be?  I know all the nodes of the first layer would be sent to all the nodes of the next layer.  So more progressive things like reading numbers would have more layers to build up an image from the ground by recognizing edges and curves in the first layer, then recognizing patterns of curves and edges in the second layer and so on.  Would this have any application to an AI learning to walk?

It's hard to say since I don't know actually know how this game works behind the scenes, but I was imagining that maybe the first layer could turn into muscle groups of actions.  Like instead of a complex leg just contracting or expanding based on basic inputs, a layer decides it wants a leg to step forward and can influence the leg nodes as more of a single unit.

Obviously something like that I assume would take many many more generations compared to learning to make a spring just expand and contract repeatedly but was wondering if I had the right idea in thinking this or if someone could explain it better.",reinforcementlearning,QuadroMan1,False,/r/reinforcementlearning/comments/gyq97i/question_about_nodes_and_layers_in_the_game/
AI with Unity3d,1591556167,"Do you know what will happen if we run an AI simulation of police and thieves. This pic is the Finite State Machine for the police. Stick around video dropping soon. AI with unity and visual studio.Just finished with the AI logic of the thieves. This thing will be an interesting simulation. Video dropping soon.

 OK this is what we will expect in this AI simulation.

1. We will create an AI police(They will be many) which will be responsible for protecting a jewel. They must make sure that the AI thieves can't steal the jewel.

2. We will create an AI thief, which will be responsible for stealing the jewel. They must make sure they are not caught or else the will retry. Every thief will have a maximum of 3 retrys, on the third retry if you are caught you will die.

3. So this is how the simulation goes: the police will be patrolling about the jewel, the jewel will be in a building so the AI police will go around the building . So its the AI thief duty to steal the jewel when the police is around the back of the building,so the thief needs to calculate at what time the police will not be at the front of the building, steals the jewel and flee or run to the safe house.

So yer that's all this simulation is about. AI police and thieves, trying to best out each other.

https://preview.redd.it/cmzmmfb3cj351.jpg?width=676&amp;format=pjpg&amp;auto=webp&amp;s=4722153f33b857f444012cbedc1bf12cfe00d940",reinforcementlearning,akwasikonadu,False,/r/reinforcementlearning/comments/gyi1xq/ai_with_unity3d/
What makes Off-Policy Algorithms better at using TD targets,1591534044,"Hi, 

I have been experimenting with On- and Off-Policy algorithms for a while now and found that Off-Policy algorithms such as SAC seem to be able to handle bootstrapped targets much better than On-Policy algorithm such as TRPO and PPO. 

Using the Generalized Advantage Estimate for PPO and TRPO and setting lambda to 0 (but also even for values such as 0.5) had dramatic impact on performance for both of these algorithms even asymptotically, while SAC seems to be doing just fine while it uses single-step transitions.  Now I know there is a ton of differences between these algorithms, but I have not really understood what exactly makes SAC so much better at handling approximated bootstrapped targets. 

For example, using a target network seems rather aimed at stability than asymptotic behavior, which is in my understanding somewhat covered by clipping the critic losses (to get a trust-region like update) in the On-Policy algorithms. 

Thanks in advance for any suggestions on this question !",reinforcementlearning,anyboby,False,/r/reinforcementlearning/comments/gybqqp/what_makes_offpolicy_algorithms_better_at_using/
Deepbots framework for Reinforcement Learning in Webots simulator,1591530095,"Hello folks. We are a team of researchers working in the Artificial Intelligence and Information Analysis lab of the School of Informatics in the Aristotle University of Thessaloniki, Greece.

We created the open-source *deepbots framework* while trying to solve RL problems in Webots. *Deepbots* is currently under development, but can already be used. It has been created mostly for research and educational purposes.

*Deepbots* is a framework which facilitates the development of RL in Webots, using OpenAI gym style interface. All gym-compatible agents work out-of-the-box with *deepbots* environments, running in the Webots simulator, which provides a powerful physics and graphics engine. Any neural network backend, like PyTorch or Tensorflow  can be used with *deepbots*.

Using RL in Webots is not straightforward and requires development overhead for an RL agent to work in Webots, so *deepbots* comes in to help by guiding the development process and providing implementations for common functionality needed for running RL in Webots. Using *deepbots*, the resulting work is a gym environment integrated with the Webots physics and graphics engine.

*Deepbots* is distributed through pip. It is accompanied by the deepbots-tutorials repository and the deepworlds repository. The *deepworlds* repository can be used either for contributing your created environments, or using the existing ones for RL benchmarking and research.

The framework can be used in multiple ways:

* Build your ideas of custom problems, robotics (or not) and solve them with RL
* Develop an RL agent which works with gym, but to test it in a different environment, maybe with 3D graphics and a realistic physics engine
* Benchmarking different agents in premade environments

We welcome everyone to provide feedback or even contribute to any of the three repositories. :)",reinforcementlearning,aidudezzz,False,/r/reinforcementlearning/comments/gyatij/deepbots_framework_for_reinforcement_learning_in/
Is it ok to use negative reinforcement if it works?,1591523371,"New question on Reinforcement Learning forum:

Is it ok to use negative reinforcement if it works?

[https://www.stemiac.com/forum/reinforcement-learning/is-it-ok-to-use-negative-reinforcement-if-it-works/](https://www.stemiac.com/forum/reinforcement-learning/is-it-ok-to-use-negative-reinforcement-if-it-works/#post-101)",reinforcementlearning,stemiac,False,/r/reinforcementlearning/comments/gy9ekd/is_it_ok_to_use_negative_reinforcement_if_it_works/
RL -&gt; IL Transfer Learning?,1591480039,"Does anyone know of research into using RL to train a DNN and then reusing some portion of the weights to then do imitation learning?

I’m working on video games that use ML and training some deeper models with pure IL tends to have long convergence time. I’m thinking pretraining the weights with RL could be interesting.",reinforcementlearning,Deathcalibur,False,/r/reinforcementlearning/comments/gxzq03/rl_il_transfer_learning/
Carla vs AirSim vs Torcs,1591458509,"Hi! I am working in multiagent autonomous driving research. I would like to know which simulator would be better to use for RL research 

At the moment I am using Sumo, but I would like something else. Thanks!",reinforcementlearning,IamKun2,False,/r/reinforcementlearning/comments/gxtbyp/carla_vs_airsim_vs_torcs/
What are the best Julia packages for Reinforcement learning?,1591455777,,reinforcementlearning,stemiac,False,/r/reinforcementlearning/comments/gxsjqo/what_are_the_best_julia_packages_for/
How to implement RAM versions of Atari games,1591430514," I have coded breakout RAM version, but unfortunately it's highest reward was 5.I trained it about 2 hours and never reached higher score.Code is huge so I can't paste here,but in short i used double deep Q-learning, and trained it like it was CartPole or lunar-lander environment.In CartPole observation was vector of 4 components, here my double deep Q-learning agent solved the environment, but in breakout-ram version whose observation was vector of 128 elements,it was not even close.Did i miss something?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gxmvda/how_to_implement_ram_versions_of_atari_games/
Reinforcement learning,1591417812,Need some suggestions on how to start basic Rl codings!,reinforcementlearning,leoron34,False,/r/reinforcementlearning/comments/gxkd13/reinforcement_learning/
virtual meetup: Q–Learning and Sarsa,1591386597,"[https://www.eventbrite.com/e/reinforcement-learning-explained-qlearning-and-sarsa-tickets-108259356650?aff=rd](https://www.eventbrite.com/e/reinforcement-learning-explained-qlearning-and-sarsa-tickets-108259356650?aff=rd)

* Brief introduction to model-free reinforcement learning
* Sarsa algorithm
* Q-learning algorithm
* Showcase: Robotics, financial trading, etc.

Speaker teaches graduate-level RL class at Columbia university",reinforcementlearning,oyolim,False,/r/reinforcementlearning/comments/gxbndo/virtual_meetup_qlearning_and_sarsa/
Tomorrow I will interview with a RL (PhD MIT) professor if you have questions shoot,1591378029,"Hello I'm one of the co -owners a Youtube RL channel called RL Turkiye. Tomorrow I'm gonna interview a MIT PhD graduate who works as professor and his research area is multi-agent systems mostly.  


So If you have any questions regarding to DeepRL, academia , how and when to apply RL to industry please leave a comment. I will take screenshot of questions and ask them in live YouTube stream.

&amp;#x200B;

 [https://www.youtube.com/watch?v=ZR1QpKHQRYE](https://www.youtube.com/watch?v=ZR1QpKHQRYE) 

10 AM GMT+3 , will be recorded as well for rewatch.",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/gx8x12/tomorrow_i_will_interview_with_a_rl_phd_mit/
Are Reinforcement Learning Techniques Marketable?,1591373201,"Hello Everyone. I am currently focusing my attention to reinforcement learning after learning some of the basics of ML/DL. Coming from a background in evolutionary dynamics, RL seemed more familiar to me unlike the other areas of research in AI. Seeing as I aim to transition to an ML/Data Science career after I complete my PhD, I was wondering if learning RL would be worth my time. That is, are there any jobs in high demand where RL techniques are used? If not, since computer vision is commonly used in RL would that help me land a job?",reinforcementlearning,bass581,False,/r/reinforcementlearning/comments/gx7e9e/are_reinforcement_learning_techniques_marketable/
Trained Neural Network for MR,1591367017,Does anyone have a trained neural network for Montezuma's Revenge? Only for the first room? That would be fine. Urgently need help. Thanks.,reinforcementlearning,weirdshitishappening,False,/r/reinforcementlearning/comments/gx5iib/trained_neural_network_for_mr/
OpenAI Stable Baselines,1591349433,"Hey guys, OpenAI's stable baselines has readymade implementations of Hindsight Experience Replay (HER) and Generative Adversarial Imitation Learning (GAIL). But how can I use these to train Atari's Montezuma's Revenge? The code they have on the website is for the Bit Flipping Experiment. But I can't figure out how do I tweak the code around to use for HER &amp; GAIL. Can someone help me with this?",reinforcementlearning,weirdshitishappening,False,/r/reinforcementlearning/comments/gx1d82/openai_stable_baselines/
looking for repo about pong-ram-v0,1591342704,"Do you know any repository on github that solves ""pong-ram-v0"" environment using pytorch?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gx03ai/looking_for_repo_about_pongramv0/
Can anyone explain the nonmonotonic payoff matrix in MARL?,1591332991,"In this [paper](https://papers.nips.cc/paper/8978-maven-multi-agent-variational-exploration.pdf), the author mentions that Q-MIX cannot represent nonmonotonic Q-functions.  I am having a hard time understanding the example presented in the paper where Q-MIX would fail to learn an optimal policy. Specifically, why is the payoff matrix nonmonotonic? Can someone explain to me the 1st definition mentioned in the paper?

[Example of nonmonotonic payoff](https://preview.redd.it/ndvxxe6fw0351.png?width=879&amp;format=png&amp;auto=webp&amp;s=a3eface35892711f1e26ce853af7fffe8c6e5dc1)",reinforcementlearning,Reddit_Psych_Hops,False,/r/reinforcementlearning/comments/gwy41w/can_anyone_explain_the_nonmonotonic_payoff_matrix/
DQNs and Action Spaces.,1591329664,"To begin with let's assume a base implementation where network has n\_states as inputs and n\_actions as output. Each output estimates the expected reward for taking an action at a given state.   
3 scenarios where I would like to get a better idea of using DQNs in. As in how would one modify the base network/flow to handle these cases better.

1. When multiple actions are possible simultaneously? (e.g. Let's assume 4 (n\_action) buttons Red, Blue, Green and Yellow, and any combination of above buttons is a valid action. e.g Pressing only Blue or pressing all buttons except Yellow )
2. When each action can take non binary values ?(e.g -2, -1, 0, 1, 2 (n\_values))
3. The combination of above 2? Will the network need to have (n\_action \* n\_values) output nodes? Are there other frameworks apart from DQNs that deal with this better?  


For situations like 2. I can see how having action as input to the network could help. However that would mean making n\_action forward passes of the network. Also I do not see how this would help with scenario 1.",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/gwxc5c/dqns_and_action_spaces/
Reviewing DRL and related papers.,1591322006,"Greetings.

While only a first year doctoral student focusing on DRL, I was wondering if it would be possible, as well as valuable for me to contribute by reviewing papers submitted to conferences or journals.

If that is the case, does anyone have some recommendations to what venues or journals I could register to receive review requests.  


Thank you for your time.",reinforcementlearning,dosssman,False,/r/reinforcementlearning/comments/gwvc3k/reviewing_drl_and_related_papers/
Stochastic Gradient Descent vs RMSprop in deepminds original atari paper (2013),1591260594,"I apologize if this has been asked before, I looked around and didn't find anything concrete.

Did deepmind use SGD or RMSprop in their original nips paper? Here: https://arxiv.org/pdf/1312.5602.pdf

The paper specifically states 'The network is trained with a variant of the Q-learning [26] algorithm, with stochastic gradient descent to update the weights.' in the Introduction.

However later in the paper it says 'In these experiments, we used the RMSProp algorithm with minibatches of size 32.' under Experiments

I understand the 2 are similar, I just need to know exactly what algorithm was used for this paper and I can't read the source lua code. can anyone help me out?",reinforcementlearning,Huffman_Elite,False,/r/reinforcementlearning/comments/gwdnib/stochastic_gradient_descent_vs_rmsprop_in/
Why are my activations skewed to the left for my custom environment,1591259914,"**Note** I'm very new to reddit (my account is a few minutes old), I hope I'm not breaking any (spoken or unspoken) rules...

I am working on creating a custom environment and training a RL agent on it.

I am using stable-baselines because I've read it implements all the latest RL algorithms, and seems to be as close to ""plug and play"" as possible (I'd like to concentrate on creating the environment and reward function rather that the implementation details of the model itself)

My environment has an action space of size 127, and interprets it as a one-hot vector: Taking the index of the highest value in the vector as an input value. For debugging, I create a bar chart, showing how many times each value has been ""called""

Before training, I would expect the graph to show a roughly uniform distribution of ""events"":

&amp;#x200B;

https://preview.redd.it/z9aphdxcuu251.png?width=368&amp;format=png&amp;auto=webp&amp;s=6dd903004ee1f4a31b976ee18f168a25b17a741d

but instead the ""events"" in the lower end of the action spec are massively more likely than the others:

https://preview.redd.it/yguwenjfuu251.png?width=375&amp;format=png&amp;auto=webp&amp;s=e979035802f802e7469357f6ad23295fcbe081ea

Some people have told me that this is expected, but I still don't understand why, and I'd really appreciate some explanations

I have created a [colab](https://colab.research.google.com/drive/12Lj5VjiArqxa5DfXnjYAUkU78qWwn7on?usp=sharing#scrollTo=fQHjRhnatof3) to explain and reproduce the issue

I asked this question in a [github issue](https://github.com/hill-a/stable-baselines/issues/877), but they recommended I post the question here",reinforcementlearning,Stock_Cattle3493,False,/r/reinforcementlearning/comments/gwdipq/why_are_my_activations_skewed_to_the_left_for_my/
Intuitive explanation of Q Learning and python code from scratch,1591259757,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/gwdhi7/intuitive_explanation_of_q_learning_and_python/
World simulation for deep RL running on GPU,1591256151,"In my last machine learning project, I tried to make a deep learning agent solve Snake. It's doing okay, but I noticed that most of the compute time is spent on transferring data between GPU, where the CNN is evaluated, and CPU, where all the other things run. The problem isn't the bandwidth, but the latency. For every simulation step, data has to be transferred to the GPU as well as back to the CPU. It would be great if everything could run on the GPU, with for example a CUDA program.

I've looked into CuPy, which even allows for custom CUDA code. However, I wasn't able to get this working with TensorFlow in a way that all data would stay on the GPU. But maybe someone else knows a way? Otherwise, it's not a huge problem to switch to a different library, so does anybody know something else I can use that allows for running everything on GPU? Or maybe there is another solution? How do people in the field deal with this?

Thanks in advance.",reinforcementlearning,stroop_wafel_man,False,/r/reinforcementlearning/comments/gwcql6/world_simulation_for_deep_rl_running_on_gpu/
About calc_potential() function based on gym in RL?,1591252356,"Hello dear contributors,

I study with husky robot in a virtual environment about RL. I used *(progress)* variable that stands for 

`progress = potential - potential_old` . So, I aim that the agent must make progress through target location in environment. The function is *calc\_potential()* for this purpose. I need to understand specific point of the function. Function code is:

    def calc_potential(self):
            # progress in potential field is speed*dt, typical speed is about 2-3 meter per second, this potential will change 2-3 per frame (not per second),
            return - self.walk_target_dist / self.scene.dt

This force the agent to go towards to target. But, I need change this situation at some points such as walls, stairs and holes. Let's back to the question:

&amp;#x200B;

* Could you describe definition of potential for husky robot?
* What is the effect of division \`self.scene.dt\` ?
* What happened if I change self.walk\_target\_dist with constant parameters? (I think, It forces the agent go faster but I need different sights.)

Thank you in advance. Have a nice day.",reinforcementlearning,Beko_35,False,/r/reinforcementlearning/comments/gwbxrt/about_calc_potential_function_based_on_gym_in_rl/
Do we really need a target network in Batch RL?,1591249355,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/gwb8x9/do_we_really_need_a_target_network_in_batch_rl/
Linear function approximation - feature vector,1591216424,"In case of using linear function approximation instead of state discretization on 1D state, its ""easy"" to apply this equation, because it is a scalar value. However, how should I compute it if its 2D or more dimensional state in order to get one scalar value from this equation?

https://preview.redd.it/s898x0359r251.png?width=468&amp;format=png&amp;auto=webp&amp;s=40748c0d16412b2549ba45e1f527b7410ec81678",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/gw20yw/linear_function_approximation_feature_vector/
Edgy DeepRL teens react only (I hope memes are allowed),1591214169,,reinforcementlearning,Demboobiez,False,/r/reinforcementlearning/comments/gw1apk/edgy_deeprl_teens_react_only_i_hope_memes_are/
Feature vector,1591196631,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/gvvkli/feature_vector/
Are there any good tutorials about training RL agent from raw pixels using pytorch,1591183361,"Is there any good tutorials about training reinforcement learning agent from raw pixels using pytorch?I don't understand official pytorch tutorial.I want to train the agent on atari breakout environment, unfortunately I failed to train the agent on RAM version, now I am looking the way to train the agent from raw pixels.",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gvs61k/are_there_any_good_tutorials_about_training_rl/
About large-scale environment Stimulation.,1591177091,"I have read a paper  [https://arxiv.org/abs/2001.12004](https://arxiv.org/abs/2001.12004). It proposes a MARL-framework named NeuralMMO, which is aimed to dealing with massively multi-agent game. However, I find that it seems to be uncapable of stimulating complex and large-scale environment(e.g. the real world), since the environmental state transfer(i.e. s\_t  --&gt;  s\_{t+1}) of every single time step may take too much time.

 So I try to seek a MARL-framework that can support large-scale environment, but nothing. It seems that in the MARL community people are more interested in small-scale enviroment, which is easier to implement. 

Do we really need to simulate large-scale world? If we do, does there exist any multi-agent framework that can efficiently stimulate a  large-scale environment? Dose there exist any research dealing with this problem?",reinforcementlearning,AlbertCity,False,/r/reinforcementlearning/comments/gvqv83/about_largescale_environment_stimulation/
Probably found a way to improve sample efficiency and stability of IMPALA and SAC,1591175586,"Hi, I have been experimenting with RL for some time and found a trick that really helped me. I'm not a researcher, never written a paper, so I decided to just share it here. It could be applied to any policy gradient algorithm. I have tested it with SAC, IMPALA / LASER-like algorithm, PPO. It did improve performance of first two but not PPO.

1. Make target policy network (like target network in DDPG/SAC but for action probabilities instead of Q values). I used 0.005 Polyak averaging for target network as in SAC paper. If averaged over longer periods, learning becomes slower, but will reach higher rewards given enough time.
2. Minimize KL divergence between current policy and and a target network policy. Scaling of KL loss is quite important, 0.05 multiplier worked best for me. It's similiar to CLEAR ( [https://arxiv.org/pdf/1811.11682.pdf](https://arxiv.org/pdf/1811.11682.pdf) ), but they minimize KL divergence between current policy and replay buffer instead of target policy. Also they proposed it to overcome a catastrophical forgetting, while I found it to be helpful in general.
3. For IMPALA/LASER. In LASER paper authors use RMSProp optimizer with epsilon=0.1 which I found to noticeably slow down training. But without large epsilon training was unstable. The alternative I found is to stop training for samples in which current policy and target policy have large KL divergence (0.3 KL div threshold worked best for me). So policy loss wil become L=(kl(prob\_target\[i\], prob\_current\[i\]) &lt; kl\_limit) \* -logp\[i\]. LASER also has a check on KL divergence between current and replay policy, I use it as well.

What do you think about it? Does someone wish to cooperate on making a research paper?",reinforcementlearning,sss135,False,/r/reinforcementlearning/comments/gvqklr/probably_found_a_way_to_improve_sample_efficiency/
Reward assumptions,1591162629,"Just wanted to understand better the assumptions of the reward from the environment. Is the reward assumed to come from some sort of stochastic process that is time invariant or does RL not make any assumptions regarding reward?

If the rewards are assumed to be time invariant initially, are there methods that can be taken to ensure or discover that it has changed temporally?

Thanks!",reinforcementlearning,UNIXnerdiness,False,/r/reinforcementlearning/comments/gvnwd4/reward_assumptions/
"""Learning Dexterity End-to-End"", Paino 2020 {OA} [behavioral cloning of Dactyl vs pure RL: cloning is 30x faster at cube manipulation]",1591140937,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gvimd6/learning_dexterity_endtoend_paino_2020_oa/
Stable Baselines loading a model is bugging/saturated?,1591126324,"So i train my models at Google Colab and then i load the models in the same session as well. When the model is training there is no problem, however once i load the model it just completely saturates/fails. It only takes the ""Do nothing"" action, i tried giving it a punishment for doing Do Nothing the whole episode which caused no diffrence. Again the training is totally fine and everything works as intended, to add that not all my models do this. It is inconsistent some do a lot of nothing and some do nothing all the time. I think it is a bug as this is not, especially with the do nothing punishment a desired technique for the agent. I had it working at some point until, i think, i switched up the action selection so i negated the inavlid actions for a duplicate action, aka does the same thing for both actions. I believe it was this moment it started bugging out, however if it was the problem, then that doesn't explain why the training just works fine. I printed the action and it just does the Do Nothing no matter what.

Code i am using:

    obs = env.reset()
    
    model = DQN.load(""DQN at timestamp 2020-06-02 18;23;54.pkl"")  
    for i in range(15000):
        action, _states = model.predict(obs)
        obs, rewards, dones, info = env.step(action)
        print(rewards)

Nothing weird i thought? Could anyone help out perhaps?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/gve0zy/stable_baselines_loading_a_model_is/
How should I approach RL?,1591123846,"Hi, I have been watching David silver's lectures and iam on lecture 5 currently, the dynamic programming and TD/MC lectures went over my head. I don't understand what iam lacking. 
I know deep learning well but wanted to get into RL for a project. 

Can anyone tell me if I should study something else before watching the lectures? Or follow some other lecture series?

Thanks in advance!",reinforcementlearning,GNewleaf,False,/r/reinforcementlearning/comments/gvd7ps/how_should_i_approach_rl/
Free AI Courses &amp; eBooks for Remote Learning,1591116257,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/gvask6/free_ai_courses_ebooks_for_remote_learning/
Proofs of Learning Convergence of Multi-agent Reinforcement Learning,1591114774,"Hi, I found recent MARL papers are more on intuitive ideas (new networks, etc), are there any papers on new methods including proofs of learning convergence? For example, proposing a new idea and prove its convergence?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/gvabec/proofs_of_learning_convergence_of_multiagent/
Is there any more research done in tabular reinforcement learning?,1591113258," Most recent papers in RL solve a  given problem (most of the time control problems s.a. moving forward in  MuJoCo or learning Atari games with pixels) while using some neural  networks.

I feel like, most of the  success doesn't really come from the RL part but instead from whatever  clever neural network architecture was used.

Does this mean that tabular RL has been ""solved""?  Is there no more merit in studying it since people just say ""curse of dimensionality"" and turn to neural networks?",reinforcementlearning,move78_,False,/r/reinforcementlearning/comments/gv9tfc/is_there_any_more_research_done_in_tabular/
[P] Simple DDPG implementation in TensorFlow 2 - Blogpost,1591040906,"I recently wrote a small blog about DDPG implementation on Pendulum-v0 environment in TF2. Why did i write when there are a billion other DDPG posts ?

\- I wanted to write a TF2 implementation  
\- I wanted something that did not have a huge library overhead and was simple to understand for a beginner.  
\- I could not find any TF2 implemntation.

Hope you guys find it useful.

Cheers

[https://towardsdatascience.com/deep-deterministic-policy-gradient-ddpg-theory-and-implementation-747a3010e82f](https://towardsdatascience.com/deep-deterministic-policy-gradient-ddpg-theory-and-implementation-747a3010e82f)",reinforcementlearning,sol0invictus,False,/r/reinforcementlearning/comments/gurjp9/p_simple_ddpg_implementation_in_tensorflow_2/
Acme: A new framework for distributed reinforcement learning,1591030811,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/guoa3b/acme_a_new_framework_for_distributed/
DeepMind's new RL framework for researchers ACME,1591030417,"[https://deepmind.com/research/publications/Acme](https://deepmind.com/research/publications/Acme)

&amp;#x200B;

Acme is a library of reinforcement learning (RL) agents and agent building blocks. Acme strives to expose simple, efficient, and readable agents, that serve both as reference implementations of popular algorithms and as strong baselines, while still providing enough flexibility to do novel research. The design of Acme also attempts to provide multiple points of entry to the RL problem at differing levels of complexity.

&amp;#x200B;

Acme: A research framework for reinforcement learning",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/guo5kk/deepminds_new_rl_framework_for_researchers_acme/
how to get raw pixels from breakout in atari games,1591018541,"Do you know how to get a raw pixels from breakout in gym environment?

 I wrote this code,but it only outputs zero matrix, I don't know why.I take random action at every episode.Then I decided to play that on my computer(I did that code on colab), and saw that ball only vanishes one brick and stops(or misses the ball, doesn't hit and dies), do you know what is the problem?can you suggest some github repo where breakout is solved using pytorch?

&gt;import gym  
env = gym.make('Breakout-ram-v0')  
for i\_episode in range(20):  
    observation = env.reset()  
 for t in range(10):  
        env.render()  
 print(observation)  
        action = env.action\_space.sample()  
        observation, reward, done, info = env.step(action)  
 if done:  
 print(""Episode finished after {} timesteps"".format(t+1))  
 break  
env.close()",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gukr7f/how_to_get_raw_pixels_from_breakout_in_atari_games/
e-greedy parameters fitting,1591015049,"Hi!

For a project, I need to fit a set of 2 parameters (epsilon\[0,1\], alpha\[0,1\]) of a simple RL e-greedy choice rule.

I need to fit the parameters on real data, and for that I used python to generate the Q's and then make predictions for each step.

I'm implementing a simple grid search with the full range of alpha and epsilon to find the best parameters, but it takes ages and I'm sure there is a better way to do that.

Can you recommend a better approach that could be easily implemented?

(I do not have a strong background in ML math so sorry if I didn't use the right terms)

&amp;#x200B;

Thanks",reinforcementlearning,PlainPiano9,False,/r/reinforcementlearning/comments/gujw5w/egreedy_parameters_fitting/
What is RAM state,1591002237,"hello, currently I am interested in creating AI that plays breakout. In gym documentation is written:"" the observation is the RAM of the Atari machine, consisting of (only!) 128 bytes "".my question is:what the RAM is?here  random-access-memory doesn't make any sense.Are those 128 numbers pixels? or what are they?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/guh6yq/what_is_ram_state/
"PPO, splitting tasks is that a good idea?",1590995166,"Working with Unitys ML Agents I am trying to build a drone controller. I started out with a more realism based approach, one rigidbody and 4 rotors that each could apply force to the rigibody.

This worked ok, I got the basics down, change altitude, orientation and velocity. However the whole thing was very jittery.

Thinking about I figured that I don't need a model that is close to realism, but something that works well and is fun.

So I scratched the idea of rotors and just work with one rigidbody. Then I split the different tasks. I have one Agent for Altitude, one for heading and one for velocity on X/Z(actually Y in Unity) axis.

On top of that I want to build a pilot agent that brings them together and on top of that a navigator.

Is that a good way to do this? I know I am side stepping the possibility of NNs to handle all these complex tasks together, so I am not sure.",reinforcementlearning,farox,False,/r/reinforcementlearning/comments/guft24/ppo_splitting_tasks_is_that_a_good_idea/
Which RL Algorithms for Trading?,1590988463,"Started off learning the basics with DQN, DDQN, DDDQN and the various improvements and then became overwhelmed with the large varieties of algorithms available, such as A2C, APEX-DQN, Rainbow DQN, IMPALA, MARWIL, etc.

My interests are in trying to apply RL to trading in financial markets, starting with a discrete action space of 3: Buy, Sell, Do nothing.

Which RL algorithms are the more suitable ones that you might choose for training agents for trading? Similarly which are the ones that you will want to avoid? What should you be looking for when choosing on an algorithm to test out?

Thanks!",reinforcementlearning,codehuggies,False,/r/reinforcementlearning/comments/gueh2r/which_rl_algorithms_for_trading/
Actor-Critic in Large Multi-Dimensional Discrete action space,1590973049,"I'm using RL framework to address a problem where my action is defined in a n-dimensional discrete space where n=3. For each action we can have a integer value between 1 and 50 (*e.g. one action could be At = {'a1': 3, 'a2': 45, 'a3': 23}*). 

As I'm using Actor-Critic I'm wondering about the actor network and more precisely what should it learn. Since using all possible combination with order and repetition (and thus 50^3 different actions) would force the neural network to handle a 125000 output layer shape, I'm quite unwilling to go this way. 
I thought about switching to 3-dimensional continuous action space where I could learn the mean and std of a Gaussian distributions that I would use to sample action from (and round the value). On the other hand this approach would assume that it exists a Gaussian dependency within each action dimension which is (I guess?) not the case in my project. (The Poisson distribution would have the same false(?) assumption)
So my question is: How this kind of setting (multidimensional large discrete action space) can be addressed ?

Help is very welcomed, thank you very much.",reinforcementlearning,BenoitLeguay,False,/r/reinforcementlearning/comments/guavg3/actorcritic_in_large_multidimensional_discrete/
In policy gradient methods how often do you update the network?,1590969929,"I'm implementing A2C, and I was wondering if I update my networks after every episode, or after a set number of episodes?",reinforcementlearning,1yian,False,/r/reinforcementlearning/comments/gua1ds/in_policy_gradient_methods_how_often_do_you/
Tutorials,1590962328,"I will be posting code (along with documentation) for various Reinforcement Learning environments on this GitHub repository. All of them, mostly will be notebooks so you will not have to worry to install any library as such. Run it on google colaboratory :D

Feel free to have a look at it and please let me know if you do find some mistake :D

[https://github.com/AdityaKapoor74/Reinforcement-Learning-Practice](https://github.com/AdityaKapoor74/Reinforcement-Learning-Practice)

Happy Learning! ",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/gu7wkr/tutorials/
Automatic entropy adjustment for PPO?,1590962271,"Hi, 

I've been working with the discrete version of SAC for the last few weeks, but find its performance still underwhelming compared to e.g. PPO, though of course the comparison is not apt since it's off-policy vs on-policy.

However, one detail I did find very interesting about SAC is the automatic entropy adjustment. I was wondering if there is anything like that for PPO? Has anyone seen papers on that?

I understand why automated entropy adjustments are critically important for SAC and not so much for PPO, however, even PPO profits from a decreasing entropy_coefficient over time. I don't like making the entropy_coefficient dependent on time though. Ideally it should be scaled by some intrinsic factor. Any ideas are welcome.",reinforcementlearning,NikEy,False,/r/reinforcementlearning/comments/gu7w0s/automatic_entropy_adjustment_for_ppo/
Understanding PPO with Recurrent Policies,1590951836,"Hi,

Normally when implementing a RL agent with REINFORCE and LSTM recurrent policy, each (observation, hidden\_state) input to action probability output and update happens only once. The way I see it is that when the update occurs, the hidden state is back-propagated though time (e.g since the start of the episode).

With PPO though, the action probability has to be computed again for each epoch. In the pytorch PPO implementations I have seen, the env is sampled with torch.no\_grad() , so no gradients are stored. The hidden states, action etc of that trajectory is stored in a buffer. These stored hidden states are then used to recompute the action probability and perform k epochs of PPO update. The specific implementation I looked at was: [https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail)

What I don't understand about this is that the gradients is only 'one step long' (truncated after one step). When it is truncated after only one step, does this not defeat the whole purpose of using a recurrent policy, since dependencies over time can't be learnt when the back-propagation though time is only one time step long?

Any insight would be appreciated",reinforcementlearning,acc1123,False,/r/reinforcementlearning/comments/gu4sjn/understanding_ppo_with_recurrent_policies/
PPO values converging to around 0.5 for all states,1590947183,"Hi, I'm making a 4 player card game PPO implementation.

There are two teams, and I've been struggling to get it to work, but I realized that it wasn't getting any better because the predictions for a state value is way off. My implementation is based on some pytorch examples on Github. The values range between 0 and 1 (between -1 and 1 don't make any difference), but it will return something very close to 0.5 for every state. The returns on which the loss is calculated are fine, but the network will very quickly only return values close to 0.5.

Any idea why this is happening? Do I need to change something to my rewards? I have tried different implementations of the critic and the same happens every time.

value\_loss = 0.5 \* (sampled\_returns - r\_values).pow(2).mean()

is how I calculate the loss, the same happens when I clip them like in  [https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c\_ppo\_acktr/algo/ppo.py](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail/blob/master/a2c_ppo_acktr/algo/ppo.py)

Any help is greatly appreciated, to calculate the returns I just use generalized advantage estimation",reinforcementlearning,perpetualglow,False,/r/reinforcementlearning/comments/gu3e9c/ppo_values_converging_to_around_05_for_all_states/
Actor-Critic implementation not learning,1590890173,"I have implemented a [vanilla actor-critic](https://github.com/Stephanehk/Reinforcement_Algorithm_Implementations/blob/master/actor_critic_lunarlander_NN.py) and have seem to run into a wall. My model does not seem to be learning the optimal policy. The red graph below shows its performance in cartpole, where the algorithm occasionally does better than random but for the most part lasts between 10-30 timesteps. 

https://preview.redd.it/tt9wninea0251.png?width=1108&amp;format=png&amp;auto=webp&amp;s=6dc390d028def7afffc3ae23587dae5c5a16b9f6

I am reasonably sure that the critic part of the algorithm is working. Below is a graph of the delta value (r + Q\_w(s',a') - Q\_w(s,a)), which seems to show that for the most part the predicted future reward is quite similar to the approximate future reward.

https://preview.redd.it/k4bmgcv9b0251.png?width=1136&amp;format=png&amp;auto=webp&amp;s=c638d80c34963c36b64cb7bf7991dadc1c4e8444

&amp;#x200B;

Thus, I am at a loss for what the problem could be. I have an inkling it lies within the actor, but I am not sure. I have double checked the loss function and that seems to be correct to me as well. I would appreciate any advice. Thanks!",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gtqj30/actorcritic_implementation_not_learning/
Land Your Own Falcon Rocket Using Reinforcement Learning,1590865110,"Here's a Colab notebook where you can train your own reinforcement learning algorithm to land a falcon rocket on a platform:

[https://colab.research.google.com/drive/1gFdJueFbLYBPuI8ss\_0WHG0VOXj2SNdT#scrollTo=ILXH1TpbO01R](https://colab.research.google.com/drive/1gFdJueFbLYBPuI8ss_0WHG0VOXj2SNdT#scrollTo=ILXH1TpbO01R)

I put an example solution using PPO as well as a way to try to land it manually using your keyboard on github:

[https://github.com/marcsto/rocketlander](https://github.com/marcsto/rocketlander)

The solution code might need some updates for TF 2.0

Original credit goes to [https://github.com/EmbersArc/gym](https://github.com/EmbersArc/gym).",reinforcementlearning,marcsto,False,/r/reinforcementlearning/comments/gtjmgo/land_your_own_falcon_rocket_using_reinforcement/
Actor critic loss function,1590779841,"I am confused about what the loss function is for the actor in a vanilla actor-critic model. I have found  different loss functions being used in different implementations. I am confused as to which one is correct, and what the difference is between them.

delta = r + gamma\*Q\_w(s',a') - Q\_w(s,a)

loss = mean(-delta\*log(y\_pred))

loss = sum(-delta\*y\*log(y\_pred))",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gszasu/actor_critic_loss_function/
Stopping training to prevent too much exploration,1590776173,"Hi everyone.

I came across a little problem that I wanted experienced people's advice on.

I made a very simple VPG algo that runs on the openAI gym (super original I know). 
Now I am testing my algo on the cartpol-v0 environment and realize that if I train for too many epochs, my average return starts to decrease after some time. When looking at the renders you can see that my agent is doing great for while but then decides to start moving to the right and go of the screen, this is when the return starts dropping. I would like to find ways to stop the training before my actor decides to explore no stuff and potentially become worst. I would also be great if the solutions works across different env and algos.

My initial gut reaction is to check the success of the experiment (for the cartpole env it is defined as an average return of 195 over 100 trials) at each epoch and stop the training if my actor is successful. This seems fine for a small env like the cartpole but not efficient at all for more complex problems.

Another solution I thought of is to do something similar to ""early stopping"" in ML to prevent over fitting. For example, a simple version of that would be if the return doesn't vary/increase after x epochs, then stop the training.

Correct me if I am wrong but I don't think my problem is really the classic exploitation vs exploration, although it is very linked and obviously tuning hyper parameters and/or adding terms to the algo would help.

I am sure what I am describing has a name and a multitude of papers/smart solutions to it and I would love if someone could link me papers about how to solve this and or explain me the ""standard"" solutions.

Let me know if this makes any sense.",reinforcementlearning,badpolicy_bot,False,/r/reinforcementlearning/comments/gsy5ex/stopping_training_to_prevent_too_much_exploration/
Implementing self-play and PPO on LSTMs,1590768689,"I'm trying to train a model on a partial-information card game via self-play. I've decided on using PPO as my RL algorithm, and an LSTM as the policy net.

Also, I think the value network should be a MLP that takes the LSTM's hidden state as input.

What's my best bet for implementing this? This subreddit gives me the impression that Pytorch would be better for novel architectures. That being said, should I use someone else's implementation of PPO or try to roll my own?

Any tips would be greatly appreciated.",reinforcementlearning,ChromosomeCoupon,False,/r/reinforcementlearning/comments/gsvtff/implementing_selfplay_and_ppo_on_lstms/
"Reinforcement Learning with Shimon Whiteson | Research at University of Oxford, Waymo",1590742735,[removed],reinforcementlearning,mukulkhanna1,False,/r/reinforcementlearning/comments/gspce3/reinforcement_learning_with_shimon_whiteson/
How can we improve sample-efficiency in RL algorithm?,1590741299,"Hello everyone,

I am trying to understand the ways to improve sample-efficiency in RL algorithms in general.  Here's a list of things that I have found so far: 

* use different sampling algorithms (e.g., use importance sampling for off-policy case), 
* design better reward functions (reward shaping/constructing dense reward functions), 
* feature engineering/learning good latent representations to construct the states with meaningful information (when the original set of features is too big) 
* learn from demonstrations (experience transferring methods)
* constructing env. models and combining model-based and model-free methods

Can you guys help me out to expand this list? I'm relatively new to the field and this is the first time I'm focusing on this topic, so I'm pretty sure there could be many other approaches to do this (maybe the ones that I have identified might be wrong?). I would really appreciate all your input.",reinforcementlearning,PsyRex2011,False,/r/reinforcementlearning/comments/gsp2fy/how_can_we_improve_sampleefficiency_in_rl/
Actor-Critic policy gradient implementation never does better than random,1590692545,"I have run my implementation on both lunarlander and cartpole for a couple hundred iterations, and in both environments the algorithm's performance hovers around random. [This is my implementation.](https://github.com/Stephanehk/Reinforcement_Algorithm_Implementations/blob/master/actor_critic_lunarlander_NN.py) I was wondering if you guys knew why my actor-critic algorithm does not seem to be learning. Any debugging tips or metrics I can use?",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gscenj/actorcritic_policy_gradient_implementation_never/
"""Synthetic Petri Dish (SPD): A Novel Surrogate Model for Rapid Architecture Search"", Rawal et al 2020 {Uber}",1590691621,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gsc3ko/synthetic_petri_dish_spd_a_novel_surrogate_model/
Finding Cross-Lingual Syntax in Multilingual BERT,1590688030,,reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/gsaxi9/finding_crosslingual_syntax_in_multilingual_bert/
[D] Art of Juggling and Reinforcement Learning,1590673738,,reinforcementlearning,y0b1byte,False,/r/reinforcementlearning/comments/gs6lk9/d_art_of_juggling_and_reinforcement_learning/
Some help with applying RL to a real example?,1590666828,"I am familiar with the theory of RL, however, a bit new to applying it to real problems.

For instance, if I have a production line process **P: x1 -&gt; b -&gt; x2-&gt;done**, where **x1 -&gt; b** is the time of line1 (**x1**) to a buffer (**b**), **b -&gt; x2** feeds into line2 (**x2**), and **x2 -&gt; done** is the time of line2 (**x2**). 

I can take the actions of changing **x1** and **x2**, with **10&lt;x1&lt;20** and **5&lt;x2&lt;15**, and I want to keep the state **b** between 1 and 5.

How do I go about creating an agent that changes **x1** and **x2** based on the state of **b**?

I have not really seen any real application of RL, and just some example to work off of would be great!

Any help appreciated.",reinforcementlearning,BadassNobito,False,/r/reinforcementlearning/comments/gs4vq2/some_help_with_applying_rl_to_a_real_example/
friendly introduction to double deep Q learning,1590664927,"Is there any friendly introduction to double deep Q learning?I know Deep Q learning, it barely solved Cart-pole so I hope DDQN can solve Cart-pole easily.",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gs4gjq/friendly_introduction_to_double_deep_q_learning/
Blog Series on Proximal Policy Optimization,1590656006,"Hi All, Recently I started writing blogs to help me better understand concepts by articulating my thoughts. Currently I am in the process of writing a three-part blog series explaining all the theory and implementation details behind PPO in PyTorch. I have completed the first part (link below) where I explain Policy Gradients  Methods and would love to hear your thoughts and suggestions, so that I can improve upon it. 
[Understanding Proximal Policy Optimization Part 1: Policy Gradients](https://www.digdeepml.com/2020/05/24/Understanding-Proximal-Policy-Optimization-Part-1/)",reinforcementlearning,learner_version0,False,/r/reinforcementlearning/comments/gs2mj5/blog_series_on_proximal_policy_optimization/
[R] Navigating the Landscape of Games,1590655178,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/gs2gx3/r_navigating_the_landscape_of_games/
Help with maze solver,1590650561,"I have made an environment where an agent is to traverse a maze from a start position to an end position. There are obstacles in the maze which it needs to avoid and get penalized if it walks into one. I am also penalizing the agent if it is near the obstacle so that it avoids it completely. On every transition, it gets a reward of -0.1. I am using DQN to solve this as it is a smaller version of a bigger problem, I am not using Table method. The problem I am facing is that after training when I test it, the agent is iterating over 2 coordinates and not progressing towards the goal position. Can someone help me with solving this?

&amp;#x200B;

I am attaching the link to my notebook here. 

[https://colab.research.google.com/drive/1tZF-grzT9OlJRALzuj8b-lcvze0cBWTo?usp=sharing](https://colab.research.google.com/drive/1tZF-grzT9OlJRALzuj8b-lcvze0cBWTo?usp=sharing)

&amp;#x200B;

Thanks :D",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/gs1kjc/help_with_maze_solver/
Training RL models with MacBook Pro Blackmagic eGPU?,1590643888,Is it OK to train RL models with MacBook Pro Blackmagic eGPU?,reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/gs08lo/training_rl_models_with_macbook_pro_blackmagic/
Educational Material for RL,1590639640,"I created a website teaching RL from scratch (mostly following Sutton's book)
http://www.jabrahtutorials.com/
Feel free to check it out, it's all free.",reinforcementlearning,JCMLight,False,/r/reinforcementlearning/comments/grzbm8/educational_material_for_rl/
"[D] Issues reproducing CURL, algorithm seems broken??",1590637831,,reinforcementlearning,hardactorcritic,False,/r/reinforcementlearning/comments/grywat/d_issues_reproducing_curl_algorithm_seems_broken/
[Distributed Pytorch] Preserving torch autograd when sending tensors,1590616215,"Hello all,

I’m currently working on some distributed frameworks for RL, particularly Ape-X for a TD3 implementation and another similar one for PPO. I can’t get the thing to converge like it should, and I think I’ve found the answer if anyone agrees:

I’m using the library Ray for the remote calls. In the PPO implementation, I send about 30 workers off with a copy of the policy and the environment, and they return experience. The problem being, when I send the experience log probabilities, the Tensor.grad_fn gets set to None. The Tensor.requires_grad attribute is still True, but how do the network params know how to update I’d the derivatives are gone?

When I try to explicitly save the grad function itself and send it over the network, I get a TypeError: can’t pickle SubBackward0 objects. 

My theory is the RPC zeros out the gradient info so the network just keeps looping in the first train loop: trains on first batch of experience, sends the parameters to the workers, workers return experience, backward() does nothing bc no grad_fn, repeat. 

Any helps/ workarounds? Anybody work with torch’s RPC or torch.multiprocessing? This is essential work IMO for capable programmers and researchers to run distributed RL solutions. Thanks to any help.",reinforcementlearning,memelord_5517,False,/r/reinforcementlearning/comments/grt6a3/distributed_pytorch_preserving_torch_autograd/
Hidden Markov Models ~ Baum Welch Algorithm,1590600384,,reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/gro2y7/hidden_markov_models_baum_welch_algorithm/
making custom open ai gym environment,1590586674,I have made a small game in Python using PyGame. (Whirly Bird) Now I want to modify this code to make it OpenAi Gym Compatible such that observation function returns the actual image slices from the game. Most of the tutorial I have seen online returns only some kind of low dimension observation state. I am new to OpenAi gym so any help is highly appreciated.,reinforcementlearning,muddy_danger,False,/r/reinforcementlearning/comments/grjvp6/making_custom_open_ai_gym_environment/
Looking for research opportunities,1590567012,"Hi everyone, I am looking for remote research opportunities to develop my skillset in RL, and while doing so get to have research experience. Please DM me if anyone has any open positions or suggestions on how to land one.

Thank you :-)",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/grflai/looking_for_research_opportunities/
How to use different map in car racing environment in openai gym,1590566177,I want to know how to change the  map in car racing environment . I read the code in github but i am not able to find when the picture is present in openai repo .[https://github.com/openai/gym/blob/master/gym/envs/box2d/car\_racing.py](https://github.com/openai/gym/blob/master/gym/envs/box2d/car_racing.py),reinforcementlearning,ajithvallabai,False,/r/reinforcementlearning/comments/grff2f/how_to_use_different_map_in_car_racing/
Get access to Unlimited GPU Power! Get $100 in GPU credits from Genesis Cloud,1590551784,,reinforcementlearning,sanchit2843,False,/r/reinforcementlearning/comments/grcbt2/get_access_to_unlimited_gpu_power_get_100_in_gpu/
A day in the life of RL researchers??,1590536268,"Hey people! For the RL researchers out there, what does ur daily schedule look like? 
I recently started full time working from home and find myself with a lot of free time waiting for experiments which usually take days.. so for RL researcher, what does your day look like? How many projects are you working on? Any daily routines (eg. x papers a day)? And any habits you think young researchers should develop? 
Thanks!",reinforcementlearning,CauchyBirds,False,/r/reinforcementlearning/comments/gr8cwx/a_day_in_the_life_of_rl_researchers/
I made Self-driving A.I. with Reinforcement Learning In Unity3D,1590532051,,reinforcementlearning,Mingoooose,False,/r/reinforcementlearning/comments/gr74xl/i_made_selfdriving_ai_with_reinforcement_learning/
Increasing invalid action punishment does not work?,1590518704,"So i tested a DQN agent with an environment where action 3 is an invalid action, which is made clear in the state representation by either a 0 or 1. With an invalid punishment of -0.01 the amount of invalid chosen actions actually increase over the run. However the end rewards are still possible but it could also achieve the same with the valid actions and don't have the problem of the invalid actions decreasing the total reward? I tried increasing the punishment which did not lead to anything. Does anyone have an idea?

https://preview.redd.it/yzxtua9u65151.png?width=1324&amp;format=png&amp;auto=webp&amp;s=abe00614475378b229ad37c650056f77d5fa5b04",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/gr2x14/increasing_invalid_action_punishment_does_not_work/
What are some simple benchmarks for non-stationary environments used in RL?,1590511491,"Are there simple non-stationary environments in Open AI gym/elsewhere that can be used for starting to delve into the topic of non-stationarity in RL? So far I have found this repo called [dyna-gym](https://github.com/SuReLI/dyna-gym) and [this one](https://github.com/HankyuJang/Non-Stationary-Reinforcement-Learning-). I am trying to avoid computation heavy stuff like some non-stationary [3D game environments](https://github.com/vlomonaco/crlmaze) as I want to understand the core issues using simple examples.

As an aside, any survey paper or some current papers explaining this topic somewhat intuitively will be helpful. I started looking at these papers to begin with: [1](http://proceedings.mlr.press/v32/tamar14.pdf) and [2](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewPDFInterstitial/16825/16216) but they are so math-heavy it is difficult to loose their idea in the jargon.",reinforcementlearning,AvisekEECS,False,/r/reinforcementlearning/comments/gr0lj3/what_are_some_simple_benchmarks_for_nonstationary/
Budgeted discrete action for SAC,1590502918,"Hello everyone.

I have recently added a discrete action option for my SAC using a GumbelSoftmax distribution. Which allows sampling from probability vectors while remaining differentiable.


My agent has 16 possible actions but I know for sure that I want it to take each action a maximum of 4 times. I know that the reward will be optimal with a policy that chooses each action 4 times. I thought of enforcing it by keeping track of the amount of times the agent has taken each action and action N has been chosen more than 4 times I will always zero it out in the output of the network but currently it doesn't seem to work very well. I then tried pushing this budget to the replay buffer so that I can zero out the correct actions when I'm sampling during the offline update.


Is there any literature on this kind of problem/have you ever done something similar?",reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/gqxy1d/budgeted_discrete_action_for_sac/
hello. which is better Tensorflow or Pytorch for reinforcement learning?,1590492260,,reinforcementlearning,L_G_S_H,False,/r/reinforcementlearning/comments/gqv7q7/hello_which_is_better_tensorflow_or_pytorch_for/
Have We Caught 'Em All? Can you find another reason for sample inefficiency in on-policy deepRL?,1590484788,,reinforcementlearning,ml_keychain,False,/r/reinforcementlearning/comments/gqtq40/have_we_caught_em_all_can_you_find_another_reason/
Have We Caught 'Em All? Do you know other reasons for sample inefficiency in on-policy DeepRL?,1590484408,,reinforcementlearning,ml_keychain,False,/r/reinforcementlearning/comments/gqtno0/have_we_caught_em_all_do_you_know_other_reasons/
Is reinforcement learning viable for this?,1590480053,I have a system of 36 independent parameters. Each of which can have be sampled with N values. That makes the number of combinations for all possible values of my parameters N\^36. This is astronomically huge. Is there a way to employ reinforcement learning for this type of problem?,reinforcementlearning,PetrosOratiou,False,/r/reinforcementlearning/comments/gqstrb/is_reinforcement_learning_viable_for_this/
A Clearer Proof of the Policy Gradient Theorem,1590478285,,reinforcementlearning,bluecoffee,False,/r/reinforcementlearning/comments/gqshl7/a_clearer_proof_of_the_policy_gradient_theorem/
From mocap data to an activity grammar,1590477830,"&amp;#x200B;

https://preview.redd.it/grb10ysj92151.png?width=268&amp;format=png&amp;auto=webp&amp;s=132b552e13592c42dd6bbb9486e0940d4df88f8a

Computer science is devoted to algorithms. An algorithm is a heuristic to solve a problem. A typical example for an algorithm is bubblesort or the A\* search algorithm. More advanced examples for transforming knowledge into a computer program are backtracking search and neural network learning algorithms. All these concepts have in common that they are based on scientific computing. There is a high speed CPU available which is able to run an algorithm, and the task for the programmer is to minimize the amount of processing steps, so that the task can be solved in a small amount of time.\[1\]

The main problem with algorithm oriented computer science is, that it is ignoring non-algorithmic problem solving strategies. The computer provides more functionality than only the ability of number crunching, it is a data processing engine too. Data processing doesn't work with algorithms but with databases. A database is a table which stores information from the real world.

Data oriented processing is the key element in developing artificial intelligence. If a computer should recognize spoken language or control the movements of a robot he doesn't need advanced algorithms but the machine needs a corpus. A typical fileformat format for a corpus is the CSV format, but MS-Excel sheets and json data are providing the same amount of information.

The main aspect of corpus data is, that it provides not a heuristics and doesn't contains of computer programs, but data are representing something which has nothing to do with computing at all. The Turing machine was invented as a device for running an algorithm, but the harddrive of a computer was constructed as a passive element which is doing nothing.

The work hypothesis is, that advanced Artificial Intelligence doesn't need a certain software program to behave intelligent, but a corpus of data. There is no need to program a computer, but the human operator has to provide a csv file which contains the input data.

**Motion capture**

Let us talk about how motion capture is working. Motion capture is a computer based recording strategy in which the position of a marker is stored in a database. The table contains of a frame number which is increasing and it provides the 3d position which is equal to x, y, z. Basically spoken a mocap recording produces an excel sheet which contains of numbers stored in a table. This sheet can't be executed on a turing machine, but it's size is measured in bytes. A small table contains of 10 kb of data, while a larger one has 1000 kilobyte of information.

After the mocap table was recorded, the next step is to convert the information into a motion graph.\[2\] A motion graph is similar to the original recording a datastructure, but not an algorithm. The difference is, that motion graphs are reordering the information as a transition system. From the starting node0, it's possible to wander to the follow up node 3 or 4. And from node4, it's possible to move towards node 8 or 10. It's a choice based movement in the mocap data.

The usefulness of a motion graphs can be increased with a grammar based representation. A grammar is used for constructing languages, and in case of mocap data, the language is about the movement of arms and legs.

**References**

* \[1\] Korf, Richard E. Artificial intelligence search algorithms. Computer Science Department, University of California, 1996.
* \[2\] Kovar, Lucas, Michael Gleicher, and Frédéric Pighin. ""Motion graphs."" ACM SIGGRAPH 2008 classes. 2008. 1-10.",reinforcementlearning,ManuelRodriguez331,False,/r/reinforcementlearning/comments/gqsecw/from_mocap_data_to_an_activity_grammar/
[D] Uber AI's Contributions (RIP),1590431493,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gqge09/d_uber_ais_contributions_rip/
Is there any article that fully covers deep Q-learning,1590422150,"Do you know any article about deep Q-learning? I need articles that fully cover what deep Q-learning is and how to implement it.Sometimes i read different things on internet.I still don't understand cost function, and how to optimize network, what is target network and how to use it",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gqdg2e/is_there_any_article_that_fully_covers_deep/
[Question] Tracking a nonstationary problem: Sutton's book.,1590400225,"&gt;Sometimes it is convenient to vary the step-size parameter from step to step. Let a\_n(a) denote the step-size parameter used to process the reward received after the nth selection of action a. As we have noted, the choice a\_n(a) = 1/n results in the sample-average method, which is guaranteed to converge to the true action values by the law of large numbers.

What I do not understand is, if we use the step size as 1/n, with every selection of the action, the update value becomes smaller and smaller. Say the probability distribution is changing with time (nonstationary problem), then as we keep on selecting the action, the update is smaller, hence the new quality or value can't express the changed probability distribution. Then how will the sample-average method converge to true action value?",reinforcementlearning,evilmorty_c137_,False,/r/reinforcementlearning/comments/gq88bd/question_tracking_a_nonstationary_problem_suttons/
Graph Neural Networks with Reinforcement Learning,1590393168,"Can someone point me to some good resources to start with GNNs and gradually move towards using it with RL?

Thanks :D",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/gq6v4e/graph_neural_networks_with_reinforcement_learning/
Dual Double DQN to train agent to play Breakout,1590384770,"hi guys, I am training an agent with Dual Double DQN to play Breakout, but after 7m steps the average rewards still oscillate around 30. Any one can point me out what may be the potential reasons?",reinforcementlearning,xz1249,False,/r/reinforcementlearning/comments/gq57g5/dual_double_dqn_to_train_agent_to_play_breakout/
Video games using RL?,1590379124,"Hi,

I’m wondering if there are any somewhat modern video games, especially 3D open worlds games, using RL for NPC AIs? Are there any good places to read more about it? (I’ve found a 2016 Quora convo, but I’m looking more for research papers or actual games)

It really interests me how RL could provide variance in NPC AIs.

Thanks",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/gq3yy2/video_games_using_rl/
Temporal difference learning - SARSA,1590343454,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/gpu8zq/temporal_difference_learning_sarsa/
Clarification about REINFORCE,1590325018,"Hello,

I'm wrapping my head around the REINFORCE implementation presented in [OpenAI SpinningUp codebase](https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/vpg/vpg.py#L216). From the code, we can see that the loss is computed as:
```
-log(p) * advantage
```

Imagine a scenario in which an agent has an action space of 20 actions. All the episodes have length 1 so the agent, given a state, selects one of the 20 actions according to its policy. The agent will receive a reward of +1/-1 depending on whether the action is correct or not.

My main issue is with the case in which the agent is highly confident about a given action. So we basically have 2 possible cases to consider when we evaluate the loss:

1. Agent is confident about the action and the game is successful: this means that log(p) ~= 0 and the reward is +1. The total loss will be something really small which makes sense. The agent won't be penalised because it's correct in its predictions.
2. Agent is confident about the action and the game is unsuccessful: this means that log(p) ~= 0 and the reward is -1. The total loss will be again really small just like in case 1. 

I can see a massive issue in 2). The agent, differently from case 1), should be heavily penalised because it's confident about an action that is completely wrong for the current state. Am I missing something here? Shouldn't REINFORCE increase the probability of successful actions and discourage wrong actions?

Thank you in advance for your help!",reinforcementlearning,ale_suglia,False,/r/reinforcementlearning/comments/gpp9je/clarification_about_reinforce/
Does anyone know if deepmind has published their code for Agent57?,1590316070,"Does anyone know if deepmind has published their code for Agent57? And if they didn't, has anyone managed to reproduce the results? Would absolutely love to checkout the implementation but I couldn't find it anywhere. 

[https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark)",reinforcementlearning,83d08204-62f9,False,/r/reinforcementlearning/comments/gpngfl/does_anyone_know_if_deepmind_has_published_their/
[D] Examples of good science in reinforcement learning experiment-focused papers,1590315622,,reinforcementlearning,tmpberk,False,/r/reinforcementlearning/comments/gpndcy/d_examples_of_good_science_in_reinforcement/
[Project] Using DQN (Q-Learning) to play the Game 2048.,1590309983,,reinforcementlearning,FelipeMarcelino,False,/r/reinforcementlearning/comments/gpmcen/project_using_dqn_qlearning_to_play_the_game_2048/
Soft Actor Critic doesn't converge when the done parameter is passed correctly,1590306609,So in the SAC algorithm when you are in a termination state which in my case is just the last timestep of an episode you need to pass a 0 to the done parameter instead of a 1 to nullify the value estimation of the next timestep. Unfortunately for me the algorithm converges nicely when I always pass 1 to that parameter. If I engage the option to pass a 0 on the last timestep to down it just stops converging. Any idea why would that happen? Implementation is correct and taken from credible sources.,reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/gplr7l/soft_actor_critic_doesnt_converge_when_the_done/
Project Malmo: How to get the agent’s position?,1590286873,"How do I get the xyz position of the agent (single agent env) in the world? I have been through their Gitter Forum, docs, and reached out to the creators on LinkedIn and by email, but haven’t gotten an answer. It should be simple, but I just can’t find it. Any ideas?",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/gphnxw/project_malmo_how_to_get_the_agents_position/
New and Stuck,1590241338,"I want to create an OpenAI Gym environment for a wireless network that consists of a receiver and N transmitters, including the potential spoofers that can impersonate another node(transmitter) with a fake MAC address.

So I have a project due tommorow where I need this. I don't have any clue how to create a cuostom environment to run my Q-learning algo. There is not enough time to do anything right now, can anyone of you help me out?",reinforcementlearning,Mrs_Newman,False,/r/reinforcementlearning/comments/gp5493/new_and_stuck/
How to pass live data to gym env?,1590196621,"I have built a stock trading env  that accepts a pandas dataframe. Now I need to pass live market data into the env for trading. I'm using OpenAI Gym and Stable Baselines. If anyone knows how this can be done, please help me out. Thanks!",reinforcementlearning,3ventHoriz0n,False,/r/reinforcementlearning/comments/gow00q/how_to_pass_live_data_to_gym_env/
Do these results look right for episodic actor-critic on cartpole?,1590179687,"The algorithm does much better than a random agent, however, does not solve cartpole. Are these the correct results or is something wrong with my implementation? Note the graph is the moving weighted average total reward. 

&amp;#x200B;

https://preview.redd.it/z210vj63nd051.png?width=1180&amp;format=png&amp;auto=webp&amp;s=049978836f5d83932651a5d7ec832bd0679c6161",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gor9kf/do_these_results_look_right_for_episodic/
"""Learning to Simulate Dynamic Environments with GameGAN"", Kim et al 2020 {Nvidia} (learning environment models with GANs augmented with NTM-like memory)",1590165589,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gomqbh/learning_to_simulate_dynamic_environments_with/
Study Group,1590159779,"Hi everyone. I am looking for people who are interested to learn DRL and MARL in a more project-centric way. Starting from theory to practical implementation. 

&amp;#x200B;

If there are already such groups please let me know 

&amp;#x200B;

Thanks :D",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/gokymr/study_group/
Train multiple agents with different behaviors together,1590154512,"In my environment, I have multiple agents of different types.

My environment `step()` returns a dictionary with the step data for each of these agent types. How can I train such an environment? What RL library can I use?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/gojgi8/train_multiple_agents_with_different_behaviors/
Creating human like robots,1590148937,"The robots, developed by computer science students, have in common that they are performing not very well. Usually the robot contains of an algorithm which struggles in more complex tasks. Improving the algorithm isn't possible.

&amp;#x200B;

To overcome the bottleneck a robot needs to be created with human level intelligence. The biggest surprise for computer science students is, that such a technology is available but it works different from normal computer programming. Human level AI can be realized with teleoperation which is a technique used in animatronics. The idea is, that not an algorithm determines what the robot is doing next, but the software transmits the signals from a human operator which stands behind the robot. These robots are working different from search algorithm which are analyzed by computer experts, but they have more in common with a remote controlled crane.

&amp;#x200B;

The technology for realizing human like robots consists of a motion capture suite for recording the motion, a motion library which stores the movement in the XML format and a replay module which retrieves the trajectories from the database. Such a system isn't limited by a certain algorithm but the motion database restricts the possible movements. If the database is a larger one, the robot behaves like a human. The term robot programming doesn't mean to invent an algorithm which allows the robot to do an action, but robot programming is equal to fill the database with information. These information are equal to sample trajectories from different demonstrations.

&amp;#x200B;

A database of motions is the key element of a robot. It stores recorded motions and it allows to recall existing motions.

&amp;#x200B;

\##Motion capture database

&amp;#x200B;

An easy to follow tutorial for realizing a complex robotics system is based on the CMU Motion capture database. The file is imported in the bvh format into the blender animation software and then a human character gets animated. The CMU bvh file provides the keyframe and the blender software is rendering the character. If the overall pipeline is connected with the keyboard, the user can activate certain motions like jump and walk.

&amp;#x200B;

The interesting point in the pipeline is, that no dedicated algorithm is needed but the robot movements are stored in the bvh file.\[1\] This file is the key element for human like animations.

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/1bxzrozk3b051.png?width=630&amp;format=png&amp;auto=webp&amp;s=ffc3ff4027affc3601fe0cfaa4e5ca56658b0c23

&amp;#x200B;

\##From game logs to motion graphs

&amp;#x200B;

A typical step in creating a game log is realized with the sqlite database. The player's position is stored 10 times a second into the database, together with the frame number and events like “jumping”.\[2\] This produces a large amount of data. In the next step the stored game log is converted into a motion graph. A motion graph is a tree based database in which the information are stored in a semantic order. For example, in the beginning the player has the choice to move to the left, or to the right. Both options are stored in the motion graph. Such a graph structure can be used by a solver to find a path in the action space.

&amp;#x200B;

The conversation from a game log into a semantic annotated motion graph isn't defined precisely. It's some sort of alchemy knowledge how to realize the workflow. What we can say for sure is, that learning from demonstration, motion capture and trajectory generation needs a well formatted database in the loop. It's the task of the programmer to make suggestions about the database diagram and specify which field names are needed.

&amp;#x200B;

\##References

&amp;#x200B;

\[1\] Hosseini, Seyed Ramezan, et al. ""Teaching Persian Sign Language to a Social Robot via the Learning from Demonstrations Approach."" International Conference on Social Robotics. Springer, Cham, 2019.

&amp;#x200B;

\[2\] Karpov, Igor V., Jacob Schrum, and Risto Miikkulainen. ""Believable bot navigation via playback of human traces."" Believable Bots. Springer, Berlin, Heidelberg, 2013. 151-170.",reinforcementlearning,ManuelRodriguez331,False,/r/reinforcementlearning/comments/goi2og/creating_human_like_robots/
"So many ""relatively"" advanced new areas , which ones to focus on",1590146381,"Well this might be awkward thing to say but really here   
After exploring &amp; learning basic &amp; classical &amp; modern stable algorithms and methods ( dynamic programming,monte carlo , tabular methods , DQNs , PGs and Actor critics such as PPO,DDPG,DD4G,A2C etc. I Feel comfortable with these approaches which they are solid enough &amp; proven in various tasks.  
I used them in some envs and created some custom envs myself but here I'm stuck which areas to explore. 

Things I have seen that might be promising to learn &amp; iterate on.

\- **Meta RL and Deep Episodic  Control - &gt;** Requires to learn RNN and LSTM's in general. Is this area promising enough to pour time into it? 

\- **Model Based Algorithms in general**  = I didn't do much work regarding to this area considering most courses/book parts here talking about GO,Backgammon and out of reach / reproducable things like Dota2,Self learning agents which require huge compute clusters

  
\- **Evolved Policy Gradient - &gt; Evolution Strategies  =** Again looks promising but is it future of RL , should it be learned or are they just not prominent / proper yet to be investigated

**- Curiosity Based Algorithms =** I have no info about them

**-  Self attention agents =**   I have no info about them

 **- Hybrid methods like Imaginative Agents =** Which tries to combine Model free and model based approaches

**-  World model based algorithms =** Sutton seemingly pushing this?

 **- Exploration Techniques**

If you have enough experience with these please tell me about your experience , which ones are worth looking into ?  Which ones are seem rubbish (kinda harsh but :) )",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/gohilo/so_many_relatively_advanced_new_areas_which_ones/
help normalizing observations for PPO,1590121580,"Hello, I am currently stuck trying to normalize the observations for my PPO implementation. Could anyone offer some help in implementing a method for normalizing the states using a running mean and standard deviation? I am working in pytorch and I am using parallel environments if that helps. Thanks a lot..",reinforcementlearning,hanuelcp,False,/r/reinforcementlearning/comments/gocj1n/help_normalizing_observations_for_ppo/
Difference between state space and feature space,1590098550,I have seen this in numerous places where learning in state space is considered distinctly from learning in feature space. But I am not sure I understand the difference between them. Would highly appreciate if someone could properly explain these two terms and difference between them.,reinforcementlearning,namuradAulad,False,/r/reinforcementlearning/comments/go6br9/difference_between_state_space_and_feature_space/
Question about the sign of the advantage and log probability in the policy gradient,1590098548,"This is something that's puzzled me for a while and I'm hoping someone can clarify it for me.

I'm doing a typical REINFORCE algorithm, subtracting a value function as the baseline. To keep it simple, let's say I have one NN for the value function, V(s), and a whole separate one for the policy, pi(s).

For a single step, the discounted total R for that step `t` is `R_disc = R_t + gamma*R_t+1 + gamma^2*R_t+2 + ...`.

So every time I update, I do:
```
adv = R_disc - V(s_t)
loss_V = 0.5*adv.pow(2)
loss_policy = -adv.detach()*log_prob(pi(s_t, a_t))
# Do backwards(), step optimizer, etc
```

What I'm getting confused about is how the sign of `adv` and the `log_prob` interact. For example, my intuition tells me that if I plot `loss_policy` throughout a successful training, I should see it only decrease, because we're doing gradient descent on it (i.e., gradient ascent on `adv*log_prob`).

However, I often don't see that. Is that assumption correct? 

It seems like as V learns to better approximate `R_disc`, `adv` will go to zero. From what I understand, the policy gradient is basically just trying to increase the log probability of actions that gave higher R's. It seems like the `log_prob` term can be either positive (if pi is a Gaussian for example) or negative (if pi is a discrete set of probabilities). So then it seems like if `log_prob` is positive and `adv` starts as positive and goes to zero, the `loss_policy` term will start negative and *increase* to 0.

So what's right here?",reinforcementlearning,FirmRub3,False,/r/reinforcementlearning/comments/go6bq8/question_about_the_sign_of_the_advantage_and_log/
how often unity is used by scientists,1590091738,"how often does researchers,scientists engineers use unity for RL? I mean, I have seen how they teach agent how to walk, is that environment unity?Is unity very popular in RL community?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/go47rj/how_often_unity_is_used_by_scientists/
"Newbie: My first AI (Deep Convolutional Q-Learning), how can I optimize it for faster training?",1590081843,"I have been learning a bit about AIs by reading the book “AI Crash Course”. After reading it, I got inspired by the snake AI he made in the book, and wanted to make my own little AI. I have then tried to follow the same approach as he did in the book, to make an AI, Deep Convolutional Q-Learning, using a Convolutional Neural Network (CNN), for the openAI atari game Breakout-v0.

So far I have made a program, where I set the AI up and train it. But it is at this stage I have run into a problem, or rather two problems:

1. The AI seems to simple move to the right and nothing else, and I don't know why.
2. The program is really taxing on my pc, making the training take forever. It seems the ""dqn.get\_batch()"" and/or “train\_on\_batch()” function(s) is the performance problem. Is this just how it is, or is there a possible way for me to improve the function(s), so the program isn’t so taxing and the AI can be trained faster?

Or do you think the problem lies else where?

I have written it in Jupyter Notebook, and here is the whole code so far, split into the cells:

    # Section in which we build our CNN - Brain
    
    # Importing the libraries
    import keras
    from keras.models import Sequential, load_model
    from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten
    from keras.optimizers import Adam
    
    nRows = 210                  #Number of rows in the image.
    nColumns = 160               #Number of columns in the image.
    
    # Creating the Brain class
    class Brain():
        def __init__(self, iS = (nRows,nColumns,3), lr = 0.0005):  # is = input shape    lr = learning rate
            self.learningRate = lr 
            self.inputShape = iS
            self.numOutputs = 3         # three actions total, left, right and nothing
            self.model = Sequential()
            # Adding layers to the model
            self.model.add(Conv2D(32, (3,3), activation = 'relu', input_shape = self.inputShape))  #32 3x3 filters with the ReLU activation function. You
            self.model.add(MaxPooling2D((2,2)))                                                    #adding a max pooling layer. The window's size is 2x2, which will shrink the feature maps in size by 2.
            self.model.add(Conv2D(64, (2,2), activation = 'relu'))                                 #second convolution layer. This time it has 64 2x2 filters, with the same ReLU activation function.
            self.model.add(Flatten())                                                              #flatten to a 1D vector. Here the 2D images is flattened to a 1D vector, which we then will be able to use as the input to your neural network.
            self.model.add(Dense(units = 256, activation = 'relu'))                                #full connection step – building the traditional ANN. This specific line adds a new hidden layer with 256 neurons and the ReLU activation function to the model.
            self.model.add(Dense(units = self.numOutputs)) #last layer of the neural network – the output layer. It has as many outputs as there are actions. By not mentioning an activation function, it defaults to a linear.
            # Compiling the model
            self.model.compile(loss = 'mean_squared_error', optimizer = Adam(lr = self.learningRate)) #how to calculate the error (indicated by loss), and which optimizer to use when training the model
    
            # Making a function that will load a model from a file
            def loadModel(self, filepath):
                self.model = load_model(filepath)
                return self.model

&amp;#x200B;

    # Section containing the environment (Breakout game) - Environment
    import gym
    import atari_py
    env = gym.make(""Breakout-v0"")

&amp;#x200B;

    # Section that builds the Experience Replay Memory - DQN
    
    # Importing the libraries
    import numpy as np
    
    # IMPLEMENTING DEEP Q-LEARNING WITH EXPERIENCE REPLAY
    
    class Dqn(object):
        
        # INTRODUCING AND INITIALIZING ALL THE PARAMETERS AND VARIABLES OF THE DQN
        def __init__(self, max_memory = 100, discount = 0.9):
            self.memory = list()
            self.max_memory = max_memory
            self.discount = discount
    
        # MAKING A METHOD THAT BUILDS THE MEMORY IN EXPERIENCE REPLAY
        def remember(self, transition, game_over):
            self.memory.append([transition, game_over])
            if len(self.memory) &gt; self.max_memory:
                del self.memory[0]
    
        # MAKING A METHOD THAT BUILDS TWO BATCHES OF INPUTS AND TARGETS BY EXTRACTING TRANSITIONS FROM THE MEMORY
        def get_batch(self, model, batch_size = 10):
            len_memory = len(self.memory)
            num_outputs = model.output_shape[-1]
            
            # Input batch which works with 3D states
            inputs = np.zeros((min(len_memory, batch_size), self.memory[0][0][0].shape[1],self.memory[0][0][0].shape[2],self.memory[0][0][0].shape[3]))
            
            targets = np.zeros((min(len_memory, batch_size), num_outputs))
            for i, idx in enumerate(np.random.randint(0, len_memory, size = min(len_memory, batch_size))):
                current_state, action, reward, next_state = self.memory[idx][0]
                game_over = self.memory[idx][1]
                inputs[i] = current_state
                targets[i] = model.predict(current_state)[0]
                Q_sa = np.max(model.predict(next_state)[0])
                if game_over:
                    targets[i, action] = reward
                else:
                    targets[i, action] = reward + self.discount * Q_sa
            return inputs, targets

&amp;#x200B;

    # Section where we will train our AI to play Breakout - Trainer
    
    # Importing the libraries
    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    
    # Defining the parameters (hyperparamteters)
    memSize = 50                #The maximum size of the experience replay memory.
    batchSize = 2                #The size of the batch of inputs and targets gotten at each iteration from the experience replay memory for the model to train on.
    learningRate = 0.0001        #The learning rate for the Adam optimizer in the Brain.
    gamma = 0.9                  #The discount factor for the experience replay memory.
    nLastStates = 3              #How many last frames we save as our current state of the game. Remember, the input is a 3D array of size nRows x nColumns x nLastStates to the CNN in the Brain.
    epsilon = 1.                 #The initial epsilon, the chance of taking a random action.
    epsilonDecayRate = 0.0002    #By how much we decrease epsilon after every single game/epoch.
    minEpsilon = 0.05            #The lowest possible epsilon, after which it can't be adjusted any lower.
    filepathToSave = 'model2.h5' #Where we want to save the model.
    
    # Creating the Environment, the Brain and the Experience Replay Memory
    #env = environment(0)  --- maybe just remove this line
    brain = Brain()
    model = brain.model
    dqn = Dqn(memSize, gamma)
    
    # A function that will initialize game states
    def resetStates():
        observation = rgb2gray(env.reset())
        currentState = np.zeros((1, nRows, nColumns, nLastStates))
        for i in range(nLastStates):
            currentState[:,:,:,i] = observation
        return currentState, currentState
    
    # A function that converts the picture to grayscaale
    def rgb2gray(rgb): 
        return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])
    
    # Starting the main loop
    epoch = 0
    scores = list()
    maxScore = 0    #the highest score obtained so far in the training
    score = 0      #the score in each game/epoch
    
    while True:
        # Resetting the environment and game states
        currentState, nextState = resetStates()
        epoch += 1
        gameOver = False
        previousLifes = 5
        lifes = 5
        livesStep = 0
        score = 0
        bricks = 84   # 14 bricks in a row and there are 6 rows in total, so a starting amount of 84 bricks
            
        # Starting the second loop in which we play the game and teach our AI
        while not gameOver:
            # Choosing an action to play
            if np.random.rand() &lt; epsilon:        #Checks if a random action sould be made, or just take the action with the highest Q-value.
                action = np.random.randint(0, 3)
            else:
                qvalues = model.predict(currentState)[0]
                action = np.argmax(qvalues)
            
            # Updating the enviroenment
            state, reward, gamOver, livesStep = env.step(action)
            lifes = livesStep['ale.lives']
            # converting the rgb image state to grayscale
            state = rgb2gray(state)
    
            # Adding new game frame to the next state and deleting the oldest frame from next state
            state = np.reshape(state, (1, nRows, nColumns, 1))
            nextState = np.append(nextState, state, axis = 3)
            nextState = np.delete(nextState, 0, axis = 3)
            
            # Remembering the transition and training the AI
            dqn.remember([currentState, action, reward, nextState], gameOver)
            inputs, targets = dqn.get_batch(model, batchSize)
            #model.train_on_batch(inputs, targets)
            
            # Sets the currenState to the nextState
            currentState = nextState
            
            # Render the game
            env.render()
            
            # Updates the score
            score += reward
            score -= bricks * 0.001 #gives a penalty of -0.001 times the number of bricks that are remaining
            
            if reward &gt; 0:
                bricks -= reward
                print(bricks)
            
            # If a life was lost, reset environment
            if lifes &lt; previousLifes:
                # Updating lifes
                previousLifes -= 1
                # granting it a negative reward for loosing a life
                score -= 5
            if lifes == 0:
                gameOver = True
        
        # Checking if score record was beaten and if yes then saving the model
        if score &gt; maxScore:
            maxScore = score
            model.save(filepathToSave)
        
        # Lowering the epsilon
        if epsilon &gt; minEpsilon:
            epsilon -= epsilonDecayRate
        
        # Showing the results each game
        print('Epoch: ' + str(epoch) + 'Score: ' + str(score) + ' Epsilon: {:.5f}'.format(epsilon))",reinforcementlearning,mufasDK,False,/r/reinforcementlearning/comments/go1316/newbie_my_first_ai_deep_convolutional_qlearning/
Interesting idea to deal with large discrete action spaces,1590077148,\[[paper here](https://arxiv.org/abs/2004.12485)\] The idea is to predict the action is continuous space and then the environment maps it to k valid discrete reactants using k-NN. This enables them to use any continuous action space RL algorithm and the results are pretty impressive,reinforcementlearning,saince96,False,/r/reinforcementlearning/comments/gnzm4t/interesting_idea_to_deal_with_large_discrete/
Interesting idea to deal with large discrete action spaces,1590075976,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/gnz8xo/interesting_idea_to_deal_with_large_discrete/
Counterfactual Regret Minimization with Kuhn Poker,1590066416,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/gnwioa/counterfactual_regret_minimization_with_kuhn_poker/
Way to plot good-looking rewards plots?,1590062548,"I am currently working on my bachelor diploma thesis on reinforcement learning, and this is the point, where I should include graphics of the results of my work. As I was going through my research, I have encountered multiple papers with really good-looking plots of rewards(most-especially with smoothed rewards and faded real reward plot). Like this one

&amp;#x200B;

[Good looking plot](https://preview.redd.it/kch1wmf5y3051.png?width=1502&amp;format=png&amp;auto=webp&amp;s=731fefb967a025796e4bae9530c609c66d4b9014)

&amp;#x200B;

However, I have no idea, how they are creating these. I am doing all the computing with tensorflow/pytorch so I am using tensorboard for looking at the learning curves myself, but they are pretty bad-looking, if I want to use them in terms of bachelor thesis.   


I've wrote some tensorboard parser and try to plot with matplotlib, but results are not as good looking as these are.

Maybe there is a library or something that does that and I am reinventing the wheel with processing rewards and putting filter on them to plot, which is really not that straitghtforward?  


Would appreciate any suggestions",reinforcementlearning,alex_karavaev,False,/r/reinforcementlearning/comments/gnvlcp/way_to_plot_goodlooking_rewards_plots/
Anyone Familiar with any AMIDAR enemy-avoidance strategies? [Atari] [Reasoning about the map],1590042302,"Hi all,

I am not too sure if this fits in this subreddit but I know of no other place this may go in. I am currently hand coding some AMIDAR bots to compare to some RL policies and was wondering if anyone is aware/suggest of a good strategy to be able to avoid enemies for as long as possible? 

I currently have something that finds the enemy closest to the player and then chooses the direction that maximizes the distance between the enemy and the player. The problem with this implementation is that it considers one enemy at a time and not all enemies at once, so a move which may take you further away from one enemy may send you right in to  another enemy. I was wondering if anyone can think of something that is more efficient than this, or some fix to be able to to pick the most efficient move?",reinforcementlearning,skidjoe1,False,/r/reinforcementlearning/comments/gnrm2s/anyone_familiar_with_any_amidar_enemyavoidance/
Learning to explore using Active Neural SLAM,1590032030,,reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/gnpgcw/learning_to_explore_using_active_neural_slam/
Sutton and Barto Exercise 2.5,1590020543,"Hello everyone,

I'm working through Sutton and Barto and was wondering if anyone could take a look at [my results](https://github.com/kylerkrenzke/Reinforcement-Learning-Exercises/blob/master/ch2/exer_ch2.pdf) (and/or code) from exercise 2.5 and verify that my graphs look ok.

My rewards plot looks accurate, but my optimal actions looks a little suspect. From the text, it seemed like I should be seeing a bigger difference between the two methods, but there is also huge variance for the second plot, so I might just be overthinking it.",reinforcementlearning,qyler_,False,/r/reinforcementlearning/comments/gnmhqy/sutton_and_barto_exercise_25/
"""Towards Interpretable Reinforcement Learning Using Attention Augmented Agents"", Mott et al 2019 {DM}",1589999451,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gngxdt/towards_interpretable_reinforcement_learning/
"""'Other-Play' for Zero-Shot Coordination"", Hu et al 2020 {FB} (domain randomization/data augmentation of which Hanabi player one is)",1589997687,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gngcdc/otherplay_for_zeroshot_coordination_hu_et_al_2020/
"""Towards Learning Multi-agent Negotiations via Self-Play"", Tang 2020 {Apple} (car merging simulation)",1589997081,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gng595/towards_learning_multiagent_negotiations_via/
"""Gradient Surgery for Multi-Task Learning"", Yu et al 2020",1589996926,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gng3e3/gradient_surgery_for_multitask_learning_yu_et_al/
"Danijar Hafner on TalkRL: Deep learning &amp; neuroscience, PlaNet, Dreamer, world models, latent dynamics, curious agents, Plan2Explore",1589995566,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/gnfnnt/danijar_hafner_on_talkrl_deep_learning/
Issue training quantile regression agent on pong,1589993107,"Hi all, although I initially set out to make a Rainbow Agent, I found that L2 projection step kinda unwieldy and didn't want to implement it, instead I jumped to quantile regression networks because they were way easier to work with (see [https://arxiv.org/abs/1710.10044](https://arxiv.org/abs/1710.10044)).

Anyway I ended up using that instead of the C51 algorithm in rainbow, it manages to train well and converge on cart-pole (no pixel input) but then when I try to drop in a CNN and train it on Pong, it doesn't learn anything, at all! I have tried a zillion different hyperparameters and inspected every aspect of my CNN and I can't find what the issue is. Something between putting it on cart-pole and running it on pong is messing up the learning procedure, does anyone have any ideas/ similar experiences they could share?

I've created a git with the main training loop, hyper parameters, and the relevant files for both cartpole and pong, any help is much appreciated! I've been staring at this for weeks now:

[https://github.com/honne23/QuantileAgent](https://github.com/honne23/QuantileAgent)

My implementation is based in part on these sources:

[https://github.com/Curt-Park/rainbow-is-all-you-need](https://github.com/Curt-Park/rainbow-is-all-you-need)  
[https://github.com/higgsfield/RL-Adventure](https://github.com/higgsfield/RL-Adventure)  
[https://github.com/qfettes/DeepRL-Tutorials](https://github.com/qfettes/DeepRL-Tutorials)",reinforcementlearning,btwhy_,False,/r/reinforcementlearning/comments/gnev9u/issue_training_quantile_regression_agent_on_pong/
Newbie: What are the internal components of a Deep RL agent?,1589992905,"Hello,

I come from the CV field where we mostly use static, fixed CNNs. Training once and (mostly) forgetting for later. Usually, we also deal with issues such as model size, inference time, clips per second, etc. since CNNs are pretty computation heavy and the data is high dimensional.

1. I'm trying to get my head around how agents work, what do they try to optimize? How are they fed inputs and decide on the outputs? Normally we use something like cross-entropy loss for multi-class classification optimized with SGD over mini-batches. What sorts of loss functions do DRL agents use and how are they fed the input data?
2. In terms of computational cost, I was unable to find a form of literature review or paper comparing the FLOPs or inference time of different DRL or RL algorithms. If anyone knows anything about this please share it with me!

Let's say we have this problem: We have a lightweight CNN that understands basic images. How do we use such a CNN to make our agent decide better actions? For example, feeding a learned embedding? Using the full CNN inside the RL building block? If anyone can link me to any resources or papers that would be very helpful! Thank you.",reinforcementlearning,ronthethrowaway,False,/r/reinforcementlearning/comments/gnesty/newbie_what_are_the_internal_components_of_a_deep/
Some questions about predicting a vector using Actor-Critc,1589989844,"Hi all,

I am currently building an A2C model that will eventually have to predict 4 parameters for another model. In other words, I want my A2C model to find an optimal vector, containing 4 real numbers. 

I have currently built a model that can predict 1 real number. The neural network predicts the mu and sigma for a gaussian distribution and then the policy samples from that distribution to choose the right parameter number. However, I will of course need this for 4 numbers, not 1. 

I though of some ways in which I could implement this:

- I can't just sample all 4 numbers from the same distribution, because all 4 lie in a different range. For example, my parameter 4 ideally lies between 0 and 0.5, but my second parameter should probably be around 50-100. 
- Then can I just make a separate network for each parameter? The parameters are all highly dependent on each other and adjusting one will mean that another one needs to be changed according to that as well. That's why I don't think separate networks will be a good choice, because they won't depend on each other anymore. Although I could be wrong?
- Another option I thought of was that the network had to predict 4 mu's and 4 sigma's, one for each distribution. Then I could sample from a distribution specific to each of the 4 parameters, but again, I don't know if this would work. 

I would be really grateful if you could provide some pointers. I'm a beginner in reinforcement learning and I would love to hear some insights on this. I've also looked around on the internet, but I haven't been able to find code examples of how I could implement this. If you happen to know any, I would love to hear. Thanks in advance!",reinforcementlearning,Nymuee,False,/r/reinforcementlearning/comments/gndtl8/some_questions_about_predicting_a_vector_using/
I changed TF1.X code to TF2.X version. but error ...,1589987497,"I changed TF1.X code to TF2.X version.

But there is several error in TF2.X.

(ex :  'tensorflow' has no attribute 'trainable\_variables' )

Anybody who can explain the previous contents

How can i change the following code...

please let me know..

&amp;#x200B;

1.TF1.X

\# Create the target networkwith tf.variable\_scope('target\_network'):

target\_qv = qnet(obs\_ph, hidden\_sizes, act\_dim)# 학습가능한 variable에 대한 리스트 생성

target\_vars = tf.trainable\_variables()2.TF2.X

\# i would like to make function like the following code

def get\_target\_qv(obs\_ph, hidden\_sizes, act\_dim): target\_qv = qnet(obs\_ph, hidden\_sizes, act\_dim)return target\_qvtarget\_vars = tf.trainable\_variables()",reinforcementlearning,sabumjung,False,/r/reinforcementlearning/comments/gnd3l9/i_changed_tf1x_code_to_tf2x_version_but_error/
Why is tanh default for PPO?,1589973235,"Hi! I'm working on a problem using PPO, and am a bit unsure which activation function to use. It seems ReLU generally performs better, but tanh is nevertheless defined as default in Stable Baselines. Is anyone familiar with the reason for this?",reinforcementlearning,Amumfie,False,/r/reinforcementlearning/comments/gn9hef/why_is_tanh_default_for_ppo/
About Continuous Action Space,1589967616,"As far as I know, when dealing with continous action space, the most common method is training a network that output parameters ***a*****,*****b*****,** and then sample action from a normal distribution **N(a, b).**

This method seems to introduce some priori knowledge(i.e. we assume that the optimal action should be normally distrubuted), so I think in some cases this method won't work well. But I am not quite sure about this because I haven't done any experiments to prove it.

Do we have any alternative ways to deal with continous action space?",reinforcementlearning,AlbertCity,False,/r/reinforcementlearning/comments/gn8e5f/about_continuous_action_space/
ICLR 2020: Evolutionary Population Curriculum for Scaling Multi-Agent Reinforcement Learning,1589927412,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/gmyv62/iclr_2020_evolutionary_population_curriculum_for/
"""Road defect detection using deep active learning"", Element AI (description of BaaL active learning library using MC-dropout+BALD for efficient semantic segmentation data annotating)",1589917047,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gmvlb5/road_defect_detection_using_deep_active_learning/
How to handle invalid actions while training an agent,1589912156,"I'm trying to train my own RL agent for the first time using the REINFORCE algorithm and a Neural Network, which plays the game battleship. 

The state and action spaces both are grids of 10x10 (so ideally, no episode should be longer than 100 moves). However, while training the agent and generating episodes, the agent keeps picking invalid actions (actions it has already taken before) and this is affecting the learning process. Each episode takes up to 2000-2500 steps to complete.

What steps can I take to avoid this problem? I thought of giving a huge negative reward for each invalid action that the agent tries to take but that didn't help in training either.  

Reward structure I'm currently using - 

* \-10 for invalid actions
* \-0.5 for actions which constitute a miss - penalizing long episodes
* \+1 for actions which lead to a hit
* \+5 for the final hit - rewarding a completed episode

Should I consider trying some other algorithm such as DQN?

Any help would be appreciated!",reinforcementlearning,user123user987,False,/r/reinforcementlearning/comments/gmtzey/how_to_handle_invalid_actions_while_training_an/
10 Must-Read AI Books 2020 - Part 2,1589905098,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/gmroxs/10_mustread_ai_books_2020_part_2/
"Below you will find a link to a Zoom recording where our team discusses Reinforcement Learning. Topics covered: Markov Decision Process, Double Q-Learning, the math behind Q-Learning, and the Bellman Equation. We also walk through the algorithms and provide coded examples.",1589896916,"Topic: Reinforcement Learning Math Discussion

Meeting Recording:

[https://us02web.zoom.us/rec/share/xcdlLPLzrmxLfNbNuFHud4UtFaTVeaa823IYr6dYzUw-uzo3Q0gjSQwweD9oLgzf](https://us02web.zoom.us/rec/share/xcdlLPLzrmxLfNbNuFHud4UtFaTVeaa823IYr6dYzUw-uzo3Q0gjSQwweD9oLgzf)",reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/gmp4nm/below_you_will_find_a_link_to_a_zoom_recording/
Looking for friend to work towards RL goals together,1589866292,"I'm looking for someone who I can work with to bounce ideas off of each other and reinforce (pun intended) each others goals around RL. As a little background, I recently graduated from undergrad, and while I am working a job now, I want to do a PhD soon. I previously worked on RL research in a lab during undergrad, but was never able to publish. I'm aiming to publish now, but working alone, I feel like it is incredibly difficult to hold myself accountable.

I'm looking for another goal-driven individual working in RL that could help critique my ideas and check in with me on a regular basis. Of course this would be a 2-way street and I would be happy to offer any help I could, whether that be critiquing ideas, debugging code, or anything else. If you are interested comment or pm me!

If this post gets enough interest, I would also be willing to help match other people together that comment/pm.",reinforcementlearning,ejmejm1,False,/r/reinforcementlearning/comments/gmihnj/looking_for_friend_to_work_towards_rl_goals/
imitation learning - roll-in policies,1589864658,,reinforcementlearning,dhecloud,False,/r/reinforcementlearning/comments/gmi56v/imitation_learning_rollin_policies/
Question about state representation for a particular game ? Help,1589842070,"So I am trying to build a DQN agent for the [bidding game](https://www.hackerrank.com/challenges/the-bidding-game). 

**About the Game :**

The game is basically a simple bidding game. There is an expensive item in the middle of a 1D board and each players are given 100$. Each turn both players make a bid, if a player wins the bid the item moves 1 step towards the player and the bid is subtracted from your current money pool. The goal is to get the item all the way to your end of the board.

**Question :**

So the board representation is pretty straight forward, I'm just confused how to represent the money each players have while imputing it to the NeuralNet

1. Represent it as just a plain integer
2. One hot encode, where the index of the 1 represents the current money 
3. Multi one hot encode where all the indices below the current money value are marked as 1.

Also since the output is also a number, (i.e. amount to bid) what should I do there ? What do I do if the neural net outputs a higher bid amount than what you have ?",reinforcementlearning,DollarAkshay,False,/r/reinforcementlearning/comments/gmc5wu/question_about_state_representation_for_a/
Constant part of observation,1589827838,"Is there a term for the part of the observation/state not dependant on the agent policy? For example, torso size of a robot might be a feature but this cannot be changed with any agent policy. 

I've got a hunch that there must be some control theory related terminology for this. Any help would be much appreciated!",reinforcementlearning,yoki_n,False,/r/reinforcementlearning/comments/gm7jfu/constant_part_of_observation/
[2001.11279] Improving the Robustness of Graphs through Reinforcement Learning and Graph Neural Networks,1589823163,,reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/gm5zlv/200111279_improving_the_robustness_of_graphs/
Are there any new research works addressing the issue of generalization in Reinforcement Learning?,1589812299,"At the time of posting this, I just know of L2 regulatization and Dropout being used in Atari 2600.",reinforcementlearning,zarrokx,False,/r/reinforcementlearning/comments/gm2icv/are_there_any_new_research_works_addressing_the/
Is anyone here working on Inverse Reinforcement Learning (IRL)? What are the best IRL method for very large state/action spaces?,1589811382,"I'm studying IRL and I've seen that it's typically very hard to apply to very large state and/or action spaces.


I've found a couple of papers that target this problem, such as [this](https://arxiv.org/abs/1707.09394) or [this](https://www.aaai.org/Conferences/AAAI/2017/PreliminaryPapers/11-Shimosaka-14918.pdf).


Do better approaches exist? What is the the state of the art?",reinforcementlearning,Fab527,False,/r/reinforcementlearning/comments/gm28xr/is_anyone_here_working_on_inverse_reinforcement/
Confidence interval - standard deviation or bootstrapping?,1589797288,"I was going through [this paper](https://arxiv.org/pdf/1802.09477.pdf) proposing the TD3 algorithm. The authors simply plot the area covered by 1 standard deviation along with the mean. I've also heard about the bootstrapping-based confidence intervals. So, what is the norm here?",reinforcementlearning,AceTheRex,False,/r/reinforcementlearning/comments/glywlp/confidence_interval_standard_deviation_or/
Is Swarm Intelligence(SI) Hopeful to Achieve Aritificial General Intelligence(AGI)?,1589793150,"AGI has been widely discussed these years, and I believe that Swarm Intelligence is the most promising way to achieve AGI. My main point is : (1) It is a common sense that some species(e.g. Ants, Bees,...) shows great intelligence as a whole group. (2) Individuals in a swarm are simple enough such that it may be possible to model them using NN(Nueral networks) or something else.

Do you has any ideas to share about this topic? I will appreciate it if you can reply this post！",reinforcementlearning,AlbertCity,False,/r/reinforcementlearning/comments/gly3qy/is_swarm_intelligencesi_hopeful_to_achieve/
Creating powerful robot programs with trajectory generation,1589788775,"From a technical perspective a robot program is a fixed trajectory. A list of points is traversed in sequential order. Sometimes, this idea is improved with a **STRIPS notation** in which pre- and post-conditions are given. With a pre-condition, it's possible to monitor the execution and stop the program flow if something unusual takes place.

The problem is, that even with a STRIPS notation the resulting robot program is very primitive. The robot will behave repetitive and it's not possible to solve more **complex tasks**. The trick is to accept this constraints and modify the task so that it can be solved within the STRIPS notation.

Let me give an example. The initial task for the robot is located in the kitchen. The robot should clean all the dishes. Programming a script for such a task is not possible. The mentioned STRIPS notation is not powerful enough. So the project fails. In the next step the task is modified. It's up to the **human operator** to invent a task which can be solved by a script. A possible simpler task is the assumption, that there is only one size for the dish, it's located always at the same position and the robot arm should pick the object, put it into the water and move on the dish with a circular movement. Then the dish is put into the second position which is also fixed.

Creating a STRIPS program for solving such a task is possible. The task is nearly static, and the robot can repeat the same action over and over. The capabilities of the robot programming are matching to the requirements of the task. This results into a successful **automation project**. Successful means, that the task is executed by the robot and a lot of human work gets saved.

It's important to be aware of the limits of robot programming. If a simple STRIPS notation is used, the robotarm is able to follow a predefined trajectory. That means, the points in the **3d space** are given in advance and the robotarm moves along that trajectory. The only improvement is, that the precondition can be checked, this allows a minimal robustness against disturbance. Other features are not available, that means the programming technique has limits. So the question is: which kind of problems can be solved with a robot program? Right, the amount of tasks is limited. The interesting point is, that most tasks can be modified. They can be simplified so that a robot program can execute it **autonomously**.

## Robot program

The term “robot program” is used by the **CAD** community in a certain way. In contrast to normal computer programming it's not about programming languages, compilers and operating system but a robot program is simulating a **numerical control** machine. The most basic form of a robot program is a list of points which has to reached in a sequential order.

What can be added for reasons of more robustness is a **monitoring** feature. After executing an sequence, the desired state with the real state is compared. If the error is to high the program quits with an error message. That is the basic idea of robot program. The interesting problem is which kind of “Robot program” is needed to solve a certain task. In most cases, it's defined by the human operator who provides the **absolute coordinates** and tests if the script make sense. The surprising fact is, that with this technique it's possible to create longer scripts. The only condition is, that the task is static and is repeating very often. A typical example is, that the robot takes an object, is doing something with it and places it to the target position.

Instead of arguing for a certain robot hardware or a robot programming language it's important to know, that in the basic form a **robot program** is equal to a list of absolute coordinates. At timecode t0, the robot is at position p0, at timecode t1 he is at p1 and so on. The sequence of points are producing sense, not because the programming was so advanced, but because the robot program solves the task. If the task is, that the robot is welding something, then the robot program is doing so.

Modern robotics engineers are trying to achieve two things: first, they are interesting in programming a robot which contains of a **complex structure**. For example a dual arm robot which has fingers. And secondly the idea is to solve tasks which are a bit different each time. Realizing both goals is complicated. If the robot has two arms plus fingers, the amount of joints is high. The resulting list of points are larger and it's difficult to create and maintain a new script. If the task needs a flexible robot program, the generated points have to be adapted in realtime which makes the overall system more complex.

Right now, there is no standard answer to the problem. What we can say is, that it's possible to control a simple robot for a static task. While controlling an advanced robot for a complex task is hard. A best practice method is to solve only problems which have a high **success probability**. That means, the robot is a simple model and the task is static.

## Domain models for AI planners

STRIPS based AI planning is a powerful technique to control robots. Basically spoken the idea is that the robot is able to **predict future states** of the domain and then a solver is searching in the state space for the needed actions to reach a goal. The principle is the same like a chess engine is searching for the next move. The difference is, that for the chess game the prediction engine is easy to realize but for robotics problems it's much harder. In case of chess, the outcome of a move can be simulated with the normal chess rules. Each chess figure can do a certain move and the rule who stands in check is fixed. The chess rules don't change over time but they are formalized in the FIDE rulebook.

In case of robotics, it's unclear which game exactly is played. In a pick&amp;place task it's obvious, that the robot should move the objects but how exactly is unclear. An additional problem is, that the behavior of the objects follows **physical rules**. The idea of AI planning is to create a symbolic model of the domain in the STRIPS notation. This model allows to plan the next high level action. The only problem is, that the action primitives, the outcome and the preconditions are unknown. But according to the STRIPS syntax these information can't be empty, otherwise the planner won't work.

There are two options for dealing with the problem. The first one is to come to the conclusion, that STRIPS based AI planning doesn't work for robotics domains and can be ignored. The other option is to see as a here to stay technique and figure out the details.

The dominant question in AI planning is how to generate the STRIPS domain file which is equal to a **symbolic action model**. What is missing is some kind of framework in which the domain file fits into. Sure, the syntax of the Strips language is known, but this is not enough to formalize a concrete domain, for example a pick&amp;place task. The needed actions are given by the domain not by the robot. A pick&amp;place task has a different mechanics than a parking robot. And here comes Learning from demonstration (LfD) into the game. According to the LfD paradigm, a human operator is in charge to provide an example plan. The baseline for creating the activity grammar is the human demonstration. This understanding works surprisingly well, because a human operator is able to solve all of the tasks. He can manual control a pick&amp;place robot, steer a parking car or bring all the Lemmings into the goal.

The task isn't located within AI, but it has to do with **human-machine interaction**. The human demonstrates a task which produces a gamelog, and the robot has to parse the information. The needed translator in between is a language interpreter. He takes the human demonstration and generates the STRIPS domain file. The perhaps most efficient way in doing so would be an automatic converter which is working without human intervention. That means, the human operator demonstrates the task 3 times and then the Strips file is generated on the fly.

Such an automatic pipeline is too complicated for the initial prototype. The better idea is to translate manual between the demonstration and the Strips file. The overall task can be identified as a software engineering problem. The problem is to **annotate** a given gamelog with semantic information. In the easiest case this is done with a python script which goes through the raw data and adds annotations. For example the first part of the demonstration is labeled with “pickup object”, while the second part gets the label “release object”. This annotation is possible because the Python scripts has access to the raw data which means, the position of the gripper, the object and the table is known. This allows to formulate a rule to identify a certain event. The result of the rule is written direct into the datastream.

The most interesting aspect is that on top of the event recognizer a more complex **task recognizer** can be realized. If the robot arm has successful transported an object from A to B, then the highlevel task “transport object” was made. This can also be annotated in the raw data. During building the plan recognizer, the domain model will become visible. It is generated slowly with the improvements of the activity parser. If the software engineering process for programming the plan recognition script was successful, a machine readable domain model is available. A parser, which can recognize events, can be used for planning events in the reverse order.

Or let me explain the situation from the other direction. A fully functional Strips file can be used as a plan recognizer as well. That means, the strips file is not utilized for producing the next action of the robot, but to annotate a **human demonstration**. The human operator is doing the pick&amp;place task and the strips file recognizes which action the human is doing.

Why is this important? Because this is equal to **model checking**. A given Strips domain file is monitored if it's able to parse a gamelog. This allows to identify, if the Strips file is working or not. If the human operator is doing an action which is not mentioned in the Strips syntax, then something is wrong. The model and the reality are out of sync.",reinforcementlearning,ManuelRodriguez331,False,/r/reinforcementlearning/comments/glx92q/creating_powerful_robot_programs_with_trajectory/
Are recurrent states needed in the replay buffer for training QRNNs?,1589764371,"I'm using TF-Agents to train a QRNN Agent (A `DqnAgent` with a `QRnnNetwork`) and I'm setting up the data collection for the replay buffer. However, I've noticed that I don't seem to need the policy states to be recorded according to the specifications (`DqnAgent.data_collect_spec`). There is a `policy_info` field but it is blank - I would expect some shape requirements. 

I've noticed that in source code when the loss is computed it uses a [""dummy state""](https://github.com/tensorflow/agents/blob/91ad5bed12232027be47bd38b6691324f765f919/tf_agents/agents/dqn/dqn_agent.py#L546). This goes against my intuitions so I feel like I must be misunderstanding something.

I don't know the nitty-gritty details of training RNNs, but my guess is that we need the observation, the action, the resultant reward, *and* the state of the recurrent cells at the time in order to properly calculate the loss and gradients? 

Thanks for any corrections to my understanding here.",reinforcementlearning,drcopus,False,/r/reinforcementlearning/comments/glrn9w/are_recurrent_states_needed_in_the_replay_buffer/
Softmax policy gradient clarification,1589760826,"I am struggling to understand how to implement the REINFORCE policy gradient algorithm. According to [this post](https://ai.stackexchange.com/questions/16951/eligibility-vector-for-softmax-policy-with-policy-gradients), given a state s and action a the eligibility vector, `∇𝜃log(𝜋𝜃(𝑎0|𝑠))`, takes the form of `(s,a)`. 

The update equation is `𝜃 += ∇𝜃log(𝜋𝜃(𝑎0|𝑠)) * r`, where `r` is the return at the current time step.  I am confused about the nature of `𝜃`. For example, if `s = (1,1)` , and `∇𝜃log(𝜋𝜃(𝑎0|𝑠)), = (0,0,-3)` , then does that mean that `𝜃` has the shape (3,) , where `𝜃 = (s1, s2, a)`?",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/glqowp/softmax_policy_gradient_clarification/
Temporal Different Error in DQN?,1589759712,What is the error term for DQN? Is it Temporal Different Error?,reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/glqdx5/temporal_different_error_in_dqn/
Weed and reincarnation,1589758921,"Do you guys think that week helps in the meditation when trying to remember past lives?
Please share your experiences.",reinforcementlearning,hasmin99,False,/r/reinforcementlearning/comments/glq5x8/weed_and_reincarnation/
Number of hidden layers and layer size for BipedalWalker-v2,1589746674,"Hi,  
I am working on Natural Evolutionary Strategy algorithm to learn different Box2D environments. When I ran the algorithm for BipedalWalker-v2 env for 1e7 steps 5 times, the learning curves look like random noise centered at around -100. They don't seem to learn. I think the problem is with the neural network. Currently, I have 2 hidden layers with 64  tanh activation units each. 

It would be really helpful if you could share some inputs on a good ballpark figure for the number of hidden layers, the size of each hidden layer and other parameters to help learn the env better.

Thanks.",reinforcementlearning,red_dhinesh_it,False,/r/reinforcementlearning/comments/glmiit/number_of_hidden_layers_and_layer_size_for/
"Interested in RL, what do I do next?",1589742903,"There's a chance I'll graduate next semester. I'm an international Master's student at a Top-5 CS school in the US. I've taken a couple of courses related to RL, done a couple of projects in Python related to Bandits, RL (Policy Iteration, Value Iteration, TD-Learning) and IRL.

I'm interested in RL (fascinated by Imitation Learning) but I don't really have any solid research experience. It's mostly coursework and a couple of projects.

What's the best way forward? I'll mostly be looking out for jobs next semester. Do I target RL roles or do I go for MLE or SWE roles in companies that are also into RL and then make my way through.

Thanks.",reinforcementlearning,K_33,False,/r/reinforcementlearning/comments/glldpc/interested_in_rl_what_do_i_do_next/
Question : Sutton and Barto RL Book Exercise 6.12,1589738483,"Suppose action selection is greedy. Is Q-learning then exactly the same

algorithm as Sarsa? Will they make exactly the same action selections and weight

updates?

In my view they should behave same by taking same greedy actions. But when i plotted SARSA and Q-Learning in the Cliffwalking  problem, the graphs are slightly different. What point am i missing ? The convergence is same I guess. But why are policies also different ??",reinforcementlearning,eaglessky,False,/r/reinforcementlearning/comments/glk0y5/question_sutton_and_barto_rl_book_exercise_612/
Choosing learning rates,1589718948,"When doing deep RL, there are 3 learning rates / parameter change rates:

* learning rate for actor
* learning rate for critic
* parameter update rate for target critic network (e.g. Polyak averaging)

How do i balance those out? Should i do hyperparameter search or are there any sophisticated algorithms that dynamically adapt the rates (e.g. if critic error is too big, slow down the learning of the actor)?",reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/gleojb/choosing_learning_rates/
Building my own simpler Atari-alternative - worth it?,1589715243,"&amp;#x200B;

I have started a project of building a simple game platform.

The screen consists of a 2D numpy array with 0’s and 1’s. 

0 = pixel off. 1 = pixel on.

In this array I can create simple games and render them in tkinter, as seen in the following picture with corresponding array - the user controls the basket at the bottom and scores points for catching falling objects:

&amp;#x200B;

[tkinter rendering](https://preview.redd.it/uee7x6hv9bz41.png?width=304&amp;format=png&amp;auto=webp&amp;s=a26b058f6842e9028cf05ab1b946172fd8dddb70)

&amp;#x200B;

[corresp. numpy array](https://preview.redd.it/6mh6lfxm9bz41.png?width=280&amp;format=png&amp;auto=webp&amp;s=4655a163f1e228c10901ecd0749144d1ff2ee076)

I’m partly doing this because it’s fun and a good experience, but also because I’ve heard that running the atari emulator takes up a lot of processing power that could be used for training the RL-agent. So instead I thought this could be used to train a DQN or similar in this simpler platform by feeding the numpy array's as input.

So my question for you guys,

Do you think it will be faster to train Deep RL agents on a simple game like this compared to the benchmark Atari-games, and so much that it is worth it?

I’d also like to hear a bit what you think about this initiative, or if something similar already exists. Also if someone would like to help out please let me know! I’d like to practice programming with others.",reinforcementlearning,etthusmedkossor,False,/r/reinforcementlearning/comments/gldx31/building_my_own_simpler_atarialternative_worth_it/
Invalid Action punishment causes the amount of invalid actions to increase?,1589714533,"I give -0.1 for doing an invalid action. I have 4 actions in which only 1 or 2 actions are invalid. Instead of using the invalid actions less, it actually started to increase more. I tried without punishment and it gave similar results like this. i do indicate very clear in my state, in binary, when certain actions can be chosen. Eg 1 if action 123 can be chosen and 0 if action 1234 can be chosen. Does anybody know why this is happening?

[Bad Plotting, because written in a Third Party Excel Writing Engine, hence you can not plot the data like you should normally.](https://preview.redd.it/09ngga387bz41.png?width=1190&amp;format=png&amp;auto=webp&amp;s=fb74f67cbd3333cb697563e4fdae440af5e73ebf)",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/gldsa2/invalid_action_punishment_causes_the_amount_of/
how to get hired from Reinforcement learning companies,1589699892,"what kind of projects you have done that got you in RL companies, such as **Deepmind**, **OpenAI**,  **Microsoft,** **Amazon** ,**Facebook?**",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/glb736/how_to_get_hired_from_reinforcement_learning/
Is this the correct gradient for log of softmax?,1589687440,"I am currently implementing the very basic version of the Monte Carlo policy gradient algorithm. I was wondering if this is the correct gradient for the log of softmax. I am not sure if my interpretation of the equation is correct. 

&amp;#x200B;

https://preview.redd.it/2tr0knsbz8z41.png?width=1122&amp;format=png&amp;auto=webp&amp;s=533734526613434991919b8a1ffbc314233b00b2",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gl8owi/is_this_the_correct_gradient_for_log_of_softmax/
Possible to optimize a policy where sub-optimal actions can sometimes yield a reward greater than optimal actions?,1589682762,"I've been diving into RL in the past month and am starting to experiment with ways in which algorithms could be applied in real-world environments. 

I'm trying to figure out the cases in which an agent would be able to optimize a policy in a simple environment where the agent is blind to some elements of the state.

Say you're using reinforcement learning to optimize a 1GB file transfer. The lower the transfer time, the greater the reward. Generally the faster the sender sends packets, the faster the transfer completes. However, if the sender sends packets too fast, packets will be dropped and the transfer will take longer to complete than if the sending rate had been slower.

Assume the agent had access to only the current number of dropped packets as an observation. Would it be able to optimize the file transfer time with only this information? 

It seems like the policy wouldn't be able to converge if network variables are changing in a way that the agent is blind to. For example, if there is low latency the agent could receive a large amount of reward when taking the optimal actions. However, if there is high latency, the agent would receive a small amount of reward even if it took all the optimal actions.

From my limited amount of reading, I believe this would be considered a ""partially observable markov decision process"". Not sure if there's a way to approach this, since most examples of RL I've been shown don't have the issue with there being the possibility of optimal actions potentially resulting in a reward less than sub-optimal actions due to changes in state that the agent is blind to. 

From the algorithms that I've seen, my intuition is that an optimal policy could never ""converge"" in this scenario.

In which case, a follow-up question would be: Would it be possible to optimize the policy if the agent is still only aware of the single ""number of packets dropped"" value, but the reward is now scaled up in states with reduced performance (high latency, low bandwidth)? Then, the reward if all optimal actions are taken would be similar to that of the state with ""best"" performance (low latency, high bandwidth). 

My intuition is that this would work, but I wonder if it's the best way to approach the problem.

&amp;#x200B;

Thanks!",reinforcementlearning,tbutlah,False,/r/reinforcementlearning/comments/gl7lhk/possible_to_optimize_a_policy_where_suboptimal/
Updated Notebooks for ML Agents 1.0,1589662871,"Hello , Ugurkan here. 

Since 1.0 release version of ML Agents they have removed example Jupyter Notebooks due fast changing syntax of ML Agents. Basically all releases make other version code obsolete (which sucks but what can you do)

I saw devs were talking to each other of possible Colab version in down the line and wanted to beat them :D just kidding I have to work on Colab since my local GPU is terrible (840M 2GB)  


So yeah I updated Rainbow DQN to work with Colab &amp; Local PCs. All binaries are also uploaded to github , which isn't ideal and makes repo like 128MB but this repo will be obselete in some time so doesn't matter much.

[https://github.com/ugurkanates/MLAgents-Google-Collab](https://github.com/ugurkanates/MLAgents-Google-Collab)",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/gl2akt/updated_notebooks_for_ml_agents_10/
"Why would anyone use PPO over SAC, TD3, DDPG, and Other off-policy Algorithms?",1589660678,"Hello, could someone please explain to me why \^?

I've ran 10 model averages of some Mujoco environments, HalfCheetah, Hopper, Walker2d, and Ant, of PPO, SAC, TD3, and DDPG but I can't seem to understand why anyone would use on-policy policy gradient methods over off-policy ones besides the fact that on-policy methods, particularly TRPO and PPO, are less hyperparameter sensitive and have more stable convergence properties.

Could someone try to persuade me why on-policy methods might ever be chosen over off-policy methods? Because the off-policy methods I used seem to crush PPO in average rewards.",reinforcementlearning,hanuelcp,False,/r/reinforcementlearning/comments/gl1ov2/why_would_anyone_use_ppo_over_sac_td3_ddpg_and/
Is Deep Reinforcement Learning Ready for Practical Applications in Healthcare? A Sensitivity Analysis of Duel-DDQN for Sepsis Treatment,1589646270,,reinforcementlearning,davidstroud1123,False,/r/reinforcementlearning/comments/gkxfa1/is_deep_reinforcement_learning_ready_for/
Q-Learning with Linear Function Approximation - tictactoe,1589645932,"I'm doing the reinforcement learning course on edX by Microsoft. I am trying to implement the algorithms for ""tic-tac-toe"" game. So far I solved the game with a tabular solution using Q-learning. Can you check if my way is correct, because even before I corrected some parts it was converging to good win rates. It plays against a random agent. I stored Q values in a dictionary. I intend to choose the ""available"" action with the highest Q value.

    def act(self, state, epsilon):
            stateStr = self.stateToString(state)
            self.epsilon = epsilon
            
            av_actions = np.where(state == 0)[0]
            self.Q.setdefault(stateStr,np.array([0]*self.num_actions, dtype = np.float64))    
         
            if(np.random.random() &lt; self.epsilon):
                action = av_actions[np.random.randint(0, len(av_actions))] 
            else:
                maxv = max([self.Q[stateStr][act] for act in av_actions])
                maxes = np.where(self.Q[stateStr] == maxv)[0]
                maxes = np.intersect1d(maxes, av_actions)
                action = np.random.choice(maxes)
                
            return action

Also while doing the Q learning update, I intend to choose the maximum value among the ""available actions"".

    def learn(self, state1, action1, reward, state2, done):  
            state1Str = self.stateToString(state1)
            state2Str = self.stateToString(state2)
            
            av_actions = np.where(state2==0)[0]
            self.Q.setdefault(state2Str,np.array([0]*self.num_actions, dtype = np.float64))
            if len(av_actions)!=0:
                maxv = max([self.Q[state2Str][act] for act in av_actions])
                maxes = np.where(self.Q[state2Str] == maxv)[0]
                maxes = np.intersect1d(maxes, av_actions)
                av_max = self.Q[state2Str][np.random.choice(maxes)]
            else:
                av_max = max(self.Q[state2Str])
            td_error = reward + self.gamma*av_max - self.Q[state1Str][action1]
            
            self.Q[state1Str][action1] += self.alpha*td_error    

Is it what should I do? In the examples of the course, there was no need to determine available actions.

Also I tried to implement the same logic with a linear function approximator. I used 81 parameters(thetas, observation space x action space). Act method is same and did the q learning update:

    def learn(self, state1, action1, reward, state2, done):
    
            av_actions = np.where(state2==0)[0]
            q = [np.sum(self.thetas.transpose()*self.featureExtractor(state2,a)) for a in self.actions]
            #Q-learning update
            if len(av_actions)!=0:
                maxv = max([q[act] for act in av_actions])
                maxes = np.where(q == maxv)[0]
                maxes = np.intersect1d(maxes, av_actions)
                av_max = q[np.random.choice(maxes)]
            else:
                av_max = max(q)
                
            maxqnew = av_max # replace 0 with the correct calculation
            oldv = np.sum(self.thetas.transpose()*self.featureExtractor(state1,action1)) 
            
            td_target = reward + self.gamma * maxqnew
            td_delta = td_target - oldv
            self.thetas += self.alpha * td_delta*self.featureExtractor(state1,action1)

Linear function approximator code was working fine with the examples in the course. However, in tic tac toe theta values overflow, they decrase or increase really fast and it gives an error. How can I solve this problem? Also I'm not sure even if I solve the overflowing problem it will converge. Epsilon decays exponentially, gamma=1, alpha=0.5. Thanks in advance.",reinforcementlearning,bugattieb,False,/r/reinforcementlearning/comments/gkxbq4/qlearning_with_linear_function_approximation/
how to choose action in dynamic discrete action space,1589621616,"Hello everyone,  I know that in reinforcement learning algorithm like DQN, the agent can select one action from N discrete actions. Now the problem is that the action space is dynamic, that N is variable in each slot. So how to select action in dynamic action space?",reinforcementlearning,Toxicist,False,/r/reinforcementlearning/comments/gkrxiz/how_to_choose_action_in_dynamic_discrete_action/
Replicate result from spinning up implementations,1589611167,"Hi there, I'm new to this field and I am trying to implement DDPG. I found that it was included in the benchmarks here: [https://spinningup.openai.com/en/latest/spinningup/bench.html](https://spinningup.openai.com/en/latest/spinningup/bench.html). It didn't work on an environment I wanted to try it in and so I simply just ran with halved epochs. I ended up with this:

&amp;#x200B;

https://preview.redd.it/h15erhycn2z41.png?width=824&amp;format=png&amp;auto=webp&amp;s=9ee7bd63c5a03a0e3441d8af9104f1ea27784982

Here's OpenAI's benchmark:

https://preview.redd.it/8o3am0cfn2z41.png?width=1380&amp;format=png&amp;auto=webp&amp;s=2e3ddcc994087ffb8c4d886904652f011cde3e08

I don't know how I should feel about this because I noticed my curve is way noisier and also it was negative for the first like 75k steps, the ""performance"" seemed to be at different scale too. Is this a common scene and how should I understand what is going on here? Feeling a bit frustrated here because all hyperparamters are the same except for epochs. 

Extra information on how I log the reward: in this file [https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py](https://github.com/openai/spinningup/blob/master/spinup/algos/pytorch/ddpg/ddpg.py) at line 277 I simply just logged the ep\_ret.",reinforcementlearning,the_real_adi,False,/r/reinforcementlearning/comments/gkq444/replicate_result_from_spinning_up_implementations/
"Hey all, started a study group for Berkeley's CS287 Advanced Robotics Course",1589610500,"X-post with this https://www.reddit.com/r/learnmachinelearning/comments/gjcetj/berkeleys_cs287_study_partner/

Created a new discord study group in order to organize resources and coordinate efforts among those choosing to self study

Course Material: https://people.eecs.berkeley.edu/~pabbeel/cs287-fa19/

Discord Study Group: https://discord.gg/Bk8PKTZ",reinforcementlearning,scikud,False,/r/reinforcementlearning/comments/gkpzm4/hey_all_started_a_study_group_for_berkeleys_cs287/
Simulation software for robotics?,1589601071,"What software would you use if you wanted to try to train a Rc car in a 3D environment and try to apply the model to the real physical version of itself?  

* [PyBullet](https://pybullet.org/wordpress/)
* [Unity3d](https://unity3d.com/machine-learning)
* [Gazebo](http://gazebosim.org/)
* [UnrealEngine (Carla)](http://carla.org/)
* [Mujoco](http://www.mujoco.org/)",reinforcementlearning,thinking_computer,False,/r/reinforcementlearning/comments/gko1l8/simulation_software_for_robotics/
Switching/invalid actions in RL,1589577164,"When you have a controller (like the Nintendo) as the possible actions for an action for a 2D game like Mario. The you’d end up with a lot of invalid actions for that environment. For example the Y button which is used to active te ability, if there is one active. 

Besides a lot of invalid actions I think it would also experience a difficult time training when these actions are useful or not if the states are rarely seen. I was thinking of switching actions mid-run like the action action changes. So say the Y button would mean jump, but when he has a power-up it switches to use ability. This is a bad example because you would still need both actions in this case, though I am trying to sketch the idea.

What would be more efficient, a lot of invalid actions or switching actions? And how does an agent handles all these invalid actions in an environment? I would happily read your suggested resources about this topic or your comment.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/gkhn5u/switchinginvalid_actions_in_rl/
Handling rewards on turn-based multi-agent games,1589559167,"What's the correct way of handling rewards in turn-based games if the reward is revealed after opponents move? Poker, for example, agent bets and all other remaining agents fold, how do I adjust the reward for bet action?

I am using openai gym, I have seen [multiagent-particle-envs](https://github.com/openai/multiagent-particle-envs) but passing multiple actions into step function doesn't work for turn-based games. 

Currently, I am returning all payouts for all agents at the end of an episode (and returning array of zeros at other steps). I am planning to wrap the environment in some sort of buffer which plays until the end of an episode and adjust the reward done flags for each agent's last action. Is this the correct way of handling this or is there a better way?",reinforcementlearning,yakame,False,/r/reinforcementlearning/comments/gkc1nd/handling_rewards_on_turnbased_multiagent_games/
How do you decide the discount factor ?,1589558429,What are the things to take into consideration when deciding the discount factor in an RL problem?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/gkbt45/how_do_you_decide_the_discount_factor/
Are there any RL jobs or internships available,1589557894,"Are there any RL jobs(internships)? I am from Georgia(small country),so do i have a any chance to get a job at any country?(there are no RL companies in my country)",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gkbmzp/are_there_any_rl_jobs_or_internships_available/
A Brief History Of Reinforcement Learning In Game Play.,1589557429,,reinforcementlearning,shehio,False,/r/reinforcementlearning/comments/gkbhrk/a_brief_history_of_reinforcement_learning_in_game/
[News] Distill article on Bayesian Optimization,1589556060,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gkb26d/news_distill_article_on_bayesian_optimization/
MADDPG Algorithm,1589522279,"Hello everyone,

I have a question about the MADDPG algorithm and I hope that someone has answers :)

In the MPE (multiagent-particle-environment) environment that the Lowe et al paper used. It looks like it has continuous observation spaces and discrete action spaces. However, if actions are discrete, then how is the algorithm deterministic since we would sample from some kind of distribution over the action space instead of getting an actual action value directly from the network. 

I'm rather puzzled about how the algorithm generates actions, does it do so deterministically or does it output parameters to distributions which we then sample from. If someone could shed some light I'd be very appreciative.",reinforcementlearning,rlylikesomelettes,False,/r/reinforcementlearning/comments/gk3ikx/maddpg_algorithm/
Questions about enhancing Double DQN and A3C and introducing self-play to adapt to more complex fighting games,1589489164,"Hello there,

Wonder if you could answer a few questions for an ML and RL newbie on various enhancements to RL algorithms to help me better understand if I'm on the right track. For my thesis, I wrote an environment for the arcade version of Ultimate Mortal Kombat 3 using the MAMEToolkit ([https://github.com/M-J-Murray/MAMEToolkit](https://github.com/M-J-Murray/MAMEToolkit)) which evaluated a Double DQN and an A3C agent that used vision and some minimal hand-selected memory values and were trained on single-player mode using a single coin, while experimenting with hyperparameters. I was only able to have an agent reach up until stage 4 out of 9 (increasingly difficult) stages, which wasn't so good. Now that I have some time, I decided to try and see how good I can make them. I'm looking to experiment with A3C+ (A3C + LSTM with concatenated features), ACER, Rainbow enhancements for DQN and Soft Actor-Critic and compare. I have added features to my environment so it can now do both self-play in 2 player VS mode and play against the game's AI periodically through stages that can be used as a benchmark. My questions about enhancements are:

1. Would increasing the agent history length (number of past frames as state) strictly help it,  given sufficient memory? The timestep is every 2 frames, which I found  to be the sweet spot in performance for this game, and the game runs in  60 fps. In the vanilla implementations, I was using the 8 latest frames,  but I was thinking something like encapsulating the last 2 seconds by  having the last 64 frames as a state, and maybe reducing them by taking  the first 16 frames, then every second frame for 16 frames, then every  fourth... to make 16 + 16/2 + 16/4 + 16/8 = 30 frames, due to memory  considerations. Not sure if this would be a good idea or if it would be  preferable to just record the last 30 frames.
2. Would using prioritized replay memory for compatible algorithms, such as DDQN, strictly help it and reduce training time? Is it a case-by-case basis? I am thinking of that or Prioritized Sequence Replay Memory ([https://arxiv.org/pdf/1905.12726.pdf](https://arxiv.org/pdf/1905.12726.pdf)). Hindsight Experience Replay not so much since the rewards aren't really sparse. The vanilla implementation used standard experience replay.
3. I strongly believe that an LSTM  layer would help, in this case, but I'm not sure by how much this would  increase the computation time. Would using an LSTM mean that, for every frame given in the state, each  grayscale image would need to be processed seperately in its own conv  layer before it is passed to its LSTM cell rather than be passed once  through a conv layer as a 3D Tensor (timestep, height, width)?, such as in here ([https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7](https://medium.com/neuronio/an-introduction-to-convlstm-55c9025563a7))? Would it be sufficient to just flatten the conv output into a 1-D vector, maybe concatenate some memory features, and then feed it to an LSTM, such as was done in the A3C+ paper?
4. Will using deeper but more successful deep conv architectures, with or  without pre-trained weights, strictly help it? I've been reading that  ResNet-50 was the current cutting edge, but I'm not sure if it would be  an overkill for 200 x 127 frames, which were zero padded to 224 x 224.  It was anyway infeasible with just CPU, taking 45-50 seconds for each  timestep. I'm not sure if it would make a great difference if I load  pre-trained weights to the ResNet layers and set them all but the 10  last to not be trainable. I also tried AlexNet but it was also  unfeasible, taking around 1.5 seconds per timestep, inc contrast to 0.2 -  0.7 seconds of the base 2-3 layer conv architectures. Not sure if I  should bother with these advanced conv arhitectures.
5. When training in Vs self-play, is my assumption correct that both agents (P1 and P2) share the same neural network(s), where, for each training timestep, there are two forward propagations to select the input for each and two backprops? There is no reason for two instances of the neural network, one for each player? I am wondering because I used separate final layer weights for each character, which are swapped every time the character changes. But if my assumption is correct, I don't think it's a good idea to save and load weights after each backprop.
6. Can I use Google Colab and make use of their free GPU to train it? The  MAMEToolkit library with the MAME Emulator resides on my PC but they can  be installed using pip and it can launch without graphics. I do have an  AMD RX 480 and I am aware of ROCm, but after many attempts to make it  work, I concluded that my CPU (an i7 2600K) was too old to be supported  and the cause of the failed installations. I believe a GPU could  potentially greatly help with the conv processing. I know that google  colab has a 12 hour runtime limit but, would it be feasible, if I saved  my weights periodically and loaded them at start, to resume training  after each timeout?

Sorry for the long post. If I get these clarified or pointed to the right way, it would greatly help.

Thanks in advance! :)",reinforcementlearning,tasoulis12,False,/r/reinforcementlearning/comments/gjuoum/questions_about_enhancing_double_dqn_and_a3c_and/
Projects to exhibit RL skills for research?,1589487545,"I am an undergraduate student who is finishing David Silvers RL course on Youtube. I have implemented all the algorithms in the course, and feel I have a pretty good grasp on the courses concepts. I would like to get involved in RL research at my university, however, all RL courses are only offered to Masters students. I figure that if I work on a project (or a couple) and show them to some professors, they might let me get involved in their labs. Do you guys have any project ideas that would help me accomplish this goal? 

Note, I have plenty of experience with computer vision, so that might tie in nicely with a project.",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gju68o/projects_to_exhibit_rl_skills_for_research/
"Using AI for Education - The latest developments from Coursera, Stanford &amp; Workera",1589472137,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/gjp5r9/using_ai_for_education_the_latest_developments/
"Made a tabular q-learning web app for openAI's ""frozen-lake"" environment. Feel free to play around with it on my coding blog (best viewed on a larger screen). I'm an undergrad trying to learn RL, and would love to hear you feedback on this!",1589452742,,reinforcementlearning,_yeah_i_reddit_,False,/r/reinforcementlearning/comments/gjjwnk/made_a_tabular_qlearning_web_app_for_openais/
DQN reward fluctuates after converging,1589447038,"Hi,

I am new to RL, and struggling to understand how I can solve this problem. I am running upto 1000 episodes as seen in the figure. The reward (cost) seems to converge but diverges again. Any help appreciated.

&amp;#x200B;

Thanks",reinforcementlearning,ssthapit,False,/r/reinforcementlearning/comments/gjipwr/dqn_reward_fluctuates_after_converging/
RL masters at University of Alberta,1589436941,"Hello Reddit!!
Hope you are all safe and sound.
I finished my 4th year, majoring in Communication and Information engineering and specializing in Machine learning. I am here asking about what should I do to have a chance to get accepted in AI/RL master at University of Alberta. I have a CGPA of 3.55/4.0(with slightly higher GPA for the last 2 years), I have many project (school +side projects), one internship (looking for another), and a research (that we are not sure yet, if it will come to light or not).
If there is something I can do to increase my chances and how to prepare, I will appreciate it.

Note: I am from Egypt and here, Engineering is 5 years so I have one more year to go and should apply in the next December or something.

Thank you.",reinforcementlearning,moustafa-7,False,/r/reinforcementlearning/comments/gjgo17/rl_masters_at_university_of_alberta/
"""NLE: The Nethack Learning Environment"", Küttler et al 2020 {FB}",1589405641,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gj8bp4/nle_the_nethack_learning_environment_küttler_et/
"""Plan2Explore: Planning to Explore via Self-Supervised World Models"", Sekar et al 2020 (ensembling for information gain)",1589402575,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gj7bp8/plan2explore_planning_to_explore_via/
GARDEN-LAYOUT-PROBLEM,1589400343,"I have been given this assignment and I can't make sense out of it, can somebody direct me or explain this please?

&gt;You are a gardener working on a large estate. There are n plants available to be planted in k garden beds. Each plant has a beauty rating b1,b2, b3.....bn. In order to maximize the diversity of plants and obtain the most beautiful garden as a whole, the BEST-GARDEN- LAYOUT problem finds an allocation of plants maximizing the beauty of the least beautiful garden bed (sum of the beauty ratings of its plants). Consider the decision version of this problem, that is, BEST-GARDEN-LAYOUT returns TRUE if there is an allocation of plants where the least-beautiful garden bed is at least L, otherwise FALSE.

The inputs are:

• a list of n non-negative integers (beauty ratings for plants),

• the number of garden beds k,

• a lower bound L

a) Prove that BEST-GARDEN-LAYOUT is in NP.

b) Prove that PARTITION S, BEST-GARDEN-LAYOUT.

Note that both of these problems are decision problems.",reinforcementlearning,_whitezetsu,False,/r/reinforcementlearning/comments/gj6l5l/gardenlayoutproblem/
Bike Sharing Rebalancing,1589398713,"Hi, I'm looking for existing work on solving the problem of redistributing rentable bicycles with reinforcement learning techniques. I have found a couple of papers on google scholar, but I might miss relevant material simply because I'm not searching for the right words. In particular I'm looking for existing simulated environments. Thanks a lot in advance!",reinforcementlearning,Taxtro1,False,/r/reinforcementlearning/comments/gj61pc/bike_sharing_rebalancing/
average time to learn reinforcement learning,1589398374,how long you have been learning RL? i need people who knows this field.,reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gj5xnq/average_time_to_learn_reinforcement_learning/
"How to pre-train/set up skills with DIAYN ""diversity is all you need""",1589394164,"I recently read the paper ""Diversity is all you need"" and some of the related posts here. But it's still not obscure for me how to set up the skill sets. What I understand is that they have a fixed number of skills (pre-defined) represented as a categorical distribution p(z). The distribution p(z) is updated by maximizing its discriminability w.r.t. the states, q(z|s) , with the entropy regularization coming from the SAC part  (as Eq 3 in the original paper).  Please give me some insights, thanks!",reinforcementlearning,nancywhr,False,/r/reinforcementlearning/comments/gj4ibt/how_to_pretrainset_up_skills_with_diayn_diversity/
How to achieve row translational invariance?,1589385947,"As  the title asks, how should one  achieve row translational invariance?  What I mean by this is if you  were to say randomize rows of pixels of an  image, but get the same  softmax output for each random permutation?

For   another example, say you have 10 features that are each vectors   representing the same thing, say characteristics of a person, and want  to get softmax output from  a model that will be independent of the   order of these features vectors. What would be the best way to get this   independenc",reinforcementlearning,Ziinxx,False,/r/reinforcementlearning/comments/gj1wjm/how_to_achieve_row_translational_invariance/
Best library to use DQN to solve a real-world problem. (not gym environment),1589373411,Can anyone tell me how should I proceed if I want to use some DQN implementation from a library if I'm looking to solve a real-world problem?,reinforcementlearning,beethoven27,False,/r/reinforcementlearning/comments/giy729/best_library_to_use_dqn_to_solve_a_realworld/
How to read papers &amp; Career Advice from Andrew Ng (PDF),1589369663,"I took some time to summarize advice from the one and only Andrew Ng in this [**free PDF**](https://patreon.com/posts/37060963) (download the attachment in the post) - Feel free to download and share with others! If you would like to see more concepts or summaries in the future, feel free to [**follow**](https://twitter.com/Jousefm2).

Source: Stanford University School of Engineering (CS230)",reinforcementlearning,g-x91,False,/r/reinforcementlearning/comments/gixaod/how_to_read_papers_career_advice_from_andrew_ng/
Experimenting simple exploration methods in DQN,1589363151,"Greetings.  


I went back to the basics lately, trying to get DQN working before tackling more complex pixel-based environments.  
In the process, I tried to experiment with various exploration schemes that one can come across in various implementations, which can be found here:  [https://dosssman.github.io/dqn-exploration-experiments/#3-acrobot-v1](https://dosssman.github.io/dqn-exploration-experiments/#3-acrobot-v1)

Any feedback would be appreciated, especially regarding the Boltzmann-based exploration method, which performs relatively poorly to a purely greedy policy, despite being quite similar to it.

Thanks in advance.",reinforcementlearning,dosssman,False,/r/reinforcementlearning/comments/givxd2/experimenting_simple_exploration_methods_in_dqn/
Agent found a bug in Montezuma's Revenge,1589359535,"[https://youtu.be/Yi4DEHh9Zog](https://youtu.be/Yi4DEHh9Zog)

Experimenting with exploration with intrinsic motivation, found unexpected behavior in the environment.",reinforcementlearning,crush-name,False,/r/reinforcementlearning/comments/giv7hy/agent_found_a_bug_in_montezumas_revenge/
DeepMind Distributional RL ZOO - TensorFlow2,1589355433,,reinforcementlearning,marload,False,/r/reinforcementlearning/comments/giucxs/deepmind_distributional_rl_zoo_tensorflow2/
Reinforcement Learning + MCTS for non-game scenario？,1589335379,"RL( + MCTS)  has achieved great success in game like Go, ... So why RL can solve these problems?

The following is what i thought:

(1) Game always has certain ends (win or loss, can be used for rewards)

(2) Most of games has two players, which produce a zero-sum problem. So for one single player, after enough training, the rate of win or loss is about 50%. This benifits the training process of RL, with win case as positive samples, loss case as negative  samples.

So my question is how to do RL(MCTS) for one-player game like maze?

The main difference is in one-player game, this win rate can be far less than 50%. May there is only one successfuly path which can lead to win. If we use MCTS, the positive samples(win case) is very sparse.

So in these scenario, how to use RL + MCTS?",reinforcementlearning,liqiangniu,False,/r/reinforcementlearning/comments/gipnzt/reinforcement_learning_mcts_for_nongame_scenario/
And that is why you should (always) normalize your rewards(?),1589317035,"This is a follow up post on a post I made earlier (deleted). 

So when it comes to state normalization we tend to normalize the features to avoid saturation/exploring gradient. This is caused by the high value inputs compared to the low initialized weights (taken you use the default initializers). We use several normalizing techniques to negate this effect, which is why, I would say, that you should always normalize your state_features to a sane reach (per feature normalization (aka diffrent scales) is applicable, which is what I learned recent). 

Taken this theory on why we normalize the state_features, does this same theory apply to rewards in Deep RL? Though most rewards are not that high scaled, in theory the same theory would apply, atleast I think? Please let me know what you think about this.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/gik8j3/and_that_is_why_you_should_always_normalize_your/
Html Boşlluk Doldurma Testi | Teknobu,1589312205,,reinforcementlearning,teknobu,False,/r/reinforcementlearning/comments/giilvr/html_boşlluk_doldurma_testi_teknobu/
Reinforcement Learning using Genetic Algorithm,1589307019,"I'm trying to create an AI player for Hearts card game (without card swapping). I'm using Genetic Algorithm for Parameter Optimization.

Have created a virtual environment for playing the games, 3 random players and 1 player using the neural network.

Neural Network has 156 input nodes (giving all available information till now) -&gt;  hidden layer 500 nodes -&gt; output 52 nodes (gives the card which is to be played).

For GA I'm using a population size of 500 and each individual plays 500 games every generation.

All the 500's used here have no specific reason behind them.

I'm new to the field trying to learn and explore the field, so the approach could be very naive or completely wrong.

Any comments or suggestions.",reinforcementlearning,darp87,False,/r/reinforcementlearning/comments/gigrfk/reinforcement_learning_using_genetic_algorithm/
Solving Multi-Armed Bandit problem via ε-Greedy agents,1589303508,,reinforcementlearning,Noblesse_Coder,False,/r/reinforcementlearning/comments/giffyt/solving_multiarmed_bandit_problem_via_εgreedy/
Some of our favourite films featuring AI - Any you would add? How many have you seen?,1589301079,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/giem9v/some_of_our_favourite_films_featuring_ai_any_you/
Solutions manual for Sutton &amp; Barto 2nd Edition,1589296910,"I've seen an archived post looking for a solutions manual.  


There is one here:

[https://github.com/brynhayder/reinforcement\_learning\_an\_introduction](https://github.com/brynhayder/reinforcement_learning_an_introduction)",reinforcementlearning,berterskine,False,/r/reinforcementlearning/comments/gid83v/solutions_manual_for_sutton_barto_2nd_edition/
[BLOG] Deep Reinforcement Learning Works - Now What?,1589283336,,reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/gi9ivg/blog_deep_reinforcement_learning_works_now_what/
What is the road to follow to become a researcher in reinforcement learning?,1589274226,"I like reinforcement learning and I like to become a researcher in this field.  I am studying it for nearly a year (not consistently). I watched David Silver lectures on reinforcement learning. Now I am watching the reinforcement learning specialization course at Coursera and reading the book ""Introduction to Reinforcement Learning"" by Rich Sutton and Barto. I completed the majority of the chapters from the book, but I think it will take 2-3 months for me to complete the book thoroughly. So, what should I do after completing those things to become an expert in this field?",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/gi7mtv/what_is_the_road_to_follow_to_become_a_researcher/
"Hello, everyone. Need help with fast start-up with RL for my Bsc. More info in the desc.",1589234228,"Hello, everyone,

Okay, so here is the situation. I am currently doing my Bsc. degree. The topic I am exploring is utilizng RL for managing network congestion over WAN.

I already have an good foundational background in networks, but have no idea about anything concerning AI.

The other kind of intense part is that I have to deliever results within 4 weeks. The short time is mainly due to me focusing on the networking part over RL part and that was a huge mistake.

I already gathered a number of resources mainly talking about RL, like: [OpenAI: Spinning Up](https://spinningup.openai.com/en/latest/), OpenAI's resource is my main goto.

But to my despair I could not understand some of the concepts, this is mainly due to the lack of foundation in deep learning and some statistics. With that being said I do understand the intuition behind a lot of the concepts explained but not necessarily the how everything works.

A bigger probelm is that after further research I found that some algorithms work in sync with environment, ie we wait for some learning to occur before resumming with our interactions. In my case I also need to have async learning happening in parallel with the network so that no delay occurs and changes are applied periodically.

I already know that a lot of work done by applied AI, is done with TensorFlow or Pytorch. But I am still kind of lost

My request is if you could help, I need resources that help me jump start with applied RL as fast as possible, also if anyone could also include good tutorials about how to apply it with frameworks such as TF or Pytorch I would be grateful. *Of course I would prioritize understanding how RL concepts works before working with frameworks*

**Also I understand that I will not be able to achieve knowledge compared to people that spent months/years studying this, and understand with this time frame that I need to kill myself with work to achieve anything good.**

I am extermly grateful in advance for everyons help.

&amp;#x200B;

Thank you.",reinforcementlearning,The_Vaxeon,False,/r/reinforcementlearning/comments/ghxeu8/hello_everyone_need_help_with_fast_startup_with/
"[P] Stable-Baselines3 beta, PyTorch edition of the RL Baselines is out!",1589186648,"Github repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)

Documentation: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/en/master/)

RL zoo: [https://github.com/DLR-RM/rl-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)

![img](313mcgyul3y41)

Simple example:

    import gym
    
    from stable_baselines3 import PPO
    from stable_baselines3.ppo import MlpPolicy
    
    env = gym.make('CartPole-v1')
    
    # Define and train
    model = PPO('MlpPolicy', env, verbose=1)
    model.learn(total_timesteps=10000)
    
    # Enjoy the trained agentt
    obs = env.reset()
    for i in range(1000):
        action, _states = model.predict(obs, deterministic=True)
        obs, reward, done, info = env.step(action)
        env.render()
        if done:
          obs = env.reset()
    
    env.close()",reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/ghjvgv/p_stablebaselines3_beta_pytorch_edition_of_the_rl/
Does anyone have student free license for Mujoco physics engine that they can share?,1589179753,"I was not provided a school email address from my college , so I'm not able to apply for student license in Mujoco. Need help.",reinforcementlearning,jerry_francis,False,/r/reinforcementlearning/comments/ghijjs/does_anyone_have_student_free_license_for_mujoco/
Mlagents and unity,1589175725,"I have trouble setting up mlagents and unity in Ubuntu 19.04 , CPU. Can anyone recommend any good setup tutorials pages or resources for it?",reinforcementlearning,jerry_francis,False,/r/reinforcementlearning/comments/ghhpxb/mlagents_and_unity/
"Learn the concepts and fundamentals of deep reinforcement learning, it's relation to AI and ML, and how to formulate and solve a problem in the context of reinforcement learning with hands-on experiments.",1589167321,"[https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?referralCode=6EFFEF951DDE7B78FBA0](https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?referralCode=6EFFEF951DDE7B78FBA0)

![video](dfqakacl02y41)",reinforcementlearning,mehdi_mka,False,/r/reinforcementlearning/comments/ghfswx/learn_the_concepts_and_fundamentals_of_deep/
"State Feature scaling is important, or is it and how far should you go?",1589154008,"Hello everyone,

I have been struggeling the past few days regarding state feature normalization. I am unsure about whether you A, “really” want to scale/normalize your state values and why and B, to what extend you wanna scale/normalize them and why.


A, so the theory goes, atleast as far as I learned, “when using a NN you should “always” scale/normalize your inputs, for computational sake, but also because it appears to “perform better”. Though if you really do some digging about other people discussing this topic, they say that same results can be achieved with and without scaling/normalizing the input. I don’t think any “side” is per se “wrong”, though most researches i’ve seen created a scaled environment by default or manually scaled/normalized an exisiting environment. I think inputting values with the size &gt;1000 if not 100 is not a good idea, though it does happen in SL/UL, I rarely see it happening in RL. This brings me to the next point.

B, say you do wanna scale/normalize, how much do you wanna normalize? I have questions whether you should do a fixed scaling for all state features with the same beta or have diffrent once, when you are scaling with a fixed point. For example if you have [10,10000] and you wanna scale to range(-1,1), then most logicaly, you’d divide this by 10000, creating [0.001, 1], this gap between values is large. Though in theory they are now scaled equally, I found by my own experiments I did that the RL agent or NN doesn’t “like” this method of scaling as it inbalance. Scaling [10,10000] to range(-1,1) with variable scaling according to eg minmax or fixed variable scaling per state feature would then make you scale 10/10 and 10000/10000 causing both to be 1, which is also not what it should be representing, hence I believe variable fixed scaling is not a method to use in NN or RL, because it causes the magnitude of the values to collide, but this is just an assumption for now. Some scale to ranges like range(-10,10) or range(-100,100), but why? Why not always scale to the range(-1,1) if you are doing scaling anyway? I have seen this been done where people prefer to scale to higher ranges than range(-1,1) which I don’t understand. Is it because you else get the highap which you would have when you divide all values in [10,10000] by 10000, which gives [0.001, 1] or is this not a problem to have this “gap” between state values. Shouldn’t more decimal values, which are caused by this scaling method, not cause more possible posistions, thus causing more bad than good for an algorithm? If not, why is nobody doing it this way and rather choosing diffrent scaling methods like maxmin scaling, no scaling, or scaling per state feature.

I am just a bit uncertain about the situation since everyone has their intake on this and I don’t wanna make conclusions as a non researcher in this field. Could anyone point out to or explain how this works, in RL? I am confused :)",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/ghcbf0/state_feature_scaling_is_important_or_is_it_and/
Monte Carlo Tree Search Simple Implementation,1589136819," This weekend, I've written Monte Carlo Tree Search, the algorithm that was used in Alpha Go, and a demo on Tic Tac Toe. This implementation can be expanded to more perfect information games. Let me know what you all think! [https://github.com/shehio/monte-carlo-tree-search](https://github.com/shehio/monte-carlo-tree-search)",reinforcementlearning,shehio,False,/r/reinforcementlearning/comments/gh74p4/monte_carlo_tree_search_simple_implementation/
Soft Actor Critic's losses don't seem to converge even though the policy is training very well.,1589117087,"Hey everyone. I implemented the SAC algorithm in python. While it does seem to learn very well, the Q,V and policy losses seem to only go **up** as the algorithm trains. So the rewards gets better and better and almost reaches the maximum possible performance but the losses withon the SAC just keep going up. Is this normal behaviour? It is worth notig that I'm using the automatic entropy alpha tuning.",reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/gh1g7p/soft_actor_critics_losses_dont_seem_to_converge/
[R] Questions about Ape-X with RLlib,1589112662,"I'm getting started with RLlib. I really like it so far! but I feel sometimes the documentation is a bit sparse...

A few questions I need your help with:

**How to limit training intensity?**

With Ape-X, I have multiple rollout workers executing the current policy and dumping experiences in a central replay buffer. Since the rollout workers and the learner are fully independent, it means that the number of times each experience is used for training will depend on the number (and speed) of the rollout workers. This is sometimes called the ""training intensity"".

In other RL frameworks (eg rlpyt, Catalyst) you can tune the min and max training intensity, to make sure it doesn't influence the training dynamics. In other frameworks you can say eg:

- If each experience in the replay buffer has already been used more than 32 times for training, wait to receive new ones before training more
- If some experiences in the replay buffer have been used less than 8 times for training, stop adding new experiences for a bit

In my experience this really has an impact on training performance. You can easily overfit on too few samples if you are not careful! 

The only relevant config I found in RLlib is `min_iter_time_s`, which limits how often the learner can learn. But that's not convenient at all. The same `min_iter_time_s` will result in very different training intensities depending on number of workers and system performance.

**How to read the ""throughput"" metrics?**

I am plotting the number of timesteps per seconds (`info_num_steps_sampled/time_total_s`) and seconds per iterations (`time_total_s/training_iteration`). These plots look like I would expect.

https://i.imgur.com/KBpBTDn.png

However, RLlib also logs the following metrics: `sample_throughput` and `train_throughput`, and I have no idea how to read those!

I would expect `sample_throughput` to be the number of experiences sampled from the replay buffer. But in that case why would the number be different from the number of ""trained"" experiences indicated by  `train_throughput`?

Does a low value of `sample_throughput` indicate that my system is under too much pressure to sample efficiently from the replay buffer?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/gh0ekf/r_questions_about_apex_with_rllib/
First Q-Learning project!,1589105505,,reinforcementlearning,gerryvanboven,False,/r/reinforcementlearning/comments/ggywpe/first_qlearning_project/
What is the difference between REINFORCE algorithm with baseline as state value function and the ACTOR-CRITIC algorithm?,1589104954,"In the REINFORCE algorithm with state value function as a baseline, we use return ( total reward) as our target but in the ACTOR-CRITIC algorithm, we use the bootstrapping estimate as our target. In my sense, other than that those two algorithms are the same. Then why we are using two different names for them?  Also what is the job of the critic in the ACTOR-CRITIC algorithm, it seems like critic is just acting as a baseline in the ACTOR-CRITIC algorithm.",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/ggystn/what_is_the_difference_between_reinforce/
Reinforcement Learning Discord?,1589088197,"Hello, 

I am currently a beginner studying RL and it is really fascinating. I have found a couple of other interested people to learn with, but I would love to be part of a larger community studying and helping each other with RL. I have seen a number of different Discords advertised in r/learnmachinelearning. Sometimes they will have a RL channel, but I want to find a server devoted to RL. Does this exist?

If not, would anybody (or multiple people :)?) be interested in making one? Hopefully a mixture of skill levels can join. 

If anyone is interested, please let me know in the comments. I can do all server setup for you (welcome msgs, roles, bots, etc.) and really anything else if it would be helpful. 

I look forward to seeing the RL community grow,

Thanks",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/ggvjf7/reinforcement_learning_discord/
Is it possible to pass two states at once?,1589086364,"I want to pass time series through a 1d convolution and then pass other single state information through the dense layer.  Is this ok architecture for neural networks and PyTorch?

    def forward(self, state1, state2):
        x = F.relu(self.conv1(state1))
        x = F.relu(self.conv2(x))
    
        # Flatten before dense layers
        x = x.view(-1, self.linear_input_size)
        
        # Concat flatten data and second state
        x1 = torch.cat([x, state2], dim=1)
        x = F.relu(self.fc1(x1))
        return torch.tanh(self.fc2(x))",reinforcementlearning,thinking_computer,False,/r/reinforcementlearning/comments/ggv5fq/is_it_possible_to_pass_two_states_at_once/
Will weights initialized code be executed again during training after being initialized?,1589082742,"In the Actor Critic framework, suppose I have an actor like this(fan\_in\_initializer() is just a self-defined method),

Will the weights in fc layers be initialized once and never used again even during training? Specifically when I create an actor instance, line 10, 13 and 16 will be executed once, then if I train the model, will these lines of code be executed again?

    class Actor(nn.Module):
    
        def __init__(self, s_dim, a_dim):
            super(Actor, self).__init__()
        
            self.s_dim = s_dim
            self.a_dim = a_dim
        
            self.fc1 = nn.Linear(s_dim, 512)
            self.fc1.weight.data = fan_in_initializer(self.fc1.weight.data.size()) # line 10
            
            self.fc2 = nn.Linear(512, 256)
            self.fc2.weight.data = fan_in_initializer(self.fc2.weight.data.size()) # line 13
            
            self.fc3 = nn.Linear(256, a_dim)
            self.fc3.weight.data.uniform_(-EPS,EPS) # line 16
        
        def forward(self, s):
            x = F.relu(self.fc1(s))
            x = F.relu(self.fc2(x))
            a = F.tanh(self.fc3(x))
            return a

Thanks in advance.",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/gguccc/will_weights_initialized_code_be_executed_again/
[D] Does a constant penalty incite the agent to finish episodes faster?,1589048078,"Ok, so the obvious answer to this question is: yes! but please bear with me.

Let's consider a simple problem like MountainCar. The reward is -1.0 at each step (even the final one), which motivates the agent to reach the top of the hill to finish the episode as fast as possible.

Let's now consider a slight modification to MountainCar: the reward is now 0.0 at each timestep, and +1.0 when reaching the goal. 

The agent will move around randomly, not receiving any meaningful information from the reward signal, just like in the standard version. Then after randomly reaching the goal, the reward will propagate to previous states. The agent will try to finish the episode as fast as possible because of the discount factor.

So both formulations sound acceptable. 

Here is now my question:

&gt; Will the agent have a stronger incentive to finish the episode quickly using
&gt; 
&gt; - a constant negative reward: -1.0 all the time
- a final positive reward: +0.0 all the time except +1.0 at the final timestep
- a combination of both: -1.0 all the time except +1.0 at the goal

My intuition was that the **combination** would have the stronger effect. Not only would the discount factor give a sense of urgency to the agent, but the added penalty at each timestep would make the estimated cumulative return more negative for slower solutions. Both of these things should help!

However, a colleague came up with this illustration showing how adding a constant negative reward **does not change** the training dynamics if you already have a final positive reward!

https://imgur.com/a/xOvjE1u

I am now confused quite confused. How is it possible that an extra penalty at each step does not push the agent to finish faster?!",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/ggkhwq/d_does_a_constant_penalty_incite_the_agent_to/
why DQN can't be used for self driving cars,1589047219,I am wondering why DQN and similar algorithms of RL can't be used for self driving cars?reason why I am curious is because it successfully plays go and other multistate games.,reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/ggk7yc/why_dqn_cant_be_used_for_self_driving_cars/
Posting a paper alone vs as a first author,1589044342,"I am new to the world of RL research. I was wondering, if I publish a paper as the sole author, as opposed to being the first author + a professor as mentor, would I get less credit for the paper? 

For instance, if I want to apply to a top firm or program, would people consider the paper less of my achievement and assume part of the ideas were that of the professor? Is it in any way more impressive to publish a paper as sole author?",reinforcementlearning,kakushka123,False,/r/reinforcementlearning/comments/ggjbqi/posting_a_paper_alone_vs_as_a_first_author/
how to represent pixel input as a state.,1589043757,"how do they train RL algorithms which plays from raw pixels.For example in breakout(atari game), they represent the state as the last four frames of the game, but i still don't understand how it learns,(i know CNN), do they feed those 4 frames in CNN and what it outputs? or how do they represent features of state?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/ggj4tx/how_to_represent_pixel_input_as_a_state/
Unit Neurons v1.0 (C++ Neural Network Library) Release Trailer,1589040795,,reinforcementlearning,johnlime3301,False,/r/reinforcementlearning/comments/ggi5yo/unit_neurons_v10_c_neural_network_library_release/
courses on self-driving cars,1589040772, are there any self-driving car courses that teach you how to build self driving car?,reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/ggi5pz/courses_on_selfdriving_cars/
Upcoming interview with Andrew G. Barto,1589027376,[removed],reinforcementlearning,g-x91,False,/r/reinforcementlearning/comments/ggemmb/upcoming_interview_with_andrew_g_barto/
RL library for Tensorflow 2,1589026099,Are there some good python RL libraries that use Tensorflow 2.1 and allow to use custom networks defined with it?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/gged28/rl_library_for_tensorflow_2/
[D] Languages and frameworks for RL environments,1589021934,"What do people use to have efficient RL environments for experiments? (so preferably accessible from Python)

Some that I'm familiar with:

- pycolab (gridworlds, pure Python)

- ML Agents (Unity)

- MuJoCo (physics sim, proprietary)

- PyBullet (physics sim)

Is it a common thing to write your own environments from scratch in a fast language like C++ or Rust?",reinforcementlearning,Laser_Plasma,False,/r/reinforcementlearning/comments/ggdigv/d_languages_and_frameworks_for_rl_environments/
[P] Lab: Organize Machine Learning Experiments,1589016599,"[**🧪 Lab Github Page**](https://github.com/lab-ml/lab)

[**📚 Documentation**](https://lab-ml.com/)

Lab is a library of small tools that work together to improve your machine learning workflow.

I posted updates to the project on this subreddit before. We've received some valuable feedback directly on this subreddit and /r/MachineLearning and later from users who found out about the project here. (I think it's more relevant in the RL subreddit because of most of the experiments I've run with Lab are RL experiment) These feedback has helped us improve the project. So, thank you.

Here's some of the updates to the project and we are glad if you find them useful. Please let us know if you have any suggestions or feedback.

**Configurations module** has improved a lot in the last couple of months. Now you can write less code to train model, close to Pytorch Lightening levels, but with full customizability. It also forces you to have good programming practices like not passing a large config object around.

For instance, [this MNIST example](https://github.com/lab-ml/samples/blob/master/pytorch/mnist/lab_latest.py), is only 80 lines of code.

[Comparison of an MNIST classifier](https://preview.redd.it/cijcq5nvipx41.png?width=1550&amp;format=png&amp;auto=webp&amp;s=1e44fe6948bf20a0014a41d319daf7bd9a148b95)

It uses these components: [Device](http://lab-ml.com/_modules/lab/helpers/pytorch/device.html#DeviceConfigs), [Training &amp; Validation](http://lab-ml.com/_modules/lab/helpers/pytorch/train_valid.html#TrainValidConfigs), and [MNIST Dataset](http://lab-ml.com/_modules/lab/helpers/pytorch/datasets/mnist.html#MNISTConfigs). Anyone can write similar components for re-use in their machine learning projects. We have included some of common components we developed.

We have also been working actively on the [**Dashboard**](https://github.com/lab-ml/dashboard) too. You can view all your experiment results and hyper-parameters in a single screen.

&amp;#x200B;

[Dashboard Screenshot](https://preview.redd.it/rezh4j01jpx41.png?width=2800&amp;format=png&amp;auto=webp&amp;s=c07387c024cdfd998cf704dfc36612a993a3343c)",reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/ggcgrv/p_lab_organize_machine_learning_experiments/
How do I set up a state space?,1589016475,"I've been messing around with q-learning in python, and I'm trying to make my own environment to put one in.

How should I create a state space if I have several variables? Can someone point me to a resource, or give a quick explanation?

In essence,

self.stateSpace = ????",reinforcementlearning,RichKat666,False,/r/reinforcementlearning/comments/ggcfvr/how_do_i_set_up_a_state_space/
RL Survery Paper or Report,1589000616,I'm looking for a survey paper for SOTA RL algorithm,reinforcementlearning,schongwe,False,/r/reinforcementlearning/comments/gg98v1/rl_survery_paper_or_report/
Sub bot idea,1588997994,"I have an idea for a bot I would like to create for this sub, and was wondering what you guys thought. Any feedback or advice is appreciated.

**Problem:**

I want to preface this post by saying that I am new to this sub but have had a really positive experience so far. I ask beginner questions and always receive very thoughtful and encouraging  responses from redditors who are clearly invested in teaching me. That being said, I have also noticed I receive responses from redditors who clearly do not know what they are talking about. These blatantly wrong answers can be confusing to learners such as myself, and are most definitely not unique to this subreddit. Stackoverflow gets around the problem of uneducated responses by heavily moderating their site. I believe that Stackoverflow has the very important job of documenting general programming problems, but the same practices that ensure this (removing duplicate/badly worded posts, restricting certain actions to those with enough reputation, etc.) can limit the type of questions one can ask. This essentially gate-keeps the site from beginners, those who want to ask broader, more discussion based questions, or those who want to ask very project specific questions.

**Proposed Solution**

I would like to create a bot that determines a subreddit members credibility based on their post-history. A credibility score would be generated and updated automatically, and ideally appear on a redditors flair. This credibility score, however, should not be used to prevent people from posting responses or questions. Instead the score would allow an OP to determine if the individual helping them out is knowledgable in their field (in this case RL). There are many different features to look for in order to determine a redditors sub-reddit credibility. Some features that immediately come to mind are number of upvotes and number of posts in given/related sub, sentient of responses to each of the users posts/comments in given/related sub, etc.",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gg8nfk/sub_bot_idea/
[R] Exploring Exploration: Comparing Children with RL Agents in Unified Environments,1588996947,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/gg8ef2/r_exploring_exploration_comparing_children_with/
Simplification of expected reward under the limit in continuous tasks.,1588943362,"I was reading the average reward setting for continuous tasks from rich sutton's book (page 202, 2nd edition). There he perform a simplification over the expected reward under the limit approaching to infinite. I mark this point in this picture:

&amp;#x200B;

[average reward for continuous tasks](https://preview.redd.it/3w8zpzhyhjx41.png?width=375&amp;format=png&amp;auto=webp&amp;s=a52d63d2da58f477e9a71aa987bfb702c6249bcb)

The book does not clearly mention the steps to simplify the above expression. I search on the web to find the solution but there is no clear explanation on that.  Can anyone explain the marked point?",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/gfsxcn/simplification_of_expected_reward_under_the_limit/
Methods for adapting the optimization steps in the learning process,1588942381,"Hey,

I'm looking for ideas, papers, or methods to adapter the number of optimization steps in the learning process of a neural network.

Imagine for each iteration your NN takes 10 optimization updates. During training this number shall become lower and lower if performing better. Or increase if performing worse. So I'm looking for some kind of criteria that defines to automatically adjust these numbers of updates (based on the learning progress).  
Do you guys have any ideas?

I already tried with some kind of variance over the last x losses. Worked okay somehow but here as well you have to define a threshold value... so no real improvement in reducing the hyperparameter.

I'm really interested to hear your ideas!",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/gfsnvu/methods_for_adapting_the_optimization_steps_in/
Mathematics for Sutton and Barto's textbook?,1588932152,"Hi.

I am exploring Sutton and Barto's textbook on reinforcement learning. I think I need to learn some more of the underlying maths first.

I have a high-school level understanding of calculus, probability and statistics. I have taken a college course on linear algebra.

Real analysis, \_serious\_ probability and statistics are a weak spot. On the bright side, I am more interested in (as of now) implementing the key algorithms and techniques, more than proofs (though I will definitely revisit RL in a more rigorous way at a later time).

Could you please point me in the right direction?

&amp;#x200B;

Thanks,

Pakodanomics",reinforcementlearning,pakodanomics,False,/r/reinforcementlearning/comments/gfqe6n/mathematics_for_sutton_and_bartos_textbook/
Mapped action for RL agent,1588923504,"Many prior works have been researched for agent's various action spaces. For instance, recently, Chandak et al. has been proposed stochastic action sets and continuously changing action sets.

Here, I want to ask how to handle the action set composed of causal action components. For instance, for the Uber/Lyft dispatch system, the RL agent (dispatcher) needs to consider which car and which passenger should be scheduled. Then, the agent action would be \`a = (car, passenger)\` where the car and the location are correlated. I've found the NLP games are closely related to the action set because the agent needs to understand both the language script and the game screen in order to perform an action. 

As in the Uber dispatch example, the agent needs to select a car and then a corresponding passenger. Is there any related work for addressing a paired or mapped action in which its components have a causal relationship?",reinforcementlearning,TK-SZ,False,/r/reinforcementlearning/comments/gfonhm/mapped_action_for_rl_agent/
What is wrong with my DQN implementation?,1588890582,"I [implemented a DQN](https://github.com/Stephanehk/Reinforcement_Algorithm_Implementations/blob/master/DQN_cartpole.py) in python to solve Cartpole. After 200 iterations, however, the results are not very promising. As seen with the graphs below, the ANN's MSE increases and then plateaus. The ANN's accuracy seems to be generally increasing, although sporadically. The total reward, however, seems to mostly hover around the total reward generated by a random model (between 20 and 30). There are some instances where the model does much better, however the results still do not seem to be where they should be. I would appreciate it if anyone could point out what might be wrong with my implementation. Thanks!

&amp;#x200B;

https://preview.redd.it/1ouijaih5fx41.png?width=1202&amp;format=png&amp;auto=webp&amp;s=8a4addb85e91c92c694e2183a5bcbd5fc59eec03

https://preview.redd.it/8ypmsaih5fx41.png?width=1206&amp;format=png&amp;auto=webp&amp;s=02071a64605d707fa0d0d8036979f2584192e42d

https://preview.redd.it/nc7dbbih5fx41.png?width=1242&amp;format=png&amp;auto=webp&amp;s=a73e3c0a92f9254f357d35ea33a62e94b0d0695b",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gfgqxq/what_is_wrong_with_my_dqn_implementation/
"""Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems"", Levine et al 2020",1588876262,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gfc8dq/offline_reinforcement_learning_tutorial_review/
RL Conference Questions,1588862959,"I had a few questions about the RL conference process that I couldn't find answered in other threads, and I was hoping for some advice. For reference I'm a graduate student, not in a CS department, so I don't really have much guidance from my advisor since we are both new to this area. This will be broad, but we created an expansion/improvement on an existing DRL method and applied it to a new problem that while can be said to be similar to current Atari tests, is applicable to real world scenarios. My questions are namely about publishing this research at a conference:

1. I gather that ICML/NeurIPS/ICLR are the top three conferences and roughly equivalent for a theory/application paper, is this accurate and/or should there be others I should be aware about?
2. The review process and acceptance rate seems brutal, how often do people apply to these, and if rejected, apply to other conferences? 
3. It seems like generally there is a series of reviews, the authors write a rebuttal, and then a final reviewer decides whether to accept or reject. Is this accurate and are they any tips for what to do during these steps?

I've looked briefly at the recent ICLR open reviews, but those are the only data points I could find to compare my research too. Further, with the NeurIPS deadline coming up, we're trying to decide our course of action using any additional data points. My field's conferences act very differently so I appreciate any advice.",reinforcementlearning,hellz2dayeah,False,/r/reinforcementlearning/comments/gf7x2d/rl_conference_questions/
Top AI &amp; Data Science Podcasts,1588862470,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/gf7rxg/top_ai_data_science_podcasts/
[R] Social diversity and social preferences in mixed-motive reinforcement learning,1588824111,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/gezgmj/r_social_diversity_and_social_preferences_in/
Recommendations for Multi-agent RL,1588820423,"I'm gonna work on some defensive multi-agent RL problems, and I'm pretty new in MARL. 

Can I have some recommendations on online resources (e.g., blogs, tutorial), classes, books and environments for a better start? Any suggestions would be highly appreciated!",reinforcementlearning,nancywhr,False,/r/reinforcementlearning/comments/geyl1x/recommendations_for_multiagent_rl/
"[Youtube][AI Podcast CLIPS] What is Deep Reinforcement Learning? David Silver, DeepMind &amp; Lex Friendman",1588803832,,reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/geu12f/youtubeai_podcast_clips_what_is_deep/
Help with some Sutton/Barto Exercises - Chapter 4,1588780550,"I've been working through the Sutton RL book and have been doing the exercises. My answers for 4.7 and 4.9 are a bit off compared to what's in the book and I was hoping someone could take a quick look to make sure I'm not making any fundamental error. 

code here:  [https://github.com/dipplestix/rlbook/tree/master/ch4](https://github.com/dipplestix/rlbook/tree/master/ch4)

&amp;#x200B;

Thanks a lot for any help",reinforcementlearning,dieplstks,False,/r/reinforcementlearning/comments/gemnap/help_with_some_suttonbarto_exercises_chapter_4/
[R] [ICRL2020][Model-based] Combining Q-Learning and Search with Amortized Value Estimates,1588770379,,reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/gejnfg/r_icrl2020modelbased_combining_qlearning_and/
Path finding in single agent games with prior policy,1588768983,"Hi everyone,

I'm currently facing the problem of finding best paths in a single agent game with the following characteristics:

\- the branching factor is roughly **30**

\- the max depth is roughly **50**

\- the reward is very **sparse**: a reward is given only when reaching a terminal node, and is 0 otherwise

\- the reward can be either computed very fast from the terminal state, or quite slowly (like 10 ms vs 30 s) and I would be interested in both use cases

\- I have a **prior probability** on all paths in the game tree (encoded by a neural network implemented in PyTorch). Most paths have a very low probability and can be safely discarded from the potential solutions. In fact, we can define a threshold probability p such that we admit that **every path with a probability less than p must be discarded**. This is wanted, and also clearly reduces the search space

\- I could have access to a lot of CPUs and would be very interested in an algorithm that can leverage **parrallelism**

\- There could be multiple (and potentially a lot of them) solutions with a satisfying reward; ideally I'd like to find the highest number of solutions, and the most diversified set (with respect to a measure of diversity that I can compute between two terminal states). This is a nice-to-have, not a must-have.

\- The current solutions that I have implemented are either learning a policy through hill climbing (sample solutions according to prior probability, keep the best, backprop to update the policy defined by the neural network as to maximize probability of sampling the best solutions, and repeat), and MCTS where I discard paths that have too low prior probability (but I'm not convinced by the relevance of MCTS in this context). Both algorithms yield statisfying solutions for different interesting reward functions, but there is room for improvement

I wanted to know:

\- do you think of any search algorithm that would fit my needs better than those solutions?

\- if the problem is too specific such that there is no perfectly suited algorithm, do you have any idea for improvement?

Thanks a lot!",reinforcementlearning,Max314156,False,/r/reinforcementlearning/comments/geja5k/path_finding_in_single_agent_games_with_prior/
how policy gradient methods work,1588758047," *Can* you explain policy gradient methods?*What* *parameterizes* it? *I* am reading *Sutton* and *Barto* book on reinforcement learning and didn't understand well what it is*,* can you give some examples?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gegw9i/how_policy_gradient_methods_work/
Q table not converging,1588741772,"This is an experiment in order to understand the working of Q table and Q learning.

I have the states as 

`states = [0,1,2,3]`

I have an arbitrary value for each of these states as shown below (assume index-based mapping) -

`arbitrary_values_for_states = [39.9,47.52,32.92,37.6]`

I want to find the minimum of the state which will give me the minimum value.
So I have complimented the values to 50-arbitrary value.

`inverse_values_for_states = [50-x for x in arbitrary_values_for_states]`

Therefore, I defined reward function as -

```
def reward(s,a,s_dash):
    if inverse_values_for_states[s]&lt;inverse_values_for_states[s_dash]:
        return 1
    elif inverse_values_for_states[s]&gt;inverse_values_for_states[s_dash]:
        return -1
    else:
        return 0
```

Q table is initialized as - 
`Q = np.zeros((4,4))` (np is numpy)

The learning is carried out as -
```
episodes = 5
steps = 10
for episode in range(episodes):
    s = np.random.randint(0,4)
    alpha0 = 0.05
    decay = 0.005
    gamma = 0.6
    for step in range(steps):
        a = np.random.randint(0,4)
        action.append(a)
        s_dash = a
        alpha = alpha0/(1+step*decay)
        Q[s][a] = (1-alpha)*Q[s][a]+alpha*(reward(s,a,s_dash)+gamma*np.max(Q[s_dash]))
        
        s = s_dash
```

The problem is, the table doesn't converge.
Example. For the above scenario - 
np.argmax(Q[0]) gives 3
np.argmax(Q[1]) gives 2
np.argmax(Q[2]) gives 2
np.argmax(Q[3]) gives 2

All of the states should give argmax as 2 (which is actually the index[state] of the minimum value).

Another example, 
when I increase steps to 1000 and episodes to 50, 
np.argmax(Q[0]) gives 3
np.argmax(Q[1]) gives 0
np.argmax(Q[2]) gives 1
np.argmax(Q[3]) gives 2

More, steps and episodes should assure convergence, but this is not visible.

I need help where I am going wrong. 
PS: This little experiment is needed to make Q-learning applicable to a larger combinatorial problem. Unless I understand this, I don't think I will be able to do that right.",reinforcementlearning,devprabal,False,/r/reinforcementlearning/comments/gedt6q/q_table_not_converging/
Discrepancy between deep Q-Learning implementation's results and NN error,1588715920,"I am attempting to implement a deep Q-Learning algorithm to solve CartPole. Below is a graph of my results so far. The graph on the left is the number of timesteps achieved, while the graph on the right is the mean squared error of the action-value function neural network. As you can see, the neural network's error is decreasing, however, the number of timesteps achieved is not improving. 

&amp;#x200B;

The neural network I am using uses MSE and gradient descent. The NN samples batches of 64 s,a,r,s pairs from memory, and transfers it's weights to the target NN every 128 iterations. [Here is my implementation for reference](https://github.com/Stephanehk/Reinforcement_Algorithm_Implementations/blob/master/DQN_cartpole.py). I am really not sure why the algorithm is not improving, and why despite this the NN is improving.

&amp;#x200B;

Note: for the first few episodes no memory replay is done, instead random actions are chosen. This is why there is a large jump in the first graph.

https://preview.redd.it/bg1hfxp4p0x41.png?width=2466&amp;format=png&amp;auto=webp&amp;s=73836c076b959becbdf83f1377f0428eea1afeee",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/ge74w6/discrepancy_between_deep_qlearning/
Need help with my RL assignment.,1588695811,"This is not for a course, so you won't be helping me cheat. 

I need help incorporating code from a RL library into my existing code. I am willing to pay. PM me for more details.",reinforcementlearning,zikrlamcy786,False,/r/reinforcementlearning/comments/ge0upe/need_help_with_my_rl_assignment/
13 must-read AI paper recommendations from experts,1588694022,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/ge0a8j/13_mustread_ai_paper_recommendations_from_experts/
Creating Artificial Life with Reinforcement Learning,1588687625,,reinforcementlearning,MaartenGr,False,/r/reinforcementlearning/comments/gdycws/creating_artificial_life_with_reinforcement/
[R] On Bonus Based Exploration Methods In The Arcade Learning Environment,1588685924,,reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/gdxvh0/r_on_bonus_based_exploration_methods_in_the/
[R][MetaRL] Improving Generalization in Meta Reinforcement Learning using Learned Objectives,1588684396,,reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/gdxh58/rmetarl_improving_generalization_in_meta/
Optimal strategy for point multiplication game,1588683666,"Hi, I've posted this on a couple different math-related subreddits and was pointed in the direction of RL. I am trying to find the most optimal strategy for the environment explained below. Any help is very much appreciated! Cheers.

Take a fake game where the goal is to end up with as many points as possible at the end of a certain number of rounds. You start with 1 point. Every round, your point(s) are multiplied by a certain random number that increases to a random maximum as the round progresses. The number always starts at 1.5 and the highest it can possibly reach is 20. Once the round starts and the number starts increasing, it does not drop back down, and when the number reaches its random max, the round ends. At any time during the round, and however many times you want, you can choose to ""use"" any or all of your points, or use any fraction of a point. When you use points, they are multiplied by the value that the random number is currently at. The multiplied points are then locked in place for the next round, and cannot be used again during the round. At the end of every round, any points that are not used are kept and can be used in the next round.",reinforcementlearning,anonymous38458,False,/r/reinforcementlearning/comments/gdxart/optimal_strategy_for_point_multiplication_game/
prerequisites for sutton and barto's book,1588665071,"are there any prerequisites to read sutton and barto's book? i have read 200 pages and sometimes i don't understand formulas and concepts. Is it because RL is hard or i don't have prerequisite knowledge?(i know multivariable analysis, functional analysis, linear algebra, basic probability and statistics)",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gdth1t/prerequisites_for_sutton_and_bartos_book/
How to handle multi action scenario in RL.,1588656498,"Suppose e.g. task is to arrange n elements on a canvas. Action that can be applied on each element is two dimensional \[move up/down, move left/right\]. Agent has time limit to finish the task and once time is up it will be given reward if arrangement is right. Next task again will be same but number of elements and canvas dimensions can change.  Can you please help me how to handle this scenario using RL and how to handle variable number of actions from one input to another.",reinforcementlearning,shivangg27,False,/r/reinforcementlearning/comments/gdrutu/how_to_handle_multi_action_scenario_in_rl/
Nice Overview RL,1588642616,"Hi everyone,

I found this paper today (from Google Brain and UC Berkeley), which is a nice survey on reinforcement learning and its applications. I wish I had a paper like that in my bibliography when I did my first project on RL !

[https://arxiv.org/pdf/2005.01643.pdf](https://arxiv.org/pdf/2005.01643.pdf)",reinforcementlearning,sheepsody,False,/r/reinforcementlearning/comments/gdooa6/nice_overview_rl/
[D] ICLR 2020 | Virtual Conference Openly Available Online; No Best Paper Awards This Year,1588629914,"The ICLR 2020 virtual conference wrapped up this weekend, with generally favourable reviews from participants and a number of areas for future improvements identified by organizers.

A surprise came from ICLR 2020 General Chair Alexander (Sasha) Rush of Cornell Tech, who revealed without elaboration in an April 30 conversation on the conference general group chat that “PCs \[program chairs\] decided against having best paper this year.”

Like other AI conferences impacted by Covid-19, this year’s International Conference on Learning Representations (ICLR 2020) was moved completely online, where it ran relatively smoothly from April 26 to 30. **The ICLR yesterday made the entire virtual conference** [**available**](https://iclr.cc/virtual_2020/) **in open-access, enabling anyone to access the content and explore the virtual conference portal.**

One of the world’s major machine learning conferences, **ICLR 2020 accepted 687 out of 2,594 papers and drew over 5,600 participants from nearly 90 countries — more than double from 2,700 physical attendees of ICLR 2019. Each of the papers was presented by its authors through pre-recorded videos, and every paper was presented twice (in two separate sessions) considering global time zone differences.**

Read more:[ICLR 2020 | Virtual Conference Openly Available Online; No Best Paper Awards This Year](https://medium.com/syncedreview/iclr-2020-virtual-conference-openly-available-online-no-best-paper-awards-this-year-9940ec53e4dd)",reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/gdl8e2/d_iclr_2020_virtual_conference_openly_available/
Diffrent results on GPU than CPU?,1588616499,"Hello,

On Google Colab I use Stable baselines, usually CPU, but tried GPU with the exact same settings. When I used, it my results were consistantly diffrent. It also appeared slower. 

Is there any connection between hardware that could influence the results for RL?

(my apologies for the kinda low quality post, but it is a quick question which shouldn’t need more content)",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/gdgz7i/diffrent_results_on_gpu_than_cpu/
Building simple context for Multi-armed bandit,1588615121,"Hello,

I am currently struggling to implement context in my multi-armed bandit.  I have a 5-armed bandit working to test 5 selling prices for an item and I did managed to use Vowpal Wabbit to do so. But unfurtunally, i just discovered that VW doens't work well with rewards ( negative costs).  I was wondering if you guys know any reference on how to implement context in my simple 5-armed bandit, i Just want to use two things as context:  Day of the week and a percentage of how high the current arm (price) is related to the avg price of all items.

&amp;#x200B;

Really thank you",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/gdgiyp/building_simple_context_for_multiarmed_bandit/
DeepMind's original DQN Breakout Score of 428 - was it one life?,1588572093,"I was going through the original DQN paper by DeepMind - [https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236), and was specifically interested in the score of Breakout - because I plan to attempt to solve it using Dueling DQN with PER. 

However, I could not find any mention of whether their best score of 428 was over the 5 available lives in the game of Breakout or a single life. Any idea?",reinforcementlearning,prabhatverma,False,/r/reinforcementlearning/comments/gd61k2/deepminds_original_dqn_breakout_score_of_428_was/
Can reinforcement learning be used to help artificial neural networks learn?,1588542647,"I was wondering if there were any attempts at using reinfrocement learning to help ANNs learn.

I just have two main ideas:

1. Replace backpropagation and SGD with an agent (or collection of agents?). The idea is - for small ANN a human can update the weights and biases manually by trial and error process and can get pretty good results,  so maybe replace the human with an agent and scale up?
2. Speed up learning process. Probably we could have an agent (or agents) which by taking some actions it has an influence on learning process maybe hyperparameters, activation function, network architecture etc.

Not sure how to define a state in either case.

I'm pretty sure someone has already thought about this and probably it cannot be applied or the results are miserable, but  I'm just curious.",reinforcementlearning,datblubat,False,/r/reinforcementlearning/comments/gcyzsr/can_reinforcement_learning_be_used_to_help/
Paper on unsupervised doodling reproduced (First project that I completed),1588526916,"This is my first DL/RL project. I just graduated high school so I’m not experienced or good. So it might be hard to read and some parts could be wrong. But I had a lot of fun and I learned a lot.

You can check it out [here](https://github.com/urw7rs/spiralpp)",reinforcementlearning,urw7rs,False,/r/reinforcementlearning/comments/gcuh59/paper_on_unsupervised_doodling_reproduced_first/
"Where do NGU, R2D2, MuZero and Agent57 fit on the Taxonomy of Reinforcement Learning?",1588491501,"OpenAI has a great Taxonomy of Reinforcement Learning Algorithms. I was wondering where the following papers would go on this tree?

1. [Never Give Up](https://arxiv.org/abs/2002.06038)
2. [Agent57](https://arxiv.org/abs/2003.13350)
3. [MuZero](https://arxiv.org/abs/1911.08265)
4. [R2D2](https://openreview.net/pdf?id=r1lyTjAqYX)

https://preview.redd.it/d7oaapp87iw41.png?width=1960&amp;format=png&amp;auto=webp&amp;s=c3bf95a2e302fd331b158f717ac7abc762d169d2",reinforcementlearning,kiraora,False,/r/reinforcementlearning/comments/gcmno0/where_do_ngu_r2d2_muzero_and_agent57_fit_on_the/
UC Berkeley Researchers Open-Source 'RAD' To Improve Any Reinforcement Learning Algorithm (Paper and Github link in article),1588480441,,reinforcementlearning,ai-lover,False,/r/reinforcementlearning/comments/gckmc0/uc_berkeley_researchers_opensource_rad_to_improve/
"""Meta-Reinforcement Learning for Robotic Industrial Insertion Tasks"", Schoettler et al. 2020",1588465611,,reinforcementlearning,aviennn,False,/r/reinforcementlearning/comments/gch58r/metareinforcement_learning_for_robotic_industrial/
RaveForce: An OpenAI Gym style toolkit for music generation experiments,1588461009,"[https://github.com/chaosprint/RaveForce](https://github.com/chaosprint/RaveForce)

Basically RaveForce is an environment that allows you to define your musical task in SuperCollider, and train an agent to do the task in Python with APIs similar to the [OpenAI Gym](https://gym.openai.com/).

The observation space is the current synthesised audio and the reward can be based on the MFCC/STFT/raw audio MSE difference btw target audio and the generated audio.

Please feel free to pull requests and make this tool better!",reinforcementlearning,chaosprint,False,/r/reinforcementlearning/comments/gcfy49/raveforce_an_openai_gym_style_toolkit_for_music/
How do you approach to read a new paper usually?,1588431343,"Hi it's a interesting question. There are interesting papers releasing almost weekly these days. Just wanted to learn how do you guys read them ?  Do you read/dismiss paper based on abstract due lack of time? Do you read them in night or at morning when you are fresh?

If you are working , do you read papers at free time in work or after work

Do you print them out to actual papers or using a PC or tablet?

While reading it  do you annotate them  and take notes if so which tools you use ?

Also other tricks &amp; helps - hacks would be fine.

\---

I mostly %99 of time read digitally. Abstracts definitely is one of the keypoints when deciding to take a look to papers. And how graphically its constructed. I just don't read  full text papers unless its so much important / classic paper. I absolutely value comparisons with other SOTA algorithms and such.  Bonus if code provided i would be more inclined to read &amp; implement or test / reproduce.",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/gc7rq6/how_do_you_approach_to_read_a_new_paper_usually/
"PPO Training speed decreasing with a change in Kp, Kd of motors of robot",1588431235,"I am working on the project to learn a hexapod walking gait with RL in simulation. I made the custom gym env with Pybullet and using Stable baseline for training. For the joints of Hexapod, I am using setJointMotorControl2() which takes some arguments and some of them are Kp, Kd and force. Initially, I started my training with hyperparameters Kp=5, Kd=0.02 and force=0.5, and training speed on PPO2 (baseline) was good \~200 iterations/s. But when I was debugging my pybullet custom env I felt that Kp=1 Kd=1 and force=0.3 would be good for hexapod. And with these parameters setup when I tried to train on PPO2, training speed drastically decreases to \~2 iterations /s. I am worried if training speed can depend that much on these parameters(Kp,Kd...)? Or have I done something terribly wrong? For now, I am not concerned with results but this change in training speed only.

ps.

(for debugging I created env in different file and sample actions from action space and generated rollout of 10000 obs,act,red pairs. I measured the time and it was almost same for different vales of Kp,Kd and force)

Thank you.",reinforcementlearning,Scarlet_22,False,/r/reinforcementlearning/comments/gc7qrs/ppo_training_speed_decreasing_with_a_change_in_kp/
Quake 1 movement physics RL environment and project code,1588431092,,reinforcementlearning,kipi,False,/r/reinforcementlearning/comments/gc7pf5/quake_1_movement_physics_rl_environment_and/
Can I train agents using CFR?,1588410644,"Hello, I'm new to reinforcement learning so my knowledge is very limited. I have a school project in which I need to train an agent to be able to play Leduc poker successfully. 

After extensive research, I found that the CFR (and the Deep CFR) is the most successful algorithm for finding approximate Nash equilibria in imperfect information games.  But I am confused about one thing, **can I train my agent using the CFR algorithm or this algorithm is a solver for the game?**

Also, what is the difference between an agent and a solver? I'm seeing the term solver in multiple papers and git repositories but I'm still confused about it.

Any help, would be appreciated.",reinforcementlearning,rustom37,False,/r/reinforcementlearning/comments/gc3h64/can_i_train_agents_using_cfr/
"When I gave reward at the end of the trajectory, the agent does not learn anything after large number of episodes.",1588400756,"I was using deep Q learning to train my model in atari games. The reward was +1 when it manages to win and -1 if it loses the game. But surprisingly I saw when I use reward at the end of the trajectory it did not learn anything. For example, in the mountain car problem it took 5000 episodes still does not manage to climb the hill. But when I  give reward along the trajectory it can learn quickly and less than 5000 episodes are needed for similar(mountain car) tasks.",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/gc1rfu/when_i_gave_reward_at_the_end_of_the_trajectory/
[Q] Resources for Muli-Agent RL,1588398489,"Hello, recently I've been exploring field of Multi-Agent RL, I am currently reading some papers from this list https://github.com/LantaoYu/MARL-Papers and it doesn't seem to be updated anymore. So here is my question - do you have some interesting papers on topic outside of this list?

Second question - do you have any environments for this kind of problems?
Here are few that I found:
- https://github.com/oxwhirl/smac
- https://github.com/geek-ai/MAgent",reinforcementlearning,RvuvuzelaM,False,/r/reinforcementlearning/comments/gc1btp/q_resources_for_muliagent_rl/
Integrating RL and Google Maps/ Open street maps,1588395001,"I got an idea of combining the problems like Frozen Lake with Google Maps for optimal pathfinding.  The start and endpoints will be the Latitude and Longitude values of places. Some barriers in between to avoid those roads.

Does anyone have any idea how to do it?",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/gc0mk5/integrating_rl_and_google_maps_open_street_maps/
Monte Carlo linear value function approximation,1588384522,"\*\*Problem:\*\*

I am attempting to code a Monte Carlo linear value function approximation algorithm for Gym's CartPole-v0. I am running into the following problem, however. After a few iterations the weights become very large, so the term !\[formula\]\[1\] then becomes infinite, and consequently each weight is updated to nan. I have the pseudocode below.

\*\*Things I have tried:\*\*

I have already tried decreasing the learning rate. The algorithm improves (ie: from lasting 10 timesteps to over 200) up until a certain point when !\[formula\]\[1\] becomes infinite. Could the problem be with how I am defining/using the weights?

\*\*Pseudocode:\*\*

Note that when generating the episode, epsilon greedy is used to select the next action, and state is an array of features such as pole angle, velocity, etc.

        W = [[0 for i in range(n_features)] for j in range (len(possible_actions))]
        for each iteration:
            episode = generate_episode()
            for state, reward, action in  episode
                Q = dotproduct(state.T,W[action])
                for j in range (len(state))
                    W[action][j] += state[j]*alpha*(reward-Q)^2

\[1\]:[https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7Bq%7D(s%2Ca%2Cw)%20%3D%20x(s%2Ca)%5ETw](https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7Bq%7D(s%2Ca%2Cw)%20%3D%20x(s%2Ca)%5ETw)",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/gbxhtf/monte_carlo_linear_value_function_approximation/
"""Distributional Soft Actor Critic for Risk Sensitive Learning""",1588347418, [https://arxiv.org/pdf/1806.04640.pdf](https://arxiv.org/pdf/1806.04640.pdf),reinforcementlearning,olivierp9,False,/r/reinforcementlearning/comments/gbkcp6/distributional_soft_actor_critic_for_risk/
"When evaluating a policy, do you take argmax or sample from the action distribution?",1588341433,"In the case of value-based policy, you take argmax of the value output from the agent but in the case of direct policy optimization (e.g., policy gradient)  what is considered optimal. Argmax or sample from the distribution?

Since the optimization goal of direct policy optimization is to optimize the expected discounted reward (over environment transition and action distribution), I guess sampling is the correct choice?",reinforcementlearning,51616,False,/r/reinforcementlearning/comments/gbijp4/when_evaluating_a_policy_do_you_take_argmax_or/
How to evaluate policy performance on Zero-Sum Game 3vs3 Multi-Agent Environment that was trained using self-play?,1588324750,"I've just created a 3vs3 multi-agent environment zero-sum game, where they move simultaneously (which mean there isn't exist any perfect strategy that could beat every strategy CMIIW). I'm trying to train using PPO algorithm that has been available on RLLib, but because it is a zero-sum game and I train using self-play, it means that I couldn't benchmark the policy performance using the policy\_reward\_mean on every iteration (because the agents play with itself and normally the policy\_reward\_mean will converge near-zero CMIIW).  


My questions are:  
1. What is the correct metrics that I need to see whether my policy is well or not?   
2. What is the usual reward if the game is a draw? I notice that if I give +0 reward for draw game, the agents behavior is they didn't want to win and think that draw is enough, while if I punish both teams, there is a strange behavior where one of the agent will trying to lose and let the other team win.  
3. I read the OpenAI Five paper and they said that to solve the overfitting issue on self-play, they play 80% with the latest policy and 20% with old policy. I've tried to implement this by having a pool of old policies with size 3, where each 10 iterations, I will throw the oldest policy and append the recent policy to the pool. Is this the correct approach? Is my pool size too small?  


Thank you so much, I'm very new to reinforcement learning and hope to learn from you guys. Cheers!",reinforcementlearning,nicholaz99,False,/r/reinforcementlearning/comments/gbevu1/how_to_evaluate_policy_performance_on_zerosum/
How long should RLLib CartPole take to run?,1588284782,"I ran `rllib train --run=PPO --env=CartPole-v0 ` on my computer. So far, it has ran for three hours and counting. Around how much time does it take you? Is 3+ hours par for the course or should I look into other resources?",reinforcementlearning,SpicyMemery,False,/r/reinforcementlearning/comments/gb5jfr/how_long_should_rllib_cartpole_take_to_run/
Where do you run your RL code?,1588265413,"My laptop has lots of CPU power but no GPU. While letting a program run overnight, I only got a few thousand iterations. Where should I look for faster performance?",reinforcementlearning,SpicyMemery,False,/r/reinforcementlearning/comments/gazdc7/where_do_you_run_your_rl_code/
Applications of GANs - 5 Influential Video Presentations,1588255286,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/gaw7jt/applications_of_gans_5_influential_video/
"""The AI Economist: Improving Equality and Productivity with AI-Driven Tax Policies"", Zheng et al 2020 {Salesforce} [bilevel optimization]",1588199994,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/gajhql/the_ai_economist_improving_equality_and/
Free Online Talk | Reinforcement Learning Explained: Overview and Applications,1588191442,"[https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd](https://www.eventbrite.com/e/reinforcement-learning-explained-overview-and-applications-tickets-103486575132?aff=rd)

Outline:

\- Introduction to reinforcement learning and its framework  
\- RL solutions: model-based methods  
\- RL solutions: model-free methods  
\- Deep reinforcement learning  
\- Real-world applications: Alpha Go, Self-driving cars, Robotics, finance, etc.",reinforcementlearning,oyolim,False,/r/reinforcementlearning/comments/gagsel/free_online_talk_reinforcement_learning_explained/
question about autopilot,1588185837,do you know what kinds of techniques autopilot uses? does it use Reinforcement learning? which types of neural network architecture it uses recurrent or linear?any other additional information would be helpful,reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/gaezcu/question_about_autopilot/
How to make different DIAYN skills focus on different parts of the observations from the environment,1588183237,"Hi,

I am using DIAYN like a setup to train an action policy. One deviation from DIAYN is, I provide rewards at the end of the trajectory based on the end observation only.

The action space of the agent is {left,right,up,down,stay}

The observation space of the agent is a 2D pixel window around its position and the position coordinates of the agent. The agent has partial observability and needs to move around to look at different parts of the environment.

Please note that for each trajectory, objects are spawned at random positions in the environment. Say for one trajectory football happens to be in the center, for other football happens to be in the southwest corner, etc.

Initially, the action policy learned skills such as ""go to the southwest corner"", ""go to the northeast corner"", etc.

I want the agents to learn skills such as ""go to a &lt;&gt; corner"" and also ""go to this &lt;&gt; object"" together.

I added a lot of noise to the position coordinates part of the observation.

The action policy learned skills such as ""go to the football"", ""go to the basketball"", etc and learned to ignore the noisy coordinates altogether.

Please let me know how to achieve my goal which is to make agents learn skills such as ""go to a &lt;&gt; corner"" and also ""go to this &lt;&gt; object"" together.

Also please let me know about your opinion on two ideas I have in mind:

1. In order to achieve my goal, I am running a sweep on the amount of noise I add to the position coordinates. I hope that the right amount of noise will result in the agent focusing on all parts of the observation and learn skills such as ""go to a &lt;&gt; corner"" and also ""go to this &lt;&gt; object"" together.

2)

a) Train multiple discriminators in separate DIAYN like setups where each discriminator sees a random sample of the observation.

Say discriminator 1 gets to only see the pixels and say discriminator 2 gets to only see the position coordinates. Both the discriminators are trained with separate randomly initialized action policies.

Eg:

Discriminator 1 learns skills S11(find football), S12(find basketball), S13(find golf ball)

Discriminator 2 learns skills S21(go to northeast corner ), S22(go west), S23(go east).

b) Throw the action policies and retain discriminators. Have a different step where skills identified by different discriminators are used to train a single network.

Eg: Single network will have 6 skills such as

S1(find football), S2(find basketball), S3 (find golfball), S4(go north), S5(go west), S6(go east).",reinforcementlearning,Crazy_Plant,False,/r/reinforcementlearning/comments/gae5ny/how_to_make_different_diayn_skills_focus_on/
How to make different DIAYN skills focus on different parts of the observations from the environment,1588182895,"Hi,

I am using DIAYN like a setup to train an action policy. One deviation from DIAYN is, I provide rewards at the end of the trajectory based on the end observation only.

The action space of the agent is {left,right,up,down,stay}

The observation space of the agent is a 2D pixel window around its position and the position coordinates of the agent. The agent has partial observability and needs to move around to look at different parts of the environment.

Please note that for each trajectory, objects are spawned at random positions in the environment. Say for one trajectory football happens to be in the center, for other football happens to be in the southwest corner, etc.

Initially, the action policy learned skills such as ""go to the southwest corner"", ""go to the northeast corner"", etc.

I want the agents to learn skills such as ""go to a &lt;&gt; corner"" and also ""go to this &lt;&gt; object"" together.

I added a lot of noise to the position coordinates part of the observation.

The action policy learned skills such as ""go to the football"", ""go to the basketball"", etc and learned to ignore the noisy coordinates altogether.

Please let me know how to achieve my goal which is to make agents learn skills such as ""go to a &lt;&gt; corner"" and also ""go to this &lt;&gt; object"" together.

Also please let me know about your opinion on two ideas I have in mind:

1) In order to achieve my goal, I am running a sweep on the amount of noise I add to the position coordinates. I hope that the right amount of noise will result in the agent focusing on all parts of the observation and learn skills such as  ""go to a &lt;&gt; corner"" and also ""go to this &lt;&gt; object"" together.

&amp;#x200B;

2) 

a) Train multiple discriminators in separate DIAYN like setups where each discriminator sees a random sample of the observation.

Say discriminator 1 gets to only see the pixels and say discriminator 2 gets to only see the position coordinates. Both the discriminators are trained with separate randomly initialized action policies.

Eg:

Discriminator 1 learns skills S11(find football), S12(find basketball), S13(find golf ball)

Discriminator 2 learns skills S21(go to northeast corner ), S22(go west), S23(go east).

b) Throw the action policies and retain discriminators. Have a different step where skills identified by different discriminators are used to train a single network.

Eg: Single network will have 6 skills such as

S1(find football), S2(find basketball), S3 (find golfball), S4(go north), S5(go west), S6(go east).",reinforcementlearning,Crazy_Plant,False,/r/reinforcementlearning/comments/gae245/how_to_make_different_diayn_skills_focus_on/
Decreasing reward in vertex cover problem,1588177898,"I am trying to solve the minimum vertex cover problem using DQN based on this [paper](https://arxiv.org/pdf/1704.01665.pdf). I have looked at the codes. and trying to implement my own code. I am unable to understand why the following issue is occurring.

i)  I have set the reward to -1  and when done = true (i.e the episode has been completed and all the vertices are covered) the reward is 0 similar to the paper. The issue is that my DQN agent keeps choosing the same nodes every time, trying to increase the negative reward.  So if I have a 100 node graph, it will say select 20 nodes initially, but as epsilon decreases, it will start repeating the same 20 nodes (or a subset of it)  multiple times and my reward start decreasing from -20,-24,-36........-203,-204.  Why is the agent trying to increase the negative reward or decrease the reward.

ii) Also, in code for this paper the state is defined as a  `n` dimensional vector (where `n=number of nodes`), and every time a action (nodes) is selected, the value at that index is set to 1. e.g if it is 5 node graph than the state is 5 dim vector \[0 0 0 0 0\] and if the action (vertex) is 4 the state becomes \[0 0 0 0 1\]. Is this an appropriate state representation? 

And I also want to implement [Hindsight experience Relay](https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf) to deal with the sparse reward issue. So, what would my goal state look like.

 I am sorry for the lengthy doubt, but I appreciate any  input!",reinforcementlearning,choudab,False,/r/reinforcementlearning/comments/gacn7i/decreasing_reward_in_vertex_cover_problem/
why don't we hear about deep sarsa?,1588176893,"Hello everyone,

I wonder why we only hear about deep q-learning. Why  is deep sarsa not more widely used?",reinforcementlearning,tarazeroc,False,/r/reinforcementlearning/comments/gacd8o/why_dont_we_hear_about_deep_sarsa/
"Secure Deep Reinforcement Learning - Dawn Song, Professor, UC Berkeley",1588174737,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/gabrhg/secure_deep_reinforcement_learning_dawn_song/
How is Compatible function approximation true for neural networks in policy gradient?,1588174019,"I was looking at the paper Sutton's [Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf) and he mentions in a footnote that the compatible function approximation holds true for a linear class of functions as he has figured from his talks with Tsitsiklis. Obviously NNs are being used now for function approximation but it is not apparent to me how NNs satisfy the compatible function approximation conditions 1 and 2.

A followup question is that some RL packages like stable-baselines consider using the same network for value and policies. Is compatible function approximation the rationale behind using shared layers?",reinforcementlearning,AvisekEECS,False,/r/reinforcementlearning/comments/gabjqd/how_is_compatible_function_approximation_true_for/
"""Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels"", Kostrikov et al 2020",1588130933,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ga1z0d/image_augmentation_is_all_you_need_regularizing/
"""Deep neuroethology of a virtual rodent"", Merel et al 2019 {DM/Harvard}",1588120391,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/g9zbez/deep_neuroethology_of_a_virtual_rodent_merel_et/
does self driving car use RL,1588101678,"does self-driving cars use reinforcement learning? did george holtz used RL to build his own self-driving car?do we have more like CV problems in self driving cars, rather than RL?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/g9tmax/does_self_driving_car_use_rl/
why do we need dyna algorithm,1588101513,"why do we need learning and planning tabular methods? i currently read sutton and barto's book(either he is bad explainer, or i am not getting it right,please let me know), i don't understand existence of 8th chapter, why do we have models and planning? dyna algorithm? trajectory sampling? it is so confusing, we have already learnt how to evaluate value function even in model-free environment, and i don't get the ide of these algorithms.please let me know.",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/g9tkfr/why_do_we_need_dyna_algorithm/
"""First return then explore"", Ecoffet et al 2020 {Uber} [Go-Explore 2: no longer requires resets/determinism, still requires feature-engineering]",1588099117,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/g9st8i/first_return_then_explore_ecoffet_et_al_2020_uber/
OpenAI Gym Question,1588086853,"I’m creating a custom gym environment for trading stocks. The current action_space is Discrete(3): Buy, Hold, or Sell.

I’m struggling to represent the amount of shares (or amount of portfolio) to buy, hold, or sell in the action space. 

For example if I am long 200 shares and the algorithm decides to sell, how many shares should be sold? Does the algorithm want to close the position and open a short position or just close the position?

I’ve looked at using a Box action_space but it is still difficult to represent number of shares or amount of portfolio this way (or perhaps I’m missing something).

Any suggestions to help represent this in the action space would be much appreciated!",reinforcementlearning,duhfrazee,False,/r/reinforcementlearning/comments/g9p07g/openai_gym_question/
Task Allocation Using Deep Reinforcement Learning,1588074962,"Hello everyone,

I was recently looking into how to use deep reinforcement learning to allocate workers to tasks. I would like with the use of a DRL to input some parameters about the task(5-12 inputs in the range of (0,1) ) and get how many workers to allocate to the task. 

I've been dwelling into DRL for the last week and learned/implemented some basic DRL algorithms (DQN and all of it's variations). 

My problem is I'm not sure which DRL algorithm is best suited for the job and I can't seem to find any information on this.

What's the state of the art DRL algorithm for taking a 5-12 float inputs in the range of (0,1) and outputting a discrete output of how many workers to assign to this task.

All the workers are the same there is no differentiation in performance between them.

I have an idea for the upper problem, by using an action branching dueling Q network as described in [github](https://github.com/atavakol/action-branching-agents) with a shared representation for all the tasks. 

But I would like to be able to assign shared workers between tasks, if for example some task under utilize a worker or if the number of tasks are more than the workers.

So to summarize my question are:

1) Which is the best DRL algorithm for simply assigning workers to tasks without worker sharing?

2) Is there a way to actually be able to consider worker sharing when assigning workers to tasks?

Another idea I had was to just find how many workers each task needs with the use of DRL and if I need to share workers then I should find it without DRL with some greedy method.

Sorry for the long post and thank you very much in advance.",reinforcementlearning,kimonides9,False,/r/reinforcementlearning/comments/g9lutt/task_allocation_using_deep_reinforcement_learning/
"[R] ""State-only Imitation with Transition Dynamics Mismatch""",1588050828,"Method for efficient Imitation-learning when the expert and the learner environments are dissimilar (in transition dynamics function).

Paper: [https://arxiv.org/abs/2002.11879](https://arxiv.org/abs/2002.11879)

Code: [here](https://github.com/tgangwani/RL-Indirect-imitation)",reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/g9h475/r_stateonly_imitation_with_transition_dynamics/
SAC seems to be achieving optimal performance only when using a logarithmic reward,1588039631,"Hey guys. 


I'm trying to train an RL agent to pass data through a noise channel as accurately as possible, I won't go into details but the agent has two very simple actions and it can manipulate value cells on the other side of the channel. My goal currently is to get around a thousand cells to the same value. Anyway I'm using a score based reward but the reward for timestep t is:     the score for state t+1 - the score for state t. 


Here's the thing, the read noise on the channel is around 20 (max is 6000) which should be the optimal std that can be achieved by the agent. This optimal performance is achieved  **only** if I make the score logarithmic. If I use a linear score of just - L1 of the distance from the target it coverges to an std of around 200 (the starting std is like 5000). 


Has anyone encountered such a phenomenon with SAC? Because the L1 linear reward worked perfectly with a classic a2c model. But it seems that SAC reacts only to exponential/logarithmic rewards. My question is: is that normal, and if not is there a fix?",reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/g9eih6/sac_seems_to_be_achieving_optimal_performance/
[R] Self-Tuning Deep Reinforcement Learning,1588037673,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/g9dzw6/r_selftuning_deep_reinforcement_learning/
first visit v. every visit MC example,1588032415,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/g9cm8x/first_visit_v_every_visit_mc_example/
How to output a continuous action with a range of [0-1] in DDPG?,1588019605," Hi everyone, I know that DDPG USES the tanh activation function to output a continuous action in the range of \[-1-1\].But now in my question, my action is a percentage, so I need to change the action range to \[0-1\] to correspond to the percentage value.Can I change the activation function of the last layer of Actor to sigmoid?Because the output range of the sigmoid activation function is \[0-1\].  Is this the right approach?",reinforcementlearning,Toxicist,False,/r/reinforcementlearning/comments/g98sy6/how_to_output_a_continuous_action_with_a_range_of/
Meta reinforcement learning for assembly task,1588011938,"Hi everyone, 

I'm a PhD student from a Poland. I'm working on intelligent assembly process which is based on reinforcement learning. So, there problem is: robot is learning e.g. how to assemble electronic component A on product A'. And then he try to assemble component B on product B'. I don't want to relearn the agent for new product. So, my question is, can I treat each product as a separate task and use meta reinforcement learning? Or maybe this is overkill and better try to achieve some kind of generalization (e.g. domain randomization).

Greg",reinforcementlearning,Souphis,False,/r/reinforcementlearning/comments/g96b0k/meta_reinforcement_learning_for_assembly_task/
Hey Everyone! Tried writing a small introduction to Markov Decision Process. This is my first technical blog! Feedback and Suggestions would be greatly appreciated.,1588003289,,reinforcementlearning,mitesh1612,False,/r/reinforcementlearning/comments/g93gk0/hey_everyone_tried_writing_a_small_introduction/
How can a single sample represent the expectation in gradient temporal difference learning?,1587980481,"I was reading the gradient temporal difference learning version 2(GTD2) from rich Sutton's book page-246. At some point, he expressed the whole expectation using a single sample from the environment. But how a single sample can represent the whole expectation.

 I marked this point in this image. 

&amp;#x200B;

[GTD2 alogrithm](https://preview.redd.it/3rywamkkzbv41.png?width=714&amp;format=png&amp;auto=webp&amp;s=2bf9b052874e975524f39c23a5d91f9f956ab1b0)",reinforcementlearning,RLnobish,False,/r/reinforcementlearning/comments/g8xoqm/how_can_a_single_sample_represent_the_expectation/
How to combine discrete and continuous actions?,1587941315,"I'm trying to implement PPO with discrete and continuous actions combined.

All I found related to this was [this poor guy](https://www.reddit.com/r/MachineLearning/comments/c7tct4/d_reinforcement_learning_with_combined_continuous/) who couldn't find a solution.

Any ideas? Thank you.",reinforcementlearning,adkyary,False,/r/reinforcementlearning/comments/g8ob5j/how_to_combine_discrete_and_continuous_actions/
"Hey people! I recently learned about bandits and thought I'd make an interactive blog post explaining bandits. This is my first time making any sort of blog post, so all suggestions and ideas from you would certainly be of value to me. (would recommend a larger screen for best viewing experience)",1587940046,,reinforcementlearning,_yeah_i_reddit_,False,/r/reinforcementlearning/comments/g8nxt7/hey_people_i_recently_learned_about_bandits_and/
"Hey people! I recently learned about bandits and thought I'd make an interactive blog post explaining bandits. This is my first time making any sort of blog post, so all suggestions and ideas from you would be valuable to me.",1587939794,,reinforcementlearning,_yeah_i_reddit_,False,/r/reinforcementlearning/comments/g8nv3u/hey_people_i_recently_learned_about_bandits_and/
"Hey people! I recently learned about bandits and thought I'd make an interactive blog post explaining bandits. This is my first time making any sort of blog post, so all suggestions and ideas from you would be valuable to me.",1587939520,,reinforcementlearning,_yeah_i_reddit_,False,/r/reinforcementlearning/comments/g8ns72/hey_people_i_recently_learned_about_bandits_and/
Getting Started with DRL,1587931787,"Hey everyone, I just published a Medium story on getting started with Deep Reinforcement Learning. If you're new and would like some tips, go check it out!

[https://medium.com/@lukas.mautner98/getting-started-with-deep-reinforcement-learning-5e973fec1e28](https://medium.com/@lukas.mautner98/getting-started-with-deep-reinforcement-learning-5e973fec1e28)",reinforcementlearning,luk_dev,False,/r/reinforcementlearning/comments/g8lg9y/getting_started_with_drl/
Question on Exploiting Temporal Structure for reducing variance in policy gradient estimate,1587929759,"I was looking in retrospect at the reasoning behind dropping the past rewards in the policy gradient estimation step in  J. Peters, S. Schaal's paper \[ Reinforcement learning of motor skills with policy gradients \]( [https://is.tuebingen.mpg.de/fileadmin/user\_upload/files/publications/Neural-Netw-2008-21-682\_4867%5B0%5D.pdf](https://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/Neural-Netw-2008-21-682_4867%5B0%5D.pdf) ) equation (23) and trying to understand whether we can remove the reward from past while estimating future actions. I was trying to trace the steps of removing future rewards explicitly. Not sure if the equations 66 and 67 \[here\]( [https://imgur.com/a/9IF5wgZ](https://imgur.com/a/9IF5wgZ)) (Separate article, not the paper) are related by equality, or are they approximately equal? Can anybody explain why they might be equal or not.",reinforcementlearning,AvisekEECS,False,/r/reinforcementlearning/comments/g8ku5d/question_on_exploiting_temporal_structure_for/
Could you tell me some conferences or journals where Reinforcement Learning research is published?,1587925830,"And, if you'd like, a brief description of them. Thank you very much!",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/g8jl5a/could_you_tell_me_some_conferences_or_journals/
difference between off-policy and on-policy,1587914683,"I read sutton and barto's  book. So my question is what is difference between on-policy and off-policy methods(reason why i didn't get it was because of my poor english, or maybe they didn't mention enough examples).",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/g8g67j/difference_between_offpolicy_and_onpolicy/
Handling varying action spaces in RLLib,1587892823,"Hey,

Could anybody provide me some guidance on how to use the Ray RLLib with varying action space? Although I understand the ""purpose"" of [this link](https://docs.ray.io/en/latest/rllib-models.html#variable-length-parametric-action-spaces), I cannot quite understand how to apply it in my scenario.

At each step, I have a varying number of actions to choose from. Each action is encoded using continuous variables, i.e. is represented by some continuous values, however since there are only a limited number of possible actions, the action space is still discrete.

So far, in my own PyTorch environment, I have simply passed the encoded action as an input to my network, combined with the state. That is, my network receives input of size (action\_shape + state\_shape) and produces one output. I then iterated through the possible actions (basically, in practice I used a fully convolutional network...).

Does anybody have any experience of the RLLib that could provide me some guidance here? At the moment, I am thinking that I could encode the available actions into my state vector through some recurrent network, thus utilizing the link provided above -- but that would require training more networks.",reinforcementlearning,jakkes12,False,/r/reinforcementlearning/comments/g8beau/handling_varying_action_spaces_in_rllib/
RL tutorials for coding,1587891190,"are there any video tutorials about reinforcement learning for coding?i know Sendtex who teaches you how to code algorithms,are there any other youtube channels?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/g8b4f5/rl_tutorials_for_coding/
What does multiplying the value loss by 0.5 do?,1587877592,"I've been reading many A2C and PPO implementations to learn these algorithms.

People tend to multiply the value loss by 0.5 before update, here are some examples:

* value\_loss = 0.5 \* (return\_batch - values).pow(2).mean() 
* loss = actor\_loss + 0.5 \* critic\_loss
* vf\_loss = .5 \* tf.reduce\_mean(tf.maximum(vf\_losses1, vf\_losses2))

OpenAI's baselines PPO implementation even does it twice.

What does it do? Does it reduce variance?",reinforcementlearning,adkyary,False,/r/reinforcementlearning/comments/g88kky/what_does_multiplying_the_value_loss_by_05_do/
[R] Implementation Matters in Deep RL: A Case Study on PPO and TRPO,1587844832,,reinforcementlearning,PartiallyTyped,False,/r/reinforcementlearning/comments/g807zo/r_implementation_matters_in_deep_rl_a_case_study/
Generalized Policy Iteration,1587832151," what is Generalized Policy Iteration?i know value iteration, which is iterative algorithm taking the maximum value of adjacent states, also policy iteration, but don't know what the Generalized Policy Iteration is.sutton and barto is the material what i am studing now.Can you tell me what Generalized Policy Iteration is?",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/g7wdbv/generalized_policy_iteration/
ML/RL based control for a beginner,1587794611,"Hey guys, 

Do you suggest me some Reinforcement Learning/Machine Learning method based control? I am a beginner and I just would like to try things out, applying some method in a simple system. 

I have my own system so I do not think that the gym environment would help me. 

If you could also give me some code examples, Matlab or Python makes no difference! 

Thank you so much!",reinforcementlearning,alphack_,False,/r/reinforcementlearning/comments/g7olct/mlrl_based_control_for_a_beginner/
Policy_kwargs in stable baselines,1587770572,"Hello  

I would like to write a custom feedforward NN, which is just 2 layers of 64 nodes each of tanh activation function. However I am still learning stable baselines documentation and am a bit confused. Please refer to the following url for my question 

 [https://stable-baselines.readthedocs.io/en/master/guide/custom\_policy.html](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html) 

So in the first snippet of code, they train using PPO2 by passing ""MlpPolicy,"" which is 2 layers of 64 nodes (with the identity activation function). But they also pass policy\_kwargs which is 2 layers of size 32 each (with tanh activation function). So what exactly is the policy here and what is hyperparamater search in reference to with these 2 policies? I have tried training my model using similar code but the mean\_reward literally peaks at some constant value when it should go to like 500.  I am not sure what policy\_kwargs is referring to here

 [https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html)",reinforcementlearning,professor_oulala,False,/r/reinforcementlearning/comments/g7izqh/policy_kwargs_in_stable_baselines/
"""Efficient Adaptation for End-to-End Vision-Based Robotic Manipulation"", Julian et al 2020 {G}",1587764175,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/g7h6je/efficient_adaptation_for_endtoend_visionbased/
What are the current state of the art ways to increase exploitation for PPO?,1587753091,"Hi, 

I am running PPO on a 2 player connect4 Multi Agent environment. The agent plays against itself and every time it beats the average of the previous 50 agent checkpoints by 10% it will release a new checkpoint (which I call generation). After a few hundred generations/checkpoints the agent will be able to play at a high level, but will have totally forgotten how to play vs the first agents and lose vs really trivial strategies.

Now I know that this could be easily fixed by using population based training, but instead, I'd rather have a more exploitative agent. Just increasing the entropy coefficient doesn't really help, as over time the entropy loss will go down, and the same outcome will be produced (since the early strategies from the early models will basically never be replayed). 

So my question is: What are the current state of the art ways to increase exploitation for PPO?",reinforcementlearning,buildmeapcnyc,False,/r/reinforcementlearning/comments/g7ds37/what_are_the_current_state_of_the_art_ways_to/
My first attempt at RL with openAI gym,1587750108,,reinforcementlearning,Larsderoitah,False,/r/reinforcementlearning/comments/g7ctsa/my_first_attempt_at_rl_with_openai_gym/
Learning outcome changes,1587748826,"Hey everyone,

im doing my masters thesis currently and use PPO to solve my task( actions: continuous from 0 to 1, observation space 0 to +inf) 

Before spamming with all hyperparameter maybe there is a chance someone can interpret my problem:

I have the problem that I can’t reproduce a certain result. For example one day i train my agent and he learns what he is supposed to really well. Another day SAME hyperparameter etc. and he learns something else.. it’s not that he doesn’t learn something at all but not what I want and not what it learned on another day.. it seems arbitrary when he learns Solution 1 and when he learns solution 2. I use always the same seed...

Implementation is in tf2 

Any ideas why this may happen? I’m really clueless..
Thanks",reinforcementlearning,slippy_1993,False,/r/reinforcementlearning/comments/g7cf69/learning_outcome_changes/
Help for selecting the right rl-algorithm,1587735087,"I am currently working on my bachelor thesis. My goal is to develop a controller for a ship which is self learning. It should be able to receive a goal and drive to that goal using the propeller and bow thruster the ship is equipped with. I got the simulation in gazebo up and running (see image below). It is compatible with open ai gym so i can try out a wide variety of algorithms. My first naive guess was using qlearning (because I know it) but that was very unsuccessful. 

The state space is 6 dimensional and the action space is 2 dimensional. I choose the reward to be 1 if the ship reaches its goal. If the ship gets more than 5 meters away from the goal the learning episode ends and the reward is 0.

  
What learning algorithm would you recommend to try out? It should be not too advanced as I'm not that experienced in rl. 

https://preview.redd.it/nh81vxu0qru41.png?width=1049&amp;format=png&amp;auto=webp&amp;s=76c8ca6db3dabfec42c226f59414000f06d67a3f",reinforcementlearning,The-Lemon-Chicken,False,/r/reinforcementlearning/comments/g78f2f/help_for_selecting_the_right_rlalgorithm/
help for RL,1587724347," **I started learning reinforcement learning, I have questions but unfortunately I don't know anyone to ask questions.Can you suggest RL platforms? Math exchange is for math, Stack Overflow is for software.What can you suggest?**",reinforcementlearning,datonefaridze,False,/r/reinforcementlearning/comments/g763oj/help_for_rl/
Getting Started on Multi-Agent RL,1587714381,"Hoping to get into MARL in games, particularly something like board games that don’t require image/pixel classification. Anyone got recommendations on how to get started? Thanks all!",reinforcementlearning,ubermensch-56,False,/r/reinforcementlearning/comments/g749zl/getting_started_on_multiagent_rl/
Unity ML Tennis environment,1587713959,"I am trying out running MADDPG on tennis environment. I am wondering how can I see the code of this environment, I mean the step(), reset() functions to understand how the rules are defined.",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g7474w/unity_ml_tennis_environment/
Doubt in MADDPG environment design,1587699828," 

Hello all. I am new to reinforcement learning. But over the past 4 months, I have learnt all the algorithms of RL. **I was given a task to design a multi-agent system especially using MADDPG**. I will give the details of what I have designed so far.

1. Total no of agents = 3
2. Continuous actions \[-1 to 1\] and discrete observation space (3) \[x,y,z\] for each agent.
3. Each agent A, B, C will have **its own rules, environment.**
4. **Observation space of A = \[input from B, input from C, specific to A\]**  
**B = \[input from C, input from A, specific to B\]**  
**C = \[input from A , input from B, specific to C\]**
5. Depending on the action A takes, some value from A goes to B and C. Similarly for B, C.

&amp;#x200B;

[This is the outline of the environment](https://preview.redd.it/f9x1dru5tou41.jpg?width=576&amp;format=pjpg&amp;auto=webp&amp;s=f4e905362750e720393934cafd16b6b2c4a7977f)

 

I have developed individual environments for A, B, C. When tested individually on random inputs, the agents all performing correctly (taking actions which are expected). How can I design the agents to **take inputs from other agents to form its own observation space?**

Please advice. Thank you.

PS: If my explaining is not clear, please comment, I will clearly explain.",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g71ajd/doubt_in_maddpg_environment_design/
need some advice for training DQNs,1587689537,"I've heard that a properly trained dqn network with experience replay can take upwards of 1 million steps to reach an optimal and functioning algorithmic state. 

But with training my own network I'm puzzled by the fact that my algorithm plateaus with rewards very early on, under 100_000 steps. From then on it stagnates and gets worst even if i leave it training to 1 million steps.

I've experimented with learning rate but there always seems to be an odd plateau followed by nothing but worst. Is this typical for DQNs?",reinforcementlearning,Qtbby69,False,/r/reinforcementlearning/comments/g6yt9z/need_some_advice_for_training_dqns/
"""Chip Placement with Deep Reinforcement Learning"", Mirhoseini et al 2020 {GB}",1587688975,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/g6yo0p/chip_placement_with_deep_reinforcement_learning/
"The RL Contest: Threadripper vs. the Cloud, Setup and Benchmarks",1587682753,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/g6wy6b/the_rl_contest_threadripper_vs_the_cloud_setup/
PPO and learning rate,1587667978,"I'm training agents with PPO to play a game against another AI. I've been using curriculum learning, increasing the complexity of the task each time. But now my agents are playing against pretty advanced AI, and I feel they are not improving much (there is room for improvement). The accumulated rewards between episodes do not gradually increase, but are kind of unstable (part due to variance in opponent behavior) and I can't see any real upwards trend anymore.   


So my question is, does it make sense to decrease learning rate as you introduce more complexity to the problem?",reinforcementlearning,Saint_Meowingtons,False,/r/reinforcementlearning/comments/g6sg39/ppo_and_learning_rate/
10 Must-Read AI Books in 2020,1587657370,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/g6p70y/10_mustread_ai_books_in_2020/
A2C to DDPG,1587635635,"I have an environment with discrete action and observation spaces.  Actions are 0,1,2 

It worked perfectly on A2C. I wanted to extend it to DDPG which accepts only continuous actions. So I have **binned the action values** as  -1.0 to -0.33 = **Action 0**, -0.33 to 0.33 = **Action 1**, 0.33 to 1 = **Action 2**. But I think this doesn't seem to work. These are the graphs

**A2C**

https://preview.redd.it/luaq6he0iju41.png?width=432&amp;format=png&amp;auto=webp&amp;s=8ea2b105bbfbf25f62dbaff146a7a94365f26a9b

**DDPG**

&amp;#x200B;

https://preview.redd.it/hym5mva3iju41.png?width=384&amp;format=png&amp;auto=webp&amp;s=8b9a4cdd1b50352316a7e245ae7e085d776c67f6

Can someone explain why isn't it working? What changes do I need to consider?

Thank You",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g6jwf2/a2c_to_ddpg/
Observation space of OpenAI Gym Continuous Lunar Lander - v2 environment,1587620877,"I am working on solving OpenAI Gym's Continuous Lunar Lander - v2 environment using DDPG. The observation space consists of 8 values. But I couldn't understand what are those 8 values. Can someone explain what do they exactly stand for?

&amp;#x200B;

![img](qihhootdaiu41)",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g6h7x6/observation_space_of_openai_gym_continuous_lunar/
Deep Deterministic policy gradient implementation in TF 2.0,1587587348,"Hi guys,

I have implemented DDPG on TensorFlow 2.0.  The current code is written for the 'pendulum-v0' in the gym environments. It's working fine and can easily be extended to other similar environments. I still have to add comments. You can find the codes here:

&gt;!https://colab.research.google.com/drive/1VDgQMSTdN-p31DdyF6ezpmJgGxBEMkxy!&lt;

or 

&gt;! https://github.com/asokraju/Rienforcement-learning-Algorithms/tree/master/DDPG !&lt;

Any suggestions or comments are welcome.

&amp;#x200B;

PS: I followed closely the code of  [https://github.com/pemami4911/deep-rl/blob/master/ddpg/ddpg.py](https://github.com/pemami4911/deep-rl/blob/master/ddpg/ddpg.py)",reinforcementlearning,asokraju,False,/r/reinforcementlearning/comments/g68qsh/deep_deterministic_policy_gradient_implementation/
Mujoco or PyBullet from images?,1587585993,"I am trying to collect all the RL algorithms that solve Mujoco (or PyBullet) default tasks (HalfCheetah, Ant, Walker, Hopper, Humanoid) from image observations.

As of now, I can find these model free algo:

* DDPG [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)
* SLAC [https://arxiv.org/abs/1907.00953](https://arxiv.org/abs/1907.00953)
* D4PG [https://arxiv.org/abs/1804.08617](https://arxiv.org/abs/1804.08617)

And model-based:

* PlaNet [https://arxiv.org/abs/1811.04551](https://arxiv.org/abs/1811.04551)
* Dreamer [https://arxiv.org/abs/1912.01603](https://arxiv.org/abs/1912.01603)

Which other papers use high-dimensional image renderings of the environment to solve these tasks? 

I am sure there are more and will update the post with the ones that get proposed in the comment.",reinforcementlearning,jeeeen,False,/r/reinforcementlearning/comments/g68buo/mujoco_or_pybullet_from_images/
How to code a Multi Objective DQN agent?,1587566742,Could anyone guide me through on coding a custom MODQN agent with torch? The only code sample I find available online is [https://github.com/ttajmajer/morl-dv](https://github.com/ttajmajer/morl-dv) . Is there anyone experienced enough to walk me through the steps? Thanks!,reinforcementlearning,ltbd78,False,/r/reinforcementlearning/comments/g62di8/how_to_code_a_multi_objective_dqn_agent/
"Working on a POMDP problem, and used value iteration to solve it. The utility function that is returned, has all the same values for all the actions. What can be done to fix it? What other algorithms can be used?",1587545186,I am using AI:MA as reference for the algorithm on pomdp value iteration.,reinforcementlearning,WorldDomIsFun,False,/r/reinforcementlearning/comments/g5xlim/working_on_a_pomdp_problem_and_used_value/
Deep Q Network vs REINFORCE,1587534540,"I have an agent with discrete states and action spaces. **It always has a random start state when env.reset() is called.**

Now I have tried this algorithm on **Deep Q Learning** and the rewards have significantly increased and the agent learned correctly.

https://preview.redd.it/72p4bdg45bu41.png?width=432&amp;format=png&amp;auto=webp&amp;s=9c210b431f10536de11393f2bc653a892ae199f6

**REINFORCE:** I have tried the same on REINFORCE, but there is no improvement in the rewards. 

https://preview.redd.it/kxksngig5bu41.png?width=397&amp;format=png&amp;auto=webp&amp;s=be9be196099ed432eebf5cc9b5115f1e130a4a73

Can someone explain why is this happening? Does my environment properties suit Policy gradients or not? 

Thank You.",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g5vlt2/deep_q_network_vs_reinforce/
Understanding the W term in off policy monte carlo learning,1587527321,"In Sutton and Barto's RL textbook they included the following pseudocode for off policy monte carlo learning. I am a little confused, however, because to me it looks like the W term will become infinitely large after a couple thousand iterations. 

For example, say that the MC algorithm always follows the behavioral policy for each episode (ignoring epsilon soft/greedy for examples sake). If the probability of the action specified by the policy is 0.9, then after 10,000 iterations W would have a value of 1.11\^10,000. Clearly I am misunderstanding something. 

&amp;#x200B;

https://preview.redd.it/nxhrbji0kau41.png?width=1408&amp;format=png&amp;auto=webp&amp;s=a5413fae2454b2514347dc987ce008261105531b",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/g5u3j2/understanding_the_w_term_in_off_policy_monte/
Using model-based method to turn Actor Critc to off-policy algorithm,1587523675,"I'm thinking that the memory replay of DQN only saves {s, a, r, s'}, which essentially has nothing to do with policy, but the model itself (i.e. p(s', r | s, a)). However, the actions {a} are selected based on epsilon-greedy, which is the expected optimal policy. If we insert those values into Q function, the result will therefore depend on policy, because Q not only depends on transition matrix but also policy. That's the reason why we can't apply such trick to on-policy methods like Actor Critic.

However, if we no longer use those memorys to calculate Q, instead, we build a model and use them to infer the model p(s', r | s, a), there should be no problem. Such model can be a neural network, which has {s, a} as input and the probability distribution of {s', r} as output. We can sample lots of ""simulated"" experience according to such probability distribution to help evaluate Q, given the current policy. And then apply policy gradient to improve policy, just like normal Actor Critic. 

Does it make sense? Is there any current algorithm that has already applied such approach?",reinforcementlearning,wsjyhaozi1,False,/r/reinforcementlearning/comments/g5t8tk/using_modelbased_method_to_turn_actor_critc_to/
"[D] ""Deep Reinforcement Learning in Action"" book by Alexander Zai and Brandon Brown",1587514190,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/g5qwd9/d_deep_reinforcement_learning_in_action_book_by/
[R] A hypothesis that the Federal Reserve can set interest rates based on the movements of the planet Mars. Here I have data going back to 1896 that shows how the Dow Jones performed when Mars was within 30 degrees of the lunar node. (- from appendix of Ares Le Mandat 4th ed),1587511719,,reinforcementlearning,thedowcast,False,/r/reinforcementlearning/comments/g5q82u/r_a_hypothesis_that_the_federal_reserve_can_set/
Which PUCT formulation does AlphaZero use?,1587503112,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/g5nos5/which_puct_formulation_does_alphazero_use/
"""Specification gaming: the flip side of AI ingenuity"", Krakovna et al 2020",1587499495,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/g5mjg3/specification_gaming_the_flip_side_of_ai/
"[R] Dimitri Bertsekas: ""Distributed and Multiagent Reinforcement Learning""",1587495229,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/g5l63v/r_dimitri_bertsekas_distributed_and_multiagent/
"Confused about TRPO ""old policy"" and ""new policy""",1587493155,"I know that TRPO is an on-policy algorithm, where the transitions used for learning are obtained by the same policy that is being optimized. Then, what exactly is the ""old policy"" and the ""new policy"" when computing the surrogate loss or the KL divergence?

I've seen a couple of TRPO  implementations (such as [ikositrov's](https://github.com/ikostrikov)) that use [old\_policy = new\_policy](https://github.com/ikostrikov/pytorch-trpo/blob/master/main.py) (see line 122-129) for computing the KL divergence. Wouldn't this always yield D\_KL = 0, and hence making the fisher vector product always zero out? What am I missing?",reinforcementlearning,cyoon1729,False,/r/reinforcementlearning/comments/g5ki6c/confused_about_trpo_old_policy_and_new_policy/
Servo motor modeling in Python for RL based robot,1587491978,"I want to implement a simple robot with RL. I have done some work in robotics in the past. So I was wondering how can I model the servo motor in python for my robot? I want to make the env from scratch with Pybullet and gym. Any help will be appreciated. Thank you.

(I don't have any special robot in my mind, but I am thinking of implementing simple walkers like from star wars ;) )",reinforcementlearning,Scarlet_22,False,/r/reinforcementlearning/comments/g5k4ih/servo_motor_modeling_in_python_for_rl_based_robot/
Robotics with RL,1587477407,"Hello everyone, I have recently started off with RL. I wanted some kind of a roadmap for RL in Robotics? Are there any good tutorials or repositories I can watch closely?",reinforcementlearning,aditya_074,False,/r/reinforcementlearning/comments/g5fnxl/robotics_with_rl/
Start state for a Policy gradient algorithm REINFORCE,1587475936,Is it necessary that the start state must always be the same while training using policy gradient REINFORCE algorithm?,reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g5f9sj/start_state_for_a_policy_gradient_algorithm/
MADDPG for a beginner. Please suggest.,1587472751,"Hello all. I am new to reinforcement learning. But over the past 4 months, I have learnt all the algorithms of RL.  **I was given a task to design a multi-agent system especially using MADDPG**. I will give the details of what I have designed so far.

1. Total no of agents = 3
2. Discrete actions (3) \[0,1,2\] and discrete observation space (3) \[x,y,z\] for each agent.
3. Each agent A, B, C will have its own rules, environment.
4. Observation space of A = \[input from B, input from C, specific to A\]  
B = \[input from C, input from A, specific to B\]  
C = \[input from A , input from B, specific to C\]
5. Depending on the action A takes, some value from A goes to B and C. Similarly for B, C. 

I have developed individual environments for A, B, C.  When tested individually on random inputs, the agents all performing correctly (taking actions which are expected). But as MADDPG uses DDPG; which only works on continuous actions, I am confused how I can apply MADDPG in this case. And also how can I design the agents to take inputs from other agents to form its own observation space.  

Please advice. Thank you.

PS: If my explaining is not clear, please comment, I will clearly explain.",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g5egtl/maddpg_for_a_beginner_please_suggest/
DDPG for discrete actions ?,1587463707,"I have an environment with discrete states and discrete actions. Can I use DDPG on that? If not, then how can I make the changes in the environment to make it run with DDPG?

Thanks in advance for your answer",reinforcementlearning,parshu018,False,/r/reinforcementlearning/comments/g5cjzh/ddpg_for_discrete_actions/
Creating a Custom OpenAI Gym Environment for reinforcement learning!,1587459269,,reinforcementlearning,Mingoooose,False,/r/reinforcementlearning/comments/g5bquq/creating_a_custom_openai_gym_environment_for/
Bellman equation for continuous state space MDP,1587457825,Can someone please point me to papers which talk about MDP with continuous state space. How do we represent transition function and what is the corresponding Bellman update equation there?,reinforcementlearning,Busy_Stranger,False,/r/reinforcementlearning/comments/g5bhbz/bellman_equation_for_continuous_state_space_mdp/
Sparse reward signal effect on discounted return,1587438971,"I am using TRPO algorithm in Garage. Please forgive me if I misunderstand something as I am a beginner. In my environment I am giving a sparse negative reward for running too long, however this reward only gets applied at the end of the episode and thus the discounted reward is very small: reward is -10, effect on discounted reward is ~-1e-14. I plan on introducing other reward signals in the environment and I am worried they will override the effect of this run too long reward signal. Is there a practical way I can implement a run too long reward? I feel as though setting a -1e+14 reward for running too long and other rewards in the order of magnitude of -10 to 100 throughout the episode seems a bit odd.
 
Picture of graph here:

https://imgur.com/a/x2maAWm


Also note that the StdReturn is going to zero, because the environment is taking more and more steps to finish (the scenario i am trying to avoid) and only 1 trajectory is fitting into my batch size.",reinforcementlearning,alek5k,False,/r/reinforcementlearning/comments/g57jj8/sparse_reward_signal_effect_on_discounted_return/
Breakout at various stages of training (code and video link in comment),1587437807,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/g578qt/breakout_at_various_stages_of_training_code_and/
[R] Real World Games Look Like Spinning Tops,1587433467,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/g563jz/r_real_world_games_look_like_spinning_tops/
"tanh activation for action [-1, 1]?",1587422524,"Trying to implement DDPG(Actor-Critic in the continuous action space) in half-cheetach.

If the action space is continuous and the range is \[-1, 1\], the state is non-image state which is compact to make actions. I saw some actor network use ReLU as the activation function in the last Dense layer, but I changed it to **tanh** due to the fact that tanh outputs exact to the range of \[-1, 1\]. Is this correct?

In the hidden layers, I simply use Dense in Keras (fully-connected), what activation function should it be? A typical choice is 'ReLU', but it truncates the negative parts. 'elu' could be better but I wanted to remain the information in the negative parts. So is it OK to use all '**tanh**' activations in the hidden layers?

Any suggestion would be appreciated.",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/g53340/tanh_activation_for_action_1_1/
How should I approach learning RL,1587398436,"Hello, I am a 17 year old high school student. I have been learning about AI on a surface level for about half a year now(both pracitcal and theoretical). I have above average level in math due to various mathemtaical competitions and I'm willing to learn higher math for RL. My question is, what topics should I cover in what order to be able to make some RL agent for easy games (I'm thinking snake, mario etc.). Thanks in advance",reinforcementlearning,MedicaLatina,False,/r/reinforcementlearning/comments/g4vfpv/how_should_i_approach_learning_rl/
R] Knowledge-guided Deep Reinforcement Learning for Interactive Recommendation,1587393332,"**Abstract:**  Interactive recommendation aims to learn from dynamic interactions between items and users to achieve responsiveness and accuracy. Reinforcement learning is inherently advantageous for coping with dynamic environments and thus has attracted increasing attention in interactive recommendation research. Inspired by knowledge-aware recommendation, we proposed Knowledge-Guided deep Reinforcement learning (KGRL) to harness the advantages of both reinforcement learning and knowledge graphs for interactive recommendation. This model is implemented upon the actor-critic network framework. It maintains a local knowledge network to guide decision-making and employs the attention mechanism to capture long-term semantics between items. We have conducted comprehensive experiments in a simulated online environment with six public real-world datasets and demonstrated the superiority of our model over several state-of-the-art methods. 

**Link**:  [https://arxiv.org/pdf/2004.08068v1.pdf](https://arxiv.org/pdf/2004.08068v1.pdf)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/g4twjo/r_knowledgeguided_deep_reinforcement_learning_for/
A Biggeners guide,1587387231,"Hey guys, Im new into the feild (Self studying no univerity) , really intrested to go forward what I woould really like from you guys is what are the essentials to understand reinforcement learning ? like what's the math that I need , good refrences ?  Would be a massive help.",reinforcementlearning,Farouqqq,False,/r/reinforcementlearning/comments/g4s8k1/a_biggeners_guide/
Soft Hindsight Experience Replay implementation,1587376365,"I was wondering if anyone has tried implementing Soft Hindsight Experience Replay (SHER), or if you know of an implementation on Git (I have tried googling it, but can't find any)? Looking at the algorithm in [the paper](https://arxiv.org/abs/2002.02089), I don't really understand how the update to the Q-functions' parameters are being done.",reinforcementlearning,Deadsocks,False,/r/reinforcementlearning/comments/g4pywf/soft_hindsight_experience_replay_implementation/
"Let's Catch 'Em All: What are reasons for the poor sample efficiency of on-policy RL methods, besides not being able to reuse experiences collected from other policies?",1587373361,Let's collect possible reasons in separate answers to discuss individual points in isolation.,reinforcementlearning,ml_keychain,False,/r/reinforcementlearning/comments/g4penl/lets_catch_em_all_what_are_reasons_for_the_poor/
HER with penalty,1587367318,"Hello, I am student in robotics and recently I started studying reinforcement learning. I came a cross HER algorithm, and I wsnt to know if anyone have tried changing the sparse reward and still manage to train an agent?

What I am trying to achieve using HER is to give some penalty to the agent when robot configuration gets close to singularity. Would that count as reward shaping?

In papre they showed that shaped reward gives worse results than sparse. So what about adding penalty to some crucial actions?

Thank you in advance.",reinforcementlearning,Dexter_fixxor,False,/r/reinforcementlearning/comments/g4o9hb/her_with_penalty/
Deep RL from Scratch: Part III,1587365294,,reinforcementlearning,uncountably-infinite,False,/r/reinforcementlearning/comments/g4nuyn/deep_rl_from_scratch_part_iii/
Can DDPG be deterministic on a timeserie ?,1587347530,"Hi guys,

ML enthusiast and practitioner on my spare time, please share your experience with me :)

The scenario is very simple:

\- DDPG trained on a timeserie data sample we'll call D1, saved into a ZIP-ed model

\- DDPG evaluated from the ZIP-ed model, on a different timeserie data sample we'll call D2

\- As long as I keep D2, the successive evaluations I make from my trained model end up with the same (and very satisfying) predictions. So, all good, so far, this is deterministic, right ?

&amp;#x200B;

Now, here is the challenge:

\- I add few hours to D2, so now the it's like evaluating my ZIP-ed model on D3 = D2 + 10 hours of data input 

\- I have new predictions for the new 10 hours, of course, but some action/prediction of the model also change for the some hours at the end of D2 (which is now part of D3)

&amp;#x200B;

Is this ""as designed"" ? If possible, I would like my trained model to stay consistent with its predictions of a prior timestamp, even if there are now new timestamps to be evaluated

&amp;#x200B;

Thank you in advance for your feedbacks :)

Stay safe &amp; learn ML ! ;)",reinforcementlearning,PoCk3T,False,/r/reinforcementlearning/comments/g4k12f/can_ddpg_be_deterministic_on_a_timeserie/
What is the specific variables for delta J in step 4 in Actor Critic in Keras?,1587340966,"What is the specific variables for delta J in step 4 in Actor Critic in Keras?

https://preview.redd.it/a1okw6q16vt41.png?width=1288&amp;format=png&amp;auto=webp&amp;s=425fdc34c279581ae75a41891c3709777db79998

Thanks in advance!",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/g4id43/what_is_the_specific_variables_for_delta_j_in/
Getting better than the sub-optimal expert with inverse RL,1587333413,"In the 7. lecture of CS234 prof. Brunskill says, that Sergey Levine and others has done some work on getting better policy then the sub-optimal demonstrator: [https://youtu.be/V7CY68zH6ps?t=4284](https://youtu.be/V7CY68zH6ps?t=4284)

Do you know the works which would describe such approaches of getting better than demonstrator? I have found only [https://arxiv.org/abs/1907.03976](https://arxiv.org/abs/1907.03976) or [https://arxiv.org/abs/1904.06387](https://arxiv.org/abs/1904.06387) from the same group (not Sergey Levine).",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/g4gaud/getting_better_than_the_suboptimal_expert_with/
Methods which can solve lunar lander?,1587301332,Can someone point me to papers which have good results on lunar lander with sparse rewards? Probably in the domain is exploration algorithms.,reinforcementlearning,asdfwaevc,False,/r/reinforcementlearning/comments/g47b6w/methods_which_can_solve_lunar_lander/
"Is Actor-Critic the ""best"" RL algorithm?",1587288670,"Hello, I've just finished David Silver's Youtube lecture 7. It seems Actor Critic algorithm is a combination of value based algorithm and policy gradient, by training two function approximation for them respectively. Both of the functions can be neural networks. In fact if we cancel the value function, it becomes a naive policy gradient, while if we replace the policy function by epsilon-greedy, it becomes Q-learning. 

I'm not sure if my understanding is correct, but if it's true, does it mean we can give up algorithms like DQN or pure policy gradient method, while improving actor critic by modifying two function approximators respectively?",reinforcementlearning,wsjyhaozi1,False,/r/reinforcementlearning/comments/g44wg9/is_actorcritic_the_best_rl_algorithm/
Actor Critic in Atari games in Gym in Keras?,1587250852,How to write and train actor-critic in Gym n Keras?,reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/g3wxda/actor_critic_in_atari_games_in_gym_in_keras/
Unit Neurons (C++ Neural Net Library): Update with rough implementation of PPO,1587246627,"A couple of months ago, I have posted reddit post on a repository for a C++ static library that I have made in XCode called Unit Neurons, where we attempt to look at and build neural network architectures by combining neurons with different feedforward and feedback functions:  
[https://www.reddit.com/r/DecisionTheory/comments/f6u55t/unit\_neurons\_neural\_networks\_as\_complex\_systems/](https://www.reddit.com/r/DecisionTheory/comments/f6u55t/unit_neurons_neural_networks_as_complex_systems/)

Since then I have implemented Kohonen's self organizing map and stochastic gradient descent using mainly 3 types of neurons: input/output neurons, feedforward-gradient descent neurons, and neighboring neurons. You will be able to combine these 3 neurons to build your own neural networks.

A rough outline of Proximal Policy Optimization implementation is also included in the examples for application in control problems.

Unit Neurons Github Repository:  
[https://github.com/johnlime/UnitNeurons](https://github.com/johnlime/UnitNeurons)

As stated in the previous post, I'll keep working on the library to the best of my abilities in the future as well.",reinforcementlearning,johnlime3301,False,/r/reinforcementlearning/comments/g3vspo/unit_neurons_c_neural_net_library_update_with/
I think there's a similarity between how Reinforcement Learning is now and how Physics was a century ago,1587238983,"When I studied Calculus at uni I was amazed by humanity's ingenuity, and thought to make any sort of contribution even a genius would need decades of practice. In contrast, I was unimpressed with the state of AI now. Most of the things are simple, and people without much background can contribute to the field. 

I think it's in some ways similar to how physics was a century ago. Einstein was 26 when he published papers that turned the world of physics on its head. Doing something like that in Physics today would be much harder. However, it feels possible with the state of AI nowadays.

A notable difference though is that many top companies and leaders pour a lot of resources into the field of AI, which wasn't so much that in Physics a century ago.",reinforcementlearning,kakushka123,False,/r/reinforcementlearning/comments/g3tngk/i_think_theres_a_similarity_between_how/
What is wrong with my RL modelling?,1587238315,"Hi, I've written a document about a problem, I'm trying to solve it using RL. In the document I've defined the action, state and reward I use for my RL model: [Link-To-Problem](https://drive.google.com/file/d/1QIgjEUj1AT4tIfCyHi2nYbTL0gVmd68p/view?usp=sharing).

I've tried several approaches:, DQN, DDQN, none of them are working, QLearning works only for very, very small parameters. I've let the algorithm train for days with almost no improvement, mainly because the action space is huge (order of millions), and in my modelling the number of legal actions is not constant over time (i.e. at T=1, there is only one valid action, and at T=4, there can be 100 valid actions). This makes me think that the way I've modeled the action space and the observation space is not suitable, so I'm really asking for your help.

Have a nice day!",reinforcementlearning,redeyestorm99,False,/r/reinforcementlearning/comments/g3tgn5/what_is_wrong_with_my_rl_modelling/
[D] Teaching machines to behave: Reinforcement Learning,1587224750,"Here is my latest article on Reinforcement Learning, where you'll learn how algorithms can learn to behave based on their own experience.

[https://medium.com/ai-society/rl-4-all-1-3edac941fe37](https://medium.com/ai-society/rl-4-all-1-3edac941fe37?fbclid=IwAR2vRecdm8jpFNH2Z6KhK6IUndAxnCtkwMJKDTxVQkUMXuHCo1cMS0XaVT0)

Algorithms described here learn have learnt to beat the greatest masters of games like Chess, GO. Feel free to discuss with me if you have any doubts.

&amp;#x200B;

[A group of blue agents interacting with the environment. Their goal is to avoid being directly seen by the red agents. Available at: https:\/\/openai.com\/blog\/emergent-tool-use\/](https://preview.redd.it/5xp3q1zjklt41.png?width=640&amp;format=png&amp;auto=webp&amp;s=b59f927469fb93260fee23397699ed8a03e2a61d)",reinforcementlearning,diegoalejogm,False,/r/reinforcementlearning/comments/g3pl9r/d_teaching_machines_to_behave_reinforcement/
Convex theory and RL,1587209517,"Dear all, can anyone explain the relationship between convex theory and RL, are all RL problems convex problems? If so what specific convex problems do RL problems belong to?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/g3m2ty/convex_theory_and_rl/
Why rl papers use fully connected layer only?,1587195848,"Hi. I'm a graudate student studying Rl. 
As far as I know, RL algorithms usually use two hidden layers(200~300), Relu activation(Except Recurrent rl). Papers I have seen didn't vary much from this. Is there any new approach to Rl neural network?",reinforcementlearning,Cerphilly,False,/r/reinforcementlearning/comments/g3jqwa/why_rl_papers_use_fully_connected_layer_only/
Multi Armed Bandits problem,1587172130,"Hi everyone I had 2 doubts?

1. Multi Armed Bandits are single state MDPs. What's the single state?
2. In Reinforcement Learning: An introduction - Sutton can someone explain the mathematical reasoning behind sampling true action value from a Normal distribution N (0,1) &amp; then sampling  the true reward values from the Normal distributionN (True ith action value, 1) ?

Thank you in advance.",reinforcementlearning,Noblesse_Coder,False,/r/reinforcementlearning/comments/g3eodg/multi_armed_bandits_problem/
What is your favorite textbook on Deep Reinforcement Learning (other than Sutton &amp; Barto's standard work)?,1587151034,,reinforcementlearning,xyzen420,False,/r/reinforcementlearning/comments/g38iln/what_is_your_favorite_textbook_on_deep/
[Discussion] Improving exploration in PPO by adjusting the probabilities at the acting stage,1587142573,"I have implemented a small change in my distributed PPO code that seems to show success in encouraging exploration in an effective manner. It also has the added benefit of removing entropy from the equation (which is hard to balance).

Since PPO is not really on-policy and can make use of slightly older samples it is possible to use a distributed PPO approach with one learner where all the trajectories are fed-in asynchronously. As used [here](https://arxiv.org/abs/1707.02286) for example, or even in the OpenAI 5 DOTA paper [here](https://cdn.openai.com/dota-2.pdf). Furthermore, in order to compute the new gradients for the policy, we really only need action, (log-)probabilities when the action was taken and (GAE-)reward for each step in a trajectory.

Given this it seems plausible that you can adapt the probabilities right at the time when the agents/workers collect them. In my case, I am changing the probabilities for each agent, such that it is slightly more likely to pick an action with lower probability, and slightly less likely to pick an action with higher probability. In practice it looks like this (in PyTorch):

    log_probs, state_value = self.model(state)

    # BEGIN DISPERSION CODE
    dispersion = 0.1
    worker_dispersion = worker_id / (n_workers - 1) * dispersion
    adj_log_probs = log_probs / (1 + worker_dispersion)
    # END DISPERSION CODE

    dist = Categorical(logits=adj_log_probs)
    action = dist.sample()
    action_log_prob = dist.log_prob(action)

Note that same can be achieved in different ways, e.g. by having the model outputting the probabilities directly and adapting those, as opposed to the logits, however, this was the most straightforward that still showed promise.

The result of that change is that each agent has a small consistent bias from the primary policy (worker_id=0) leading to much better exploration in the process. Due to the added exploration the primary policy does not seem to get stuck in 100% probability actions like it normally would if the entropy_coefficient is zero. Consequently, entropy can be removed as parameter altogether.

I tested this on sparse reward problems (MountainCar), as well as SeaQuest (which isn't really sparse, but finding the ""trick"" to emerge from the water is very difficult with PG methods), and in both cases it seemed to significantly improve performance, similar to adding Random Network Distillation (except it was faster in wall-time, but similar in steps).

What do you guys think about this?",reinforcementlearning,tmuxed,False,/r/reinforcementlearning/comments/g35u2a/discussion_improving_exploration_in_ppo_by/
[R] A Game Theoretic Framework for Model Based Reinforcement Learning,1587136625,"**Abstract:**  Model-based reinforcement learning (MBRL) has recently gained immense interest due to its potential for sample efficiency and ability to incorporate off-policy data. However, designing stable and efficient MBRL algorithms using rich function approximators have remained challenging. To help expose the practical challenges in MBRL and simplify algorithm design from the lens of abstraction, we develop a new framework that casts MBRL as a game between: (1) a policy player, which attempts to maximize rewards under the learned model; (2) a model player, which attempts to fit the real-world data collected by the policy player. For algorithm development, we construct a Stackelberg game between the two players, and show that it can be solved with approximate bi-level optimization. This gives rise to two natural families of algorithms for MBRL based on which player is chosen as the leader in the Stackelberg game. Together, they encapsulate, unify, and generalize many previous MBRL algorithms. Furthermore, our framework is consistent with and provides a clear basis for heuristics known to be important in practice from prior works. Finally, through experiments we validate that our proposed algorithms are highly sample efficient, match the asymptotic performance of model-free policy gradient, and scale gracefully to high-dimensional tasks like dexterous hand manipulation. 

Research link:  [https://arxiv.org/abs/2004.07804v1](https://arxiv.org/abs/2004.07804v1) 

PDF link:  [https://arxiv.org/pdf/2004.07804v1.pdf](https://arxiv.org/pdf/2004.07804v1.pdf)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/g33y18/r_a_game_theoretic_framework_for_model_based/
Innovation and Reinforcement Learning,1587119213,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/g2zeqr/innovation_and_reinforcement_learning/
[R] Importance of using appropriate baselines for evaluation of data-efficiency in deep reinforcement learning for Atari,1587105570,,reinforcementlearning,hardmaru,False,/r/reinforcementlearning/comments/g2wn6l/r_importance_of_using_appropriate_baselines_for/
Deep RL from Scratch: Part II,1587102817,,reinforcementlearning,uncountably-infinite,False,/r/reinforcementlearning/comments/g2w2cg/deep_rl_from_scratch_part_ii/
Deep Neural Nets from Scratch: Part 2,1587102495,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/g2vzt0/deep_neural_nets_from_scratch_part_2/
What is stopping us from using RL for molecule generation for creating drugs against COVID-19?,1587078218,"In GCPN: Graph Convolutional Policy Network by You et al. (2018), the authors frame a Markov Decision Process for creating molecules with desired properties. So what are the difficulties in using this for creating a vaccine against COVID-19? Is it that we cannot come up with a proper reward structure to get the desired properties or something else?

Link to the GCPN paper:  [https://arxiv.org/pdf/1806.02473.pdf]( https://arxiv.org/pdf/1806.02473.pdf)",reinforcementlearning,nsidn,False,/r/reinforcementlearning/comments/g2pvxx/what_is_stopping_us_from_using_rl_for_molecule/
Understanding exploration in TRPO,1587067227,"Hi, something I'm having a hard time understanding is how TRPO explores its environment, as we constrain the next policy in the sequence to deviate some small value from the old policy. It seems like to begin with, our value functions are all initialized to roughly the same values, giving us an almost uniform policy, and then as we explore and tune our value functions more, our policy follows suit and becomes much more imbalanced, leading TRPO to get stuck in local optima, as we'd have to successively do something suboptimal to explore for quite a few iterations to get out of the local optima (which is very unlikely).

If this is the case, why don't we occasionally restart the search once we reach a local optima? Or, am I not understanding something about TRPO?",reinforcementlearning,OriginalMoment,False,/r/reinforcementlearning/comments/g2me4t/understanding_exploration_in_trpo/
Rendering pybullet environments in jupter notebook,1587064942,"Hello

I have a custom environment using pybullet.  I am unable to get rendering of the agent playing through the environment. I do not get any errors.  

I have the step, reset and render functions defined. Any suggestions to what the problem is? This is how the render function is defined: 

&amp;#x200B;

def \_render(self, mode='human', close=False):

return

&amp;#x200B;

PS I am only working in the jupyiter notebook.  I haven't registered my enviroment, i only call it in the jupyiternotebook.",reinforcementlearning,professor_oulala,False,/r/reinforcementlearning/comments/g2ln4x/rendering_pybullet_environments_in_jupter_notebook/
Reinforcement Learning - AI Experts Explain (Blog &amp; Video Library Access),1587044634,,reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/g2f5bh/reinforcement_learning_ai_experts_explain_blog/
"Q-function in DDPG: Why MSBE instead of ""just"" a classifier?",1587044513,"I've been trying to understand OpenAI's spinning up, particularly the page on DDPG. It, as well as the section on Q-Learning, describes optimizing the Q-function as something based off of the Bellman equations, leading to this definition:

[Source: https:\/\/spinningup.openai.com\/en\/latest\/algorithms\/ddpg.html](https://preview.redd.it/4qrvst54n6t41.png?width=725&amp;format=png&amp;auto=webp&amp;s=ec1f636d0274368e664944ec37f62cbafb5ff71e)

This is confusing to me, because the Q function has always been explained to me in the context of eg. chess as ""the likelihood you'll win given this state+action"" or more generally the expected reward of a particular (state, action) pair. If so, why do we not train the Q-function as a generic supervised network on ""(state, action) --&gt; actual reward"" instead of this self-referencing MSBE that apparently uses target networks and other hacks to stay stable? Is the answer just ""it works better/better sample efficiency"" or is there another reason why treating the Q-function as a classifier/reward predictor isn't a good idea?",reinforcementlearning,Orpheon73,False,/r/reinforcementlearning/comments/g2f401/qfunction_in_ddpg_why_msbe_instead_of_just_a/
DQN: LARGEST rewards with LARGEST losses?,1587043678,"I am once again confused by my RL tensorboard metrics. I've trained a DQN model with [stable-baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html) (after optimizing features, hyperparameters and reward function with Bayesian optimization). To my big surprise, the best model (it indeed performs well on the desired task) ended the training period with a **large loss.**

During training, the rewards keep going up. The losses also keep going up. The temporal difference errors become more and more negative (i.e. getting worse as reflected by the losses). See the plots below. If I let the model train even longer, then the loss eventually goes to 0, BUT this **only** happens because the agent learns not to take any actions (which is bad).

My questions are:

1. Is it normal achieve **largest rewards** *when the losses are largest* (with DQN)?
2. How can this be explained (or even justified in an academic publication)?

I'd be very thankful if you'd share your experience!

&amp;#x200B;

https://preview.redd.it/xz2n53b2m6t41.png?width=2512&amp;format=png&amp;auto=webp&amp;s=930b99fd3a88c80cab7d12648b6cd3e12aea5ed0",reinforcementlearning,thisisthehappylion,False,/r/reinforcementlearning/comments/g2evx9/dqn_largest_rewards_with_largest_losses/
Can safe exploration speed up reinforcement learning training process ?,1587038187,"Hello everyone, I am a beginner in RL and my interest is sample-efficiency in RL, I wonder if we can teach an agent a explore the environment in the safe ways, can it speed up the training process compare to some traditional model-free algorithms like Q-learning ? Thank you",reinforcementlearning,ndtquang,False,/r/reinforcementlearning/comments/g2djjw/can_safe_exploration_speed_up_reinforcement/
My next live stream will be Friday at 10pm PST about training a DQN to play Atari Breakout as well as how to deeply instrument your runs with weights and biases,1587015573,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/g2900o/my_next_live_stream_will_be_friday_at_10pm_pst/
Reward function design,1586984902,"Hi, I have a custom environment doing hourly sequence task like trading, my goal is to maximise the daily profit. (24 hours of total profits) I have a reward function calculating every hour's profit; however, sometimes the agent needs to buy stocks resulting negative reward at that timestep which could be a good action. I have tried to tune my hyperparameters, but the performance couldn't even beat my rule-based strategy. (I have trained my agent 10k episodes, e.g. 240k time steps) 

I wonder should I redesign the reward function to make it episodic, e.g. at the last timestep, calculate the total profit and compare that to my baseline profit to determine the episodic reward for my agent. (I am using TD3)

Many thanks if you could provide some suggestions.",reinforcementlearning,Lazy_cty,False,/r/reinforcementlearning/comments/g20sq1/reward_function_design/
"Can the value function NN “critic” in A2C depends on agent state that are different from the reward features, in environment that have other dynamic objects?",1586976083,"I’m using a custom gym environment and an actor-critic A2C reinforcement Learning. My agent state ms are (x-coordinate, y-coordination, speed). In the environment there are dynamic objectives that have predefined position and speed at each time (their position and speed differs fromfor each time step). The reward features are based for example on the distance between the agents.
Now, for the value function NN, can it has only the agent state (x, y, V)? Can I include the time step in the state as well? 
I included the time step in the states in order to know the other dynamic objectives info ? Is that the correct way?
Thanks!",reinforcementlearning,John_9009_,False,/r/reinforcementlearning/comments/g1xybr/can_the_value_function_nn_critic_in_a2c_depends/
A good tutorial for the understanding of the math in papers,1586975256,"As of right now, I have attempted to build models through rewriting other already written implementations. This has made my knowledge of the topics that I program almost none. So I want to understand the math in research papers like from [arxiv.com](https://arxiv.com) to build models from the ground up. Thank you for any answers you may give.",reinforcementlearning,stackoverflowcoder,False,/r/reinforcementlearning/comments/g1xogy/a_good_tutorial_for_the_understanding_of_the_math/
"""Meta-Learning in Neural Networks: A Survey"", Hospedales et al 2020",1586974332,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/g1xdn4/metalearning_in_neural_networks_a_survey/
"[R] ""WES: Agent-based User Interaction Simulation on Real Infrastructure"" - how Facebook uses RL-trained agents to test its website",1586960607,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/g1sv55/r_wes_agentbased_user_interaction_simulation_on/
What's a good book that treats reinforcement learning for imperfect information games ?,1586958013,,reinforcementlearning,joseildo_M_S_filho,False,/r/reinforcementlearning/comments/g1s3ka/whats_a_good_book_that_treats_reinforcement/
don't understand the behaviour of the mountainCar problem when I tune the reward function,1586948577,"Hello everyone, 

I just suceeded in having satisfying results with the mountainCar problem from the gym library using tile coding with q-learning.

I didn't understand at first why my algorithm worked well on the first episodes and then started to slowly get badder results.

I then found out that it is because I set the reward at +100 when the car reaches the objective. When I removed that, it started to just work fine.

I can't figure out why it behaves this way... Can someone enlighten me about this subject?",reinforcementlearning,tarazeroc,False,/r/reinforcementlearning/comments/g1pufr/dont_understand_the_behaviour_of_the_mountaincar/
"[R] Summary of the A3C paper (""Asynchronous Methods for Deep Reinforcement Learning"")",1586914190,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/g1idvl/r_summary_of_the_a3c_paper_asynchronous_methods/
"What does the term ""bias plane"" mean in the muzero paper?",1586913075,"Last line of this paragraph:

https://preview.redd.it/czshevjitvs41.png?width=989&amp;format=png&amp;auto=webp&amp;s=e3485ec8ec1db595b36bc684f11c317f2c24a476

Is there a definition of this somewhere?  Is it just a 96x96 plane with all 0s, 1s, ... Ns depending on the particular action from (0..N) taken at that step?",reinforcementlearning,jefferyodwyer,False,/r/reinforcementlearning/comments/g1i3ly/what_does_the_term_bias_plane_mean_in_the_muzero/
An Optimistic Perspective on Offline Reinforcement Learning,1586910173,,reinforcementlearning,smallest_meta_review,False,/r/reinforcementlearning/comments/g1hbog/an_optimistic_perspective_on_offline/
Offline Reinforcement Learning,1586909193,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/g1h1v9/offline_reinforcement_learning/
Interesting/good sources regarding State and Reward designs?,1586905976,"Hello everyone,

I am going to attempt creating my own Environment with State and I am looking for a lot of resources on tips, rule of thumbs, past experiences and more on the topic’s state and reward creation. 

I know a well writen/defined state and reward combination could aid quite a lot in making your agent complete the given Environment, hence I wanna give it a lot of attention. 

Are there any resources you prefer about this topic or do you have anything to add about your experiences for example. Any contribution is greatly appreciated and I am sure others will also find it useful, since making your own Environment with RL is just so fun.

Thank you for your contribution in advance!",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/g1g5p4/interestinggood_sources_regarding_state_and/
Is this the correct implementation of policy iteration and value iteration?,1586897725,"I am following David Silver's reinforcement learning course, and have implemented policy iteration and value iteration. My code produces correct results in a 3x3 grid world, however, I know it is possible to achieve correct results with an incorrect algorithm. I was wondering if anyone would be willing to look at my implementation of each algorithm and tell me if it is correct and/or give me some feedback. Thanks!

Policy Iteration:

    import numpy as np
    import random
    
    states = [[0,0,0],
              [0,0,0],
              [0,0,0]]
    R = [[-1,-1,-1],[-1,10,-1],[-1,-1,-1]]
    #stored as translation to x,y: nothing, up, down, left, right
    actions = [[0,0],[-1,0],[1,0],[0,-1],[0,1]]
    #probability of actually going up,down,left,right
    probability = {(0,0):1,(-1,0): 0.9,(1,0):0.9, (0,-1):0.9,(0,1):0.9}
    gamma = 0.9
    
    def evaluate_policy(policy,V):
        V_prime = V.copy()
        for i in range(len(states)):
            for j in range (len(states[0])):
                #store value given the policy at current state
                #try catch statement is only here for the initial policy
                try:
                    a = actions[policy[i][j]]
                    V_prime[i][j] = probability.get(tuple(a))*(R[i+a[0]][j+a[1]] + gamma*V[i+a[0]][j+a[1]])
                except IndexError:
                    continue
        return V_prime
    
    def improve_policy(policy, V):
        for i in range(len(states)):
            for j in range (len(states[0])):
                s = states[i][j]
                res = []
                #one state look ahead
                for a in actions:
                    #get next state
                    try:
                        Q = probability.get(tuple(a)) * (R[i+a[0]][j+a[1]] + gamma*V[i+a[0]][j+a[1]])
                        res.append(Q)
                    except IndexError:
                        continue
                #store the optimal action to take at current state
                policy[i][j] = res.index(max(res))
        return policy
    
    
    def policy_iteration():
        V = states.copy()
        #initialize policy with random actions
        policy = [[random.randint(0,3) for i in range (len(states))] for j in range (len(states[0]))]
        #compute value function for each state
        while True:
            V = evaluate_policy(policy,V)
            new_policy = improve_policy(policy, V)
            if new_policy == policy:
                break
            else:
                policy = new_policy
        return policy

Value Iteration:

    import numpy as np
    
    states = [[0,0,0],[0,0,0],[0,0,0]]
    values = states.copy()
    R = [[-1,-1,-1],[-1,10,-1],[-1,-1,-1]]
    #stored as translation to x,y: up, down, left, right
    actions = [[-1,0],[1,0],[0,-1],[0,1]]
    #probability of actually going up,down,left,right
    probability = {(-1,0): 0.9,(1,0):0.9, (0,-1):0.9,(0,1):0.9}
    
    
    def value_iteration():
        gamma = 0.9
    
        max_iters = 1000 #number of iterations before convergence
    
        #compute value function for each state
        for x in range (max_iters):
            for i in range(len(states)):
                for j in range (len(states)):
                    s = states[i][j]
                    res = []
                    #one step look ahead
                    for a in actions:
                        #get next state
                        try:
                            s_prime = states[i+a[0]][j+a[1]]
                            r = R[i][j]
                            Q = probability.get(tuple(a)) * (r + gamma*s_prime)
                            res.append(Q)
                        except IndexError:
                            continue
                    #store optimal value given an action
                    values[i][j] = max(res)
        return values
        #computer optimal policy
    
    print (value_iteration())",reinforcementlearning,Stephanehk,False,/r/reinforcementlearning/comments/g1dojf/is_this_the_correct_implementation_of_policy/
Getting sawtooth like performance in TD3 and BipedalWalker,1586892346,"I am trying to debug my TD3 implementation and I am consistently getting sawtooth like performance. I would really appreciate any tips or ideas for what the source may be.

What I observe is a very large increase in performance that slows down and is followed by a sharp decrease, like [this](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Cosine-Annealing-Learning-Rate-Schedule.png) but mirrored horizontally.",reinforcementlearning,namedlbda,False,/r/reinforcementlearning/comments/g1c1u9/getting_sawtooth_like_performance_in_td3_and/
[R] Neural Game Engine: Accurate learning of generalizable forward models from pixels,1586876092,,reinforcementlearning,RichardRNN,False,/r/reinforcementlearning/comments/g16z1l/r_neural_game_engine_accurate_learning_of/
Using TF-Agents for a Multiagent problem with multiple reward functions,1586872151,"I'm looking into using [TF-Agents](https://github.com/tensorflow/agents) for my problem where two agents interact with a single environment with differing objectives. I'm sure I can hack something together that will do this with the `PyEnvironment` (following [this example](https://github.com/tensorflow/agents/blob/master/docs/tutorials/2_environments_tutorial.ipynb)), but I'm wondering if anyone has any elegant approaches.

My hacky solution is to have the environment alternate which reward function it uses on each timestep. However, I would rather the environment has references to the agents and is aware of which agent is performing any given action. 

Am I wasting my time with TF-Agents? I'm open to suggestions that may fit my problem more naturally.",reinforcementlearning,drcopus,False,/r/reinforcementlearning/comments/g15umm/using_tfagents_for_a_multiagent_problem_with/
Training using Stable Baselines and Open AI Gym Retro,1586867896,"Hi,

I'm having trouble implementing the ACKTR algorithm from Stable Baselines into Retro environment because of the way that Stable Baselines requires you to vectorise a gym environment and I don't know how to do that to a retro environment. If anyone has any ideas please let me know.",reinforcementlearning,TalfrynKenobi,False,/r/reinforcementlearning/comments/g14rj8/training_using_stable_baselines_and_open_ai_gym/
how model is saved?,1586860766,I am new to this so I have this question. I watched coding train videos to make a genetic algorithm for my simple game of maze in python. My code works perfect to find the best solution for the maze. but i have to run the whole code everytime to train model. I want to know how to save the model so i can use the same model for my new maze.,reinforcementlearning,sdzeeros,False,/r/reinforcementlearning/comments/g138k2/how_model_is_saved/
Why does reduced variance increase sample efficiency?,1586838390,"This is something I've taken for granted, and I'd like to understand more clearly why reduced variance improves the sample efficiency of a simple algorithm such as REINFORCE. Could someone shed some light on this for me? It's surprisingly hard to find an explanation that's not just relying on intuition (albeit, I don't think I've ever seen sample efficiency clearly defined).",reinforcementlearning,OriginalMoment,False,/r/reinforcementlearning/comments/g0ytkt/why_does_reduced_variance_increase_sample/
Inverse reinforcement learning on low-level computer vision tasks.,1586832612,"Are there some applications on low-level cv tasks(image segmentation, image registration, etc) by RL?",reinforcementlearning,ohazyi,False,/r/reinforcementlearning/comments/g0xf5b/inverse_reinforcement_learning_on_lowlevel/
"Learning Diverse Skills via Maximum Entropy Deep Reinforcement Learning (on ""Reinforcement Learning with Deep Energy-Based Policies"", Haarnoja et al 2017)",1586825668,"Hello, I was wondering if someone could help me out with this proof, I'm pretty close to the answer but i am missing something ! I can recover the kl divergence but not the most right term.

https://preview.redd.it/gyxg0a8n6ts41.jpg?width=4032&amp;format=pjpg&amp;auto=webp&amp;s=048e67cfed5c2dbf928d65ea488e699a70ccda2b

https://preview.redd.it/zteae35ylos41.png?width=881&amp;format=png&amp;auto=webp&amp;s=6beab07dc111f6b5f9bdcc4737e7e139aa0aba14",reinforcementlearning,olivierp9,False,/r/reinforcementlearning/comments/g0vmqk/learning_diverse_skills_via_maximum_entropy_deep/
[R] What is your RL research workflow?,1586822105,"I want to advantage of the confinement to improve my RL workflow!

I am curious how people are managing their research and experiments:

- How do you write code: text editor like Sublime? full-fledged IDE like IntelliJ? notebooks?
- What do you use for experiment tracking: TensorBoard? Weights &amp; Biases? ...
- How do you keep track of your research ideas and prioritize tasks?
- How do you keep track of the papers you read and organize your notes?
- What frameworks do you use? PyTorch, TensorFlow, Keras?
- What baselines do you start from: [stable baselines](https://github.com/hill-a/stable-baselines)? [rlpyt](https://github.com/astooke/rlpyt)? your own?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/g0unuu/r_what_is_your_rl_research_workflow/
Solutions for Hard Exploration Problems,1586794544,"Hi guys,

can anyone give me a list of solutions proposed in the past for solving hard exploration problems?

My focus lies on somewhat modular solutions, i.e. ""modules"" I can put in different algorithms, that help tackle hard exploration environments.

Thanks in advance, Jonas Stepanik",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/g0m1xu/solutions_for_hard_exploration_problems/
How to handle keyboard events for training data?,1586786305,"I want to train a model to play a racing game. For that I need input/output pairs which are screenshots of the game and corresponding actions (keyboard presses).

For debugging purposes I have a listener and controller classes. Listener simply listens to keyboard presses and saves them to a text file. The problem is, when I run the controller against this saved text file, I would expect the agent (car) to repeat all the actions I made (basically repeat the key presses). But it does not do that, or at least the timing is off.

Essentially, my keyboard events are stored as a sequence of presses: UP, UP, UP, RIGHT, RIGHT, etc. So then the program (controller) goes through every key stored and sends a keyboard.press() signal to press the corresponding key and releases the key after some or no delay (tried both, neither worked). But as I said, the agent seems to fail to repeat the commands.

What could be the problem?",reinforcementlearning,Yajirobe404,False,/r/reinforcementlearning/comments/g0jg2w/how_to_handle_keyboard_events_for_training_data/
Teach agent to reach objective in empty environment.,1586783019,"Hey guys.

&amp;#x200B;

i am posting this because i am a beginner in reinforcement learning and am a little desperate as my agent is not learning.

Here is a picture of my setup (i am done with all the gym env, wanted to create my own):

&amp;#x200B;

https://preview.redd.it/btbyhgc73ks41.jpg?width=309&amp;format=pjpg&amp;auto=webp&amp;s=9de1c76c351b9df801145e3a312609439d97295b

&amp;#x200B;

I am implementing my own environment right now - the agent's goal (green square) is to reach the big green bar and avoid the red ball (ball movement angle is random). I am implementing this with PyTorch.

&amp;#x200B;

Currently my obervation is:

* x and y coordinate of the agent
* x and y coordiante of the ball relative to the agents position
* y coordinate of the big bar

Action space is:

* move left, up, right, down, do nothing

Reward system:

* \+ 5 for reaching the big bar
* \- 5 for collision with red ball
* \-0.01 for each step (to make the agent learn to find the shortest path)

My Network:

* self.fc1 = nn.Linear(state\_size, 32)self.fc2 = nn.Linear(32, 32)self.fc3 = nn.Linear(32, action\_size)
* target network
* lr = 0.001
* batch size = 32

The problem is, even after 3000 episodes with 100 max\_steps each the agent seems to still act randomly and does not even learn to go straight up (obviously the best solution)

&amp;#x200B;

My question: Any tips on how i could make the agent more ""smart"" ? I was thinking about giving a higher reward for a higher y-coordinate. (to force it moving ""up""). Or is my observation simply too small to make it learn?

&amp;#x200B;

I dont want do use convolutional networks as I later want to make it a multi-agent environment, where multiple squares have to act coopertively.

&amp;#x200B;

I appreciate every hint &amp; tip, if i can provide further info just tell me.

&amp;#x200B;

Edit: If this is not the correct place to post a question i am sorry. If so, i would appreciate a link to where ever i can ask questions about RL =)",reinforcementlearning,SonnencremeSuchti,False,/r/reinforcementlearning/comments/g0ije6/teach_agent_to_reach_objective_in_empty/
Streamlined Off-Policy Learning with Emphasizing Experience Replay implementation (Better performance than SAC),1586778661,"Hello RL community,

I recently read the paper [Striving for Simplicity and Performance in Off-Policy DRL: Output Normalization and Non-Uniform Sampling](https://arxiv.org/abs/1910.0.2208), which analyses, amongst other things, the entropy maximization framework recent model-free algorithms like SAC integrate to achieve SOTA performance. Based on their analyses they propose a different framework consisting of additive Gaussian noise for exploration and action normalization. Together with another addition of this paper, the Emphasizing Experience Replay, this new algorithm called Streamlined Off-Policy Learning (SOP) is not only faster than SAC (in terms of FPS), it also exceeds its performance and sample-efficiency.

&amp;#x200B;

This paper is still undergoing the reviewing process however, its promises sounded great so I implemented it.

This is my implementation: [https://github.com/Fable67/Streamlined-Off-Policy-Learning](https://github.com/Fable67/Streamlined-Off-Policy-Learning)

I only tested it on the Ant and HalfCheetah environments using one seed and the results look promising. Unfortunately, I have not enough time and resources for a complete, publishable evaluation. I would be very thankful for your help!!! &lt;3

&amp;#x200B;

Greetings from Germany, Jonas Stepanik",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/g0hhgo/streamlined_offpolicy_learning_with_emphasizing/
Discord server for RL Community,1586760988,"Hi Reddit ML community,

Hope everyone is safe from the virus and finding productive ways to pass time (like self-studying ML or playing Animal Crossing)! Personally, I’ve spent the past weeks in quarantine doing my research projects and learning about various topics in the realms of ML, Robotics and Math. I thought it would be useful to create a Discord channel to serve as a unified platform for people to share ideas and learn together. Hopefully this channel would be beneficial to everyone: for beginners it will be a valuable learning resource and for others it serve as a breeding ground for inspiration.

Another purpose for this channel is to find collaborators for some personal project ideas which I’ve been meaning to work on but haven’t found the time until now. One of which I thought would be a fun project which is not only practical but also helpful in learning about some of the algorithms/methods in ML + Robotics is to build a mobile delivery robot. This would be a multidisciplinary project involving people of diverse backgrounds in ME, Controls, CS, etc. I think it could be a great application project, networking opportunity, and an effort to help prevent the spread of the virus.

In summary, I hope this channel could serve as a platform for sharing knowledge (particularly in ML and Robotics) and also for collaborating on project ideas. Anyone is welcome to join and pitch their ideas. Feel free to invite your friends! Looking forward to talking to some of you!

Discord server: 
https://discord.gg/yuvErS

EDIT: Thank you to those who join the server and gave this post an upvote! Really appreciate you guys for showing support. :)",reinforcementlearning,aliangdw,False,/r/reinforcementlearning/comments/g0dtlz/discord_server_for_rl_community/
Representation learning in RL?,1586740785,"Hi guys,

I'm looking to do a research project, and not sure where to start. Is representation learning in RL an active area with interesting problems? I would like to do something fairly theoretical.

What other areas that are heavy on theory/math are there?

Thanks!",reinforcementlearning,preoccupied-penguin,False,/r/reinforcementlearning/comments/g08xtv/representation_learning_in_rl/
DGN-I: Deep Geometric Network with Integrated Bounding Box Proposal Pipeline 15 GFLOPS [Swaayatt Robots],1586711257,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/g00i05/dgni_deep_geometric_network_with_integrated/
What's your recommended Learning Resource for starting with RL ?,1586702719,For an absolute Beginner. Have heard of David silver's series in YT and currently am doing the University of Alberta RL Course on Coursera. Learning if fun and all but I wish I could do some projects with the learned concepts side by side.,reinforcementlearning,F1restartXr,False,/r/reinforcementlearning/comments/fzxx3d/whats_your_recommended_learning_resource_for/
Looking for Deep Reinforcement Learning forum,1586693261,"Just curious, Is there any specific forum for Deep Reinforcement learning, to discuss abt new algorithms or personal problems when training beside this credit, like discord, FB chat group, etc...   
Thank you in advance, have a nice weekend guys",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/fzvls2/looking_for_deep_reinforcement_learning_forum/
"Is it legal to add ""artificial"" rewards in a self-play setting which would make the game to be non-zero-sum?",1586689999,"I have a zero-sum game with very sparse reward and I am attempting to train 3 vs 3 agents (two teams).

Seeing how long it is taking to learn, I would like to speed the training process by giving some rewards for doing certain actions (help guide the agents in the right direction). This would make the reward system to sum more than zero if we add both teams rewards.  
Would this be a problem? If so, why? Do you guys have ideas on how I could approach this problem?",reinforcementlearning,OleguerCanal,False,/r/reinforcementlearning/comments/fzuy0j/is_it_legal_to_add_artificial_rewards_in_a/
Dueling Network without CNN layers = 2 separate networks?,1586688082,"It seems to me that people generally consider dueling networks as 1 network, because of the shared CNN layers (below 3 CNN and 1 Flatten layer). If we however, **don't use these layers**, then we'd basically have 2 completely detached networks, is that right? I'm thinking about the aggregation layer as something beyond the networks themselves: There would be 1 state-value network with V(s) in 1 output node and 1 action-value network with 3 output nodes A(s, a1), A(s, a2), A(s, a3). These outputs get aggregated afterwards.

The part where I am not sure is: does the aggregation layer affect the training? If so, then we'd still have 1 network instead of 2 detached networks (because the aggregation layer would tie together the state-value and the action-value network). If it does not affect training, then we'd have 2 detached networks, right? Thanks a lot for helping me to get a better understanding!

&amp;#x200B;

[With CNN](https://preview.redd.it/kq05zfzb8ds41.png?width=821&amp;format=png&amp;auto=webp&amp;s=b9e3cd3d1e1c58fb4580c0c61ce2a8774c7610eb)

&amp;#x200B;

[Without CNN](https://preview.redd.it/vy0ndjow9ds41.png?width=1287&amp;format=png&amp;auto=webp&amp;s=0adaa9714ae8924bd48e95e65f842b63f802a9ad)

&amp;#x200B;

As further info: I was training a DQN with the \`MlpPolicy\`, i.e. without CNN layers, using the parameter \`dueling=True\` on [stable-baselines](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html). Printing out the weights and their dimensions resulted in the following. This made it seem to me like the action-value network and the state-value network are 2 detached networks (both for the model and the target model).

\`\`\`

\['eps:0  |  ()',  'model/action\_value/fully\_connected/weights:0  |  (140, 64)',  'model/action\_value/fully\_connected/biases:0  |  (64,)',  'model/action\_value/fully\_connected\_1/weights:0  |  (64, 64)',  'model/action\_value/fully\_connected\_1/biases:0  |  (64,)',  'model/action\_value/fully\_connected\_2/weights:0  |  (64, 3)',  'model/action\_value/fully\_connected\_2/biases:0  |  (3,)',

'model/state\_value/fully\_connected/weights:0  |  (140, 64)',  'model/state\_value/fully\_connected/biases:0  |  (64,)','model/state\_value/fully\_connected\_1/weights:0  |  (64, 64)',  'model/state\_value/fully\_connected\_1/biases:0  |  (64,)',  'model/state\_value/fully\_connected\_2/weights:0  |  (64, 1)',  'model/state\_value/fully\_connected\_2/biases:0  |  (1,)',

'target\_q\_func/model/action\_value/fully\_connected/weights:0  |  (140, 64)',  'target\_q\_func/model/action\_value/fully\_connected/biases:0  |  (64,)',  'target\_q\_func/model/action\_value/fully\_connected\_1/weights:0  |  (64, 64)',  'target\_q\_func/model/action\_value/fully\_connected\_1/biases:0  |  (64,)',  'target\_q\_func/model/action\_value/fully\_connected\_2/weights:0  |  (64, 3)',  'target\_q\_func/model/action\_value/fully\_connected\_2/biases:0  |  (3,)',

'target\_q\_func/model/state\_value/fully\_connected/weights:0  |  (140, 64)',  'target\_q\_func/model/state\_value/fully\_connected/biases:0  |  (64,)',  'target\_q\_func/model/state\_value/fully\_connected\_1/weights:0  |  (64, 64)',  'target\_q\_func/model/state\_value/fully\_connected\_1/biases:0  |  (64,)',  'target\_q\_func/model/state\_value/fully\_connected\_2/weights:0  |  (64, 1)',  'target\_q\_func/model/state\_value/fully\_connected\_2/biases:0  |  (1,)'\]

\`\`\`

References:

\-  [https://adventuresinmachinelearning.com/dueling-q-networks-tensorflow-2/](https://adventuresinmachinelearning.com/dueling-q-networks-tensorflow-2/)

\-  [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581)",reinforcementlearning,thisisthehappylion,False,/r/reinforcementlearning/comments/fzukpz/dueling_network_without_cnn_layers_2_separate/
Are there any zoom chats,1586672978,"Are there any zoom chats related to reinforcement learning, I'm currently an amateur in this field but I would love to pick the brains of more experienced folk and hear the ideas of everyone",reinforcementlearning,rtgb3,False,/r/reinforcementlearning/comments/fzrlz4/are_there_any_zoom_chats/
Understanding why there isn't a log probability in TRPO and PPO's objective,1586666484,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/fzq7rp/understanding_why_there_isnt_a_log_probability_in/
Is hyperparameter tuning legitimate in reinforcement learning?,1586639558,"Reinforcement Learning is different from traditional supervised learning settings in that there is no distinction between the training phase and the test phase. RL, therefore, is more like an online optimization process in which any optimization steps, both parameter optimization and hyperparameter optimization, would require real interactions with the environment.

It seems to me that only one of the two following protocols is legit for hyperparameter optimization in reinforcement learning:

1. Both parameter optimization and hyperparameter optimization are performed on the environment of interest, and **both processes should count towards the agent's sample complexity in solving the problem**. For example, if parameter optimization takes 1M steps, and 100 different sets of hyperparameters are evaluated, then the total sample complexity for the method should be 100M.
2. Perform **hyperparameter optimization on auxiliary/simulator tasks** and only train on the environment of interest with one or a few sets of hyperparameters. In this case, if parameter optimization takes 1M steps, and only 1 set of hyperparameters is evaluated, then one can legitimately claim that the method has a sample complexity of 1M.

Most if not all paper I've read so far is claiming the sample complexity in (2) while following the protocol in (1). So I wonder what the community's thought is on this issue. Would love to hear different opinions.",reinforcementlearning,zhangxz1123,False,/r/reinforcementlearning/comments/fzj9k1/is_hyperparameter_tuning_legitimate_in/
"Parameterizing and Sampling from Continuous, Multimodal Action Spaces",1586639328,"What are the best methods, both in terms of computational complexity and agent performance, for parameterizing and sampling from continuous, multimodal action spaces in model-based RL? 

Are multivariate GMMs efficient and effective? Are there any notable distributions for bounded action spaces to avoid clipping (e.g. multivariate Beta)? Any recommended libraries or practical tips for integrating into TF? I don't see anything at the moment that would be good out-of-the-box from TFP.",reinforcementlearning,bigrob929,False,/r/reinforcementlearning/comments/fzj7au/parameterizing_and_sampling_from_continuous/
Pendulum on OpenAI Gym...Help!,1586637533," I have coded something to try and solve the problem for the pendulum. The problem is that it is not getting better at all. Even after an hour, it is still only swinging to one side and overshooting the top. I was confident in it because the same basic format helped me solve the CartPole and MoutainCar envs. Are there any problems with this code? 

    import gym
    import random
    import numpy as np
    from collections import deque
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.models import Sequential
    
    EPISODES = 300
    
    class DQNAgent:
        def __init__(self, state_size, action_size):
            self.state_size = state_size
            self.action_size = action_size
    
            self.render = True
    
            self.epsilon = 1
            self.epsilon_min = 0.01
            self.epsilon_decay = 0.99
            self.learning_rate = 0.01
            self.discount_rate = 0.8
            self.train_start = 1000
            self.batch_size = 64
    
            self.memory = deque(maxlen=3000)
    
            self.model = self.build_model()
            self.t_model = self.build_model()
    
            self.update_t_weights()
    
        def build_model(self):
            model = Sequential()
            model.add(Dense(24, input_dim=self.state_size, activation=""relu""))
            model.add(Dense(24, activation=""relu""))
            model.add(Dense(self.action_size, activation=""linear""))
            model.summary()
            model.compile(loss=""mse"", optimizer=Adam(lr=self.learning_rate))
            return model
    
        def append_sample(self, state, action, reward, next_state, done):
            self.memory.append((state, action, reward, next_state, done))
    
            if self.epsilon &gt; self.epsilon_min:
                self.epsilon *= self.epsilon_decay
        def update_t_weights(self):
            self.t_model.set_weights(self.model.get_weights())
        def get_action(self, state, env):
            if np.random.rand() &lt; self.epsilon:
                e = env.action_space.sample()
                return e
            else:
                q_value = self.model.predict(state)
                return q_value[0]
        def train(self):
            if len(self.memory) &lt; self.train_start:
                return
            batch_size = min(self.batch_size, len(self.memory))
            mini_batch = random.sample(self.memory, batch_size)
    
            target_input = np.zeros((batch_size, self.state_size))
            t_input = np.zeros((batch_size, self.state_size))
            actions = []
            rewards = []
            dones = []
            for i in range(self.batch_size):
                target_input[i] = mini_batch[i][0]
                actions.append(mini_batch[i][1])
                rewards.append(mini_batch[i][2])
                t_input[i] = mini_batch[i][3]
                dones.append(mini_batch[i][4])
    
            target = self.model.predict(target_input)
            t_target = self.model.predict(t_input)
    
            for i in range(self.batch_size):
                if dones[i]:
                    target[i] = rewards[i]
                else:
                    target[i] = rewards[i] + self.discount_rate * t_target[i]
            self.model.fit(target_input, target, epochs=1, batch_size=self.batch_size, verbose=0)
    
    
    def main():
        env = gym.make(""Pendulum-v0"")
    
        state_size = env.observation_space.shape[0]
        action_size = env.action_space.shape[0]
        agent = DQNAgent(state_size, action_size)
        scores = []
        episodes = []
    
        for e in range(EPISODES):
            done = False
            score = 0
    
            state = env.reset()
            state = np.reshape(state, [1, state_size])
    
            while not done:
    
                if agent.render == True:
                    env.render()
    
                action = agent.get_action(state, env)
                next_state, reward, done, info = env.step(action)
                next_state = np.reshape(next_state, [1, state_size])
                reward = reward if not done or score == 0 else -16
                agent.append_sample(state, action, reward, next_state, done)
                agent.train()
                score += reward
                state = next_state
    
                if done:
    
                    agent.update_t_weights()
    
                    print(""episode:"", e, ""  score:"", score, ""  memory length:"",
                          len(agent.memory), ""  epsilon:"", agent.epsilon)
        env.close()
    main()",reinforcementlearning,UnsecuredConnection,False,/r/reinforcementlearning/comments/fzim6t/pendulum_on_openai_gymhelp/
what is env.reset openai gym,1586626813,what does the function env.reset() do in openai gym?,reinforcementlearning,salma_gabr,False,/r/reinforcementlearning/comments/fzb1rl/what_is_envreset_openai_gym/
Training a twin stick shooter AI,1586623564,"Hello,

I am currently trying to train a bot of a twin-stick shooter, linked the gameplay below.

[https://www.youtube.com/watch?v=W4XNFGT7t1I&amp;feature=youtu.be](https://www.youtube.com/watch?v=W4XNFGT7t1I&amp;feature=youtu.be)  
This is real-time gameplay but I can run close to 1000 runs per second.

The bot can either move in an X and Y with values(-1 and 1) or shoot in a particular direction with an angle that I am normalizing to a range of -1 to 1.  
The firing of the bullet and the movement are 2 separate neural networks.

For the agent state, I am filtering the values and giving only the important things that the agent needs to know. Like the projectiles that might hit the opponent, the projectile that might hit itself, the last 5 positions of the opponent over last 0.5 seconds to give it an idea where the enemy is currently moving at. All these data are then normalized.  
All these things I am calculating mathematically and providing only the important variables for the agent to learn.

My goal here is to see how much less performance I would be able to run this system in and if I would be able to run something like this in a real-time game environment taking 5-20% of the GPU.

For now, I am storing the gameplay data in csv files for each run and plan to later use them to train to get the h5 files and use the h5 files to run the next set of games. This would help me to run the game in multiple locations in parallel and send the data to be trained.

I want to know what would be the best way to train something like this to e? I am starting with reinforcement learning so I am a bit unsure about something like this.  
If anyone also wants to partner up to help or mentor with the project, that would also be great. My next goal is to introduce more features to it after this.",reinforcementlearning,Envenger,False,/r/reinforcementlearning/comments/fz96jp/training_a_twin_stick_shooter_ai/
[R] A3C vs A2C - did I get this right?,1586618063,"I am not sure I fully understand the differences between A3C and A2C.

- A3C: https://arxiv.org/abs/1602.01783
- A2C: https://openai.com/blog/baselines-acktr-a2c/ (anyone has a more formal description?)

A3C
---

In A3C, you have multiple independent actor-learners, as seen in Algo S3 of the paper. Each actor-learner receives globally shared parameters, performs a number of timesteps, then accumulate gradients for both the actor and the critic, just as a standard actor-critic method would do. 

Except that, instead of updating its own local parameters, it updates the **global parameters** in an asynchronous way, then synchronizes its local parameters to the global parameters. Sounds pretty straightforward! 

This is optimised for use on machines with many CPU threads, and without a GPU. This allows updates to be performed Howgwild-style, ie using shared memory without any locking.

A2C
---

A2C is introduced in this way in the OpenAI blog post:

&gt; As an alternative to the asynchronous implementation, researchers found you can write a synchronous, deterministic implementation that waits for each actor to finish its segment of experience before performing an update, averaging over all of the actors. One advantage of this method is that it can more effectively use of GPUs, which perform best with large batch sizes. This algorithm is naturally called A2C, short for advantage actor critic. (This term has been used in several papers.)

I initially thought that it means A2C works in the same way as A3C, but that the global parameters would be updated all at the same time instead of asynchronously.

However, this wouldn’t make sense: the goal is to use ""large batch sizes"". This would mean that instead of sending **gradients**, A2C actually transmits **experiences** to the central learner. 

This seems to be a big difference! Essentially you go from a model with multiple independent actor-learners, to a model with a single learner and multiple actors, which end up being nothing more than rollout workers.

Is my understanding correct? 

If yes, then I find the description given by OpenAI a bit misleading - this is not only a matter of Synchronous vs Asynchronous, but a more fundamental change in the way the framework works!",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/fz79hw/r_a3c_vs_a2c_did_i_get_this_right/
Online Paper Reading Group,1586547530,"Are there any online paper reading groups (RL/ML/Optimization) that I can participate in? 

It would be nice to have that during this quarantine season at least.",reinforcementlearning,elitalobo1995,False,/r/reinforcementlearning/comments/fyn0yo/online_paper_reading_group/
"David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning | AI Podcast #86 with Lex Fridman",1586547089,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fymv5q/david_silver_alphago_alphazero_and_deep/
[100% Off] 2-Days limit - Deep Reinforcement Learning: A Hands-on Tutorial in Python,1586542482,"Deep Reinforcement Learning: A Hands-on Tutorial in Python

Register here:

https://preview.redd.it/oyvsqkst71s41.jpg?width=750&amp;format=pjpg&amp;auto=webp&amp;s=8858267f8a6e6ad9542d2a91bea5e07a3998c142

[https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=STARTDRL](https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?couponCode=STARTDRL&amp;fbclid=IwAR30dC_ArrrslmAFyKsafmkYLLnie0yf6CVfAVEzxOl_YL2mezvUfqvkoXc)",reinforcementlearning,mehdi_mka,False,/r/reinforcementlearning/comments/fyl8v9/100_off_2days_limit_deep_reinforcement_learning_a/
Nice tips about a better video presence (useful for all the conferences/presentations going remote these days),1586536087,,reinforcementlearning,Wookai,False,/r/reinforcementlearning/comments/fyj3dk/nice_tips_about_a_better_video_presence_useful/
AI Learns to Parallel Park - Deep Reinforcement Learning with Unity ML-Agents,1586535799,,reinforcementlearning,SamuelArzt,False,/r/reinforcementlearning/comments/fyj00v/ai_learns_to_parallel_park_deep_reinforcement/
DDQN and Add-ons,1586507117,"I implemented a modularized version of DQN algorithms and their add-ons. Making it possible to stack every single improvement towards rainbow together and compare their performance for Atari games based on pixel input and ram input. 

[DQN-Atari-Agents](https://github.com/BY571/DQN-Atari-Agents)

&amp;#x200B;

I would be interested in feedback and possible improvements. Also planning to add DRQN and curiosity and novelty exploration modules.",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/fybreb/ddqn_and_addons/
Silly rules improve the capacity of agents to learn stable enforcement and compliance behaviors,1586499493,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fy9xwk/silly_rules_improve_the_capacity_of_agents_to/
Jelly Bean World: A Testbed for Never-Ending Learning,1586499315,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fy9whq/jelly_bean_world_a_testbed_for_neverending/
Normalization in PPO,1586499158,"What does normalization of inputs mean in the context of PPO? At each time step of an episode, I only know the values of this time step and of the previous ones, if I take track of them. This means that for each observation and for each reward at each time step I will do:

value = (value - mean) / std

before passing them to the NN, right?",reinforcementlearning,kosmyl,False,/r/reinforcementlearning/comments/fy9vcs/normalization_in_ppo/
Can anyone guide me to get start coding in RL,1586471789,I am reading and enjoying RL algorithms during the quarantine.(almost completed Sutton and Barto). But I have not been coding along it. Can anyone share their experiences and advise how to get start coding for RL.,reinforcementlearning,muddy_danger,False,/r/reinforcementlearning/comments/fy29s2/can_anyone_guide_me_to_get_start_coding_in_rl/
World Models in Tensorflow 2,1586467971,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/fy154s/world_models_in_tensorflow_2/
[R] Summary of the CURL paper (contrastive loss for RL),1586463396,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/fxzsld/r_summary_of_the_curl_paper_contrastive_loss_for/
Topics for a PhD in Reinforcement Learning,1586458668,"I am planning to start a PhD in RL next semester. I do not have any specific topic yet, but I would like to work on something useful in pursuing general intelligence. I am making a list of topics so my tutor and I can narrow it down to one. These are some interesting things I have listed so far:

\- multi-task learning

\- meta-RL

\- active / self-supervised RL

\- hierarchical RL

\- neural programming synthesis

\- RL to solve the ARC benchmark ([https://arxiv.org/abs/1911.01547](https://arxiv.org/abs/1911.01547)) which would require a bit of everything of the above topics

Which of these topics do you think are most promising ? Which applications worth the most exploring in 2020 ? If you are an experienced RL researcher, what topic would you choose if you were in my position knowing what you know now ? Are there other interesting topics that I may be missing ?

Thanks for your help ! I appreciate any suggestions :)",reinforcementlearning,xSensio,False,/r/reinforcementlearning/comments/fxyb1x/topics_for_a_phd_in_reinforcement_learning/
RL to predict score of football games,1586437928,"Hello everyone ,

&amp;#x200B;

I'm pretty new to all RL methods and i have a project to do using reinforcement learning. 

&amp;#x200B;

The idea of the project is to predict the scores of all games of a football competition given the scores of this same competition from previous years and differents stats from each teams (like if they are good in attack, defense etc....). 

&amp;#x200B;

I know we can solve this problem with classic ML algorithm but i specificly need to do it with RL.  This is some kind of 'research'  project. 

&amp;#x200B;

I have some issue to determine what could be the possible best algorithm, how to manage the reward etc... 

  
Can anyone could give me some clues ? 

&amp;#x200B;

Thanks in advance",reinforcementlearning,Mr_Le_Pain,False,/r/reinforcementlearning/comments/fxs6av/rl_to_predict_score_of_football_games/
Questions for POMDP and delayed outcome,1586419561,"In the real-world environment, the agent generally observes a part of the environment. By the missing information, the agent can have a delayed action/reward, and we can consider this as a POMPD setting \[1\] (correct me if I am wrong).

&amp;#x200B;

In general, deep RL models have an LSTM layer to have temporal information, and in traditional RL, the eligibility trace is used to incorporate previous values which serve as short-term memory. As referred to \[2\], eligibility traces have been used in the POMDP setting although it did not use a nonlinear function approximator. In my opinion, these approaches could alleviate the POMDP problem (mentioned above) to some extent. Am I right?

&amp;#x200B;

Moreover, it seems that both LSTM and trace have past knowledge but used in different perspectives, one for network features and the other for optimization. For the above POMDP problem case, which method is more preferred, LSTM, trace, or both? Is there any distinction between LSTM and traces?

&amp;#x200B;

The questions are

1. Are there any theoretical papers regarding differential reward in the infinite-horizon problem?

2. Can past knowledge solve the POMDP problem?

3. Any suggestion for delayed consequences, and what is difference between LSTM and eligibility traces?

&amp;#x200B;

\[1\] Katsikopoulos et al. ""Markov decision processes with delays and asynchronous cost collection"", IEEE TAC 2003

\[2\] Loch &amp; Singh, “Using Eligibility Traces to Find the Best Memoryless Policy in Partially Observable Markov Decision Processes”, ICML 1998",reinforcementlearning,TK-SZ,False,/r/reinforcementlearning/comments/fxoglt/questions_for_pomdp_and_delayed_outcome/
Sutton book + Silver lectures - How to combine them?,1586415409,,reinforcementlearning,rnishtala,False,/r/reinforcementlearning/comments/fxnmsc/sutton_book_silver_lectures_how_to_combine_them/
WeChat Group for Discussion,1586408939,"Hi guys, I have created a WeChat group for discussion. No matter whether you are researchers or students, feel free to join the group to share your problems and opinions about UC Berkeley CS 285 and deep RL.

&amp;#x200B;

https://preview.redd.it/uf13o6dq6qr41.png?width=1080&amp;format=png&amp;auto=webp&amp;s=f8e040284110c06de6c2ede911be04ca4f08620b",reinforcementlearning,Tao_Qing,False,/r/reinforcementlearning/comments/fxmbyg/wechat_group_for_discussion/
Deep RL from scratch,1586404994,,reinforcementlearning,uncountably-infinite,False,/r/reinforcementlearning/comments/fxlgrk/deep_rl_from_scratch/
[R] CURL: Contrastive Unsupervised Representations for Reinforcement Learning,1586395866,,reinforcementlearning,hardmaru,False,/r/reinforcementlearning/comments/fxj9qb/r_curl_contrastive_unsupervised_representations/
"Predictive Maps in the Brain (Sam Gershman, Harvard University)",1586380920,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fxf6fv/predictive_maps_in_the_brain_sam_gershman_harvard/
Updating two agents who are playing against one another (tic tac toe),1586345633,"Hello everyone! 

I am trying to implement a simple Q Learning based Tic Tac Toe game (or naughts and crosses). The way I am trying to implement this is, I have two independent agents, who each have their own Q tables. They play against one another, and the first play is with equal probability. However, I am having trouble with the rewards. See, when an agent wins a game, I can easily attribute a reward (1000 in this case) to the action taken in that particular state, but the other agent has not really made any action, so I cannot update it. In this sense, the agents have no sense of negative rewards at all (only positive rewards, or neutral for next state). How do I change this? I also saw some tutorials where people had implemented a history thing to keep track of all moves since the beginning of the game, and when a win/loss/draw happens, all states in history are updated. However, while solving the grid world problem, I did not implement anything like that and it worked just fine.

How should I approach this problem?

Cheers from Nepal!",reinforcementlearning,evilmorty_c137_,False,/r/reinforcementlearning/comments/fx4war/updating_two_agents_who_are_playing_against_one/
Pytorch Model Based RL,1586329184,"Seems like there are very few pytorch implementations of model based rl algorithms.. is anyone aware of pytorch implementations of any of the following algorithms: 
- MBPO
- PETS 
- MVE
- STEVE
Help appreciated :)",reinforcementlearning,CauchyBirds,False,/r/reinforcementlearning/comments/fx1na6/pytorch_model_based_rl/
[P] Stream 2 of my DQN in Pytorch series will be Wednesday at 10:20pm PST,1586323048,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fx0czv/p_stream_2_of_my_dqn_in_pytorch_series_will_be/
PPO: Reward completely flat but high value estimate,1586287712,"I'm training an agent with PPO (Unity ML-Agents). My environment rewards are sparse (only one reward after 1.6k steps). After a few thousands of steps, the reward becomes completely flat. However, the Value Estimate remains quite high with completely flat loss at basically zero. 

&amp;#x200B;

From my understanding it doesn't make any sense: if the loss is almost zero it means that the Critic is almost perfect in predicting the reward. This would make sense if the prediction was zero since the reward is flat at zero. However, the prediction (Value Estimate) remains high.",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/fwqwix/ppo_reward_completely_flat_but_high_value_estimate/
Ideal Beginner Path,1586285990,"Someone asked me a question and I thought I should ask here.
Would you recommend a beginner to read Barton and Sutton cover to cover or rather Follow a course like the ones by David Silver or Sergey Levine?",reinforcementlearning,riley_kel,False,/r/reinforcementlearning/comments/fwqckf/ideal_beginner_path/
Deep RL from scratch stream series,1586275989,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fwn8sp/deep_rl_from_scratch_stream_series/
Csaba Szepesvari of DeepMind on TalkRL,1586275645,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/fwn4z0/csaba_szepesvari_of_deepmind_on_talkrl/
PPO: how big batch size and buffer size,1586275531,"PPO: How big should batch size and buffer size be with respect to the episode length?

Another question: If batch size is 2000, does it mean that 2000 consecutive steps are taken from the buffer and used for updating?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/fwn3jo/ppo_how_big_batch_size_and_buffer_size/
Any tips on designing a good RL state?,1586257379,"Hello,

I am looking for some times when creating a RL state. I am trying to make one and had some diffrent aspects in mind, for example normalization and a sense of time, however I am looking for tips and or efficiency you can bring in a state, for example whether distance from an object would be more useful than the location of one. Small things you’ve find useful to know and use when creating a RL state. I also read up on framestacking. This all to of course create the ultimate MDP for your RL problem, however you can never be sure enough. Please let me know if you have any tips and or articles related to this as I am curious to know more to designing one.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/fwil49/any_tips_on_designing_a_good_rl_state/
Simple RL Design Question - Q Learning,1586246117,"Hi, quick question about Q Learning (simple q table with epsilon greedy) with infinite loop until dead / run out of cumulative score

For example, a very simple mario platformer:
1. ground 'moves' towards agent. The only actions available at anytime is just jump up or do nothing (take would mean be take 1 step forward, since the frames are moving towards the agent). 
2. We would like the agent to survive as long as possible. 

Q1) Am i right to say that the reward function is 0 for any actions taken, but if agent falls into hole, reward=-10 (or something big)? Should we then reward agent for successfully crossing the pit, reward = 1?

Q2) For the update algorithm when agent falls into pit, is max Q(s',a) = 0 to signify end of the game?

Q3a) once agent falls into the hole, do we repeat the whole game from start with the current Q table? Or reset the table?

Q3b) Can we continue rolling the game forward since the ""death"" reward is embedded into the Q table already (and that the Q table doesnt ""know"" it died)?

Now lets tune up the game complexity 1 notch - there is a starting score=100. At each time step, score decrements by 1. If agent collects the cherry, score increments by 10. If score runs to 0, agent is rewarded with -10 for losing.

Q4) am i right to say that our State now requires to have Score encoded into it?

Q5) Is it necessary for us to set reward=+1 if agent collects the cherry? After all collecting the cherry is not the objective we want the agent to learn, but kinda like side objective to survive as long as possible

Q6) how should we handle different hyper parameters eg learn rate, discount rate etc? Train all N agents in N environments and select the best performing agent for a test environment?

Thanks in advance!",reinforcementlearning,Schwarzschild-,False,/r/reinforcementlearning/comments/fwghsx/simple_rl_design_question_q_learning/
Livestream starting soon: DQN in Pytorch from scratch,1586236769,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fweo1c/livestream_starting_soon_dqn_in_pytorch_from/
PPO in control problem,1586221053,"Hi, I'm currently working on using RL in control system. So I first tried PPO in gym's Pendulum-v0, but performance of codes in github doesn't seem to converge(Stable-baselines, etc). Does PPO shows bad performance in Pendulum-v0?",reinforcementlearning,Cerphilly,False,/r/reinforcementlearning/comments/fwawku/ppo_in_control_problem/
[P] Minimal Distributional RL algorithms using TensorFlow2,1586210523,"![Imgur](https://i.imgur.com/X7f28za.png)

**NEW Project dist-rl-tf2** [github here](https://github.com/marload/dist-rl-tf2) &lt;br&gt;
**deep-rl-tf2** [github here](https://github.com/marload/dist-rl-tf2)

Hi!
I have implemented various and popular Distribution Reinforcement Learning Algorithm using TensorFlow2. Like the Deep-rl-tf2 project, this repository has five features.

- Using TensorFlow2
- Very simple code
- Correct implementation
- High readability code
- Implementation core algorithms

I hope you like this project! The next series of projects is distributed-rl-tf2.",reinforcementlearning,marload,False,/r/reinforcementlearning/comments/fw808j/p_minimal_distributional_rl_algorithms_using/
How long does training a DQN take?,1586196735,"I've been trying to train my own DQN to play pong in PyTorch (for like 3 weeks now). I started off with the 2013 paper and based on suggestions online decided to follow the 2015 paper with target q network.

Now I'm running my code and its been like 2 hours and is in episode 160 of 1000 and I don't think the model is making any progress. I can't seem to find any issue in the code so I don't know if I should just wait some more.

for your reference code is in [https://github.com/andohuman/dqn](https://github.com/andohuman/dqn).

Any help or suggestion is appreciated.",reinforcementlearning,Andohuman,False,/r/reinforcementlearning/comments/fw3s1n/how_long_does_training_a_dqn_take/
Reinforcement Learning in the Cloud,1586193287,"I tried setting up an RL environment in Azure so I could use Mujoco but was unable to get it working, has anybody here successfully setup their RL environment in a cloud VM, what provider did you use and did it work?",reinforcementlearning,riley_kel,False,/r/reinforcementlearning/comments/fw2o3c/reinforcement_learning_in_the_cloud/
Asking for help for the CS330 Stanford course,1586177453,"Hello everyone!

I was wondering if anyone here has done the CS 330 Stanford course about meta-learning and multi-tast learning.

I am stuck on the first homework, I can't get my model to converge. If anyone here has already done the assignment and is willing to help me, I would be more than grateful!",reinforcementlearning,tarazeroc,False,/r/reinforcementlearning/comments/fvy68z/asking_for_help_for_the_cs330_stanford_course/
[R] New models that outperform the popular EfficientNet models while being up to 5x faster on GPUs,1586173277,"Abstract:  In this work, researchers present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs. 

Paper link:  [https://arxiv.org/abs/2003.13678v1](https://arxiv.org/abs/2003.13678v1)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/fvx9cr/r_new_models_that_outperform_the_popular/
ML-Agents PPO entropy not decreasing,1586171467,"I'm using [ML-Agents](https://github.com/Unity-Technologies/ml-agents) to train a custom Unity Agent.

What could be the reason if the PPO entropy does not decrease (even with `beta = 1e-5`, which should make it decrease really fast)?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/fvwwlk/mlagents_ppo_entropy_not_decreasing/
Time horizon in Proximal Policy Optimization,1586166616,I'm having a hard time understanding the horizon parameter in PPO (also called `nsteps` and `timesteps_per_actorbatch` in openai-baselines). What does it do and how do I tune it?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/fvvxr4/time_horizon_in_proximal_policy_optimization/
How the human learning process reflected Artificial Intelligence (Layman),1586158549,"&lt;blockquote class=""twitter-tweet""&gt;&lt;p lang=""en"" dir=""ltr""&gt;Humans made mistakes, they learn from it \&amp;amp; adapt. In fact, that’s how we actually learn all things. Once human successfully achieve the goal by learning from failure, they passed the lesson to the new generation, that\&amp;#39;s how they becoming superhuman. &lt;a href=""https://twitter.com/hashtag/ReinforcementLearning?src=hash\&amp;amp;ref\_src=twsrc%5Etfw""&gt;#ReinforcementLearning&lt;/a&gt; &lt;a href=""https://twitter.com/hashtag/ai?src=hash\&amp;amp;ref\_src=twsrc%5Etfw""&gt;#ai&lt;/a&gt; &lt;a href=""https://t.co/A4rlH5V37P""&gt;pic.twitter.com/A4rlH5V37P&lt;/a&gt;&lt;/p&gt;\&amp;mdash; Adibzahran ⚡ (@adibzahran) &lt;a href=""https://twitter.com/adibzahran/status/1246789922846986241?ref\_src=twsrc%5Etfw""&gt;April 5, 2020&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""&gt;&lt;/script&gt;  


  
[https://medium.com/@adibzahran/the-relativity-of-human-learning-process-reflected-artificial-intelligence-laymen-a3bc882ac640](https://medium.com/@adibzahran/the-relativity-of-human-learning-process-reflected-artificial-intelligence-laymen-a3bc882ac640)

Regular or non-tech people are struggling to understand what is AI and obviously they wouldn't know what is Reinforcement Learning. Put it this way,  


The situation is almost similar as you were dropped off at an isolated island, How will you survive?",reinforcementlearning,adibzahran,False,/r/reinforcementlearning/comments/fvues6/how_the_human_learning_process_reflected/
[P][D] Altair + Jupyter Notebooks vs Tensorboard for data visualization?,1586082859,"I've been a heavy user of Tensorboard, and was using notebooks to make custom visualizations occasionally. After coming across Altair for interactive visualizations on notebooks, I've started to wonder whether it'll make sense to use notebooks exclusively for visualizations for monitoring and analyzing training.

We've coded up some helpers to generate visualizations using Altair with data from Tensorboard. Here are some samples [http://blog.varunajayasiri.com/ml/tensorboard\_vs\_notebook.html](http://blog.varunajayasiri.com/ml/tensorboard_vs_notebook.html).

We feel using notebooks is better for programmers than Tensorboard, and want to invest a little more time into this to make it easier. Let us know your thoughts. Thank you.",reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/fvbt4i/pd_altair_jupyter_notebooks_vs_tensorboard_for/
"LPT: If you're struggling to implement an algorithm, find someone else's working implementation and step through it in a debugger.",1586080110,"This seemingly obvious insight would have saved me a lot of time. Whilst studying RL, I've found that many algorithms have important implementation details that are often glossed over in papers and tutorials. Stepping through a working implementation and looking at intermediary matrix values was so important for me to learn how many algorithms work at a deeper level.",reinforcementlearning,ynmidk,False,/r/reinforcementlearning/comments/fvbbzk/lpt_if_youre_struggling_to_implement_an_algorithm/
[N] - Launch of AWS DeepRacer new virtual community race,1586056045,"AWS Machine Learning Community  would like to invite all new racers to join this virtual community race created specially for you, put together with the help of our racing enthusiasts - Juv Chan, Cyrus Wong Chun Yin, Donnie Prakoso and friends. Prizes await! 

If you’ve been wondering whether to begin racing with #AWSDeepRacer, or if you’re looking to pick up new skills, this might be a good time to start. Learn more today: 
https://blog.deepracing.io/2020/04/02/get-started-with-aws-deepracer-beginners-community-race-3-april-31-may-2020/

#AutonomousRacing #ReinforcementLearning #MachineLearning",reinforcementlearning,juvchan,False,/r/reinforcementlearning/comments/fv6t9z/n_launch_of_aws_deepracer_new_virtual_community/
Any PPO Implementation in PyTorch that match stable-baselines performance?,1586046527,"I have tried rlpyt so far. Though the code is modularized and decently documented, it doesn't match the performance of PPO in stable baselines on common Mujoco benchmarks. E.g., Average performance over 10 runs on hopper - rlpyt 2460 vs stable-baseline 2680.

I notice a couple of differences between their implementations. It would be great if someone with practical experience could point out whether these differences are likely to cause this performance gap.

1. rlpyt doesn't implement reward normalization and value function clipping. However, stable-baseline can retain its performance when I switched off reward normalization and value function clipping in its code.
2. rlpyt uses neural networks to output only the mean of gaussian action distribution, the variance is learned but is fixed for all input states. On the other hand, stable-baselines uses neural networks to predict state-dependent gaussian variances.",reinforcementlearning,mind_juice,False,/r/reinforcementlearning/comments/fv4jrk/any_ppo_implementation_in_pytorch_that_match/
Surprised me while I was listening to random podcasts Reinforcement Learning - Classroom - StrongFit,1586041583,,reinforcementlearning,bananaanagram,False,/r/reinforcementlearning/comments/fv3anb/surprised_me_while_i_was_listening_to_random/
"""Learning to Walk in the Real World with Minimal Human Effort"", Ha et al 2020 {GB}",1586033432,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fv12jg/learning_to_walk_in_the_real_world_with_minimal/
"""Learning to Fly via Deep Model-Based Reinforcement Learning"", Becker-Ehmck et al 2020",1586033034,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fv0ypb/learning_to_fly_via_deep_modelbased_reinforcement/
"""Robots Learning to Move like Animals"" {BAIR/GB} (on ""Learning Agile Robotic Locomotion Skills by Imitating Animals"", Peng et al 2020)",1586031603,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fv0kn1/robots_learning_to_move_like_animals_bairgb_on/
Why don't the popular RL papers are published in peer-reviewed journals?,1586021037,"Most of the popular RL papers (like DeepMind and OpenAI papers) are uploaded to arXiv. It is done with the notion of open-sourcing the research, I agree. But why don't the authors try to publish in a peer-reviewed journal?

It is fine if the paper comes from a popular source like OpenAI, because people value the research done by them. Will the arXiv paper be respected even if it comes from a less popular source? Say, a PhD student from an average-ranked university publishes a RL paper in arXiv. Will the future employers/guides consider his/her arXiv paper as a plus point to his potential, given the research work is good? Or would considered it a less of work since the work is not peer-reviewed?

I'm asking this because I'm fundamentally from a biotech background and in my field, the reputation of a research partially depends on which journal it is published. Is there something like that in RL, too?",reinforcementlearning,Capn_Sparrow0404,False,/r/reinforcementlearning/comments/fuxjml/why_dont_the_popular_rl_papers_are_published_in/
Value-based RL for continuous state and action space,1586009713,"Hi everybody, as the title says I am looking for value-based RL algorithms for a continuous action and state space. Actions are multidimensional (2 real values). Policy gradient methods do not work for my problem, since I explicitly need to estimate a value function. Thanks!",reinforcementlearning,thatpizzatho,False,/r/reinforcementlearning/comments/fuue31/valuebased_rl_for_continuous_state_and_action/
Can DDPG's actor and critic share representation layers the same way A2C's can?,1586006222,"I've been going through [Berkeley's deep RL course](http://rail.eecs.berkeley.edu/deeprlcourse/) and reading through open source projects to learn.

In [lecture 6](https://youtu.be/EKqxumCuAAY?t=3198) (@53:10) Levine discusses using a shared convolutional base for the actor and the critic heads for policy gradient algorithms.

I notice in open source repos it's fine to do this for A2C (and other policy-gradient methods) because they block the policy's loss from being backpropagated through the critic (for the advantage calculation).

With DDPG this isn't the case since it's an analytic gradient that needs to go through the critic; so if I do it the same way, the policy won't learn, but if I don't block it, then sum the losses and backprop(), the critic's parameters would go towards the wrong objective.

&amp;#x200B;

Is there any example on how to do this? Googling around hasn't yielded anything yet.",reinforcementlearning,ihexx,False,/r/reinforcementlearning/comments/futk1i/can_ddpgs_actor_and_critic_share_representation/
[project] Best modern Deep Reinforcement Learning Tutorial Repository,1585969963,,reinforcementlearning,marload,False,/r/reinforcementlearning/comments/fumcpz/project_best_modern_deep_reinforcement_learning/
Question about Soft Q Imitation Learning.,1585953508,"Greetings, I am new to RL and I came across some paper. 
Today I have read the paper [SQIL] and I am not sure if I understand the part where they implement it on SAC. Do they sample 50-50 from both demonstrated and expirienced replay memory, and train both action-value and policy with that batch, or they sample separate batch from just agent's self replay when training policy?
 If they sample it two times for second case, does that mean that they run both Q and policy functions (networks) two times for each sample?
Thank you.",reinforcementlearning,Dexter_fixxor,False,/r/reinforcementlearning/comments/fui58g/question_about_soft_q_imitation_learning/
Confused about frame skipping in DQN.,1585935340,"I was going through the DQN paper from 2015 and was thinking I'd try to reproduce the work (for my own learning). The authors have mentioned that they skip 4 frames. But in the preprocessing step they take 4 frames to convert it to grayscale and stack them.

So essentially do they take 1st frame, skip 2,3,4 then consider the 5th frame and with this way end up with 1st, 5th, 9th and 13th frame in a single step?

And if I use {gamename}Deterministic-v4 in openai's gym (which always skips 4 frames), should I still perform the stacking of 4 frames to represent a state (so that it is equivalent to the above)?

I'm super confused about this implementation detail and can't find any other information about this.",reinforcementlearning,Andohuman,False,/r/reinforcementlearning/comments/fucovf/confused_about_frame_skipping_in_dqn/
"""Using automated data augmentation to advance our Waymo Driver"", Waymo [PBT data augmentation of LIDAR clouds]",1585924523,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fu9h7x/using_automated_data_augmentation_to_advance_our/
Difference,1585898326,"Please  what is the difference  between  these  two books 
 REINFORCEMENT LEARNING AND OPTIMAL CONTROL Dimitri P. Bertsekas  and Reinforcement Learning: An Introduction 
Richard S. Sutton
and Andrew G. Barto
Second Edition",reinforcementlearning,Osarenomawise,False,/r/reinforcementlearning/comments/fu3u2y/difference/
Question about model based vs model free RL in context of Q Learning,1585897942,"Hello everyone! I am an absolute beginner in the field of RL. While going through some tutorials, I came across ""model based"" and ""model free"" RL methods, where model-free RL methods were the ones that were described as

&gt;An algorithm which does not use the *transition probability distribution* (and the *reward function*) associated with the Markov Decision Process (MDP), which, in RL, represents the problem to be solved ...  An example of a model-free algorithm is Q Learning - Wikipedia

What I get from this is that a model-free reinforcement learning method is the one, where the agent has absolutely no notion of the transition functions the state contains and the rewards for reaching each state. However, it contains a list of all states it can be in, and the actions it can take in the world.

However, I came across this [question](https://ai.stackexchange.com/questions/4456/whats-the-difference-between-model-free-and-model-based-reinforcement-learning) on stackoverflow about the difference between model based and model free approaches. One of the answers was:

&gt;*If, after learning, the agent can make predictions about what the next state and reward will be before it takes each action, it's a model-based RL algorithm.*

My question is, after learning through multiple iterations in the world the agent is in, it will finally build a Q table, where the action for each state and the Q values are listed, where it will take an action that will maximize the Q value (assuming epsilon decay where the agent has completed learning and epsilon = 0). After this, the agent **should** be able to make predictions about the next state, should it not?

I am an absolute beginner in the field and English is not my first language. Please feel free to point my mistakes out and suggest me some resources where I can learn more hands-on RL ( not with openAI gym )

Cheers from Nepal!",reinforcementlearning,evilmorty_c137_,False,/r/reinforcementlearning/comments/fu3rfp/question_about_model_based_vs_model_free_rl_in/
TD learning: why forward view equals to backward Views,1585885600,"I am following [this tutorial](http://incompleteideas.net/book/first/ebook/node76.html) and try to understand why in TD(*λ*) learning, the forward and backward view equals to each other. I got stuck at the following equation:

https://preview.redd.it/xe3n4ctfyiq41.png?width=790&amp;format=png&amp;auto=webp&amp;s=1051db6f15d7174c4bc82c9b74545d2ac2293184

I understand how 7.9 gets into 7.10 but not the rest of the equations. Any help would be appreciated!",reinforcementlearning,HippeTeddyBear,False,/r/reinforcementlearning/comments/fu13an/td_learning_why_forward_view_equals_to_backward/
Gave a talk about my RL work at the Weights and Biases Deep Learning Salon,1585859293,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fttv86/gave_a_talk_about_my_rl_work_at_the_weights_and/
Self Driving AI (NEAT) in browser,1585847483,"Hi everybody, I had been inspired by [code bullet](https://www.youtube.com/channel/UC0e3QhIYukixgh5VVpKHH9Q) and his videos on NEAT, and wanted to create a self driving AI which will learn using NEAT. After months of creating and starting from scratch and debating whether to just use the NEAT implementation of Code Bullet or try and code on my own, I present you the 'final' product. [Here it is](https://manassarpatwar.github.io/Self-driving-AI/).

It was a daunting task to code NEAT from scratch, and I think I have got a good understanding of what it tries to achieve (and how). 

If anyone is looking for Car physics implementations like I was, I have mentioned them in the readme on [github](https://github.com/manassarpatwar/Self-driving-AI) but I will mention them here once again.

[Car physics for games by Marco Monster](https://asawicki.info/Mirror/Car%20Physics%20for%20Games/Car%20Physics%20for%20Games.html)

[The JS implementation of the paper by Marco Monster by spacejack](https://github.com/spacejack/carphysics2d)

[2d car physics by Juha Lindstedt](https://github.com/pakastin/car)

[3d WebGL car by Matt Bradley](https://github.com/mattbradley/dash)",reinforcementlearning,Ringsofthekings,False,/r/reinforcementlearning/comments/ftq6rs/self_driving_ai_neat_in_browser/
Is it possible to train ML agents with communicating to Jetson TX2,1585846242,"Hello , I have a quite interesting situation here. I have TX2 laying around at home. So at my PC I have 2GB VRAM with NVIDIA Cuda enabled laptop GPU(GT 840M)

I'm not even sure if it's feasible but since TX2 has 8GB VRAM thought why not ? Since Jetson TX2 runs on Tegra Ubuntu ( Arm x64) and Unity doesnt support arm64 / jetson builds only android ones.  


So my goal is to run ML agent binary on Ubuntu x86\_64 PC and somehow efficiently transfer my state(let say for extreme case its only RGB frame or I can send grey if its going to help transfer - train rate , bandwith etc)   then process it on ARM64 8GB VRAM Jetson then turn back to local PC that running simulation.  


In this case Unity agent would run on local PC and actions taken with arm64 neural net output would apply to local pc.

But I have no idea if its feasible with how speed/efficiency / data optimization etc would work, I just thought since they are on same network just a easy TCP would work

&amp;#x200B;

Does anyone have idea about it",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/ftpsba/is_it_possible_to_train_ml_agents_with/
[R] Ultrasound-Guided Robotic Navigation with Deep Reinforcement Learning,1585844029,"**Abstract**: This paper introduces the first reinforcement learning (RL) based robotic navigation method which utilizes ultrasound (US) images as an input. Our approach combines state-of-the-art RL techniques, specifically deep Q-networks (DQN) with memory buffers and a binary classifier for deciding when to terminate the task.  
Our method is trained and evaluated on an in-house collected data-set of 34 volunteers and when compared to pure RL and supervised learning (SL) techniques, it performs substantially better, which highlights the suitability of RL navigation for US-guided procedures. When testing our proposed model, we obtained a 82.91\\% chance of navigating correctly to the sacrum from 165 different starting positions on 5 different unseen simulated environments. 

Paper access:  [https://arxiv.org/abs/2003.13321v1](https://arxiv.org/abs/2003.13321v1)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/ftp4b2/r_ultrasoundguided_robotic_navigation_with_deep/
An introduction to Reinforcement Learning - Put together very well,1585825996,,reinforcementlearning,rajeshpachaikani,False,/r/reinforcementlearning/comments/ftkjn4/an_introduction_to_reinforcement_learning_put/
What are SOTA algorythms that can be used in production right now?,1585803624,"I created a custom open ai gym environment for my specific problem and I want an agent that can solve it very efficiently. I have been playing around with the Stable Baselines python library and there seems to be many good algorithms but now I'm reading about MuZero, Dreamer and now Agent 57. Are these algorythms that have been implemented and can be used? What are SOTA algorythms that can be used in production right now?",reinforcementlearning,Uagir,False,/r/reinforcementlearning/comments/ftga9y/what_are_sota_algorythms_that_can_be_used_in/
Cool new preprint: Searching for Reward in Graph Structured Spaces,1585791715,,reinforcementlearning,Stauce52,False,/r/reinforcementlearning/comments/ftddyq/cool_new_preprint_searching_for_reward_in_graph/
Free Data Science Courses,1585759100," In response to the novel Coronavirus outbreak, 365datascience is making all of their #datascience courses completely free until 15 April. Be safe. Stay at home. Learn data science and share the info with your friends.

Sign up on free account to get access: [https://365datascience.com/pricing/](https://365datascience.com/pricing/)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/ft379p/free_data_science_courses/
Google DeepMind ‘Agent 57’ Beats Human Baselines Across Atari Games Suite,1585758865,"DeepMind’s breakthroughs in recent years are well documented, and the UK AI company has repeatedly stressed that mastering Go, StarCraft, etc. were not ends in themselves but rather steps toward artificial general intelligence (AGI). DeepMind’s latest achievement stays on path: Agent57 is the ultimate gamer, the first deep reinforcement learning (RL) agent to top human baseline scores on all games in the Atari57 test set.

Read more: [Google DeepMind ‘Agent 57’ Beats Human Baselines Across Atari Games Suite](https://medium.com/syncedreview/google-deepmind-agent-57-beats-human-baselines-across-atari-games-suite-9476e0f50bad)

The original paper is [here](https://arxiv.org/abs/2003.13350)",reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/ft34k9/google_deepmind_agent_57_beats_human_baselines/
What real world application could potentially be solved with reinforcement learning in the coming decade?,1585754856,"Things I could thing of:

1. Better factory robots
2. Completely selfdriving cars
3. House assistant robot (Roomba++)
4. Drones deliver system
5. Financial trading robot

Do you agree with those I mentioned, and do you have any other ideas perhaps?",reinforcementlearning,kakushka123,False,/r/reinforcementlearning/comments/ft1utp/what_real_world_application_could_potentially_be/
"[Project] Stuck on solving custom game with DQN, advices appreciated",1585737626,"Hello everyone,  

I would like to request some help / advices concerning a Reinforcement Learning project a friend and I are struggling to solve for quite some time now.  

To explain the essence of the problem with a simple case, here are few relevant concepts about the project we are trying to solve:  

&gt; Note: the state space of our real case is larger than the simple case here.

1. Let's assume we have a single Robot that receives a set of 3 different balloons based on a set of possible balloons with different probabilities:

    Balloon		|	Proba	|	Value
    ---|---|---
    White (W)	|	38.8%	|	10
    Blue (B)	|	33.4%	|	20
    Cyan (C)	|	16.6%	|	30
    Green (G)	|	11.2%	|	40
2. The set of the 3 received balloons is not divisible, and the order of the balloons in the set cannot be changed, as an example S = [ G, B, W ].

3. The Robot is then asked to place the set S in one of 3 possible grids, corresponding to the action set A = [ 1, 2, 3 ].

4. If 3 balloons of the same color are then vertically aligned, the robot receives the reward which is equal to the balloon value. The goal of the Robot is to maximize the total possible reward based on the balloons it gets over time (based on probabilities and without prior knowledge of the next coming balloons).  

         S = [ b1, b2, b3 ]
        
        
             1       2       3
         | x x x | x x x | x x x |
         | x x x | x x x | x x x |
         | x x x | x x x | x x x |
         -------------------------
           r r r   r r r   r r r
           (with r = line reward)

Let's consider this full example:

Step 0
------
      S = [ G, B, W ]
 
        1	    2       3 
    | x x x | x x x | x x x | 
    | x x x | x x x | x x x | 
    | x x x | x x x | x x x | 
    -------------------------
      0 0 0   0 0 0   0 0 0
      Step reward: 0
      Total reward: 0

Step 1
------
      S = [ B, W, W ]

        1       2       3
    | G B W | x x x | x x x |
    | x x x | x x x | x x x |
    | x x x | x x x | x x x |
    -------------------------
      0 0 0   0 0 0   0 0 0
    Step reward: 0
    Total reward: 0

Step 2
------
      S = [ C, B, C ]

        1       2       3
    | G B W | x x x | x x x |
    | B W W | x x x | x x x |
    | x x x | x x x | x x x |
    -------------------------
      0 0 0   0 0 0   0 0 0
    Step reward: 0
    Total reward: 0

Step 3
------
      S = [ C, B, W ]

        1       2       3
    | G B W | C B C | x x x |
    | B W W | x x x | x x x |
    | x x x | x x x | x x x |
    -------------------------
      0 0 0   0 0 0   0 0 0
    Step reward: 0
    Total reward: 0

Step 4
------
     S = [ G, B, W ]

        1       2       3
    | G B W | C B C | x x x |
    | B W W | C B W | x x x |
    | x x x | x x x | x x x |
    -------------------------
      0 0 0   0 0 0   0 0 0
    Step reward: 0
    Total reward: 0

Step 5
------
     S = [ B, W, W ]

        1       2       3
    | G B W | C B C | x x x |
    | B W W | C B W | x x x |
    | G B W | x x x | x x x |
    -------------------------
      0 0 10  0 0 0   0 0 0
    Step reward: 10
    Total reward: 10

Step 6
------
     S = [ W B W ]

        1       2       3
    | G B W | C B C | B W W |
    | B W W | C B W | x x x |
    | G B W | x x x | x x x |
    -------------------------
      0 0 10  0 0 0   0 0 0
    Step reward: 0
    Total reward: 10

Step 7
------
     S = [ W C B ]

        1       2       3
    | G B W | C B C | B W W |
    | B W W | C B W | W B W |
    | G B W | x x x | x x x |
    -------------------------
      0 0 10  0 0 0   0 0 0
    Step reward: 0
    Total reward: 10

Step 8
------
     S = [ C B C ]

        1       2       3
    | G B W | C B C | B W W |
    | B W W | C B W | W B W |
    | G B W | x x x | W C B |
    -------------------------
      0 0 10  0 0 0   0 0 0
    Step reward: 0
    Total reward: 10

Step 9
------
     S = [ ]

        1       2       3
    | G B W | C B C | B W W |
    | B W W | C B W | W B W |
    | G B W | C B C | W C B |
    -------------------------
      0 0 10 30 20 0  0 0 0
    Step reward: 50
    Total reward: 60



We already tried to solve this problem using the following approach:  
- Using DQN algorithm by assuming that the set of 3 balloons has to be taken as a single step reward function that gives a reward equal to 0 most of the time, and the total reward when an action makes a grid full (see example). Based on that reward function, we assume that the agent cannot determine which line / balloon gave the reward (in case of multiple lines like in step 9 on previous example).

We then assumed that the balloons should all be taken individually as multi-objectives (with same weight), but we are not sure that this is the right approach to solve this problem.

Anyone has any advice / research paper or experience on similar projects where the action taken by an agent is defined by the possible reward of a subset of values out of a set of possible rewards?

Did we miss something crucial in our approach and how we perceive the actual underlying problem?


Thanks in advance.",reinforcementlearning,Ezneh,False,/r/reinforcementlearning/comments/fsxf4g/project_stuck_on_solving_custom_game_with_dqn/
What makes the Baird's counter example diverge ?,1585733273,"I am somewhat confused as to what makes the Baird's Counter example diverge, is  it the overparameterization of the feature vector which makes it diverge. Like the TD algorithm is supposed to diverge when the A matrix in the TD update has negative eigen values, but what makes the eigen values really negative for Bairds Counter example.

&amp;#x200B;

Thanks",reinforcementlearning,intergalactic_robot,False,/r/reinforcementlearning/comments/fswiwd/what_makes_the_bairds_counter_example_diverge/
How maximizing mutual information worked in Reinforcement learning?,1585715536,"Recently I read a paper “Unsupervised State Representation Learning in Atari” which is a constrastive method about state representation learning with self-supervise. In this paper, they use mutual information between two consecutive observations(frames) and maximizing it. I wonder that why they should maximize it and how mutual information works in that procedure. And there is another question: what is high-entropy features and what is low-entropy features. Thanks a lot.",reinforcementlearning,H_uuu,False,/r/reinforcementlearning/comments/fssoiw/how_maximizing_mutual_information_worked_in/
Using NEAT (evolution strategy) to train... huh... various things,1585694997,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/fsn9nj/using_neat_evolution_strategy_to_train_huh/
Reward function problem in bilateral negotiation space.,1585690507,"I have an agent that is supposed to navigate a round-based bilateral negotiation with a time deadline. The agents take turns suggesting outcomes and each agent has their own utility for each possible outcome. The final utility is the personal utility of the agreed-upon bid or 0 if no agreement was reached. Each agent only knows it's own preferences, not the opponents. Currently, I am representing the state by the last two bids my agent made, the last two the opponent made (all mapped to personal utility space and bucketed so integers (1-10) and the time as a bucket from 1-5. A baseline agent is one that concedes based on time so would offer increasingly worse bids (from his point of view)  if the negotiation is reaching the deadline. The way it is programmed is easily abusable by simply always bidding what is best for you and waiting for the time to run out, at which time he will accept or offer something you will accept. So essentially the max utility (1) is gained by bidding 10 constantly. Since the actions are down,stay, up with respect to the last bid made and the fact that the agent is initialized with ""last bid"" being a 10 the agent must stay at 10 until the end of the timer in order to get the max utility. The problem is that if at any point he does go down once or twice the enemy will usually accept. Since getting to the deadline often takes upwards of 50 rounds the agent very rarely gets to see that last reward and usually ends up with a 0.6-7. How could I change the reward function in order to promote better learning without compromising the ability of the agent to learn vs other strategies? I thought of maybe implementing some kind of reward for ""surviving"" longer might be useful although it might not be ideal in general. I'm using online learning and am bottlenecked by the environment so not sure if something like experience replay might be possible.  I am using a2c with an entropy term in the loss. Any advice is appreciated.",reinforcementlearning,samme013,False,/r/reinforcementlearning/comments/fslvjl/reward_function_problem_in_bilateral_negotiation/
Does anyone know how to activate a MuJoCo license key? I cannot find the proper directories/files even though I definitely have them,1585687260, I cannot find the directory \~/mjpro200/bin or \~/mjpro200/model and thus I cannot test my activation key or run a test simulation. MuJoCo is successfully installed on my VMware Ubuntu 18.04 virtual machine so I believe I should be able to find the files but I cannot. If anyone could help they would be a godsend. I have spent so long troubleshooting with MuJoCo.,reinforcementlearning,AWESAM22,False,/r/reinforcementlearning/comments/fskui9/does_anyone_know_how_to_activate_a_mujoco_license/
PerAction A2C,1585678514,"Hey everyone, 

I am trying to figure out how to model a per-action Actor-Critic network. 

For my use case, my action space is in the order of a few ten thousands and not every action is available at all times. So rather than having a ACTOR with 10k output nodes, I was thinking of passing the valid actions in pair with the state and build a model around this architecture.

I am new to this and would really appreciate some advice.

Thanks",reinforcementlearning,altair9335,False,/r/reinforcementlearning/comments/fsi1lw/peraction_a2c/
"""NGU: Never Give Up: Learning Directed Exploration Strategies"", Badia et al 2020 {DM} (8,400 points on _Pitfall_)",1585667703,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fsenzv/ngu_never_give_up_learning_directed_exploration/
"""Never Give Up: Learning Directed Exploration Strategies"", Badia et al 2020 {DM} (8,400 points on _Pitfall_)",1585667398,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/fsekj5/never_give_up_learning_directed_exploration/
[R] Reinforcement Learning in Economics and Finance,1585663729,"State-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research, and finance 

**Abstract:**  Reinforcement learning algorithms describe how an agent can learn an optimal action policy in a sequential decision process, through repeated experience. In a given environment, the agent policy provides him some running and terminal rewards. As in online learning, the agent learns sequentially. As in multi-armed bandit problems, when an agent picks an action, he can not infer ex-post the rewards induced by other action choices. In reinforcement learning, his actions have consequences: they influence not only rewards, but also future states of the world. The goal of reinforcement learning is to find an optimal policy -- a mapping from the states of the world to the set of actions, in order to maximize cumulative reward, which is a long term strategy. Exploring might be sub-optimal on a short-term horizon but could lead to optimal long-term ones. Many problems of optimal control, popular in economics for more than forty years, can be expressed in the reinforcement learning framework, and recent advances in computational science, provided in particular by deep learning algorithms, can be used by economists in order to solve complex behavioral problems. In this article, we propose a state-of-the-art of reinforcement learning techniques, and present applications in economics, game theory, operation research and finance. 

Read the full paper:   [https://arxiv.org/abs/2003.10014v1](https://arxiv.org/abs/2003.10014v1)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/fsdjgk/r_reinforcement_learning_in_economics_and_finance/
[R] Agent57: Outperforming the Atari Human Benchmark,1585649370,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/fsaclv/r_agent57_outperforming_the_atari_human_benchmark/
"""Suphx: Mastering Mahjong with Deep Reinforcement Learning"", Li et al 2020 {MSR}",1585621039,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fs4hh6/suphx_mastering_mahjong_with_deep_reinforcement/
g(tau) in Analyzing variance?,1585606862,"In Slide 27 Lecture 5 in [http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf), how to get the derivative of Var w.r.p to b? In particular, what is g(tau)?

&amp;#x200B;

https://preview.redd.it/kkscl4dqxvp41.png?width=2042&amp;format=png&amp;auto=webp&amp;s=bb3cc008fd036e206a6bcc85b3c7a9fe731ebff3

Thanks in advance!",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/fs0n79/gtau_in_analyzing_variance/
Inverse of summation?,1585605805,"In Slide 12 Example: Gaussian policies, what does the inverse of the summation mean?

https://preview.redd.it/nra4kofiuvp41.png?width=940&amp;format=png&amp;auto=webp&amp;s=1e4d2398a35849dfe28fd50fa4fd5ac2b6b55ab7

Thanks!",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/fs0bfo/inverse_of_summation/
product from t=1 to T?,1585604096,"In slide 8 lecture 5 [http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)

Is the product from t=1 to T over

&amp;#x200B;

[Case 1](https://preview.redd.it/p5r5xna9pvp41.png?width=730&amp;format=png&amp;auto=webp&amp;s=ba5809cc6bd6cfe7b1a43bd2b4f9db8160504e0b)

or over

&amp;#x200B;

[Case 2](https://preview.redd.it/tr0joufapvp41.png?width=730&amp;format=png&amp;auto=webp&amp;s=2c5c7cda4185b888e1a714588091138542176928)

?

The summation of log seems to indicate that it should be the first case:

&amp;#x200B;

https://preview.redd.it/7ubpd1nhpvp41.png?width=1444&amp;format=png&amp;auto=webp&amp;s=f72f479465f8cf83c802253885e853abb991aa75

Thanks!",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/frzs47/product_from_t1_to_t/
"First Podcast &amp; Looking for a Guest! (AI, Engineering &amp; Neuroscience)",1585603898,"As you might see in the title, I am currently looking for people in my podcast. The topics should be Engineering, AI, Neuroscience and/or Self-Development!

Feel free to contact me via Twitter and send me your website or other places where I can find you. I will get things rolling from there on :)",reinforcementlearning,g-x91,False,/r/reinforcementlearning/comments/frzpyp/first_podcast_looking_for_a_guest_ai_engineering/
Stable baselines: loading agent from disk interfering?,1585597515,"Hi,

At the moment, I'm loading the most recent policy/agent from disk and applying it within the custom gym environment used to train the agent itself (using PPO2 in stable baselines). It seems that doing so leads to new processes being initiated (with my current setup, only one process should run at a time). Does anyone have an idea of why this happens, and how I can stop it from happening?",reinforcementlearning,Amumfie,False,/r/reinforcementlearning/comments/frxqis/stable_baselines_loading_agent_from_disk/
10x10 Snake Game with Categorical Q-Learning (n_atoms=11 worked best),1585596637,,reinforcementlearning,Luzzx,False,/r/reinforcementlearning/comments/frxg8a/10x10_snake_game_with_categorical_qlearning_n/
10x10 Snake Game with Reinforcement Learning,1585596552,"Thank you to [https://www.reddit.com/r/reinforcementlearning/comments/foali4/i\_realized\_i\_never\_posted\_this\_here\_its\_a\_high/](https://www.reddit.com/r/reinforcementlearning/comments/foali4/i_realized_i_never_posted_this_here_its_a_high/) 

for providing insight into this project. User jack-of-some trained a snake game on a 10x10 grid using an actor critic method. He initially attempted to use Q-Learning but found that it was ineffective. I was curious whether or not **Categorical** Q-Learning would be effective on such an environment and it was! I suspect this is primarily due to the random nature of the environment.

This is preliminary work for creating the ultimate AI snake in the popular competition Battlesnake:

 [https://play.battlesnake.com/](https://play.battlesnake.com/)",reinforcementlearning,Luzzx,False,/r/reinforcementlearning/comments/frxfc0/10x10_snake_game_with_reinforcement_learning/
RL Weekly 40: Catastrophic Interference and Policy Evaluation Networks,1585587892,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/fruncq/rl_weekly_40_catastrophic_interference_and_policy/
"Ben Eysenbach on TalkRL: Human supervision, SORB, DIAYN, exploration and more",1585584694,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/frtnc2/ben_eysenbach_on_talkrl_human_supervision_sorb/
DQN model still won't converge [UPDATE],1585583222," My first post can be found here -&gt; [https://www.reddit.com/r/reinforcementlearning/comments/fpvx99/dqn\_model\_wont\_converge/?utm\_source=share&amp;utm\_medium=web2x](https://www.reddit.com/r/reinforcementlearning/comments/fpvx99/dqn_model_wont_converge/?utm_source=share&amp;utm_medium=web2x)

People who commented mentioned that training in batches was the best way to go. So, I've changed my code to do batch training with batch size 256, replay memory size 1000. But my model still won't converge on atari breakout. I've also tried punishing my network for losing each of the 5 lives (instead of punishing only when it loses).

I can't seem to figure out where I've made a mistake. Any assistance is appreciated.

Full updated code can be found here:- [https://github.com/andohuman/dqn](https://github.com/andohuman/dqn)

Thank you.",reinforcementlearning,Andohuman,False,/r/reinforcementlearning/comments/frt6q8/dqn_model_still_wont_converge_update/
DQN model still won't converge [UPDATE],1585583172,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/frt660/dqn_model_still_wont_converge_update/
[Source code with demo] Here is my python implementation of Deep Q-learning for playing Tetris,1585493586,,reinforcementlearning,1991viet,False,/r/reinforcementlearning/comments/fr71pa/source_code_with_demo_here_is_my_python/
Neural Network Surgery with Sets paper code?,1585472847,"Does anyone know if the code developed by the OpenAI paper Neural Network Surgery with Sets [https://arxiv.org/abs/1912.06719](https://arxiv.org/abs/1912.06719) is published somewhere?

Would like to try the implementation. Thanks!",reinforcementlearning,OleguerCanal,False,/r/reinforcementlearning/comments/fr31t4/neural_network_surgery_with_sets_paper_code/
A little problem with value iteration,1585455609,"Here's my question on StackOverflow.

[https://stackoverflow.com/questions/60907874/value-iteration-example-maybe-wrong](https://stackoverflow.com/questions/60907874/value-iteration-example-maybe-wrong)",reinforcementlearning,shehio,False,/r/reinforcementlearning/comments/fqzpqo/a_little_problem_with_value_iteration/
4 legged robot doesn't learn to walk,1585429374,"Hi, I have a robot on a gym environment, which I'm trying to get to walk using stable baselines, however, despite months of effort put into it, it doens't want to walk.

I tried PPO2, but the model seems to learn to stay upright, but doesn't learn to walk, after some time, the reward drops and it falls again, I can't explain this behaviour.

I then tried SAC, but no major improvments, the robot seems to settle at a state where it doesn't move, despite the reward for speed.

You can check the environment on Github :  [https://github.com/nono9212/aida\_gym](https://github.com/nono9212/aida_gym)  I normalized the actions, the observations and even the reward, but can't get it to walk.

If you have any ideas, I will be pleased to try them, thx",reinforcementlearning,nono9212,False,/r/reinforcementlearning/comments/fqshzw/4_legged_robot_doesnt_learn_to_walk/
Advice needed for training DQN with Breakout-ram and TF2.0,1585426827,"I'm trying to train a double DQN with reply buffer on OpenAI Breakout Ram version (tried both Breakout-ram-v0 and Breakout-ram-v4). The code is [here](https://github.com/nancyhwr/DQN_Ram/blob/master/DQN.py).

I have tried so many parameter settings along with many other details. Hardware is limited, so only trained on my macbook pro for 200 episodes and about \~4k steps. (shouldn't be a problem though) The total reward in each game is still between 0\~5.

If anyone could give me any suggestions/advices, it would be super appreciated!!! Very confused now. Thank you!",reinforcementlearning,nancywhr,False,/r/reinforcementlearning/comments/fqrp1l/advice_needed_for_training_dqn_with_breakoutram/
Advice needed for training DQN on breakout-ram with tensorflow 2.0,1585426649,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/fqrn2d/advice_needed_for_training_dqn_on_breakoutram/
"""Meta-learning curiosity algorithms"", Alet et al 2020",1585324174,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fpz96f/metalearning_curiosity_algorithms_alet_et_al_2020/
Using several agents in one environment with stable baselines,1585320067,"I'm attempting to create an environment with several agents taking actions using the same model. The only way I've been able to do this so far is by training on the data from a single central agent while making predictions for the other agents by loading the most recent model from disk (somehow this seems to start a new process?). However, I'd like to use the data from all agents for training, for instance using DDPG. I'd also like to get the actions for all agents present (this is a dynamic number) for each step. I've been looking at HER, but I don't understand how to fuse this with my custom environment, as I don't see how to access the replay buffer from inside the environment. Does anyone have tips for how to do this? 

What I'd ideally like to do is something like vectorized environments where each agent in an environment is an object in the other environments. Has anyone attempted something like that?",reinforcementlearning,Amumfie,False,/r/reinforcementlearning/comments/fpy1yi/using_several_agents_in_one_environment_with/
What are artificial transitions?,1585319266,"Specifically, what are artificial transitions in the stable baselines framework? I can't find any clarification in the documentation (or anywhere else).",reinforcementlearning,Amumfie,False,/r/reinforcementlearning/comments/fpxuhf/what_are_artificial_transitions/
DQN model won't converge,1585311360,"I've recently finished David Silver's lectures on RL and thought implementing the DQN from ([https://www.cs.toronto.edu/\~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) ) would be a fun project.

I mostly followed the paper except my network uses 3 conv layers followed by a 128 FC layer. I don't preprocess the frames to a square. I am also not sampling batches of replay memory but instead sampling one replay memory at a time.

My model won't converge (I suspect it's because I'm not batch training but I'm not sure) and I wanted to get some inputs from you guys about what mistakes I'm making.

My code is available at  [https://github.com/andohuman/dqn](https://github.com/andohuman/dqn).

Thanks.",reinforcementlearning,Andohuman,False,/r/reinforcementlearning/comments/fpvx99/dqn_model_wont_converge/
DQN model won't converge,1585311295,"I've recently finished David Silver's lectures on RL and thought implementing the DQN from ([https://www.cs.toronto.edu/\~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf) ) would be a fun project. 

I mostly followed the paper except my network uses 3 conv layers followed by a 128 FC layer. I don't preprocess the frames to a square. I am also not sampling batches of replay memory but instead sampling one replay memory at a time. 

My model won't converge (I suspect it's because I'm not batch training but I'm not sure) and I wanted to get some inputs from you guys about what mistakes I'm making.

My code is available at  [https://github.com/andohuman/dqn](https://github.com/andohuman/dqn).

Thanks.",reinforcementlearning,Andohuman,False,/r/reinforcementlearning/comments/fpvwq5/dqn_model_wont_converge/
Safe Counterfactual Reinforcement Learning,1585309843,,reinforcementlearning,softmaru,False,/r/reinforcementlearning/comments/fpvliv/safe_counterfactual_reinforcement_learning/
Memory Card Game with RL,1585247200,"Hello,

I am new to RL, and I am thinking of doing a little project.

The goal of the project is to learn an agent play the memory game with cards.

I already created the program for detecting the cards on the table(with YOLO) and classifying them what kind of object they are.

Any tips how to get started to make the RL process easier?",reinforcementlearning,BobderBaumeister5,False,/r/reinforcementlearning/comments/fpgrl9/memory_card_game_with_rl/
Gold: Reinforcement Learning in Go,1585223551,,reinforcementlearning,yahyaheee,False,/r/reinforcementlearning/comments/fpa7k2/gold_reinforcement_learning_in_go/
Multi Matrix Deep Learning with GPUs - its role in Artificial Intelligence,1585221331,,reinforcementlearning,Albertchristopher,False,/r/reinforcementlearning/comments/fp9s2i/multi_matrix_deep_learning_with_gpus_its_role_in/
A talk about physical human animation with RL,1585185204,,reinforcementlearning,profbof,False,/r/reinforcementlearning/comments/fp2ejs/a_talk_about_physical_human_animation_with_rl/
Novice Issues with Pybullet,1585181233,"I did pip install pybullet and everything installed fine. However, I am now having trouble using import pybullet to run a simulation on OpenAI Gym. Please let me know if you can help.",reinforcementlearning,AWESAM22,False,/r/reinforcementlearning/comments/fp1cla/novice_issues_with_pybullet/
"""Meta Pseudo Labels"", Pham et al 2020 {GB}",1585171158,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/foyhi6/meta_pseudo_labels_pham_et_al_2020_gb/
Ben Lorica post on applications of RL,1585165898,,reinforcementlearning,bucktrends,False,/r/reinforcementlearning/comments/foww7g/ben_lorica_post_on_applications_of_rl/
Launching an RL environment for ML-Agents: The Mayan Adventure,1585146960,"Hey there 😃,

I’m launching the [Mayan Adventure](https://towardsdatascience.com/unity-ml-agents-the-mayan-adventure-2e15510d653b). An open-source **deep reinforcement learning environment on Unity ML-Agents.**

https://preview.redd.it/vltzffewxto41.jpg?width=2550&amp;format=pjpg&amp;auto=webp&amp;s=96dcbd784cdcaed8453dc6d8f823720857b6851b

In this environment, you train your agent (Indie) to find the golden statue in this dangerous environment full of traps. Your agent will learn to cross the bridge, change its physics to cross the fire etc.

I designed the project to be as modular as possible, **it means that you will be able to create new levels and new obstacles.** I’m currently working on two new levels: a rotating bridge and a rolling boulder level.

The Article: [https://towardsdatascience.com/unity-ml-agents-the-mayan-adventure-2e15510d653b](https://towardsdatascience.com/unity-ml-agents-the-mayan-adventure-2e15510d653b)

The Environment: [https://github.com/simoninithomas/the\_mayan\_adventure](https://github.com/simoninithomas/the_mayan_adventure)

The video of the trained agent: [https://youtu.be/kKng-vRy6bs](https://youtu.be/kKng-vRy6bs)

I would **love to hear your feedback** about this project.

Thanks!",reinforcementlearning,cranthir_,False,/r/reinforcementlearning/comments/forbmj/launching_an_rl_environment_for_mlagents_the/
[R] Do recent advancements in model-based deep reinforcement learning really improve data efficiency?,1585132185,"In this paper, researchers argue, and experimentally prove, that already existing model-free techniques can be much more data-efficient than it is assumed. They introduce a simple change to the state-of-the-art Rainbow DQN algorithm and show that it can achieve the same results given only 5% - 10% of the data it is often presented to need. Furthermore, it results in the same data-efficiency as the state-of-the-art model-based approaches while being much more stable, simpler, and requiring much less computation.  Check it out if you are interested?  

**Abstract:**  Reinforcement learning (RL) has seen great advancements in the past few years. Nevertheless, the consensus among the RL community is that currently used model-free methods, despite all their benefits, suffer from extreme data inefficiency. To circumvent this problem, novel model-based approaches were introduced that often claim to be much more efficient than their model-free counterparts. In this paper, however, we demonstrate that the state-of-the-art model-free Rainbow DQN algorithm can be trained using a much smaller number of samples than it is commonly reported. By simply allowing the algorithm to execute network updates more frequently we manage to reach similar or better results than existing model-based techniques, at a fraction of complexity and computational costs. Furthermore, based on the outcomes of the study, we argue that the agent similar to the modified Rainbow DQN that is presented in this paper should be used as a baseline for any future work aimed at improving sample efficiency of deep reinforcement learning. 

Research paper link:  [https://arxiv.org/abs/2003.10181v1](https://arxiv.org/abs/2003.10181v1)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/foo1sr/r_do_recent_advancements_in_modelbased_deep/
Need help with Every Visit Monte Carlo (MC Prediction),1585090769,"Hello, RL noob here and would greatly appreciate any help I can get!  I am asked to predict v\_pi for three different states (the only ones available using results obtained from two episodes from a behavior policy. Attaching an image of the question problem.  

So I proceeded as normal by calculating G\_t for all the episodes. However, when I'm calculating V\_pi(S\^C), I am getting a weighted sampling ratio of 0. Seeing how the target policy is to perform a\^C(move to state C) regardless of the state that it's in. From the results obtained from the two episodes we see that it's not always the case.  What am I missing here? Is there another way to go about it? Thank you and I hope I'm not violating any rules!

https://preview.redd.it/t1dw9k66apo41.png?width=1850&amp;format=png&amp;auto=webp&amp;s=54ecb7355414cb3cb48b164f01dea0f415165596",reinforcementlearning,melsalmi,False,/r/reinforcementlearning/comments/fof6r4/need_help_with_every_visit_monte_carlo_mc/
AlphaZero: Policy head questions,1585078057,"Having read the original paper and found that ""Illegal moves are masked out by setting their probabilities to zero, and re-normalising the probabilities over the remaining set of legal moves.""  I'm a bit confused as to how to do this in my own model (smaller version of AlphaZero).  The paper states that the policy head is represented as a 8 x 8 x 73 conv layer.  1st question: is there no SoftMax activation layer?  I'm used to architectures with a final dense layer &amp; SoftMax.  2nd question: how is a mask applied to the 8 x 8 x 73 layer?  If it were a dense layer I could understand adding a masking layer between the dense layer and the SoftMax activation layer.  Any clarification greatly appreciated.",reinforcementlearning,oldrigger,False,/r/reinforcementlearning/comments/fobd2u/alphazero_policy_head_questions/
I realized I never posted this here. It's a high level description of what I did to train a model to play Snake visually.,1585075585,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/foali4/i_realized_i_never_posted_this_here_its_a_high/
"Policy π vs Action-Value Function q(s,a)",1585065156,"Hi, I am completely new to RL.

Can someone explain Policy and Action-Value function in simple terms.

I learnt that we take an action based on Policy. Then what is Action value function which tells the best action to take at a given state?

What do we follow to take an action at a given state? Policy or Action-Value function?",reinforcementlearning,fwerop,False,/r/reinforcementlearning/comments/fo7dp7/policy_π_vs_actionvalue_function_qsa/
Intrinsic Motivation Definition,1585047895,"The definition of intrinsic motivation is the motivation to engage in activities that satisfy an individual's own goals. However, I find this definition to be recursive and not a good definition in terms of computation and RL. So, then, what is the ideal definition of intrinsic motivation? How should we compute intrinsic motivation? There are many methods which try to approximate intrinsic motivation, but I feel as if they lack some of the features of intrinsic motivation. For example, our current methods seem to always try to maximize entropy in order to aid exploration, but humans aren't exactly trying to  maximize entropy when they're intrinsically motivated. An artist wouldn't want to maximize entropy when creating a work of art, as an example...",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/fo3b6h/intrinsic_motivation_definition/
PPO - Surrogate loss has very small values. How do I fix it?,1585044084,"I implemented a PPO-Agent to learn an optimal strategy in an simulated environment (related to the topic of energy supply).  In the tensorboard charts I noticed a couple of odd things, for example the surrogate function has very small values compared the the entropy bonus. How should I interpret these small values and how do I fix it? 

&amp;#x200B;

https://preview.redd.it/5svxyj3bblo41.png?width=1046&amp;format=png&amp;auto=webp&amp;s=ddfca9fc009d89fa1bca8cd4baa4ef966762eecf

I scale all the reward with a constant factor, so the mean is about 0 and the variance is about 1. Also I use batch normalization for the GAE-values. My hyperparameter beta for the entropy term is 1e-3.

What's also  weird is that the policy needs a lot of training to get worse before it gets better. I guess that's because the random actions in the beginning are better than what the agent is trying to to, but why does is take so long?

&amp;#x200B;

https://preview.redd.it/ejfrb0syflo41.png?width=350&amp;format=png&amp;auto=webp&amp;s=29ee35012c37f91ef59883f5a6d0dfee46bac3c3

If somebody could have a quick look at my code I'd be super grateful.",reinforcementlearning,flxh13,False,/r/reinforcementlearning/comments/fo2kxt/ppo_surrogate_loss_has_very_small_values_how_do_i/
Been doing some with with the Vizdoom environment. Here's an agent finishing the corridor scenario.,1585028256,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fnzrzb/been_doing_some_with_with_the_vizdoom_environment/
OpenAI Gym help,1585009831,"I am having trouble doing a humanoid simulation for openAI Gym. I have my code but need to download MuJoCo and there are very few detailed resources that explain how to properly get it. Every time I run  pip3 install -U 'mujoco-py&lt;2.1,&gt;=2.0'   in the terminal of my VMware Ubuntu 18.04 virtual machine I get a ""failed building wheel"" error. I downloaded MuJoCo for Linux from their website and have an activation key. I followed the README's instructions regarding putting the txt file in the bin folder. I am very confused and would know if anyone here would be able to help, since i don't know anyone who can and the contributors of the MuJoCo GitHub page likely won't respond to my issue post. I even tried getting it for Windows too and that did not work either. Let me know if any more information is necessary for help.",reinforcementlearning,AWESAM22,False,/r/reinforcementlearning/comments/fnvfbr/openai_gym_help/
"""Placement Optimization with Deep Reinforcement Learning"", Goldie &amp; Mirhoseini 2020 {GB}",1585004650,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fnu0w0/placement_optimization_with_deep_reinforcement/
"""SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference"", Espeholt et al 2020 {GB} [open-sourcing TF2 library for scaling Impala/R2D2 to 64 TPUs/4.1k CPUs]",1585003952,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fnttqf/seed_rl_scalable_and_efficient_deeprl_with/
"[D] As of 2020, how does model-based RL compare with model-free RL? What's the state of the art in model-based RL?",1585001799,"When I first learned RL, I got exposed almost exclusively to model-free RL algorithms such as Q-learning, DQN or SAC, but I've recently been learning about model-based RL and find it a very interesting idea (I'm working on explainability so a building a good model is a promising direction). 

I have seen a few relatively recent papers on model-based RL, such as [TDM by BAIR](https://bairblog.github.io/2018/04/26/tdm/) or the ones presented in the [2017 Model Based RL lecture](http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_9_model_based_rl.pdf) by Sergey Levine, but it seems there's isn't as much work on it. I have the following doubts:

1) It seems to me that there's much less work on model-based RL than on model-free RL (correct me if I'm wrong). Is there a particular reason for this? Does it have a fundamental weakness?

2) Are there hard tasks where model-based RL beats state-of-the-art model-free RL algorithms?

3) What's the state-of-the-art in model-based RL as of 2020?",reinforcementlearning,anormalreddituser,False,/r/reinforcementlearning/comments/fnt80a/d_as_of_2020_how_does_modelbased_rl_compare_with/
Data-Efficient Hierarchical Reinforcement Learning,1584911789,,reinforcementlearning,anormalreddituser,False,/r/reinforcementlearning/comments/fn7emi/dataefficient_hierarchical_reinforcement_learning/
What does '~' mean in The goal of reinforcement learning?,1584903876,"What does '\~' mean in page 5 in [http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-5.pdf)?

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/h7t7zzm9v9o41.png?width=900&amp;format=png&amp;auto=webp&amp;s=f40a054d608cc5629d3be8014cb31c59c43f0eb3",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/fn533h/what_does_mean_in_the_goal_of_reinforcement/
Learn RL using a top-down approach?,1584903522,"I'm very familiar with ML and have some idea about RL as well. I want to really delve into RL by doing projects instead of just going through theory/videos. A top-down approach suits my need, so I was wondering if anyone has any idea how to start?",reinforcementlearning,-Ulkurz-,False,/r/reinforcementlearning/comments/fn4za4/learn_rl_using_a_topdown_approach/
Regarding the overestimation bias of DQN and TD3 (and overfitting?),1584902204,"So I'm studying the weel known paper that introduced TD3 ([https://arxiv.org/pdf/1802.09477.pdf](https://arxiv.org/pdf/1802.09477.pdf) )

and according to their results, the overestimation problem of the state-action value function has been significantly improved. In fact, another recent paper on the transaction on NN even states that TD3 introduces an underestimation problem ( [https://www.researchgate.net/publication/338588911\_Reducing\_Estimation\_Bias\_via\_Triplet-Average\_Deep\_Deterministic\_Policy\_Gradient](https://www.researchgate.net/publication/338588911_Reducing_Estimation_Bias_via_Triplet-Average_Deep_Deterministic_Policy_Gradient) ) and reading the papers it makes sense.

I started working with the original code for TD3 [https://github.com/sfujim/TD3](https://github.com/sfujim/TD3) , with a simple continuous cart pole environment (this one [https://gist.github.com/iandanforth/e3ffb67cf3623153e968f2afdfb01dc8](https://gist.github.com/iandanforth/e3ffb67cf3623153e968f2afdfb01dc8)) to avoid buying mujoco (and to run it on the cloud).

I have set the rewards in such a way that the maximum discounted reward is 100 (cart has to stay up 100 time steps, +1 for each time step save for the last where it gests 100,  with gamma=0.99).

&amp;#x200B;

I was expecting that the estimate of the Q function at the initial states would converge to something like 90 for the underestimation bias, but instead I see the following:

* the agent solves the environment when the estimate of Q, given by the critic's target networks, is VERY low (like 20)
* as the agent continues to learn (well before the 1e6 iteration reported in the TD3 paper), the estimation of the Q grows. slowly, but it grows.
* We get to the point where the estimation provided by the critics is beyond 5000 (with a maximum discounted reward of 100!)

I tried playing with the parameter tau (all the rest are as in the github), that regulates the update of the critic network, with no luck.

Why is an algorithm that should be underestimating the Q values overestimating them? it seems like overfitting of some sort ( as I am training the network even if the environment has been solved), but for now I'm lost.

Has anyone tested the accuracy in reconstructing the Q values of TD3? Is there some specific tuning to do that I'm missing?",reinforcementlearning,Aumanidol,False,/r/reinforcementlearning/comments/fn4l1v/regarding_the_overestimation_bias_of_dqn_and_td3/
"Model (PPO, stable) is working, however not all the time. Plot shows the amount of actions (action 1 &amp; 2, in blue and red respectively) taken per episode. It does “ok”, however it only “works” every now and then (peaks). Why are the episodes so inconsistant, Any ideas?",1584900459,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/fn41qa/model_ppo_stable_is_working_however_not_all_the/
Basics of reinforcement learning (for example- creating an agent etc) examples as a Jupyter notebook? Can someone point me to the right source ?,1584872163,"I am interested in learning absolute basics of RL, but using jupyter notebook. With my search, I have ended up with some good examples but they always assume I have coding knowledge of the subject and start directly with SARSA. I am interested in step by step code , Starting with how to create an agent, setting environments gradually building up to Deep Q learning? 

Any good pointers will be helpful !!

Thanks",reinforcementlearning,Abi92,False,/r/reinforcementlearning/comments/fmxqdu/basics_of_reinforcement_learning_for_example/
Empirical example of MCTS calculation PUCT formula,1584868437,,reinforcementlearning,promach,False,/r/reinforcementlearning/comments/fmx3pp/empirical_example_of_mcts_calculation_puct_formula/
What is the average number of episodes required to solve OpenAI gym Cartpole-V0 with DQN ?,1584843265,"Hi, I'm relatively new to machine learning and open AI gym. I'm currently trying to beat the cart pole v0  environment using DQN (implementing experience replay and target network) and using the environment observation space as input. I ran the model for 30 thousand episodes and the average score only seemed to decrease. I am not sure if this is because there is a flaw in the model or if I'm simply not training it long enough. How many episodes should I train it over?",reinforcementlearning,Za_Warudo_1000,False,/r/reinforcementlearning/comments/fms0nv/what_is_the_average_number_of_episodes_required/
"Diversity Is All You Need Implementation using RLKit, a PyTorch reinforcement learning framework",1584840517,,reinforcementlearning,johnlime3301,False,/r/reinforcementlearning/comments/fmrc8k/diversity_is_all_you_need_implementation_using/
Soft Actor Critic in TF2.1,1584762132,"I've ported the spinning up Soft Actor Critic implementation to TF2.1. If anyone's interested in using it, you can find it [here](https://github.com/zacwellmer/tf2_sac)",reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/fm8rjf/soft_actor_critic_in_tf21/
"PPO: Number of envs, number of steps, and learning rate",1584760487,"I just got my PPO implementation working and am a little confused about ho to pick the hyperparams here. Overall I've noticed that my environment performs best when I have a relatively smaller number of environments (128 in this case) and an even smaller number of steps for each before the next batch of training (4) with a low learning rate (0.0001). If I increase the number of environments or make the steps more the model's learning becomes way ... waaaayy slower.

What gives? What's a good way to tune these knobs? Can I kind soul point me towards some reading material for this? Thank you so much :)",reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fm8esm/ppo_number_of_envs_number_of_steps_and_learning/
CPU-trained agent perform better than GPU-trained agent.,1584750037,"Hi all,

Novice here.

I have identical RL code (PyTorch) running on my Mac Mini (CPU) and Ubuntu server with RDX 6000 (GPU). On CPU the average training loss decreases from 4.2689E+13 to 2.7119E+09 while at the GPU the loss goes from 2.6308E-02 to 7.1175E-03.

At the same time the GPU-trained agent performs much worse in my test environment: GPU trained agent can't make it further than 300 steps, while CPU-trained stops at my maximum of 20000 steps.

How could it be and what am I doing wrong?

Thank you in advance :-)

&amp;#x200B;

[CPU](https://preview.redd.it/i4hk90qv5xn41.png?width=1572&amp;format=png&amp;auto=webp&amp;s=da596f40adca7abcadbd37b0c557d9f5911eef5e)

&amp;#x200B;

[GPU](https://preview.redd.it/yjczl4lx5xn41.png?width=1548&amp;format=png&amp;auto=webp&amp;s=c1dc9781cd245ebb69fd50a60b88cb2d214ebf75)",reinforcementlearning,zbroyar,False,/r/reinforcementlearning/comments/fm5wp1/cputrained_agent_perform_better_than_gputrained/
TF vs Pytorch,1584739623,"Hi all. I did some reinforcement learning in Pytorch a few years ago before returning to school and would like to get back into it.  Both Pytorch and TF look like they've been updated quite a bit since then. I was wondering what the community thinks about the pros and cons of each?  (I don't remember much of the syntax for Pytorch, and I think it has changed a lot so there wouldn't be a switching cost)

Thanks!",reinforcementlearning,novas8,False,/r/reinforcementlearning/comments/fm343t/tf_vs_pytorch/
"No real trend with normalized rewards [-1,1], issue? Rewards are not very fixed. See comment for more. How to validate if it is working and is this how such results should even look? Grap contains reward per episode.",1584738241,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/fm2q5f/no_real_trend_with_normalized_rewards_11_issue/
PPO not working on OpenAI's gym Reacher_v2,1584728666,"I am using the code from [https://github.com/qqadssp/PPO-Pytorch](https://github.com/qqadssp/PPO-Pytorch).

After the agent has finished learning, it still cannot reach targets in an intelligent way and often fails to reach the targets at all. I have tried changing the learning rate, entropy coefficient, N\_steps per episode but nothing works. Any suggestions would be welcome.",reinforcementlearning,xicor7017,False,/r/reinforcementlearning/comments/flzy21/ppo_not_working_on_openais_gym_reacher_v2/
"""Enhanced POET: Open-Ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions"", Wang et al 2020 {Uber}",1584723752,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/flyiap/enhanced_poet_openended_reinforcement_learning/
Looking for Jobs in RL.,1584683926,"Hello,

Don't know if this fits here... 

I'm a 20-year-old person with physical disabilities. I'm not enrolled in any universities, because of my disability. I'm very interested in RL, and have implemented several deep reinforcement learning algorithms in PyTorch/Tensorflow. However, this information is quite useless to me, as I don't have a job or a doctorate in RL. I'd like a job in RL, but everything is so far away from me (Disability prevents me from travelling much) and the jobs all require university degrees, anyway. What should I do?",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/flpusu/looking_for_jobs_in_rl/
Anca Dragan: Human-Robot Interaction and Reward Engineering | AI Podcast #81 with Lex Fridman,1584681989,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/flpgpu/anca_dragan_humanrobot_interaction_and_reward/
Can we apply Monte Carlo tree search algorithm to an environment where it has various initial states?,1584677326,"Recently, I read a paper, [the link](https://arxiv.org/abs/1807.01672). This paper made a MCTS based model for 2d bin packing problem. But there is only an experiment result with a fixed count of the bins.  With this, we can get the generality of the model even if it has various count of bins?",reinforcementlearning,verystrongjoe,False,/r/reinforcementlearning/comments/flogfp/can_we_apply_monte_carlo_tree_search_algorithm_to/
How to create a Reinforcement Learning youtube live channel?,1584637918,"What do I need to be able to create a youtube channel, that is like this one [https://www.youtube.com/watch?v=ge9MVVEBHTo](https://www.youtube.com/watch?v=ge9MVVEBHTo) 

Except that the AI is learning a different game? (Beyond Oasis and old Sega game)",reinforcementlearning,cluhedos,False,/r/reinforcementlearning/comments/fldnin/how_to_create_a_reinforcement_learning_youtube/
Baba Is You simulator using C++ with some reinforcement learning,1584633516,,reinforcementlearning,utilForever,False,/r/reinforcementlearning/comments/flcd82/baba_is_you_simulator_using_c_with_some/
how to implement TD(lambda) by pytorch?,1584622654,"Hi everyone. 
How to implement backward view algorithm of TD(lambda).
what is the loss function? or we have to update weight manually by the formula?
are there some elegance method to update grad?",reinforcementlearning,fengyanghe,False,/r/reinforcementlearning/comments/fl9lef/how_to_implement_tdlambda_by_pytorch/
[R] Neuroevolution of Self-Interpretable Agents,1584588511,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/fl33z3/r_neuroevolution_of_selfinterpretable_agents/
[R] Neuroevolution of Self-Interpretable Agents,1584587739,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/fl2xq3/r_neuroevolution_of_selfinterpretable_agents/
"""Introducing Dreamer: Scalable Reinforcement Learning Using World Models"", Hafner et al 2020 {DM} [PlaNet]",1584571371,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fkytu2/introducing_dreamer_scalable_reinforcement/
AlphaGo - The Movie | Full Documentary,1584563437,,reinforcementlearning,PsyRex2011,False,/r/reinforcementlearning/comments/fkwkkt/alphago_the_movie_full_documentary/
Monte-Carlo Tree Search : WU-CT versus PUCT,1584547918,,reinforcementlearning,promach,False,/r/reinforcementlearning/comments/fks21b/montecarlo_tree_search_wuct_versus_puct/
Competitive Snake,1584546258,"Hello, I developed a competitive snake game and I wanted to share it with you. There are 2 snakes on a small area and the purpose of each player is to surround the opponent. You can play with arrow keys against AI. I used reinforcement learning to train the agent. \[[https://alpersekerci.itch.io/competitive-snake](https://alpersekerci.itch.io/competitive-snake)\]

&amp;#x200B;

The neural network architecture is very similar to AlphaGo, it has a policy and a value head and I used PPO algorithm. Also, I used a league system to choose an opponent for my learning agent. Simply, there is a pool of agents and each agent has an ELO rating. Initially, the only agent is the 'random agent', with ELO rating 0. When the learning agent achieves 64% win rate against the strongest opponent, it means the ELO rating of the learning agent is 100 more than the strongest. In that case, I freeze the agent and add it to the pool. Now, there are 2 agents in the pool: 0 and 100, and the training goes like this.

&amp;#x200B;

I have been practicing with RL for some time, finally this game is the first meaningful project I can share with people. I proposed a challenge to my friends; the aim is to get 2 consecutive wins (ignoring draws). Fortunately for me, it took a long time for them to achieve this (but still my agent had a better win rate). I would also like to hear your opinions, and your results if you want to accept this challenge. :D

&amp;#x200B;

If you want to learn more details about the algorithm, feel free to ask! Thank you for reading.",reinforcementlearning,AlperSekerci,False,/r/reinforcementlearning/comments/fkrl0g/competitive_snake/
Deep Reinforcement learning for UAV,1584540152,"Hi all, 

I am trying to simulate UAV environment and apply deep reinforcement learning to allocate channels for UAV. I understand the theories of RL but i am nor sure on where to implement it practically. I am not sure which tool to use. I need a suggestion on which one is better : Matlab, Unity or Gym ?",reinforcementlearning,aashmauprety,False,/r/reinforcementlearning/comments/fkpz5u/deep_reinforcement_learning_for_uav/
RL research,1584539339,"I’m planning to do my AI masters thesis on RL. Not sure of the exact topic yet, What are you favourite resources to get started?  I’m looking for papers, tutorials, books etc. Haven’t taken any RL modules yet as part of my course.",reinforcementlearning,_loopylady_,False,/r/reinforcementlearning/comments/fkpsek/rl_research/
Which baseline do you use for optimizing multiple action spaces?,1584516167,I'm trying to implement PPO on a custom environment. I need to optimize for two different action spaces simultaneously. I tried using [stable-baselines](https://github.com/hill-a/stable-baselines) but multiple action spaces are not supported in it. Have you ever come across a solution that uses baselines for optimizing two action spaces?,reinforcementlearning,Capn_Sparrow0404,False,/r/reinforcementlearning/comments/fkllzq/which_baseline_do_you_use_for_optimizing_multiple/
Upper Confidence Bound. How do I count actions?,1584512461,"I understand UCB is a count-based method. When working with small environments, I can represent all states explicitly, making counting actions on them possible. With large environments, we don't have a table of states, so how can I implement UCB?",reinforcementlearning,ritiange,False,/r/reinforcementlearning/comments/fkkz1f/upper_confidence_bound_how_do_i_count_actions/
Importance Sampling - Sutton and Barto,1584503065,"Hi, I'm a highschool student having a bit of trouble with understanding their definition of Importance Sampling as related to Monto Carlo processes --- I can't find any good videos or articles that explain it without using maths or ideas that are outside the scope of the book. Any articles or videos you would suggest?",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/fkj4y4/importance_sampling_sutton_and_barto/
Anyone down to review my PPO code?,1584488831,"I've been working to implement PPO (or rather stitching things together from existing resources, namely RL Adventure and Ilya Kostrikov's repo). I think I have something now that should be correct and I'm training my environment on it right now but was hoping someone more knowledgeable might be willing to look over the code. You can find the code here ([https://github.com/safijari/jack-of-some-rl-journey/blob/master/pytorch\_common.py](https://github.com/safijari/jack-of-some-rl-journey/blob/master/pytorch_common.py)). I love to do live code reviews with my team since that makes it easy to give context to the reviewer so if someone is willing to do that please hit me up.

Thanks :)",reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/fkfprh/anyone_down_to_review_my_ppo_code/
"""Does On-Policy Data Collection Fix Errors in Off-Policy Reinforcement Learning?"" [""DisCor: Corrective Feedback...via Distribution Correction"", Kumar et al 2020 {BAIR}]",1584469741,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fkabqz/does_onpolicy_data_collection_fix_errors_in/
SAC statistics,1584453631,"Hello, 

I am using a Soft Actor Critic implementation on a custom environment. Right now the results are not perfect and I am trying to see if I can find any useful information observing SAC losses evolution. My question is, what behaviour should I expect with the evolution of:

· Q loss (I have two Q -networks)

· Policy loss

· Entropy loss

· Alpha parameter

My guess is that alpha starts high and reduces over time and the same for Q loss.

Any other ideas?

Thanks in advance.",reinforcementlearning,LazyButAmbitious,False,/r/reinforcementlearning/comments/fk5q7g/sac_statistics/
'Diversity is all you need: learning skills' implementation with a hardcoded Discriminator,1584411274,"Hi,

I am trying to implement ""DIVERSITY IS ALL YOU NEED: LEARNING SKILLS WITHOUT A REWARD FUNCTION"" paper for a grid world.

The agent has to learn policy(action|state,z). State, for the most part, is the image patch around its location(around 150 bits) and 2 bits for its normalized x,y coordinates. The latent variable z is the skill.

I am trying to make this work with a hardcoded discriminator and later on learn the discriminator at each step.

I hardcoded the discriminator such that it predicts high log(probability(z=0|state)) i.e high rewards when the bot is in the top half of the grid world for skill=0 and high log(probability(z=1|state)) i.e high rewards when the bot is in the bottom half of the grid world for skill=1. This hardcoded discriminator only looks at the 2 bits for x,y coordinates.

I first trained the agent by making sure that skill z=0 is sampled all the time. The agent converges pretty fast in this setting.([https://www.dropbox.com/s/f7b3bo7kx8fphh0/Screenshot%20from%202020-03-16%2021-57-24.png?dl=0](https://www.dropbox.com/s/f7b3bo7kx8fphh0/Screenshot%20from%202020-03-16%2021-57-24.png?dl=0)). It just learns to go North all the time.

Later, I trained the agent by sampling z from \[0,1\] at the start of the episode. The agent rewards fluctuate around zero. My interpretation is that agent overfits at every episode and learns to either move up or down irrespective of z. It fails to capture that it has to move north for z=1 and move south for z=0( [https://www.dropbox.com/s/an64ibl3672brnp/Screenshot%20from%202020-03-16%2021-57-00.png?dl=0](https://www.dropbox.com/s/an64ibl3672brnp/Screenshot%20from%202020-03-16%2021-57-00.png?dl=0)). I reduced the ppo epoch to reduce overfitting. The magnitude of fluctuation around zero has reduced, but it still fluctuates([https://www.dropbox.com/s/utwhqhralrvybcj/Screenshot%20from%202020-03-16%2021-57-17.png?dl=0](https://www.dropbox.com/s/utwhqhralrvybcj/Screenshot%20from%202020-03-16%2021-57-17.png?dl=0)).

How do I make the agent pay attention to the skill latent variable? Please note that I am passing skill variable by passing though a randomly initialized 32 dim embedding layer which is being learned(by backpropagating) during training.",reinforcementlearning,Crazy_Plant,False,/r/reinforcementlearning/comments/fjx8t1/diversity_is_all_you_need_learning_skills/
"My bot is stupid, what do ?",1584383828,,reinforcementlearning,lifeinsrndpt,False,/r/reinforcementlearning/comments/fjpwcw/my_bot_is_stupid_what_do/
A Survey and Critique of Multiagent Deep Reinforcement Learning,1584381644,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fjp9nw/a_survey_and_critique_of_multiagent_deep/
Sparse Rewards + DQN,1584379022,"Hey, I am trying to train a DQN agent in a very sparse environment. Though so far it does not work properly:

**Environment**: I implemented a simple environment myself, which is basically a puzzle of two pieces with discrete action space. One puzzle piece is fixed, the other one can be controlled by the rl agent. The reward is 1 if both puzzle pieces are put together properly and -1 otherwise. I set the episode length to 100, though I am only saving every 4th frame in the buffer.

**Agent**: So far I am using DQN, Hindsight Experience Replay and Prioritized Experience Replay. I am also initialising the Replay Buffer with successfull trajectories. 

**Problems:**

1) The loss first converges though at some point starts diverging to infinity..

2) I was only able to sometimes train the agent successfully by customising the HER approach. 

Original version: Adding the last next\_state as goal for all samples in one episode. Set last reward to 1 and done to True.

Custom: Sample a random number between 0 and 1 for each state in one episode and check for threshold: If above threshold, change the goal to the next state, reward to 1 and set done to True.

I guess my custom approach has been working in some runs since it adds a lot more samples with positive rewards compared to the original version. On the other hand it can be quite misleading if e.g. the agent just jumps back and forth between two states.

3) I could not spot any difference when applying Prioritized Experience Replay. Shouldn't it work well in such a sparse environment?

4) Do you think policy gradients might be more suited for the problem?

I would be very grateful for any suggestions and help :)",reinforcementlearning,al_les,False,/r/reinforcementlearning/comments/fjoilx/sparse_rewards_dqn/
Sota or literature review for multi agent and hierarchical learning?,1584377085,"Hi all, I’m writing my undergraduate thesis and have put together some ideas however due to limitations in my resources I don’t have that much money to spend. As such I’m looking for some literature on some of the latest techniques (mentioned in title) that can reduce my sample complexity as much as possible.

At the moment I have an integration of rainbow DQN with Curiosity but it’s still not as effective and I would have hoped, I’m looking to combine it with some additional methods such as HRL to bring down my sample complexity. The final experiment is set in a multi agent setting. At the moment my group and I are integrating mean field multi agent learning (as it reduces computational complexity significantly). But if there have been follow up methods that perform better I have not been able to find them! If anyone could point me to any good papers or literature reviews I would really appreciate it.",reinforcementlearning,btwhy_,False,/r/reinforcementlearning/comments/fjnz30/sota_or_literature_review_for_multi_agent_and/
How to understand retrace,1584367908,"Are there some good tutorial about Retrace? It is difficult to understand by the original paper.
Thanks.",reinforcementlearning,fengyanghe,False,/r/reinforcementlearning/comments/fjlk8y/how_to_understand_retrace/
Meta reinforcement learning as task inference,1584340593,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fjgj61/meta_reinforcement_learning_as_task_inference/
DQN for Cartpole,1584306778,"I am trying to implement DQN for the cartpole environment but can't get the model to optimize the reward. I looked online for other resources that tried this and I don't see a bug in the code.

If anyone can help, please let me know. I been working on this for the past two days and don't know what is wrong. I have attached the link to the code below.

[https://github.com/domhuh/Reinforcement\_Learning/blob/master/src/dqn.py](https://github.com/domhuh/Reinforcement_Learning/blob/master/src/dqn.py)

[https://github.com/domhuh/Reinforcement\_Learning/blob/master/src/replaybuffer.py](https://github.com/domhuh/Reinforcement_Learning/blob/master/src/replaybuffer.py)",reinforcementlearning,domhuh4,False,/r/reinforcementlearning/comments/fj8osu/dqn_for_cartpole/
[D] Policy Gradients with Memory,1584261601,"I'm trying to run parallel PPO with a CNN-LSTM model (my own implementation). However, it seems that leaving the gradients piling up for 100s of timesteps before doing a backprop is easily overflowing the memory capacity of my V100. My suspicion is that this is due to the BPTT. Does anyone have any experience with this? Is there some way to train with truncated BPTT?

In this implementation: [https://github.com/lcswillems/torch-ac](https://github.com/lcswillems/torch-ac)

There is a parameter called \`recurrence\` that does the following:

a number to specify over how many timesteps gradient is backpropagated. This number is only taken into account if a recurrent model is used and **must divide** the num\_frames\_per\_agent parameter and, for PPO, the batch\_size parameter. 

However, I'm not really sure how it works. It would still require you to hold the whole batch\_size worth of BPTT gradients in memory, correct?",reinforcementlearning,BrahmaTheCreator,False,/r/reinforcementlearning/comments/fiycwh/d_policy_gradients_with_memory/
What is the name of the fancy S symbol that represents a set of states and how do I get it in Latex?,1584240618,"The one highlighted in blue/grey from Sutton and Barto's book.

Thanks.",reinforcementlearning,Trigaten,False,/r/reinforcementlearning/comments/fiul64/what_is_the_name_of_the_fancy_s_symbol_that/
Why is GAIL better than BC?,1584203132,"In what scenarios is GAIL preferred to Behavior Cloning (BC)? In my experiments, BC is much faster to train in achieving the same performance. 

The original GAIL paper does something like subsampling trajectories to make the task of imitation more difficult - but in what real scenarios would you want to do that? Why would you not take advantage of the full trajectories? All my experiments are tested with full trajectories, and I cannot see how GAIL is preferable to BC in those circumstances. 

Thanks!",reinforcementlearning,piaget-marr,False,/r/reinforcementlearning/comments/fikyrq/why_is_gail_better_than_bc/
Double Q-Learning,1584198980,"Hey!  
I'm trying to implement the algorithm DQN (2010)  
In the paper  [http://papers.nips.cc/paper/3964-double-q-learning.pdf](http://papers.nips.cc/paper/3964-double-q-learning.pdf)  pseudocode page 5 line 3, I don't quite understand how we are supposed to choose an action using Qa AND Qb   
Does someone know what is supposed to be the right method ? (randomly, ..)  
Thanks :)",reinforcementlearning,thomashirtz,False,/r/reinforcementlearning/comments/fijvi9/double_qlearning/
Gradient scaling in Muzero,1584153838,"Hello,

I am having a hard time understanding the reason behind part of the Muzero pseudocode and appreciate any help or comment. The authors scale the gradients of hidden states by 0.5 after each call of recurrent\_inference:

    for action in actions:
        value, reward, policy_logits, hidden_state = network.recurrent_inference(hidden_state, action)
        predictions.append((1.0 / len(actions), value, reward, policy_logits))
        hidden_state = scale_gradient(hidden_state, 0.5)

In the paper, they stated that ""this ensures that the total gradient applied to the dynamics function stays constant"". But why does it help to achieve a constant gradient? Why 0.5?",reinforcementlearning,mojtabamozaffar,False,/r/reinforcementlearning/comments/fibq78/gradient_scaling_in_muzero/
[Vowpal Wabbit] How to get cost/reward current estimation for all arms,1584142432,"Hello guys,

I am starting to work with Vowpal Wabbit with Python and I am kinda struggling with its lack of documentation.

Do you guys know what modeling it uses as a cost/reward estiamtion for each arm? Do you know how to retrieve this  current estimation?  


    vw = pyvw.vw(""--cb_explore 2 --epsilon 0.2"")
    input = ""2:-20:0.5 | Anna""
    vw.learn(initial_input)
    input = ""1:-10:0.1 | Anna""
    vw.learn(initial_input)
    vw.predict("" | Anna"")

Output would be: 

    [0.10000000149011612, 0.9000000357627869]

How can I also get the expected value for each arm?",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/fi94mw/vowpal_wabbit_how_to_get_costreward_current/
Stable lstm net_arch for LSTM,1584137110,"Anyone knows how I can use the net_arch parameter in stable baselines? Instead of mlp policies, Looks like net_arch=[64,64], but with the lstm policies you need to add an extra argument, lstm. I thought it was net_arch=[64,64,”lstm”]. I had it once but I deleted it and forgot what it was and its like not shown on the documentation how it should he used exact. Speaking of the policy_kwargs argument for the ppo2 policies (lstm). Anyone knows?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/fi7rny/stable_lstm_net_arch_for_lstm/
Are there any parallel implementations of SAC or other sample efficient algorithms,1584064940,"Hello, so I've been using SAC for a project for its sample efficiency. The environment for this project is pretty complex and requires a long time to take each step. I've been hoping to try and parallelize things but came across this thread from a while ago saying that it was difficult to parallelize SAC due to how experiences and gradient steps are usually taken in sequence.

Being relatively new to rl, I was wondering if anyone had any suggestions on sample efficient algorithms (like SAC) that can be trained in parallel (e.g. with MPI).",reinforcementlearning,shrekbehindu,False,/r/reinforcementlearning/comments/fhs4lb/are_there_any_parallel_implementations_of_sac_or/
[Beginner question] I'm struggling to understand the purpose of contextual bandits,1584036323,"I have a continuous state space of \[0,1\] and discrete action space of (5,). Based on my actions and the resulting state, I calculate my reward (action-based rewards). For an episode, I'm choosing only one action. Hence, I want it to be the best optimal action for that state. Based on these conditions, I was told that I should go for Contextual Bandits (CB) algorithm.

But why should I do that? What is the real-world purpose of CB? If I want to choose an action, I can calculate rewards for each action and choose the one with maximum reward. Why do I have to use CB here? I know I'm thinking short-sighted here. But most articles talk only about the slot machines as example. So it would be really helpful if someone can explain to me the bigger picture.",reinforcementlearning,Capn_Sparrow0404,False,/r/reinforcementlearning/comments/fhkmlu/beginner_question_im_struggling_to_understand_the/
[R]The MineRL Competition on Sample-Efficient Reinforcement Learning Using Human Priors: A Retrospective,1583985674,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/fhag0p/rthe_minerl_competition_on_sampleefficient/
El Monstruo Que Vive entre Puerto Rico y República Dominicana,1583957376,,reinforcementlearning,yuveris,False,/r/reinforcementlearning/comments/fh3ggb/el_monstruo_que_vive_entre_puerto_rico_y/
Hindsight experience replay -&gt; future and episode version + reward function,1583926304,"Hi I am reading the HER paper and there are few things that I do not understand. There is a few version of HER such as final, future and episode. According to paper the future version is the best one. But what is the difference between the future and the episode version?

https://preview.redd.it/qljnxim431m41.png?width=1080&amp;format=png&amp;auto=webp&amp;s=1f5884a14e89876da2df2a912fe41e549922e9a5

So far I ve been using the final version. However, in practice (at least in my implementation) I have found that the problem with reward function

&amp;#x200B;

https://preview.redd.it/kn9h8iqh31m41.png?width=1027&amp;format=png&amp;auto=webp&amp;s=4f1fcf0ceb93429c4a652c2a8bf744e8f252d344

In case of sparse rewards 0,-1 (0 if solved, -1 otherwise)... r' should have value 0 only if the g' = st. So in case of having 5 timesteps per episode and using HER, the new transitions using additional goal should look like this:  
`r0 = -1, r1 = -1, r2=-1, r3=-1, r4=0`

Unfortunately, with this I cannot solve the robotic environment `FetchPush`. It can be only solved with this: `r0 = 0, r1 = 0, r2=0, r3=0, r4=0`  


Do I miss something? Thank you in advance for your help",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/fgvo4y/hindsight_experience_replay_future_and_episode/
Meta Experience Replay,1583925750,"New to RL... I have observed that experience replay sttrategies are crafted by hand, and thus my  question is: Can you have two agents, one for determining the best experience replay strategy, and another agent determining the best action to take? The first agent would train on the latest samples from the main agent, thus avoiding 'maximizing' the reward by simply repeating the same sample.",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/fgvkcp/meta_experience_replay/
[Research] Interactive Robot Training for Non-Markov Tasks,1583925456," Abstract: Defining sound and complete specifications for robots using formal languages is challenging while learning formal specifications directly from demonstrations can lead to over-constrained task policies. In this paper, we propose a Bayesian interactive robot training framework that allows the robot to learn from both demonstrations provided by a teacher, and that teacher's assessments of the robot's task executions. We also present an active learning approach -- inspired by uncertainty sampling -- to identify the task execution with the most uncertain degree of acceptability. We demonstrate that active learning within our framework identifies a teacher's intended task specification to a greater degree of similarity when compared with an approach that learns purely from demonstrations. Finally, we also conduct a user-study that demonstrates the efficacy of our active learning framework in learning a table-setting task from a human teacher. 

Receive #AI research updates for free 👉 http://eepurl.com/ghCeNn 

PDF Link: https://arxiv.org/abs/2003.02232v1",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/fgviej/research_interactive_robot_training_for_nonmarkov/
Programming a UCB Learning Player in tic-tac-toe using R (beginner),1583895952,,reinforcementlearning,IchiEight,False,/r/reinforcementlearning/comments/fgqgi3/programming_a_ucb_learning_player_in_tictactoe/
Actor Critic Model for Recommendation System,1583891651,"Hey Everyone ! I am trying to implement **Actor-Critic Model for recommendation systems** and I am confused about the following part.

For an **ACTOR** model which takes **user states** as input, and throws out an **action** (or probability scores), how do you handle changing action spaces (Example Catalog Change, Constraints, etc) ?

Example:

Say I have top 1000 different candidates generated by Algo1 everyday and I am building the RL model to re-rank them. For the Actor Model, the user comes in with a state **St** and it gives some probability distribution of these 1000 items and then the critic can validate the error.

In this scenario, the indices/actions carry no meaning as these items change everyday. There has to be a way to tie the states to the action space, which changes. So should I consider the item embeddings as a part of the State and my Action should be a single node with a probability of selection or is there something that I am missing.

Would really appreciate any input.

Thanks :)",reinforcementlearning,altair9335,False,/r/reinforcementlearning/comments/fgpgt5/actor_critic_model_for_recommendation_system/
"Mujoco 2L How to get (x,y,z) coordinates of robot body in locomotion envs ?",1583887314,"Greetings.

I am currently trying to plot the trajectories of an RL agent that is trained in some of the standard Mujoco 2 environments such as `Hopper-v2`, `Ant-v2`, `HalfCheetah-v2` and such. Hence, I find myself in the need to access the x and y coordinates of the agent's body, or to be more precise, its base link.

First, I managed to access to the `qpos` element of simulation namely by instanciating the environment with `env = gym.make( ""Hopper-v2"")`, then accessing the object `env.env.data.qpos`.

In the Hopper-v2 case, it returns a vector of length 6, as below:
```python
# DEBUG: qpos -- Length 6
array([-0.00224839,  1.24770782,  0.00373275,  0.00394404,  0.00452884, 0.00162659])
```
The problem is, I do not have any idea of which one is which. So far, I went through the official documentation at https://openai.github.io/mujoco-py/build/html/reference.html , which allegedly describes the content and attribute of the PyMjData class that the `env.env.data` object is an instance of.

Could anyone please clarify if this is the right data object to access for the intended purpose ? Or even direct me toward a more exhaustive documnetation of the environments and Mujoco 2 in general in case I missed it ?

Thank you very much for your time.

EXTRA: I also found this `env.env.data.body_xpos` which seems to be exactly what I need, but again, its output is quite cryptic and there does not seem to be any documentation in this regard. A sample of what it returns at initizalization of the Hopper-v2 environment:

```python
# DEBUG: body_xpos 5
array([[ 0.        ,  0.        ,  0.        ],
       [-0.00271627,  0.        ,  1.24483737],
       [-0.00247414,  0.        ,  1.04483751],
       [-0.00148755,  0.        ,  0.34483887],
       [ 0.12987859,  0.        , -0.00443781]])
```",reinforcementlearning,dosssman,False,/r/reinforcementlearning/comments/fgofeo/mujoco_2l_how_to_get_xyz_coordinates_of_robot/
[D] ICLR 2020 Cancelled Over Coronavirus; Virtual Conference Arranged,1583887049,"Organizers announced today that the COVID-19 outbreak and associated travel restrictions have forced the cancellation of the “physical conference” of the Eighth International Conference on Learning Representations (ICLR 2020). A fully virtual conference is being arranged, which will still take place from April 26 to 30.

[Read more](https://medium.com/syncedreview/iclr-2020-cancelled-over-coronavirus-virtual-conference-arranged-c737279e4c49)",reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/fgocyu/d_iclr_2020_cancelled_over_coronavirus_virtual/
Hi! I am interested in turning my deep reinforcement learning interest into a startup. Please pm me if you want to do the same!,1583883636,,reinforcementlearning,OmnisciousDev,False,/r/reinforcementlearning/comments/fgnjmd/hi_i_am_interested_in_turning_my_deep/
"""AutoML-Zero: Evolving Machine Learning Algorithms From Scratch"", Real et al 2020 {GB} [evolutionary search to evolve SGD &amp; regularizations]",1583874384,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fgl35x/automlzero_evolving_machine_learning_algorithms/
Learn Variational Autoencoder separately or during training?,1583837435,"I want to use a variational autoencoder on the state space (e.g. for visual observations) for my RL agent. I have a quite big dataset of the visual observations of my environment.

Should I train the VAE separately first, and then train the RL agent using the VAE on the visual observations? Or there is some other method that I'm not aware of?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/fgbxrm/learn_variational_autoencoder_separately_or/
Didn't realize this community existed so cross posting here,1583774526,,reinforcementlearning,jack-of-some,False,/r/reinforcementlearning/comments/ffy5k2/didnt_realize_this_community_existed_so_cross/
[D] Transfer learning for RL,1583771402,"In supervised learning it is common to take a pretrained model that was originally trained on one dataset and finetune it on another dataset to enable it to learn faster or with less data. How far are we from being able to do this in RL? It seems like with models like [PopArt-IMPALA](https://arxiv.org/abs/1809.04474) that we would not be so far from being able to do this, but it isn't something I have heard much about. Is anyone aware of work that has been done in this direction? I would image there are a few challenges.

1. You would probably need a very large number of different tasks to be able to pretrain an agent that would tend to perform well off the bat in new environments. The normal Atari benchmark only has 57 different tasks, which might not be enough. [Gym Retro](https://github.com/openai/retro) has a huge number of games, but I am not aware of any work trying to train a multitask agent across that whole library of games.
2. There is probably a lot more variety in the kinds of environments that an RL agent might encounter that would make it more difficult to form good general representations due to destructive interference, but I would still expect there to be some amount of useful transfer, maybe more so with a metalearning approach.
3. Another difficulty might be in dealing with differences in the observation or action space. Still this seems like it could probably be handled with pooling/RNNs/Attention and metalearning.
4. According to [this paper](https://arxiv.org/abs/1910.00571) an egocentric perspective on the environment greatly improves an agents ability to generalize, something many environments don't have. Maybe visual attention could help?
5. Obviously, of course it would take a huge amount of computing power.

 
Still, though it is surprising I haven't heard more about this, or even negative results of people attempting it and being unsuccessful. I wonder if maybe there are some large organizations like Deepmind or OpenAI that have tried this but are keeping quiet about it. Either way it seems like it would be helpful in a lot of scenarios to have a pretrained agent that could speed up learning somewhat for new tasks. If anybody knows about this or would like to work on something in this direction I would love to discuss it further.",reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/ffxbrf/d_transfer_learning_for_rl/
[Github] Track PyTorch Experiments,1583746527,"This is a small library I've been developing to track machine learning experiments

[https://github.com/vpj/lab](https://github.com/vpj/lab#)

It also comes with a web UI to browse experiments.[https://github.com/vpj/lab\_dashboard](https://github.com/vpj/lab_dashboard). 

This is a side project that I started off several months ago by combining a bunch of helpers I had and I would like to know your opinions and suggestions.",reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/ffs5zf/github_track_pytorch_experiments/
Win free bitcoin and enjoy playing dice on the coolest dice site! Visit us today!,1583691663,,reinforcementlearning,FlakyTradition5,False,/r/reinforcementlearning/comments/ffgjoh/win_free_bitcoin_and_enjoy_playing_dice_on_the/
Value function for finite horizon setup - implementation of time dependance,1583686957,"Value function is stationary for infinite horizon setup (does not depend on timestep), but this is not the case if we have finite horizon. How can we deal with it with neural network value function approximators? Should we feed timestep together with the state to the state value network?

I remember that it was shortly mentioned during one of the CS294 lecture by Sergey Levine, I think after a student question, but I am not able to find it now.",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/fffdg5/value_function_for_finite_horizon_setup/
[GitHub] A JAX based TD3 implementation,1583658346,"As Google/DeepMind have started to push JAX/RLax/Haiku and seem to be using it pretty extensively, I decided it's worth getting to know these libraries.

[https://github.com/tesslerc/TD3-JAX](https://github.com/tesslerc/TD3-JAX)

There seem to be several differences, yet I can't fully place my finger on them yet, between certain operations in PyTorch and in JAX. As a result, the hyperparameters do not transfer perfectly, and require some tweaking.

Feel free to suggest improvements both in terms of hyperparameters and implementation.",reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/ffa4if/github_a_jax_based_td3_implementation/
SAC - log std dev unchanged,1583644047,"Any advice is appreciated. I am trying to implement Soft actor critic, and believe the algorithm is setup correctly. However, during training I notice the log standard deviations outputted from the neutral network are mostly centered about zero, whereas I would expect them to trend increasingly negative as the policy becomes confident in itself. I have tried increasing learning rate without much success. The result is the means themselves trending around zero as well, therefore the action at evaluation is usually zero. Was curious if anyone else has experienced this, and appreciate the help.",reinforcementlearning,algobar,False,/r/reinforcementlearning/comments/ff7w4x/sac_log_std_dev_unchanged/
[Beginner question] How to model a constrained tetris game,1583574233,"Hi, I am fairly new to reinforcement learning. I was trying to think of the possible States, Actions and Rewards for a constrained tetris game. 

Constraint(applied to make the game minimalistic):

1. The next coming block information is not available to the agent. 

2. Blocks are rectangles but of varied lengths. i.e. R(x, y) representing width, height of a piece where x, y \in {1, 2, 3}
And the dimension of the play area is (6, 12). 

3. Once the entire row is completed the row doesn't disappear, however, score is updated as completed_rows * 6.

4. The block piece is not moving/falling on its own. 

5. 'Place' action fixes the block in the last position and agent moves on to control a new piece generated by the environment. however, no place action can be used unless the piece is directly above another piece or the bottom floor.


State S = (center_x, center_y, w ,h). Starting state is at center_y = 12 and center_x obeying the play area width.

Actions A = {left, right, down, rot_clock, rot_anticlock, place). Rotation is 90°.

Rewards = -1 for every passing time step

                 = Area of box after the place action + score(if row completed)

Task is to maximise the score which an agent can get.

**Questions**


1. Will the agent able to play this modified game optimally?
My concern is that I have not added any penalty for invalid actions(moving into other box or outside area). I only plan to dismiss the invalid action if performed and assign state' = state and update reward -1

2. How will the agent know the current configuration of the play area i.e. how the existing pieces are placed? Do I need to add more information for this?

Cheers.",reinforcementlearning,PaganPasta,False,/r/reinforcementlearning/comments/fet2gw/beginner_question_how_to_model_a_constrained/
Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms,1583553456,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fepqkk/multiagent_reinforcement_learning_a_selective/
Improvement of an RL agent in a zero-sum card game.,1583513099,"So I am building an RL agent for a 4 player card game. I am using standard actor-critic with PPO. The card game has teammates in it such that every other person in the circle gets the same points as per rules. The problem is that I have a hard time finding out if the agent is actually learning or not. The game always has two agents who win by the last round. Normally, you’d plot the average reward the agent is getting as the episodes increase, but in this case the graph of average rewards is almost consistent because you roughly win 50% of the games even at random. So my question is, how can I really know that the agent(s) is learning the game so that I can train them for longer period of time. TIA.",reinforcementlearning,alikhan97,False,/r/reinforcementlearning/comments/feg6d8/improvement_of_an_rl_agent_in_a_zerosum_card_game/
Stanford CS330 complete course with readings and assignments,1583502780,,reinforcementlearning,tarazeroc,False,/r/reinforcementlearning/comments/fedrgo/stanford_cs330_complete_course_with_readings_and/
Practical Guide to Feature Selection and Hyperparameter Tuning in Reinforcement Learning,1583489867,"I am looking for robust methods for *feature selection* and *hyperparameter tuning* that work for *any* RL algorithm based on *function approximation* with *episodic environments*.  In particular, I am considering algorithms implemented in OpenAI baselines or stable-baselines. I am considering environments, where feature selection makes sense because the researcher designs the features for the state space (unlike in e.g. Atari games where the features are basically given as image pixels). In particular, I am considering environments based on static data sets that can be split into train/validation/test sets.

This is a complex topic, so there are many questions. However, it boils down to: **Where are the practical guides and best practice examples - for feature selection and hyperparameter tuning?** I'd be glad about links to useful materials. 

***

My impression is that the methods suggested (for feature selection/hyperparameter tuning in RL) by the literature are very specific to certain RL algorithms (and therefore non-trivial to implement in pratice). In contrast, in supervised learning we have simple algorithms that work for most methods, e.g. forward stepwise/backward stepwise selection for feature selection. In addition, there are many more practical considerations to make for feature selection/hyperparameter tuning in RL, which seem rarely addressed in the literature. Hence, my following questions.


\1. Why don't we use forward stepwise selection or backward stepwise or similar for feature selection in RL? (Using test and validation data, if the environment is based on static data)

\2. Which metric should be optimized: rewards (computed after every step) or the objective (computed at the end of each episode) assuming that the two are different?
 
\3. How to account for randomness during training? If I train and evaluate a model with a set of hyperparams once, I may get different results than when running it a second time (with different seed).

\4.There are different sets of hyperparameters: RL related (e.g. discount factor, number of steps before update etc.) and model related (e.g. neural network parameters of policy and/or value function). Optimize them both together or optimize first e.g. RL related parameters and then model related parameters?

\5. What is a good number of hyperparameters to optimize at once (e.g. with bayesian or random search optimization)? Should I provide the complete search space of all parameters or rather reduce the number of parameters to e.g. a set of 5 parameters?

\6. The literature seems to argue for bayesian or random search optimization for hyperparameters. Do you see why any of the two would be preferred over the other in the context of RL?

\7. Intuitively, I'd prefer optimizing one parameter at a time (fixing the other parameters to default parameters), to be able to interpret what is going on (and see if an optimal point can be found for a particular parameter. Shouldn't this be the first thing to try in practice (before moving on to efficient bayesian/random search optimization)?

Thanks a lot for answers to any of these questions.",reinforcementlearning,thisisthehappylion,False,/r/reinforcementlearning/comments/febitm/practical_guide_to_feature_selection_and/
"""goalGAIL: Goal-conditioned Imitation Learning"", Ding et al 2019",1583463275,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fe708p/goalgail_goalconditioned_imitation_learning_ding/
"""Reward-rational (implicit) choice: A unifying formalism for reward learning"", Jeon et al 2020",1583463185,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fe6zik/rewardrational_implicit_choice_a_unifying/
"""What Can Learned Intrinsic Rewards Capture?"", Zheng et al 2019 {DM}",1583463047,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fe6ygk/what_can_learned_intrinsic_rewards_capture_zheng/
[D] Build for a mix of research between evolutionary algorithms and reinforcement learning,1583449997,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/fe40xn/d_build_for_a_mix_of_research_between/
Multi-agent Reinforcement Learning in Sequential Social Dilemmas,1583435191,,reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fe07cg/multiagent_reinforcement_learning_in_sequential/
Old SpinningUp Pseudocodes,1583433762,"By any chance, does anyone have pdf/or any form of the older SpinningUp (only tensorflow version) pseudocodes? Especially for SAC",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/fdztvt/old_spinningup_pseudocodes/
PPO - entropy and Gaussian standard deviation constantly increasing,1583431805,"I noticed an issue with a project I am working on, and I am wondering if anyone else has had the same issue. I'm using PPO and training the networks to perform certain actions that are drawn from a Gaussian distribution. Normally, I would expect that through training, the standard deviation of that distribution would gradually decrease as the networks learn more and more about the environment. However, while the networks are learning the proper mean of that Gaussian distribution, the standard deviation is skyrocketing through training (goes from 1 to 20,000). I believe this then affects the entropy in the system which also increases as well. The agents end up getting pretty close to the ideal actions (which I know a priori), but I'm not sure if the standard deviation problem is preventing them from getting even closer, and what could be done to prevent it. 

I was wondering if anyone else has seen this issue, or if they have any thoughts on it. I was thinking of trying a gradually decreasing entropy coefficient, but would be open to other ideas.",reinforcementlearning,hellz2dayeah,False,/r/reinforcementlearning/comments/fdzbs9/ppo_entropy_and_gaussian_standard_deviation/
Reward-rational (implicit) choice: A unifying formalism for reward learning,1583429463,"Reward-rational (implicit) choice: A unifying formalism for reward learning

[Hong Jun Jeon](https://arxiv.org/search/cs?searchtype=author&amp;query=Jeon%2C+H+J), [Smitha Milli](https://arxiv.org/search/cs?searchtype=author&amp;query=Milli%2C+S), [Anca D. Dragan](https://arxiv.org/search/cs?searchtype=author&amp;query=Dragan%2C+A+D)*(Submitted on 12 Feb 2020)*

&gt;It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.",reinforcementlearning,EmergenceIsMagic,False,/r/reinforcementlearning/comments/fdypvb/rewardrational_implicit_choice_a_unifying/
"""The impact of Simulation on advances in Reinforcement Learning"" - Danny Lange (Unity VP of AI)",1583407235,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/fdttmh/the_impact_of_simulation_on_advances_in/
Variational Autoencoder or Convolutional NN for visual observations?,1583406125,I know that two common techniques for reducing a visual observation space in RL are Variational Autoencoders and CNNs. In which context do they work better? How are they trained?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/fdtni2/variational_autoencoder_or_convolutional_nn_for/
Reward-rational (implicit) choice: A unifying formalism for reward learning,1583389234,,reinforcementlearning,tcfml,False,/r/reinforcementlearning/comments/fdr3iu/rewardrational_implicit_choice_a_unifying/
[Beginner: Belief update in POMDP],1583374609,"Hi everyone, 

Can someone explain to me the derivation for the belief update in POMDP below. I am not good at probability and need some help to understand. Thanks!

&amp;#x200B;

https://preview.redd.it/fr0dsraxjrk41.png?width=774&amp;format=png&amp;auto=webp&amp;s=b5553d48aac9721c10591d4f60ee83c5434bec13",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/fdo66a/beginner_belief_update_in_pomdp/
"Cam accros this, what do you think about it?",1583363768,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/fdlqfz/cam_accros_this_what_do_you_think_about_it/
"Need an explanation for an excerpt from ""Deep RL for sepsis treatment""",1583339675,"Quoting from Deep RL for sepsis([https://arxiv.org/pdf/1711.09602.pdf](https://arxiv.org/pdf/1711.09602.pdf))

&gt;We prefer RL for sepsis treatment over supervised learning, because the ground truth of a “good” treatment strategy is unclear in medical literature \[7\]. Importantly, RL algorithms also allow us to infer optimal strategies from training examples that do not represent optimal behavior.

&amp;#x200B;

How can RL algorithms infer optimal strategies from training examples that do not represent optimal behavior?

My intuition for why it is so:

Different trajectories do specific things right. RL algorithms can identify these specific things and stitch an optimal strategy from suboptimal trajectories.

Am I correct?

Please give me examples such as environments(even atari) where this is true and algorithms which can learn from  suboptimal trajectories. On-policy algorithms, which I have worked mostly on, can't learn this way.",reinforcementlearning,Crazy_Plant,False,/r/reinforcementlearning/comments/fdfkx3/need_an_explanation_for_an_excerpt_from_deep_rl/
[D] Difference between a terminal state and a negative reward?,1583318009,"For a situation where you have multiple lives, I'm trying to figure out if it's better for each life to be an episode (death is terminal), or for the game to be an episode (loss of all lives is terminal, death is just a negative reward).

For example, Pacman can have the following states:

* Method 1 has no negative rewards.  You get points based on your score.  Pellets, ghosts, etc give you a positive score.  Death ends the episode.
* Method 2 keeps training as method 1 with death being a negative reward.  The episode ends after the loss of the final life.
* Method 3 is a combination.  You get negative rewards on each death but the terminal state on ONLY the final death.
* Method 4 is a combination.  You get negative rewards and a terminal state on EACH death.

Almost all the examples I see online do method 1 - 3.  Which confuses me.

From what I see, the terminal state is only used to know if we should discount the reward when processing the replay memory.  If that's the case, we should be using method 3 only.  The negative reward updates the q values and the terminal state means there is nothing in the future to learn from this.  Am I understanding this right?  If people are playing multiple lives with only the last life being marked as terminal, you're teaching it that actions you make after you respawn are affected by actions made before.  Right?  (I guess there might be games where you have to die to progress?  But otherwise, I would think the rounds should be kept separate.)

Thoughts?  I haven't been able to find a good explanation online.  Thanks!

TL;DR: Death should trigger the end of an episode, not the exhaustion of all deaths.  Otherwise, you train across death boundaries.  This correct?",reinforcementlearning,stridera,False,/r/reinforcementlearning/comments/fdb5n9/d_difference_between_a_terminal_state_and_a/
Is LinUCB a non-stationary modeling?,1583253407,"I am not sure if the LinUCB proposed by the paper ""A Contextual-Bandit Approach to Personalized News Article Recommendation""  models the expected reward as a non-stationary distribution. I know that what makes it non-stationary is to weight the latests rewards higher than the past ones, and we do this in the update function of the mean when we use a fixed step\_size like 1/10 or so.",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/fcx7z8/is_linucb_a_nonstationary_modeling/
Why is it fine to neglect importance weights in IRL?,1583240895,"In the paper by Chelsea Finn ""Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization"" [http://www.jmlr.org/proceedings/papers/v48/finn16.pdf](http://www.jmlr.org/proceedings/papers/v48/finn16.pdf) it is proposed to use importance sampling if we don't train the policy until convergance. Sounds like a resonable solution.

But in many later work the importance weights are ommited. For example in paper ""End-to-End Robotic Reinforcement Learning without Reward Engineering"" it is stated: ""While in principle this would require importance sampling if using off-policy data from the replay buffer R, prior work has observed that adversarial IRL can drop the importance weights both in theory \[reference 1\] and in practice \[reference 2\]"". I can believe that in practice it ""may just work"", but what is the theory behind it?

I looked into this theoretical reference 1 ""A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models"" [https://arxiv.org/pdf/1611.03852.pdf](https://arxiv.org/pdf/1611.03852.pdf?source=post_page---------------------------) but I still don't see why is it that you can omit the importance weights. In the derivation the importance weights are still always included in the paper.

Can someone explain why from theoretical perspective is it fine to omit the importance weights when updating the reward function, the discriminator?",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/fcub9y/why_is_it_fine_to_neglect_importance_weights_in/
Agent takes non-optimal actions and aplears to generalise to much,1583222909,"Hi everyone,
I have an agent (using noisy DDQN with PER) that gets less punishment the closer it gets to a goal, and then no punishment when it performs a certain action at (or around) that goal point. However I am having two problems, one is that the agent sometimes learns to only perform the action it should do at the goal state, and just performs that over and over again despite it giving no benefit over other actions . The other problem is, if it is not doing that, it goes as far away from the goal as possible, and just moves around that spot instead.
For the first issue, I assume it is over-generalising but I am stuck on how to fix this. I have tried to sort it by changing the reward scheme, as well as decreasing the lr kf the model
For the second issue, I have checked the rewards being given to the agent and they are what they should be. Furthermore, the exploration policy (I have tried both noisynet and epsilon-greedy) has led it to explore the other states, as well as getting to the goal position, but for some reason the agent stays in the least rewarding spot it can find.
Does anyone know why this might not be working?",reinforcementlearning,Lavadingdong,False,/r/reinforcementlearning/comments/fcrhhu/agent_takes_nonoptimal_actions_and_aplears_to/
OpenAI Baselines RNN version of PPO2,1583190402,"Just wondering, has anyone looked at how the RNN version of PPO2 works in the baselines [code](https://github.com/openai/baselines/tree/master/baselines/ppo2)? I'm trying to reimplement it in PyTorch, but I can't quite follow what they're doing here...

I've done a little bit of RNN stuff before with video analysis, but it seems that working on-policy makes life a little different. My plan is to simply run 64 parallel agents and apply updates in the order of the rollouts. What I'm not sure about is when to zero the gradient and when to apply the update.

At the moment I'm zeroing the gradient a the start of every 40-step 'block' and performing backwards on the mean of the losses over these steps. The states are passed between the steps with gradients so the LSTM units should be able to learn to adjust the state to make long term connections.

I wanted to check how baselines did it for reference, but I can't make head nor tail of it. :(",reinforcementlearning,VirtualHat,False,/r/reinforcementlearning/comments/fckv4n/openai_baselines_rnn_version_of_ppo2/
"""On Catastrophic Interference in Atari 2600 Games"", Fedus et al 2020 {GB}",1583189045,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/fckj3a/on_catastrophic_interference_in_atari_2600_games/
[P] cpprb: Replay Buffer Python Library for Reinforcement Learning,1583167311,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/fcesfr/p_cpprb_replay_buffer_python_library_for/
Poke-Agent: Pokemon Battling &amp; Reinforcement Learning,1583156938,,reinforcementlearning,caleb_dre,False,/r/reinforcementlearning/comments/fccb99/pokeagent_pokemon_battling_reinforcement_learning/
Reinforcement Learning vs Supervised/Unsupervised Learning!,1583156416,,reinforcementlearning,SuspiciousCompote6,False,/r/reinforcementlearning/comments/fcc7it/reinforcement_learning_vs_supervisedunsupervised/
Reinforcement Learning vs Supervised/Unsupervised Learning!,1583156395,,reinforcementlearning,SuspiciousCompote6,False,/r/reinforcementlearning/comments/fcc7cd/reinforcement_learning_vs_supervisedunsupervised/
"[D] ""Why does reinforcement learning not work (for you)?"", Prof. Shie Mannor",1583065269,"*Recently published in the* [Technion Reinforcement Learning Research Labs](https://rlrl.net.technion.ac.il/) *(RL)**^(2)* *blog.*

Author: [**Shie Mannor**](https://www.reddit.com/shie-mannor/)

&amp;#x200B;

[Robot failing](https://preview.redd.it/ja4jc9kdz1k41.jpg?width=609&amp;format=pjpg&amp;auto=webp&amp;s=3179f46fda1a2699a6b7702e81d2ac0ad04084d2)

So you run a reinforcement learning (RL) algorithm and it performs poorly. What then? Basically, you can try some other algorithm out of the box: PPO/AxC/\*QN/Rainbow/etc... \[1, 2, 3, 4\] and hope for the best. This approach rarely works. But, why? Why don't we have a ""ResNet"" for RL? By that I mean, why don't we have a network architecture that gets you to 90% of the desired performance with 10% of the effort? 

The problem, I believe, is in the questions we ask. For most problems of interest, much of the mental effort and the ingenuity is in defining the problem rather than in finding the best algorithms. Algorithms matter, but less than solving the right problem. In this post I will explain what are the right problems, and how can we address them. Rather than considering algorithmic problems, I view the problems from the design side: what would an engineer who tries to use RL need? These properties are not specific to RL systems, of course, but are rather generic. As we view the problem from a design perspective, we are interested in the interfaces from the system and how it is reflected to the outside world.

&amp;#x200B;

![img](1brdzfujz1k41 ""An autonomous vehicle should be capable of explaining its decision-making process.
Image credit: smartcitiesworld.net"")

### Accountability:

Accountability means that the system can explain why it acts in a certain way. The explanation can be reasoning in words, by example, or in any other means that are interpretable to the other entities that communicate with it. For example, consider an autonomous self-driving system. It ought to be able to explain itself: why it crossed a lane, or why it decided to hit the brakes. The explanation is important not only for the sake of investigating accidents, but also for making the human passenger feel safer and more in control, for debugging and to account for different styles of driving. An explanation can be in the form of a natural language or a computation, but without reasoning humans (users, regulators, or humans that interact with the system), will find it difficult to trust the system.  

### Adaptivity:

Adaptivity means that the system should function properly under a variety of conditions. Some of these conditions may be expected and some harder to predict. Of specific importance is adaptivity to counter-factual behavior: other agents and the environment are expected to react to the system's behavior. Consider again the case of autonomous behavior in cars, not necessarily self-driving as ABS (Absolute Braking System) and ADAS (Automatic Driving Assist System) come to mind as well. The system has to work in all weather conditions and all road conditions,  even if trained mostly in several specific conditions. Such a requirement calls for control policies more akin to adaptive control and to taking exogenous, or contextual, parameters (e.g., weather conditions) into account \[9, 10\].

&amp;#x200B;

![img](5yrho53nz1k41 ""The system should be capable of communicating its performance and being aware of when and where it doesn’t work well.
Image credit: [11]"")

### Awareness:

Awareness means being aware of how well a system performs, and that it can communicate its performance. The system should be able to identify what is happening to it and be cognizant of other entities (other agents and humans). Returning to the self-driving car example, consider a level 3 or level 4 self-driving car that can relinquish driving to the human behind the steering wheel. Relinquishing control should happen when the system recognizes its inability to perform some required maneuver (e.g., change lanes into a very busy lane), but also, and more importantly when the system realizes it does not work well enough and it is aware of its current inability to perform adequately.  

### Life-cycle consciousness:

A system will be built, and then put in the field, and eventually deprecate and die out. We need to build systems that are aware of the life cycle, including debugging tools such as unit-testing, decomposability, and interaction with other subsystems within a more complex system. While the importance of debugging is clear and has been considered by many researchers \[11, 12\], let us give an example of building a system that can work effectively with other systems. Consider again, an ADAS  or an ABS. Each such system will work with tens of other systems, from air pressure sensors to steering control to driver alertness monitoring. It should be possible to easily test when replacing the other systems with a newer version (software or hardware) would work, and it should be possible to replace the version of the ABS/ADAS system while considering the current versions of all other systems. Such testing is no easy fit, as it should be done virtually and probably no on the vehicle itself. 

&amp;#x200B;

![img](8xd77ivoz1k41 ""The system's performance should scale with the amount of resources at its disposal.
Image credit: Google"")

### Scalability with resources:

The more resources we have in terms of data and computational power the better the policy should be. To quote Richard Sutton: ""One thing that should be learned from the bitter lesson is the great power of general-purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are 'search' and 'learning'."" \[5\]. Going back once more to the self-driving example, we would like to be able to learn and improve from, essentially, all driving experience of all cars ever recorded. Beyond the algorithmic challenge of learning with very large data sets, this calls for multi-domain learning since there will be a difference between individual drivers and counterfactual learning \[8\] as well as sim2real \[6, 7\] and real2sim. 

So when building a system that is supposed to work in the real world, you should ask yourself if the above 5 principles are considered or not.More often than not, we are enchanted by the mathematical beauty of one algorithm or another and consider restricted and limited research that provides a solution that would work on some mock up domain. 

So this leads to people who run their pet RL algorithms on real systems and do not understand why they fail?A system that does not comply with the 5 principles is unlikely to work in a real-world application. Take for instance DQN based algorithms, they really are only scaleable. All other design principles are far from satisfied. I find it hard to imagine any real-world system that uses DQN as its engine.Of course, there is work on making DQN adaptive and on methods for debugging these algorithms, but in general, they are notoriously difficult to debug. My view is that any RL research should be judged according to the five principles above: which of the five principles does it advance?Of course. there is no harm in working on solving games, and ""solving"" Go or DOTA 2 is a great exercise in scaleability. But, it is nothing more: it does not bring us much closer to building RL systems that interact in the real world. 

There are two diverging views on RL at the moment. The optimists are saying that since Go is ""solved"" and so are many other games it is time to call quits and look for a new research area. The pessimist view is that RL basically does work only when there are huge amounts of data and essentially for problems where the test set itself can be overfitted. So far, we have no indication that the pessimists are wrong. The reason, we have argued, is that we, as a community, have mostly worked so far on one aspect of the design problem. 

#### Bibliography

\[1\] Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. *Nature*, *518*(7540), pp.529-533.\[2\] Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M. and Silver, D., 2018, April. Rainbow: Combining improvements in deep reinforcement learning. In *Thirty-Second AAAI Conference on Artificial Intelligence*.\[3\] Schulman, J., Wolski, F., Dhariwal, P., Radford, A. and Klimov, O., 2017. Proximal policy optimization algorithms. *arXiv preprint arXiv:1707.06347*.\[4\] Mnih, V., Badia, A.P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D. and Kavukcuoglu, K., 2016, June. Asynchronous methods for deep reinforcement learning. In *International conference on machine learning* (pp. 1928-1937).\[5\] Sutton, R., 2019. The bitter lesson. *Incomplete Ideas (blog), March*, *13*. http://www.incompleteideas.net/IncIdeas/BitterLesson.html\[6\] Peng, X.B., Andrychowicz, M., Zaremba, W. and Abbeel, P., 2018, May. Sim-to-real transfer of robotic control with dynamics randomization. In *2018 IEEE international conference on robotics and automation (ICRA)* (pp. 1-8). IEEE.\[7\] Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R. and Schneider, J., 2019. Solving Rubik's Cube with a Robot Hand. *arXiv preprint arXiv:1910.07113*.\[8\] Swaminathan, A. and Joachims, T., 2015, June. Counterfactual risk minimization: Learning from logged bandit feedback. In *International Conference on Machine Learning* (pp. 814-823).\[9\] Hallak, A., Di Castro, D. and Mannor, S., 2015. Contextual Markov decision processes. *arXiv preprint arXiv:1502.02259*.\[10\] Finn, C., Abbeel, P. and Levine, S., 2017, August. Model-agnostic meta-learning for fast adaptation of deep networks. In *Proceedings of the 34th International Conference on Machine Learning-Volume 70* (pp. 1126-1135). JMLR. org.\[11\] Zahavy, T., Ben-Zrihem, N. and Mannor, S., 2016, June. Graying the black box: Understanding dqns. In *International Conference on Machine Learning* (pp. 1899-1908).\[12\] Hohman F, Kahng M, Pienta R, Chau DH. Visual analytics in deep learning: An interrogative survey for the next frontiers. IEEE transactions on visualization and computer graphics. 2018 Jun 4;25(8):2674-93.",reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/fbtbet/d_why_does_reinforcement_learning_not_work_for/
Is there an imlpementation of Prioritized Sequence Experience Replay,1583051126,I was wondering whether anyone knows of a public implementation of [Prioritized Sequence Experience Replay](https://arxiv.org/abs/1905.12726) (PSER) to confirm the author's results.,reinforcementlearning,rrz0,False,/r/reinforcementlearning/comments/fbr74c/is_there_an_imlpementation_of_prioritized/
DDPG implementation for multiple Gym environments,1583047912,"Hello everyone,

When I first started out trying to implement RL algorithms, I often ran into the problem of not knowing why my code worked or didn't work or if the chosen environment could even be solved using a given algorithm. Further, most people don't include hyperparams or some way to efficiently reproduce the programs that they have written. I also found that with newer and 'somewhat better' algorithms such as PPO or TRPO, it's hard to find examples specifically for DDPG.

As such, I've written my own DDPG algorithm that include 

* CLI execution of code and setting of various hyper parameters for easy exploring of hyperparams
* Fully trained agents capable of solving LunarLanderContinuous, BipedalWalker, MountainCar and Pendulum, with included code and hyperparams to easily reproduce the results

In this repo, with minimal alterations and environment specific tweaks, you can solve the above 4 continuous environments.

Please have a look at it [here](https://github.com/kangtinglee/reinforcement-learning)! Hope it helps someone!",reinforcementlearning,rlylikesomelettes,False,/r/reinforcementlearning/comments/fbqqry/ddpg_implementation_for_multiple_gym_environments/
Policy and Value function,1583046829,"The value function is the unique solution to its Bellman equation. (Chapter 3, page 59)

There may be more than one Optimal Policy and they share the same State Value function. ( Chapter 3, page 62)

The above two are statements from the book,

Reinforcement Learning
An Introduction
second edition
Richard S.Sutton and Andrew G. Barto

My doubt is,
Does the above two statements mean

If two different policies have the same value function for all states, then they are optimal policies.

I would be very grateful if anyone could help me understand the actual meaning of the two statements",reinforcementlearning,ArtemisFowl98,False,/r/reinforcementlearning/comments/fbql1v/policy_and_value_function/
Where are the Atari roms for Gym hosted?,1583040486,"Just out of curiosity- where are the Atari ROMs that can be installed via via pip for Gym hosted, and how has OpenAI not gotten sued?",reinforcementlearning,MockingBird421,False,/r/reinforcementlearning/comments/fbpjrt/where_are_the_atari_roms_for_gym_hosted/
Applying OpenAI's Baselines to Custom Environment,1583024595,,reinforcementlearning,DRLkid,False,/r/reinforcementlearning/comments/fbmcnh/applying_openais_baselines_to_custom_environment/
Using RI for stacks,1583016783,"Suppose you have 3 poles. Some rings on each pole and each ring is of different size. The goal is to move the rings, one at each time, from a pole to another so that there will not be a smaller ring below a big one on any pole.

How to model this quickly?",reinforcementlearning,markovianChains,False,/r/reinforcementlearning/comments/fbkliq/using_ri_for_stacks/
What is the current SOTA on the Gym Hand Manipulation environments?,1583004561,I was just wondering if anyone knows off the top of their head what the best results are on the multi-goal hand manipulation environments in OpenAI gym ([https://github.com/openai/gym/tree/master/gym/envs/robotics](https://github.com/openai/gym/tree/master/gym/envs/robotics)). Is there anything that significantly outperforms the results of Hindsight Experience Replay reported here?:  [https://arxiv.org/pdf/1802.09464.pdf](https://arxiv.org/pdf/1802.09464.pdf),reinforcementlearning,henrythepaw,False,/r/reinforcementlearning/comments/fbho0w/what_is_the_current_sota_on_the_gym_hand/
Differentiable of the policy function,1582991236,"Hi everyone, 

I understand that REINFORCE algorithm is usually used when the loss/reward function is not differentiable. However, does the policy function need to be differentiable as well? (from what I understand, yes, there should be error signal propagated through the policy function, but I am not sure). The same question is about Evolutionary Strategies (if it has assumptions about the policy function or not).

Cheers,

Omar",reinforcementlearning,osm3000,False,/r/reinforcementlearning/comments/fbebbt/differentiable_of_the_policy_function/
[R] Taming an Autonomous Surface Vehicle for Path Following and Collision Avoidance using Deep Reinforcement Learning,1582981179,,reinforcementlearning,Heatkiger,False,/r/reinforcementlearning/comments/fbc6fl/r_taming_an_autonomous_surface_vehicle_for_path/
Q Learning In Depth Tutorials (Code Included),1582947757,,reinforcementlearning,ejmejm1,False,/r/reinforcementlearning/comments/fb6qvy/q_learning_in_depth_tutorials_code_included/
[Project] Reward scheme for Noisy DDQN with PER,1582897317,"Hi all,
I am using a Noisy DDQN-PER algorithm for use on a  2d crane game (agent moves boxes from one side of the environment over a wall to the other side). The agent has six actions, which are moving up, down, left, or right as well as grabbing and releasing boxes. A reward is given when a box is moved from left to right for the first time in an episode. The episode resets after 1000 time steps also.
Problem is, this isn't working, and I am not too sure why. I have tried other schemes also, such as giving it a negative reward for each action but still no luck.
Does anyone have any suggestions? I am very stuck, so am happy to try most things.",reinforcementlearning,roboticalbread,False,/r/reinforcementlearning/comments/fauvic/project_reward_scheme_for_noisy_ddqn_with_per/
Reinforcement Learning: Startup Ecosystem Analysis | Netscribes,1582895168,[removed],reinforcementlearning,netscribes-india2,False,/r/reinforcementlearning/comments/fauffe/reinforcement_learning_startup_ecosystem_analysis/
Actor-critic for discrete action space,1582824371,"I've seen a few actor-critic methods for discrete action spaces, however I do not seem to understand how they work.

If I have a discrete action space, isn't an actor-critic architecture equivalent to standard Q-learning where I use a softmax over the Q-values to create my policy? Why do I need to two networks?  


For continuous action spaces it makes sense to me, since I can't do a max over the action values, so instead I can use another network to evaluate my action. So I guess my question is, does actor-critic architectures only make sense for continuous action spaces?

Thanks!",reinforcementlearning,s_meldgaard,False,/r/reinforcementlearning/comments/fafi7j/actorcritic_for_discrete_action_space/
[D] Mixed precision training for reinforcement learning?,1582813326,"Has there been any result using mixed precision training in reinforcement learning?

Are there any reasons why it would work well, or not work well?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/facrt7/d_mixed_precision_training_for_reinforcement/
Artificial Intelligence | Will AI Rule the World in 2020,1582813266,,reinforcementlearning,gajanand_edu,False,/r/reinforcementlearning/comments/facrc8/artificial_intelligence_will_ai_rule_the_world_in/
PyRL - Modular Implementations of Reinforcement Learning Algorithms in Pytorch,1582793278,"[https://github.com/chaovven/PyRL](https://github.com/chaovven/PyRL)

I just implemented several RL algorithms in Pytorch, including Policy Gradient, DQN, DDPG and TD3.

Some features:

* Implemented in PyTorch
* Readable code
* Modular architecture

 Feel free to submit *issues* and enhancement requests. Hope this project could help you.",reinforcementlearning,aineqml,False,/r/reinforcementlearning/comments/fa98yj/pyrl_modular_implementations_of_reinforcement/
[Project] My model doesn't appear to be improving. Suggestions? Help? (Please?),1582765836,"So, I've been working on a RL project that plays the arcade game Robotron on the Xbox 360 using only computer vision and a custom Arduino driven controller.  I've been told by a few people I trust that this is totally doable, but I even after days of training, it doesn't appear to be improving.  The score doesn't seem to be improving at all and still appears close to what it was when the epsilons were higher.  (More random moves.)

[Image of the player after nearly 2k episodes (2 days)](https://i.imgur.com/6As2wnQ.png)

More details in the github readme files via link below.  All code is available and any questions or even random ideas I could implement would be great.  Feel free to ask any questions if you want something explained as well.

Any suggestions would be amazing.  Some links:

* [The DQN Model/Agent](https://github.com/stridera/robotron/blob/master/src/ai/dqn.py)

* [Github for the whole project](https://github.com/stridera/robotron/)

* [Video of the system training this morning](https://www.twitch.tv/videos/553410269)

* [Twitch Channel with live training (when available)](https://www.twitch.tv/stridera)",reinforcementlearning,stridera,False,/r/reinforcementlearning/comments/fa3ymp/project_my_model_doesnt_appear_to_be_improving/
Confused on comparing policies.. How ?,1582765264,"When we talk about comparing different policies(plans) what we do is basically compare the expected reward of a state. Now I'm confused as to how it works. Consider this problem:

Let 'a' be the start state, 'g' be the goal state and 'x' be an intermediate step between 'a' and 'g' such that a -&gt; x -&gt; g

the expected return for each of state be A, X, G( G doesn't matter)

Case 1: suppose according to one policy, reward X at state 'x' is suboptimal. but by the same policy, the maximum of the reward propogates to 'a' (Good path x-&gt; g, Bad path a-&gt;x)

Case 2: in another policy, reward X at state 'x' is optimal, but by the same policy, the rewards degrades a lot as it reaches 'a' ( Bad Path x-&gt;g, Good Path a-&gt;x)

In this case, how can we compare which policy is better simply on the basis of expected return/reward.  


I don't understand. Please Help.",reinforcementlearning,lifeinsrndpt,False,/r/reinforcementlearning/comments/fa3tvi/confused_on_comparing_policies_how/
[R] Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement,1582759773,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/fa2j4w/r_rewriting_history_with_inverse_rl_hindsight/
A3C (or A2C) loss function,1582738125,"Hey,  
In A3C article [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)  
Theta\_v parameters are supposed to approximate the value functions, however, in the A3C algorithm they use the following equation to update theta\_v.

https://preview.redd.it/hn5k0elewaj41.png?width=455&amp;format=png&amp;auto=webp&amp;s=bc40e3a64b2aa5816150cefb411debce54fafd0b

they say in the article that R is an approximation of the Q function and therefore R-V is the advantage.  
Arent they minimizing the advantage by using this loss? doesn't that makes theta\_v approximating Q and not V?

thanks",reinforcementlearning,What_Did_It_Cost_E_T,False,/r/reinforcementlearning/comments/f9wr6e/a3c_or_a2c_loss_function/
cooperative MARL w/ discrete action spaces,1582734665,"Hi guys,

I have an idea for a Multi Agent environment based on the game [The Mind](https://arstechnica.com/gaming/2018/08/the-mind-most-polarizing-card-game-of-the-year/). Admittedly, I am a relative newcomer to RL (and I would say to Machine Learning in general), and I have spent the past few months familiarizing myself with the more widely used tabular as well as deep learning RL methods. Ideally, I could use an existing MA learning framework, but I couldn't really find anything (apart from Ray, which for the life of me I cannot figure out). 

I wonder if anyone on this subreddit has any experience with implementing co-op Multi Agent algorithms with discrete action spaces? If so, which algorithm did you use? I would love to take a look at some (pseudo)code if at all possible. Of course there are algorithms like MADDPG, but they seem to only deal with continuous actions, and I could not find any useful info on how to adapt it to a discrete action domain. 

Thank you.",reinforcementlearning,DollaPond,False,/r/reinforcementlearning/comments/f9vu7a/cooperative_marl_w_discrete_action_spaces/
"""ANML: Learning to Continually Learn"", Beaulieu et al 2020",1582730368,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/f9uqq6/anml_learning_to_continually_learn_beaulieu_et_al/
Stanford CS330 videos are now on YouTube (Deep Multi-Task and Meta Learning),1582684038,,reinforcementlearning,zbqv,False,/r/reinforcementlearning/comments/f9m0kn/stanford_cs330_videos_are_now_on_youtube_deep/
DDPG in CarRacing-v0,1582638189,"I've been trying to implement a DDPG algorithm to solve the [CarRacing-v0](https://gym.openai.com/envs/CarRacing-v0/)  environment in OpenAI's gym.

However, no matter the combination, my car eventually ends up spinning around in circles. My guess is that because the reward function is defined as the number of new coordinates/parts of the track and my timeout is 1000 frames, it figures that the best it can do is to go in circles around the start until the episode ends.

I've tried toying with the noise function like using gaussian instead of OU noise but the results are still the same.

I've been changing bits of code and tweaking everything for days now. I would love if someone can have a look at my code and point me in the right direction.

I realise that for a lot of the solutions posted online, people tend use PPO for this. Indeed, PPO seems more robust and is probably the ""better"" solution here. But I don't see why DDPG wouldn't be able to solve this environment.

Heres my code for any kind person out there who's willing to have a look for me :)

[https://github.com/kangtinglee/CarRacing](https://github.com/kangtinglee/CarRacing)",reinforcementlearning,rlylikesomelettes,False,/r/reinforcementlearning/comments/f9ag05/ddpg_in_carracingv0/
why is ppo on policy?,1582600455,why ppo is not off policy? it updates new θ by old θ with importance sample.,reinforcementlearning,fengyanghe,False,/r/reinforcementlearning/comments/f93mft/why_is_ppo_on_policy/
DDPG - action space limitation,1582586308,"Hello,

I am using DDPG for a control problem with continuous action space and to accelerate the training speed, I want to limit the selected action with some interval \[a, b\] some states (and generally with some other intervals in some other states ).  
I've tried clipping the selected action (with added OU noise), but it didn't seem to help with finding the better policies -&gt; most of the selected actions on the test set were exactly values of the clipping limits (a and b). I assume that those actions appeared most often during the training (because of the clipping).  
I'm wondering if there is a smarter solution to action space limitation for DDPG?",reinforcementlearning,sesli994,False,/r/reinforcementlearning/comments/f909bl/ddpg_action_space_limitation/
Where do you train your agents for (OpenAI) gym environments?,1582582356,"I'm planning to work on a project that involves the gym + few DRL methods, therefore training agents on my laptop seems to be infeasible (it does not have a GPU, sigh). 

Right now I'm looking at Google Colab and Paperspace. I'm not sure which is the best so I'd really like to hear your opinion on this (or about any other platform that I can use)",reinforcementlearning,PsyRex2011,False,/r/reinforcementlearning/comments/f8z8py/where_do_you_train_your_agents_for_openai_gym/
Making an AI for match 3 games,1582566925,"Is there any way to apply a NEAT AI to a match 3 game (like candy crush) ?  


What would be the inputs and output ?  
I'm currently trying to put the 64\*64 normalized value of the grid as input nodes and a 4 \[x1, y1\] \[x2, y2\] output nodes for the two cases to switch.",reinforcementlearning,BlindAnimal,False,/r/reinforcementlearning/comments/f8v3ds/making_an_ai_for_match_3_games/
Help Error encountered when running code of Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games,1582555934,"I was trying to reproduce the plots of section three  of the paper Can Deep Reinforcement Learning Solve Erdos-Selfridge-Spencer Games  using the code  [https://github.com/maxencemonfort/dqn-attack-defense](https://github.com/maxencemonfort/dqn-attack-defense)  

when runing this : [https://i.imgur.com/Yc8HGyE.jpg](https://i.imgur.com/Yc8HGyE.jpg)

I had this error : [https://i.imgur.com/TEzvajX.jpg](https://i.imgur.com/TEzvajX.jpg) 

Thanks in advance !",reinforcementlearning,kaspersaif,False,/r/reinforcementlearning/comments/f8sapx/help_error_encountered_when_running_code_of_can/
"Reinforcement Learning, why using data from behaviour policy can be used to optimise the target policy?",1582544954,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/f8q3rt/reinforcement_learning_why_using_data_from/
Difference between A3C (or A2C) and Federated Learning,1582509562,"Hi,

I have a question of a difference between distributed RL and federated learning.

Referred to the [McMahan et al.](http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf), federated learning differs from distributed learning in which FL uses (i) non-IID and unbalanced data, (ii) massively distributed, and (iii) limited communication.

Thinking of asynchronous distributed RL with the REINFORCE algorithm, workers interact with own environments and collect experiences, send their model gradients to main agent. Then, the agent updates model and synchronize its model to distributed workers. Each worker does not share transitions and collect own experience. Also, RL transitions are generally non-IID and unbalanced. 

In that sense, I could not find any clear difference between federated learning and distributed RL other than using more numbers of workers. 

Did I miss any important points? Is there any other major difference between the two concepts?",reinforcementlearning,TK-SZ,False,/r/reinforcementlearning/comments/f8jwsb/difference_between_a3c_or_a2c_and_federated/
What does column vector mean?,1582493615,"In page xxi in Summary of Notation chapter in the public [Reinforcement Learning an Introduction](https://d18ky98rnyall9.cloudfront.net/Ph9QFZnEEemRfw7JJ0OZYA_808e8e7d9a544e1eb31ad11069d45dc4_RLbook2018.pdf?Expires=1582502400&amp;Signature=GBE6zL09TMohXkBdXnbjaYEQrHOBfHVcPo8L655P9BjXORTL-8eQMePkFyJ~2BbAmIijM-BoX0d~r7y515gH6~rlxwPfnVBtGnJhCEvTV3NhZhgnPdA5DU2O~K4e8ienYlMUemhZpGMJIN8dqMFzt4NuOxJvj-dm5yntw3GQKQ4_&amp;Key-Pair-Id=APKAJLTNE6QMUY6HBC5A) book, what does column vector mean?

&amp;#x200B;

https://preview.redd.it/nuctsh19sqi41.png?width=1618&amp;format=png&amp;auto=webp&amp;s=4d5b4e181bcfea22a2867b722b922fa83a22163a",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/f8gauf/what_does_column_vector_mean/
First Visit Monty Carlo Explanation,1582491325,"Hi, I wanted to make sure I am understanding first visit monte Carlo correctly. I made a brief  summary: 

&amp;#x200B;

https://preview.redd.it/omqb5uzhlqi41.png?width=468&amp;format=png&amp;auto=webp&amp;s=4618037ff03716ca5eec8fe8f8d15696bcbb2978

Would this be correct?",reinforcementlearning,RLAttempt2Many2Count,False,/r/reinforcementlearning/comments/f8fpm3/first_visit_monty_carlo_explanation/
How to deal with multi discrete action spaces?,1582489110,"I'm solving a task with multiple discrete action spaces. Currently I'm using separate MLPs that take the same GRU hidden state to output categorical distributions for each action space. This approach seem to work and but it does not leverage any known structures between the action spaces. For example, if I know that the action from one action space should affect the choice of action from another action space, I should probably condition the output of the MLP for the second action space on the sampled action from the first action space. Another possibility is to create a unified action space by taking the cartesian product of all action spaces, but this may lead to an impractically huge action space. I'm wondering what would be better methods for dealing with such action spaces. It'd be great that someone could share some papers on this. Thanks!",reinforcementlearning,skwaaaaat,False,/r/reinforcementlearning/comments/f8f5ab/how_to_deal_with_multi_discrete_action_spaces/
Shape of observation space for LSTM policies in OpenAI and stable-baselines,1582484785,"This is a simple question and yet one, for which I did not really find a straight forward answer. Suppose, I want to use the `MlpLstmPolicy` or any other policy that makes use of an LSTM in OpenAI or in stable baselines. Should the observation space contain information about past time steps (explicitly added by me) or only about the current time step (and the past time steps are used automatically)?

So far, I have been using the `MlpPolicy`, i.e. without an LSTM. My state space however, did contain past time steps: It is a box with dimensions TxD, where T is the number of time steps (including the current time step and past time steps) and D is the number of features. Should I leave the state space as is when switching to an `MlpLstmPolicy` or should I switch to dimensions 1xD, ony giving the current time step as information?

\-----------------

 When training an LSTM in e.g. Keras, the input X generally takes the shape:

    [number of observations, number of time steps per observation, number of features per observation]

In other words, every observation is a matrix with dimensions:

    [number of time steps per observation, number of features per observation]

Meaning that every observation contains several time steps of information. Hence, I am wondering if this applies to the context of OpenAI as well or if, instead, every observation should only contain 1 time step of information.",reinforcementlearning,thisisthehappylion,False,/r/reinforcementlearning/comments/f8e1gv/shape_of_observation_space_for_lstm_policies_in/
MY SECRET TO GETTING YOUR CHILD TO SLEEP THROUGH THE NIGHT,1582473433,,reinforcementlearning,totallytonyavlogs,False,/r/reinforcementlearning/comments/f8b8sb/my_secret_to_getting_your_child_to_sleep_through/
What is the significance of shared layers between the actor and critic in PPO?,1582462510,"I was looking into many implementations of PPO and in many of the cases the actor and critic share many layers of neural network. As far as my understanding goes in this, the critic network's job is to get better at approximating the *value* that we'll get from the environment by using the actual returns that we got from batched experiences. And the actor network's job is to produce an optimum set of logits, in the case of discrete action space or produced optimum distribution of actions in the case of continuous action space. For me, intuitively, these two jobs seems to be independent of each other, then why some implementations have shared set of layers?",reinforcementlearning,DrDewDrop,False,/r/reinforcementlearning/comments/f893iz/what_is_the_significance_of_shared_layers_between/
Training Reinforcement Learning,1582444829,How to vary the learning rate while training a reinforcement learning algorithm?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/f86m6a/training_reinforcement_learning/
Looking for ways to optimize RL experiment workflow,1582409594,"Hi, as the title suggests I am looking for ways to get the most of the computing resources I have and find ways to batch test. 

I am running\` cross-domain experiments on Atari 2600 environments.  I have access to my university's High Performance Computing resources  and I submit a script that trains and evaluates one experiment in one environment. I am not well versed in parallel computing or how to scale my experiments to cover more environments and test different hyperparameters. I would love to get any resource to make my experimentation more rigourous. 

Thank you!",reinforcementlearning,pigeonwingflaps,False,/r/reinforcementlearning/comments/f7zsjo/looking_for_ways_to_optimize_rl_experiment/
State of the art in Hierarchical Reinforcement Learning?,1582394432,"Hi, I'm reading some literature about Hierarchical Reinforcement Learning, but many of the things I'm reading are quite old -e.g. MAXQ, options. What are the most important HRL concepts today? How is the field advancing?

I'd really grateful if you could point me to some interesting papers about it.",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/f7w48f/state_of_the_art_in_hierarchical_reinforcement/
Summation expansion,1582390057,"How to expand this equation?

&amp;#x200B;

https://preview.redd.it/qp4mioeg8ii41.png?width=468&amp;format=png&amp;auto=webp&amp;s=915ff7e19f31865b5ca477af89a3368651d8c1b3",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/f7v1h1/summation_expansion/
Continuous and discrete states and actions together,1582388741,"Hi,how can I apply reinforcement learning when my state space and action space have both continuous and categorical values?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/f7uqds/continuous_and_discrete_states_and_actions/
[D] How important is the number of threads assigned to PyTorch when using a GPU?,1582368787,"You can manually specify how many threads PyTorch can use with `torch.set_num_threads`.

My assumption is that, if I do both the policy optimization and action selection using the GPU, then I could keep pretty much all my threads to run environments. 

Is this correct? Can I just set this value to 1 and create an environment per remaining thread?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/f7r3l2/d_how_important_is_the_number_of_threads_assigned/
Continuous and discrete states and actions together,1582365452,"Hi,how can I apply reinforcement learning when my state space and action space has both continuous values and categorical or Boolean values?",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/f7qn53/continuous_and_discrete_states_and_actions/
Reinforcement Learning and Optimal Control,1582364338,"Are there any good blog series or video lectures on the intersection of the control system and reinforcement learning. Specifically, it seems that optimal control and reinforcement learning are tightly coupled in the presence of a known model. It would be great if someone can point some good resources on this topic.",reinforcementlearning,Odd_Quarter,False,/r/reinforcementlearning/comments/f7qhkv/reinforcement_learning_and_optimal_control/
RL Weekly 39: Intrinsic Motivation for Cooperation and Amortized Q-Learning,1582349249,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/f7o7f9/rl_weekly_39_intrinsic_motivation_for_cooperation/
Ideas for Research Projects for Master's Capstone,1582320923,"Hi,

I'm a master's student looking to do my capstone research project in Reinforcement Learning. I'm trying to get involved in Reinforcement Learning research and prepare myself for a PhD in the field so I'd like to dip my toe in with this project. 

I'm looking for a small project which will have some amount of implementation and some amount of reading research papers / conducting original research, but since I'm not too experienced nor am I collaborating with many others - something on the easier side.

The subtopics of RL I'm interested in are: Meta RL, Multi-Task Learning etc.

I'm looking for any and all suggestions! Also advice for how to best prepare myself for a PhD in this field would be great too!

Thanks!",reinforcementlearning,siddharthjoshi804,False,/r/reinforcementlearning/comments/f7i4gp/ideas_for_research_projects_for_masters_capstone/
Soft-Actor-Critic-and-Extensions,1582314357,PyTorch Implementation of the newest (2019) [Soft-Actor-Critic Algorithm](https://github.com/BY571/Soft-Actor-Critic-and-Extensions) \+ Extensions like PER and ERE,reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/f7gh0s/softactorcriticandextensions/
Question about Asynchronous Dynamic Programming convergence from Sutton &amp; Barto (2nd edition),1582307173,"On page 85, when introducing Async Dynamic Programming, Sutton &amp; Barto say:



&gt;Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. These algorithms update the values of states in any order whatsoever, using whatever values of other states happen to be available. 

Then, in the next paragraph they say:   

&gt;If 0 ≤  ɣ&lt; 1, asymptotic convergence to v\_\* is guaranteed given only that all states occur in the sequence {s\_k} an infinite number of times (the sequence could even be stochastic). **(In the undiscounted episodic case, it is possible that there are some orderings of updates that do not result in convergence, but it is relatively easy to avoid these.)** \[bolding mine\]

Does anybody have any explanations or pointers as to how convergence can fail in the undiscounted episodic case?",reinforcementlearning,nrhodes,False,/r/reinforcementlearning/comments/f7epde/question_about_asynchronous_dynamic_programming/
Reinforcement Learning vs Supervised/Unsupervised Learning!,1582292354,,reinforcementlearning,gajanand_edu,False,/r/reinforcementlearning/comments/f7b7q3/reinforcement_learning_vs_supervisedunsupervised/
Why doesn't the PPO algorithm reset the environment if done flag is set?,1582272109,"I want to use the PPO algorithm for training a robot. Usually with the other algorithms, when the robot falls down I set the done flag and the episode terminates there itself. But while looking into the [Runner](https://github.com/openai/baselines/blob/master/baselines/ppo2/runner.py) class of openai baselines of ppo, I see that, minibatches of experience are being collected even after the done flag is set. And instead of terminating the episode, the done is appended into the *mb\_dones*. But isn't it useless to collect mini-batches of experience after the done flag has been set. If I run this algorithm, my robot will simply be lying in the ground and still will be attempting to take *actions*. Am I missing anything here?",reinforcementlearning,DrDewDrop,False,/r/reinforcementlearning/comments/f77zlh/why_doesnt_the_ppo_algorithm_reset_the/
Multiplayer Atari,1582265088,"Just wondering if anyone has any experience with training on 2-player Atari games using the Atari Learning Environment? 

I was thinking of implementing something simple where two agents play Pong vs each other (or another game), but it seems that the platform only supports input for a single player. Has anyone experimented with multi-player atari before? If so, did you need to modify ALE? I'd also be very interested in co-operative 2-player games if there are any.

Thanks.",reinforcementlearning,VirtualHat,False,/r/reinforcementlearning/comments/f76vz8/multiplayer_atari/
value_function_loss and policy_gradient_loss not changing in A2C (while discounted_rewards and episode_reward do improve),1582214203,"Hi everyone, 

I am training and A2C agent based on time series (e.g. financial) data, teaching it how to trade (e.g. buy and sell). Each episode is 1 day. The agent does beat a forecast-based and a random trader, so it seems to work. 

However, I have a question regarding the learning curves, in particular regarding the value\_function\_loss and policy\_gradient\_loss. The problem is that they don't seem to noticeably improve. I expect both to decrease from some point onwards. E.g. this here is a good reference (but not for A2C):  [https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2](https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2) After all, I am not fully sure how ""normal"" behavior would look like for the losses in A2C.

So, what happens is the following: entropy\_loss behaves as expected, decreasing up to some point and then oscillating.  discounted\_rewards and episode\_reward behave as expected, increasing slightly over time (even though it's almost not noticeable for episode\_reward in the plot) and then oscillating. However, the  policy\_gradient\_loss and value\_function\_loss behave in the same way e.g. in the first 5000 training steps and in the last 5000 training steps. Also, there are many spikes in these loss time series.

Questions: 

1. How should the policy\_gradient\_loss and value\_function\_loss behave over time for A2C or other actor critic algorithms (what is the expected behavior if everything works fine)?
2. What might be reasons for no apparent change/improvement in the policy\_gradient\_loss and value\_function\_loss over time (as described above and shown in the plots below)?
3. Why do the discounted reward discounted\_rewards and episode\_reward (i.e. the agent does learn to improve rewards) but the other loss functions don't reflect this? Shouldn't loss functions improve when rewards do?

My guess:

\- Regarding the spikes in the losses: Financial time series data is, to a large extent, unpredictable. So, it may happen by chance that in some episodes, the agent gets a huge loss on and in others, a huge profit. This is very unexpected for the value function. Hence, the huge spikes in the losses.

\- Regarding no improvement in the loss functions: I was also thinking that the interplay between state-value function and policy function has something to do with this. Both keep changing and get updated based on the most recent version of each other. Kind of like in GANS, where there is a discriminator and a generator and their losses may also oscillate. From some point onwards, however I still do expect improvements.

https://preview.redd.it/bvbuuz76n3i41.png?width=665&amp;format=png&amp;auto=webp&amp;s=76e150fec15b254bbe45a8eb000159ba0e24bf67",reinforcementlearning,thisisthehappylion,False,/r/reinforcementlearning/comments/f6v26s/value_function_loss_and_policy_gradient_loss_not/
Which approach is suitable for varied numbers of actions per every state?,1582213779,"Hi,

&amp;#x200B;

I am trying to apply RL to the real-world application which is real-time operated and follows varied transitions.

&amp;#x200B;

The environment transition has

(i) varied numbers of actions per each state,

(ii) each reward after the respective actions, and

(iii) the next state which is received when all previous actions are being completed.

&amp;#x200B;

Therefore, the transition can be shown as &lt;s\_1, a\_1\^{1:A}, r\_1\^{1:A}, s\_2, a\_2\^{1:B}, r\_2\^{1:B}, ...r\_(T-1)\^{1:Z}, s\_T&gt; where A, B, ..., and Z are the number of varied actions decided by those states. The possible number of actions range from 1 to 5. Moreover, we assume T is very large, and the problem is an infinite-horizon setting.

&amp;#x200B;

In this case, I don't know whether this transition follows the MDP assumption. From past experiments, general RL algorithms were not fit this particular environment. Currently, I use A3C and differential reward. To address the varied number of actions, I have used the ""None"" placeholder from TF.

&amp;#x200B;

After training several days, the reward (and performance) is increased for a bit but lingered at a certain range and not increased any more. I have searched a lot of research papers and could not find any approach that fits this problem setting.

&amp;#x200B;

I wonder about any related works or possible approaches to solving this problem. Thank you.",reinforcementlearning,TK-SZ,False,/r/reinforcementlearning/comments/f6uylx/which_approach_is_suitable_for_varied_numbers_of/
Unit Neurons: Neural Networks as Complex Systems,1582210392,,reinforcementlearning,johnlime3301,False,/r/reinforcementlearning/comments/f6u5u9/unit_neurons_neural_networks_as_complex_systems/
What is Reinforcement Learning | basics explained,1582205113,,reinforcementlearning,gajanand_edu,False,/r/reinforcementlearning/comments/f6t0ze/what_is_reinforcement_learning_basics_explained/
Include step number in state variables,1582148468,"Hi everyone,

I am dealing with the control problem with following properties:

\- the number of steps in the episode is fixed

\- the agent should learn to take positive actions at the beginning of the episode and negative actions later (the reward function does not penalize the action being selected to achieve this property)

Would including the number of episodes current step in state variables improve the RL training results, or would it expand the state space and worsen the results? I'm using DDPG agent.",reinforcementlearning,sesli994,False,/r/reinforcementlearning/comments/f6hw1v/include_step_number_in_state_variables/
[R] Modular PyTorch implementation of Rainbow?,1582112307,"I am looking for an implementation of Rainbow to run some largish-scale experiments.

Here's what I'm looking for:

- Implemented in PyTorch
- Modular architecture: I should be able to run a full ablation study, eg replace NoisyNetworks with eps-greedy, disable PER, etc
- Readable code
- Reaches published Rainbow results

This is harder to find as you'd expect. Here's what I found so far:

- **[rlpyt](https://github.com/astooke/rlpyt)** - the best looking so far! I had problems navigating the code in the past, but it looks like documentation was recently added

- **[Kaixhin/Rainbow](https://github.com/Kaixhin/Rainbow)** - not modular, you can't run ablation studies

- **[RLlib](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#deep-q-networks-dqn-rainbow-parametric-dqn)** - doesn't use PyTorch for the Rainbow implementation

Any other options?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/f69p5u/r_modular_pytorch_implementation_of_rainbow/
RL and LSTM prctical question,1582112221,"Hey,

Following the question about LSTM and alpha star, I want to fully understand how LSTM are trained in RL, especially in DRQN.

Which options are the correct?  
During episode:  
1. LSTM's hidden state is reset only in the beginning of the episode, and each ""inference"" rely on all the timesteps from the start of the episode.  
OR  
2. Each ""inference"" rely on N (lets say N=5) latest timesteps, and for each inference we reset the LSTM's state, and start to feed the network with that window: t-N+1, T-N+2,...,T in order to get the action for time T.  


During Training:  
1. We saved the previous hidden state for each experience in the replay buffer and we just train by feeding the input + previous hidden state and optimizing for the target.  
OR  
2. We saved the previous hidden state for each experience in the replay buffer and we feed that window: t-N+1, T-N+2,...,T for each experience in order to also get the back propagtion through time

&amp;#x200B;

I hope my question was clear, this issue really bothers me...

Thanks",reinforcementlearning,What_Did_It_Cost_E_T,False,/r/reinforcementlearning/comments/f69onv/rl_and_lstm_prctical_question/
[Q] Transfer learning for RL,1582056114,"Hello,

I recently took a project with my friends to make a game and an RL agent that will solve the game.   
Game is in 3D and is made in Unity so my first thought, as this game is very simple, I decided to feed pixel data inside of my Agent(e.g. 100x100 img).  
So I guess that involves working with CNN's, and I was wondering - has anyone tried transfer learning with a similar use-case? Meaning pretrained on Image Net ResNet34(just example architecture, can be anything).  
If so, how it came out, good, bad? I am just curious if you have any tips/tricks etc, I will do it anyway to test how it works.

On the side note, I already implemented PPO, right now I and my friends are connecting pieces together with gRPC. And I know that I have to have multiple instances of environment otherwise PPO might not just work.

Thanks for all the answers, I hope you won't kill me for the first messy post in this subreddit.",reinforcementlearning,RvuvuzelaM,False,/r/reinforcementlearning/comments/f5xqwj/q_transfer_learning_for_rl/
Question: AlphaStar vs Catastrophic Interference,1582050301,"How was AlphaStar able to train for so long without forgetting?

Is it because an LSTM was used?

Was it because of the techniques used in combination with an LSTM?

""[deep LSTM core](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&amp;rep=rep1&amp;type=pdf), an [auto-regressive policy head](https://arxiv.org/abs/1708.04782) with a [pointer network](https://papers.nips.cc/paper/5866-pointer-networks.pdf), and a [centralized value baseline](https://www.cs.ox.ac.uk/people/shimon.whiteson/pubs/foersteraaai18.pdf) ""

How can the layman go about training models without it being destroyed by Catastrophic Interference?",reinforcementlearning,Heartomics,False,/r/reinforcementlearning/comments/f5w7l7/question_alphastar_vs_catastrophic_interference/
Should I normalize the state?,1582046185,"I am using a SAC implementation from github and I do not see any mention of normalization of state.

My state is composed of two vectors of different sources and both sources have very different scales.

Does anyone know if scaling the state would affect the performance? (As in SL it totally affects).

Thanks in advace.",reinforcementlearning,LazyButAmbitious,False,/r/reinforcementlearning/comments/f5v4dy/should_i_normalize_the_state/
New to Facebook Elf Go,1582000624,"Hi, is there any study groups or forums for learning and discussing Facebook Elf Go?

Thanks",reinforcementlearning,AiLearnerXYF,False,/r/reinforcementlearning/comments/f5m57s/new_to_facebook_elf_go/
"""The messy, secretive reality behind OpenAI’s bid to save the world"", TR [""One of the biggest secrets is the project OpenAI is working on next. Sources described it to me as the culmination of its previous four years of research: an AI system trained on images, text, and other data...""]",1581983075,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/f5i4ov/the_messy_secretive_reality_behind_openais_bid_to/
Normalizing rewards with STD or fixed scale?,1581976080,"TL:DR | Do you need reward normalization in RL and if so, should you use STD or fixed scaling? 

When using RL, you have rewards where the scales are sometimes higher then you would want, for regularization sake.

I am trying to figure out which method would be the most efficient. Currently I am dividing my rewards by 4, because average reward range is around [-5,5] with a cap of [-1,1], meaning if the reward is higher or lower than this threshold, then the reward becomes -1 or 1. For example 8/4 = 2, meaning this becomes 1. I do this, because i got “told” to stay within the [-1,1] scale for normalization and computing sake, although I have seen discusions that this is not always the case. 

When I look at the stable-baselines normalization, then i see that they are using the standard deviation to normalize the rewards, if enabled. I am wondering whether this is a good idea, because Say your average reward is in the triple digits, then even with std the reward becomes quite high, this negating the normalization idea, I mean you will filter out the outliers though. Also at the start of training the std becomes quite inaccurate, since it is just like an average, it needs samples to become more accurate, i am not sure if this makes any noise in the model. I also have a terminal reward of -10, this could also shift the std to an Unwanted range.

Of course perhaps a diffrent method of normalizing your rewards.
I could be completly wrong about all this and I am just seeking for some more information and perhaps advice. Thank you in advance.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/f5gclh/normalizing_rewards_with_std_or_fixed_scale/
[Q] Using minibatches in PPO/Policy gradient updates,1581975529,"What is the purpose of using minibatches in policy updates? Also what math/theory is behind the idea?

Lets say that I have a buffer of experience with 1000 timesteps/samples. I randomly sample 250 samples (my minibatch). And now I am lost, I ve read that you can/should use this same buffer for 10 times to update policy. Should I use another minibatch from those 1000 samples? I have also read somewhere that you should compute updates but do not apply them yet. What is the benefit of this approach?

Can someone point me to the right direction or explain this to me? I cant find any relevant literature about using minibatches.",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/f5g76w/q_using_minibatches_in_ppopolicy_gradient_updates/
Designing observation space for custom environment,1581960930,"Hey, I am currently trying to implement a Custom Environment in which I can place blocks and move them on a plane. But I am struggling in deciding what kind of observation would be suited best here. My first thought was representing each block by a one hot vector with its type and its bounding box and rotation. However my objects have front and back sides which are also relevant to my problem. Any suggestions to incorporate this into my state or whether bounding box and rotation are enough to represent that?",reinforcementlearning,salah3,False,/r/reinforcementlearning/comments/f5ca9v/designing_observation_space_for_custom_environment/
"[Post] Routing Traveling Salesmen on Random Graphs Using Reinforcement Learning, in PyTorch",1581960326,,reinforcementlearning,Wookai,False,/r/reinforcementlearning/comments/f5c4gm/post_routing_traveling_salesmen_on_random_graphs/
[Concantenate discrete actions],1581953877,"Hello everyone, 

I want to ask what is the best way to concatenate discrete actions into a compact representation? My thought is that I can use concatenated hot encoding vectors but it seems wrong since these vectors are sparse. I want to find a compact representation of actions as an input together with states to predict future observations.",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/f5aftp/concantenate_discrete_actions/
[Research] Deep Residual Reinforcement Learning,1581953112,"New research finds out that Residual Algorithms (RA) is a more effective approach to the distribution mismatch problem in model-based planning 

Full research paper (PDF):   [https://arxiv.org/pdf/1905.01072v2.pdf](https://arxiv.org/pdf/1905.01072v2.pdf)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/f5a971/research_deep_residual_reinforcement_learning/
[Post] A Step-by-step Approach to Understanding Q-learning,1581952176,"Reinforcement Learning: Monte-Carlo and Temporal-Difference Learning

 [https://medium.com/ai%C2%B3-theory-practice-business/reinforcement-learning-part-5-monte-carlo-and-temporal-difference-learning-889053aba07d](https://medium.com/ai%C2%B3-theory-practice-business/reinforcement-learning-part-5-monte-carlo-and-temporal-difference-learning-889053aba07d)",reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/f5a14r/post_a_stepbystep_approach_to_understanding/
Reinforcement Learning Tutorial | Reinforcement Learning in Artificial I...,1581939579,,reinforcementlearning,raghunathsamal,False,/r/reinforcementlearning/comments/f57k29/reinforcement_learning_tutorial_reinforcement/
Any Reinforcement Learning algorithm for sim2real directly(No real world data during training)?,1581878443,"If I train my Reinforcement Learning algorithm purely in simulation without access to real world data, then deploy it in the real world, how to make it adapt to the real world efficiently? 

Note that the data here I mainly mean RGB image. So can it adapt to the RGB image in real world from simulation directly?

 On policy?

Thanks in advance.",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/f4uxs5/any_reinforcement_learning_algorithm_for_sim2real/
How are my Hindsight Experience Replay (HER) results obtained 50 times faster than original paper?,1581878151,"I am reproducing the results from Hindsight Experience Replay by Andrychowicz et. al. In the original paper they present the results below, where the agent is trained for 200 epochs.

200 epochs \* 800 episodes \* 50 time steps = 8,000,000 total time steps.

&amp;#x200B;

https://preview.redd.it/mfbdo0n6ybh41.png?width=1044&amp;format=png&amp;auto=webp&amp;s=c47b948929065cee2984460d7f9c2cccea7cd4a3

I try to reproduce the resutls but instead of using 8 cpu cores, I am using 16 CPU cores.

## Fetch Push

I train the `FetchPush` for 80 epochs, but with only 50 episodes per epoch. Therefore 80 \* 50 \* 50 = 200,000 iterations. I present the curve below, generated using **two random seeds**:

After 20 epochs = 50,000 iterations we solve this environment. In the paper above, it took the original authors 100 episodes = 4,000,000 iterations to do so.

How is my algorithm converging **50 times faster**?

https://preview.redd.it/rqlamvc7ybh41.png?width=868&amp;format=png&amp;auto=webp&amp;s=9a96f841685ce890596368ab85cea63e473ffe47

## Pick and Place

I train the `FetchPickAndPlace` for 80 epochs, but with only 50 episodes per epoch. Therefore 80 \* 50 \* 50 = 200,000 iterations. I present the curve below, generated using **three random seeds**:

&amp;#x200B;

https://preview.redd.it/4v6ee678ybh41.png?width=1538&amp;format=png&amp;auto=webp&amp;s=e81e133b2d689d015dc82f69236e57b749beaf1a

and logger output for the first two epochs, showing that indeed I have 50 episodes per epoch:

&amp;#x200B;

https://preview.redd.it/g4x8upw8ybh41.png?width=233&amp;format=png&amp;auto=webp&amp;s=2204d1f073b9ca4aaae23d2435127cd196cfded8

Now, as can be seen from my tensorboard plot, after 40 epochs we get a steady success rate, close to 1. 40 epochs \* 50 episodes \* 50 time steps = 100,000 iterations. Therefore it took the algorithm approximately 100,000 time steps to learn this environment.

The original paper took approximately 50 \* 800 \* 50 = 2,000,000 time steps to achieve the same goal.

How is it that in my case the environment was solved nearly **20 times faster**? Are there any flaws in my workings above? Surely I am doing something wrong, right?

Results are also faster than another paper which also uses 19 MPI workers:

https://preview.redd.it/9ywm98k9ybh41.png?width=873&amp;format=png&amp;auto=webp&amp;s=84e3760a41038affe4bb5d2333cfaad0865ea8d3

As stated in this paper: ""We train for  50 epochs (one epoch consists of 19  2  50 = 1 900 full episodes), which amounts to a total of 4.75 x10^(6) timesteps."" It took around 2,000,000 timesteps to reach a median success rate of 0.9.

Any suggestions on what I may be doing wrong would be appreciated.",reinforcementlearning,rrz0,False,/r/reinforcementlearning/comments/f4uv2p/how_are_my_hindsight_experience_replay_her/
Hindsight Experience Replay resutls obtained 50 times faster? How is it possible?,1581878042,"I am reproducing the results from Hindsight Experience Replay by Andrychowicz et. al. In the original paper they present the results below, where the agent is trained for 200 epochs.

&amp;#x200B;

200 epochs \* 800 episodes \* 50 time steps = 8,000,000 total time steps.

&amp;#x200B;

\[!\[enter image description here\]\[1\]\]\[1\]

&amp;#x200B;

I try to reproduce the resutls but instead of using 8 cpu cores, I am using 16 CPU cores.

&amp;#x200B;

&amp;#x200B;

\----------

\## Fetch Push ##

&amp;#x200B;

I train the \`FetchPush\` for 80 epochs, but with only 50 episodes per epoch. Therefore 80 \* 50 \* 50 = 200,000 iterations. I present the curve below, generated using \*\*two random seeds\*\*:

&amp;#x200B;

After 20 epochs = 50,000 iterations we solve this environment. In the paper above, it took the original authors 100 episodes = 4,000,000 iterations to do so.

&amp;#x200B;

How is my algorithm converging \*\*50 times faster\*\*?

&amp;#x200B;

\[!\[enter image description here\]\[2\]\]\[2\]

&amp;#x200B;

\----------

\## Pick and Place ##

&amp;#x200B;

I train the \`FetchPickAndPlace\` for 80 epochs, but with only 50 episodes per epoch. Therefore 80 \* 50 \* 50 = 200,000 iterations. I present the curve below, generated using \*\*three random seeds\*\*:

&amp;#x200B;

\[!\[enter image description here\]\[3\]\]\[3\]

&amp;#x200B;

&amp;#x200B;

and logger output for the first two epochs, showing that indeed I have 50 episodes per epoch:

&amp;#x200B;

&amp;#x200B;

\[!\[enter image description here\]\[4\]\]\[4\]

&amp;#x200B;

Now, as can be seen from my tensorboard plot, after 40 epochs we get a steady success rate, close to 1. 40 epochs \* 50 episodes \* 50 time steps = 100,000 iterations. Therefore it took the algorithm approximately 100,000 time steps to learn this environment.

&amp;#x200B;

The original paper took approximately 50 \* 800 \* 50 = 2,000,000 time steps to achieve the same goal.

&amp;#x200B;

&amp;#x200B;

How is it that in my case the environment was solved nearly \*\*20 times faster\*\*? Are there any flaws in my workings above? Surely I am doing something wrong, right?

&amp;#x200B;

Results are also faster than another paper which also uses 19 MPI workers:

&amp;#x200B;

\[!\[enter image description here\]\[5\]\]\[5\]

&amp;#x200B;

As stated in this paper: ""We train for  50 epochs (one epoch consists of 19  2  50 = 1 900 full episodes), which amounts to a total of 4.75 x10\^6 timesteps."" It took around 2,000,000 timesteps to reach a median success rate of 0.9.

&amp;#x200B;

\----------

&amp;#x200B;

Any suggestions on what I may be doing wrong would be appreciated.

&amp;#x200B;

  \[1\]: [https://i.stack.imgur.com/Bw6Sj.png](https://i.stack.imgur.com/Bw6Sj.png)

  \[2\]: [https://i.stack.imgur.com/vpOCd.png](https://i.stack.imgur.com/vpOCd.png)

  \[3\]: [https://i.stack.imgur.com/3BMhE.png](https://i.stack.imgur.com/3BMhE.png)

  \[4\]: [https://i.stack.imgur.com/aGESH.png](https://i.stack.imgur.com/aGESH.png)

  \[5\]: [https://i.stack.imgur.com/QiTSg.png](https://i.stack.imgur.com/QiTSg.png)",reinforcementlearning,rrz0,False,/r/reinforcementlearning/comments/f4uu1z/hindsight_experience_replay_resutls_obtained_50/
Mujoco and windows,1581871266,"As you guys know there is no support in windows for mujoco 2, I want to know is there a specific reason for that? I am trying to run safety gym but it needs mujoco 2 which I cannot have on windows
Also I couldn’t run it on linux too! Can anybody please help me to do these? Or show me some pointer except the github page?",reinforcementlearning,forsakenMystery,False,/r/reinforcementlearning/comments/f4t4hu/mujoco_and_windows/
Python and AI,1581857575,"Hey! Im new in AI and all that connect with it. I want to create a 3d simulation of my nn learning, but i dont know where i can do it. I know about unity, but there i need knowledge in c# for physics. So, pls, write me if you know some programs which can help me with it.",reinforcementlearning,TIKki430,False,/r/reinforcementlearning/comments/f4q9we/python_and_ai/
CS234 Winter 2020,1581850428,"I have seen, that the lectures from winter 2019 course of RL on Stanford by Emma Brunskill are available on YouTube. What about winter 2020? Are these new lectures also available somewhere?",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/f4p6k0/cs234_winter_2020/
Dilemna in Education and Finding Careers,1581841014,"I have a bit of a dilemma.

I've been teaching myself deep RL. I understand the concepts, but mostly not the math. I've been watching 3B1B videos, and they're excellent, but I'm afraid they're not helping me understand... I live in a city with an university that doesn't really have the best CS or math departments. To top it all off, I have a slight physical disability which prevents me from moving to another city with a good CS department, and I don't really have a clue what to do with my knowledge in RL career-wise, as I'm pretty much stuck here. What should I do?",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/f4nt9n/dilemna_in_education_and_finding_careers/
"""BADGR: An Autonomous Self-Supervised Learning-Based Navigation System"", Kahn et al 2020 {BAIR}",1581825864,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/f4l8vv/badgr_an_autonomous_selfsupervised_learningbased/
Can I apply experience on naive actor critic directly? Should it work?,1581792338,,reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/f4do3v/can_i_apply_experience_on_naive_actor_critic/
Deep RL with Data Already Collected,1581791621,"Hi all, I‘m currently working on a dataset containing mobility data of a university campus (like rows containing a timestamp, ID of person and the current building he/she is in). The dataset has about 250 Mio rows. I want to predict the next building a person will go to via Deep RL. I have implemented DRL algorithms before, but never with already collected data (so I‘m not able to let the agent really interact with the environment like in the classic setup).

Do you have any pointers for me?",reinforcementlearning,tcharmos,False,/r/reinforcementlearning/comments/f4dhmg/deep_rl_with_data_already_collected/
High variance while training REINFORCE,1581763507,"I am new to Reinforcement learning and was trying to implement REINFORCE in pytorch using  [https://github.com/pytorch/examples/blob/master/reinforcement\_learning/reinforce.py](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) 

However, I realized that there is  extremely high variance during training. The model is able to perform learn only 1 out of 10 times. Any guide to how I can reduce this variance.",reinforcementlearning,djin31,False,/r/reinforcementlearning/comments/f47uzr/high_variance_while_training_reinforce/
A new PyTorch framework for RL,1581758315,,reinforcementlearning,_djab_,False,/r/reinforcementlearning/comments/f475eq/a_new_pytorch_framework_for_rl/
Looking for recommendation on course for reinforcement learning.,1581752432,,reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/f46bpk/looking_for_recommendation_on_course_for/
What is difference the between MountainCar-v0 and MountainCarCountinous-v0 in OpenAi gym ?,1581704552,,reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/f3wb0v/what_is_difference_the_between_mountaincarv0_and/
Can reinforcement learning be used to speed up monte carlo process?,1581697144,"I'm trying to optimise the monte carlo process. For a simple example like estimating the value of pi, can we use reinforcement learning to arrive at a good approximation in a lesser number of random samples so that it becomes less computationally expensive?",reinforcementlearning,funnymanallinsane,False,/r/reinforcementlearning/comments/f3ugaj/can_reinforcement_learning_be_used_to_speed_up/
Does multi armed bandit algorithms do the same predictions for all users?,1581688667,"I was reading about these algorithms, but I don't understand how they can be used in a recommender system because using the MovieLens dataset these algorithms recommend the best movie for all the users and not for a specifc user. I mean does multi armed bandit do the same predictions for all users?",reinforcementlearning,fablami,False,/r/reinforcementlearning/comments/f3sj1p/does_multi_armed_bandit_algorithms_do_the_same/
RL framework for researchers,1581684888,,reinforcementlearning,_djab_,False,/r/reinforcementlearning/comments/f3rsgr/rl_framework_for_researchers/
Does changing maximum achievable reward in episodes affect learning in TRPO/PPO?,1581681012,"An agent has to go around a map by following a path composed of N checkpoints. The state is the relative position of the next 4 checkpoints. The agent receives +1 every time it takes a checkpoint, and -0.01 at every time-step. In training, maps have different sizes and number of checkpoints, therefore the total achievable reward in each episode varies according to the number of checkpoints in the episode. Does this negatively affect the training using Policy Gradient algorithms such as TRPO or PPO?  


I'm confused between two answers:  
A) Yes, it negatively affects the training because different maps have a different total return for exactly the same state.  
B) No, because TRPO/PPO consider the temporal difference error, not the total reward from a given state to the end of the episode.",reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/f3r3zw/does_changing_maximum_achievable_reward_in/
"Taxi-v3 explanation. What is meant exactly by convergence of the algo, the highest reward and optimal action for every state?",1581670018,"I started learning about Q table from this blog post [Introduction to reinforcement learning and OpenAI Gym, by Justin Francis](https://www.oreilly.com/radar/introduction-to-reinforcement-learning-and-openai-gym/).
 &gt; After so many episodes, the algorithm will converge and determine the optimal action for every state using the Q table, ensuring the highest possible reward. We now consider the environment problem solved.

The Q table was updated by Q-learning formula
`Q[state,action] += alpha * (reward + np.max(Q[state2]) - Q[state,action])`

I ran 100000 episodes of which I got the following -
```
Episode 99250 Total Reward: 9
Episode 99300 Total Reward: 7
Episode 99350 Total Reward: 6
Episode 99400 Total Reward: 14
Episode 99450 Total Reward: 10
Episode 99500 Total Reward: 10
Episode 99550 Total Reward: 9
Episode 99600 Total Reward: 14
Episode 99650 Total Reward: 5
Episode 99700 Total Reward: 7
Episode 99750 Total Reward: 3
Episode 99800 Total Reward: 5
Episode 99850 Total Reward: 10
Episode 99900 Total Reward: 5
Episode 99950 Total Reward: 9
Episode 100000 Total Reward: 11
```
I don't know what the highest reward is. It does not look like it has converged. Yet, the [graph](https://i.imgur.com/FdlLKE8.png) shows a trend in convergence that was taken on a larger scale.

What should be the sequence of actions to be taken when the game is reset() but the ""learned"" Q table is available? How do we know that and the reward in that case?",reinforcementlearning,devprabal,False,/r/reinforcementlearning/comments/f3ph40/taxiv3_explanation_what_is_meant_exactly_by/
Effective Diversity in Population-Based Reinforcement Learning,1581661722,,reinforcementlearning,hardmaru,False,/r/reinforcementlearning/comments/f3o6vg/effective_diversity_in_populationbased/
Good Descriptions of MuJoCo Tasks,1581651876,"Hey guys,

Is there any place that has good descriptions of the more popular MuJoCo tasks?  
What I'm looking for specifically is what the state space is for different environments and a description of how the reward function is set up?  
The gym website most of the time just points to the github. [An example](https://gym.openai.com/envs/InvertedDoublePendulum-v2/).",reinforcementlearning,idurugkar,False,/r/reinforcementlearning/comments/f3mcdf/good_descriptions_of_mujoco_tasks/
How do you know you're doing something sufficiently novel?,1581627124,"So I'm a PhD student at an R1 trying to pin down research for my Master's (finished classes last semester). My adviser wants us to find our own research interests and so has been very hands-off in the process. I decided I wanted to characterize hierarchical RL in POMDPs, since it seemed an underexplored niche, but now I'm having second thoughts. 

I mostly just seem to be studying the performance characteristics of different ways to address the partial observability and applying methods to problems they haven't been used on before, but I worry that I'm wasting time doing something that ultimately won't be publishable. The idea seems to be that I'll figure out the story I want to tell as I see the results from different approaches, and I realize that the Master's part isn't expecting me to revolutionize the field, but I don't know what's publishable.

How do you recognize when you're going down a dead-end?",reinforcementlearning,AlternateZWord,False,/r/reinforcementlearning/comments/f3gnzx/how_do_you_know_youre_doing_something/
Formula relating lambda return and half life,1581616682,"In Suton and Barto's ""Reinforcement Learning: An Introduction"", there is a question about the relationship between lambda and the half life of lambda return the sequence (page 312).

&amp;#x200B;

https://preview.redd.it/baahby3wcqg41.png?width=1768&amp;format=png&amp;auto=webp&amp;s=2d8df3a0b5b21937e3e169833df8063093ca5d76

https://preview.redd.it/lccdcfn6cqg41.png?width=1834&amp;format=png&amp;auto=webp&amp;s=1997322b4c74198a68c4a91d9a126793aad3a606

In essence, the question is how can you determine lambda from the half life?

So far, I have the following formula:

https://i.redd.it/kax4fji4cqg41.gif

But, any value of tau greater than 1 gives a value of lambda greater than 1.   So, I am not sure what I am doing wrong.  Any ideas?",reinforcementlearning,marcosfelt,False,/r/reinforcementlearning/comments/f3dul2/formula_relating_lambda_return_and_half_life/
[R] Growing Neural Cellular Automata: A Differentiable Model of Morphogenesis,1581614184,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/f3d7rz/r_growing_neural_cellular_automata_a/
"(question)Implementing Empowerment, intrinsic reward",1581605525," so I have been thinking  on adding intrinsic reward to my project, but after reading about empowerment im still a bit confused.  

The algorithm that I’m using is an actor-critic, policy gradient, PPO.  

My question is on calculating mutual information. I have the policy distribution given by my model, but i’m reading that you need another policy distribution, W( which they call the distribution choosing actions? isn't that just my policy). Where do I get W from.

Also from calculating empowerment, I’m reading that the hardest piece getting the probability of S+1, so the probability of the next state. From what I read, you use your model to predict  this probability. How would I go about implementing this prediction. Lest assume I have whole episodes of S, actions, and S+1.",reinforcementlearning,lost_pinguin,False,/r/reinforcementlearning/comments/f3b2pn/questionimplementing_empowerment_intrinsic_reward/
I always feel behind in this area of research,1581588583,"Hi Everyone,

I did multiple RL courses in last one year - but somehow the pace of research is always crazy in this field. How do you cope up with it? 

Is there any great PhD thesis - kind of survey paper where they discuss all recent (2015 onward) developments in this field ?

Thanks again!",reinforcementlearning,AjayUnagar,False,/r/reinforcementlearning/comments/f37vc8/i_always_feel_behind_in_this_area_of_research/
"[D] Rebuttal of the SimPLe algorithm (""Model Based Reinforcement Learning for Atari"")",1581584861,"I am reading the ""Model Based Reinforcement Learning for Atari"" paper ([arxiv](https://arxiv.org/abs/1903.00374), [/r/ML thread](https://www.reddit.com/r/MachineLearning/comments/ax406x/r_190300374_modelbased_reinforcement_learning_for/), [website](https://sites.google.com/view/modelbasedrlatari/home)).

I've been told that some time after this paper came out, someone published a rebuttal explaining how similar results could be achieved using a regular Rainbow-DQN agent.

Which paper was that? Any of those?

- [Do recent advancements in model-based deep reinforcement learning really improve data efficiency?](https://openreview.net/forum?id=Bke9u1HFwB)
- [When to use parametric models in reinforcement learning?](https://arxiv.org/abs/1906.05243)

I want to make sure I get the story straight! Also was there any further development?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/f37b56/d_rebuttal_of_the_simple_algorithm_model_based/
Cartpole-v0 doens't work when gamma is too big,1581580197,"Hello guys, I'd like to hear your insight on something.

I'm practicing DQN on gym's cartpole-v0. Interestingly, when I set the gamma below 0.9, the DQN works just fine . However  when I set the gamma above 0.9, the DQN always converges to give the same action on every state(either 0 or 1), which is literally the worst policy for the Cartpole problem. Of course I'm fine with working on gamma&lt;0.9, but I'm just curious how can this be happening and I'd like to hear your thoughts about this. Thx.",reinforcementlearning,kafljjj,False,/r/reinforcementlearning/comments/f36lfq/cartpolev0_doenst_work_when_gamma_is_too_big/
What are the current state-of-the-art reinforcement learning algorithms for discrete problems?,1581541207,I know that PPO and Soft Actor-Critic are SoTA for continuous action problem. I'm wondering what are the current state-of-the-art reinforcement learning algorithms for discrete problems?,reinforcementlearning,skwaaaaat,False,/r/reinforcementlearning/comments/f2y06x/what_are_the_current_stateoftheart_reinforcement/
Category Theory of Markov Decision Processes,1581526565,,reinforcementlearning,hoj201,False,/r/reinforcementlearning/comments/f2u6zk/category_theory_of_markov_decision_processes/
Has anybody used RecSim (Google AI) to apply RL in recommendation engine?,1581517413,,reinforcementlearning,Light_geass,False,/r/reinforcementlearning/comments/f2rxlo/has_anybody_used_recsim_google_ai_to_apply_rl_in/
[R] Learning Structured Communication for Multi-agent Reinforcement Learning,1581481600,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/f2lh8j/r_learning_structured_communication_for/
[R] Hyper-Meta Reinforcement Learning with Sparse Reward,1581481558,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/f2lgwh/r_hypermeta_reinforcement_learning_with_sparse/
Choosing suitable rewards,1581440457,"Hi all, I am currently writing a SARSA semi-gradient agent for learning to stack boxes in a way so they do not fall over, but am running in trouble assigning rewards. I want the agent to learn to place as many boxes as possible before they fall
The issue I am having is I have been giving the agent a reward equal to the total number of boxes placed, but this means it never really gets any better, as it does not recieve 'punishment' for knocking a tower over, but instead reward.
One reward scheme I tried was to give it a reward for every time step it didn't fall over, equal to the number of blocks placed, and then a punishment when it did fall, but this gave mixed results.
Does anyone have any suggestions? I am a little stuck",reinforcementlearning,roboticalbread,False,/r/reinforcementlearning/comments/f2b7wk/choosing_suitable_rewards/
Computational resources required for football environment (Multi-Agent),1581439245,"I was giving a look at EMERGENT COORDINATION THROUGH COMPETITION ([https://arxiv.org/pdf/1902.07151.pdf](https://arxiv.org/pdf/1902.07151.pdf)) and (correct me if I am wrong) they do not report details about the hardware/time required for the learning computation.

Is this information missing or has been reported somewhere else?  


In the case of omitted information, what would be a realistic estimate?",reinforcementlearning,HeavyGradient,False,/r/reinforcementlearning/comments/f2awwv/computational_resources_required_for_football/
Sutton &amp; Barto's Figure 2.2,1581438060,"I am new to Reinforcement Learning, and I am studying (self-studying) on Sutton and Barto's [Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html). I got stuck on Figure 2.2, page 29, and the authors explanation at page 30. Particularly, I do not follow when they say:

&gt;The $\epsilon = 0.01$ method improved more slowly, but eventually would perform better than the $\epsilon = 0.1$ method on both performance measures shown in the figure.

But in the figure I do not really see how the $\epsilon = 0.01$ method performs better: both average reward and optimal action selection are lower.

What am I missing??",reinforcementlearning,zzkr,False,/r/reinforcementlearning/comments/f2amcu/sutton_bartos_figure_22/
"""Causal evidence supporting the proposal that dopamine transients function as temporal difference prediction errors"", Maes et al 2020",1581351449,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/f1sk3e/causal_evidence_supporting_the_proposal_that/
A2C. Neural Network gradients are scaled poorly.,1581350726,"I am working on a multi agent reinforcement learning problem.

I am using A2C.

I am using gradient clipping so that the policy doesn't diverge a lot from the current policy.

I am observing that the neural network gradients are scaled poorly as described by John Schuman here([https://youtu.be/xvRrgxcpaHY?t=208](https://youtu.be/xvRrgxcpaHY?t=208)).

The network gradients vary by order of magnitures( 1e-2 ,1e-3, 1e-5 and in some layers 1e-10). How do I fix the problem that network gradients are scaled poorly?",reinforcementlearning,Crazy_Plant,False,/r/reinforcementlearning/comments/f1sdrc/a2c_neural_network_gradients_are_scaled_poorly/
Confusion about Minimax Q-Learning MARL algorithm,1581338428,"I am working on combating jammers in cognitive radio networks. It is a zero-sum game between the cognitive user and a jammer in multiagent setting. I applied Minimax Q-learning to let the cognitive user learn minimax strategies and then simulated the learned cognitive user against a random jammer. I don't know what's wrong with it but I am getting some random results. I am plotting the curve for the probability of jamming against no. of epochs for training. For me, the more I train my agent the less should be the probability of jamming but it shows random results. 

Can anyone tell what could be a potential mistake I am making? Anticipated thanks!",reinforcementlearning,engrkhalid01,False,/r/reinforcementlearning/comments/f1pqvs/confusion_about_minimax_qlearning_marl_algorithm/
Do you use a lower Batch and/or Buffer size with LSTM?,1581291672,"When using LSTM in pretty much all RL algorithms, do/should you tend to go towards a lower batch and/or Buffer size, because I feel that you are already sampling quite an amount more because of the Lstm. I am talking like from Batch size 64 to 32. 

Is anyone firmiliar or known with any special treatments in hyperparameters when using LSTM? I am using PPO-LSTM if it matters.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/f1hd7h/do_you_use_a_lower_batch_andor_buffer_size_with/
How is the Deterministic Policy Gradient being evaluated in Deepmid's trfl Library?,1581199291,"I don't know if this question fits the rules for posting in this subreddit but I cannot seem to wrap my head around how it is being done and it has been bothering me the past few days.

* I cannot grasp the steps for lines [87](https://github.com/deepmind/trfl/blob/e633edbd9d326b8bebc7c7c7d53f37118b48a440/trfl/dpg_ops.py#L87) to  [92](https://github.com/deepmind/trfl/blob/e633edbd9d326b8bebc7c7c7d53f37118b48a440/trfl/dpg_ops.py#L92) in the implementation of the [deterministic policy gradient op](https://github.com/deepmind/trfl/blob/master/trfl/dpg_ops.py?l=36). 
* Why is a ""target\_a"" being created? 
* The subsequent stop\_gradient is understandable since we don't want to update the Q-network's trainable variables. 
* But then, what does this loss represent in the next line? DPG to me is an application of the chain rule. ~~Why~~(Rather, how) is the optimization of this loss helping update the network? 

Any help in understanding it would be immensely appreciated.",reinforcementlearning,AvisekEECS,False,/r/reinforcementlearning/comments/f0yiiu/how_is_the_deterministic_policy_gradient_being/
Where part does the neural network play in Deep RL?,1581183853,[removed],reinforcementlearning,liviurotiul,False,/r/reinforcementlearning/comments/f0uvpo/where_part_does_the_neural_network_play_in_deep_rl/
How to apply Contextual Bandits?,1581172463,[removed],reinforcementlearning,Fender6969,False,/r/reinforcementlearning/comments/f0sbvc/how_to_apply_contextual_bandits/
Modeling Reward for Multi-armed bandit and the reward problem of variance across time,1581134621,"Hello, 

I am working on dynamic pricing on a hourly basis for the company I work and I decided to model it as   a multi-armed bandit problem. With that in mind I had on question about the reward:

Lets assume that our product sales is strongly associated with the time of the day, it sells a lot near lunch. What is the best way to tackle this problem?  


1. Should I normalize the reward by  some kind of score calculated by the representativeness of the time regarding the total sales of the day? Some thing in the lines:  
   1. if at noon, 40% of the sales are made,  the hour score normalizer for the reward at noon would be (1.0 - 0.4)  = 0.6 whereas at midnight the representativeness of the hour is 0.05 than the score normalized should be (1.0 - 0.05) = 0.95
2. Or should I create a context of hour for each product? Maybe one MAB for each hour of the day and product?
   1. With this, the convergence is slower, because each or of the day, would be a context that will only repeat once per day. If we normalize the score, we would have 24 samples a day.

What do you suggest?",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/f0majb/modeling_reward_for_multiarmed_bandit_and_the/
Why does the update rule guarantee the convergence with random state initialization(except terminal states),1581130006,"The [incompleteideas](http://incompleteideas.net/book/first/ebook/node41.html#eq:VpiP) says

&amp;#x200B;

&gt;The initial approximation, is chosen arbitrarily (except that the terminal state, if any, must be given value 0) and each successive approximation is obtained by using the Bellman equation for  (3.10) as an update rule:

&amp;#x200B;

https://preview.redd.it/kt4y7i0q5mf41.png?width=932&amp;format=png&amp;auto=webp&amp;s=3ddb1b2f7a5b85aadf7e657df685d383ef14bb96

What's the intuition about how this update rule ensures the convergence with random state initialization(except terminal states)?

&amp;#x200B;

https://preview.redd.it/b9odxztn5mf41.png?width=462&amp;format=png&amp;auto=webp&amp;s=43c5bf81493144cbabfbf0cf9a9f6b27c3fc1aaf",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/f0legt/why_does_the_update_rule_guarantee_the/
Intuition of the convergence with random state initialization(except terminal states),1581123639,"The [incompleteideas](http://incompleteideas.net/book/first/ebook/node41.html#eq:VpiP) says

&amp;#x200B;

&gt;The initial approximation, , is chosen arbitrarily (except that the terminal state, if any, must be given value 0) and each successive approximation is obtained by using the Bellman equation for  (3.10) as an update rule:

&amp;#x200B;

https://preview.redd.it/hj9n0apimlf41.png?width=308&amp;format=png&amp;auto=webp&amp;s=be31f2479c6e873aa295abf85656c6958e57eb46

&amp;#x200B;

&amp;#x200B;

https://preview.redd.it/m7byzwmjmlf41.png?width=462&amp;format=png&amp;auto=webp&amp;s=060bed6302a0ecb6b11dde697ef4ebf46d9895f8

What's the intuition about why this update rule ensures the convergence with random state initialization(except terminal states)?",reinforcementlearning,curimeowcat,False,/r/reinforcementlearning/comments/f0k4hr/intuition_of_the_convergence_with_random_state/
Deep reinforcement learning for estimating a number between 0 and 1,1581088432,Can anyone recommend any reading or method for applying reinforcement learning to a continuous variable between 0 and 1. Thank you in advance!,reinforcementlearning,prismxxx,False,/r/reinforcementlearning/comments/f0bs45/deep_reinforcement_learning_for_estimating_a/
Reward Function for two targets,1581044282,"Hello ppl,

  
I am looking for a reward function that focus on two targets of different magnitude and variance. Let's say that you want to design a reward function for a dynamic pricing problem, whereas the two targets you want to optimize are Sales and Profit.   


My first intuition for this problem, is to normalize both targets in terms of Standard deviations from the mean and apply some weight multiplier (sales\_multipler + profit\_multiplier = 1.0) to then:   


reward =   sales\_multiplier \* (|Current\_sales - mean\_sales| / STD(sales) ) + profit\_multiplier \* (|current\_profit  - mean\_profit| / STD(profit))   


The main problem here, is to find a way to calculate the standard deviation in a online environment.   


What do you think? What other reward functions you  suggest?   


Thanks,",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/f043yb/reward_function_for_two_targets/
Overview of Reinforcement Learning,1581022944,,reinforcementlearning,UppedVotes,False,/r/reinforcementlearning/comments/ezz127/overview_of_reinforcement_learning/
Overview of Reinforcement Learning and Implementation of Proximal Policy Gradient using RLKit,1581022266,,reinforcementlearning,johnlime3301,False,/r/reinforcementlearning/comments/ezyu51/overview_of_reinforcement_learning_and/
Lex Fridman discusses RL,1580990992,"Seen our summary of Lex Fridman's introduction of Reinforcement Learning discussion last week? Let me know what you think! 

https://blog.re-work.co/an-introduction-to-reinforcement-learning-lex-fridman-mit/",reinforcementlearning,teamrework,False,/r/reinforcementlearning/comments/ezrglr/lex_fridman_discusses_rl/
Reinforcement learning podcasts,1580960902,"Apart from TalkRL and the one odd episode in the AI podcast, TWiML etc., are there any good RL podcasts? Anything that focuses on discussions of foundational papers? How about audio lectures?

Also, any podcast that discusses papers in ML in general? Not necessarily by the authors. Something like the NLP Highlights but covering more of foundational aspects.

PS - a fun fact. Found this note at the bottom of 27-Jan NLP Highlights show notes:

Warning: This episode contains explicit language (one swear word). 

Seriously. One swear word. That’s how straightlaced some of these show hosts are.",reinforcementlearning,umamal,False,/r/reinforcementlearning/comments/ezmanq/reinforcement_learning_podcasts/
"""NES: Scaling data-driven robotics with reward sketching and batch reinforcement learning"", Cabi et al 2019 {DM} [learning reward models to learn from large heterogeneous unlabeled robotic datasets]",1580954099,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ezkslp/nes_scaling_datadriven_robotics_with_reward/
Why do Torch DDPG implementations use two optimizers?,1580930334,"I am aware that we step the critic, then the actor, but this can be done with a single optimizer. My thought was that because adam uses a step, it might mess things up, but after reading up the code, the step is dependent on the group variables which are independent, thus stepping once the Q doesn't affect the Pi's step.",reinforcementlearning,da_qstar,False,/r/reinforcementlearning/comments/ezeqac/why_do_torch_ddpg_implementations_use_two/
"[DRL] Using Rotation, Translation, and Cropping to Boost Generalization in Deep Reinforcement Learning Models",1580928068,"In a new paper released last week, researchers from the New York University and Modl.ai, a company applying machine learning to game developing, suggest that simple spacial processing methods such as rotation, translation and cropping could help increase model generality.

Read more: [Using Rotation, Translation, and Cropping to Boost Generalization in Deep Reinforcement Learning Models](https://medium.com/syncedreview/using-rotation-translation-and-cropping-to-boost-generalization-in-deep-reinforcement-learning-412ec3225873)",reinforcementlearning,rockyrey_w,False,/r/reinforcementlearning/comments/eze4x5/drl_using_rotation_translation_and_cropping_to/
Project Topic idea for Final Year Paper,1580926681,"Hey guys, I'm searching for a project Topic idea on Reinforcement learning for my final paper (3 months) . Please suggest any",reinforcementlearning,hojoisaac,False,/r/reinforcementlearning/comments/ezdrxb/project_topic_idea_for_final_year_paper/
Reinforcement Learning and Behavior Trees,1580922078,I'm starting a research project on the integration of RL in Behavior Trees and I'm getting up to date with the work that has been done in that direction. Could you point me to some interesting work about that?,reinforcementlearning,fedetask,False,/r/reinforcementlearning/comments/ezcl0x/reinforcement_learning_and_behavior_trees/
Festure engineering for semi-gradient methods?,1580914007,"Hello all, I was wondering if anyone knows of any good links/advice on how to actually perform feature engineering? I am currently ysing semo-gradient SARSA, which (as far as I can tell) requires features to be constructed for it.
My issue is although I can edit my features and vary the final output, I have no real idea how to make 'good' features. I have tried searching for advice but have so far found very little.
Thanks in advance",reinforcementlearning,roboticalbread,False,/r/reinforcementlearning/comments/ezal0m/festure_engineering_for_semigradient_methods/
Understanding PPO!,1580895503,"I am using the PPO Algorithm ([PPO](https://arxiv.org/pdf/1707.06347.pdf)) in my research work and I am building a few optimisations on top of it.  I am almost clear with how and why it works. But one thing which still nags me is that, why is the clipping function unbounded in the negative region for a negative advantage function.  This seems to defeat the whole purpose of having small gradient steps, because it is controlled only for the postive advantages and for the negative advantages it is not.

Can anyone explain why this is so? or if I have understood the algorithm wrong?

Also this [paper](https://openreview.net/pdf?id=r1etN1rtPB) is a great study on the effects of implementation of various optimisations in PPO, that are not explicitly mentioned in the original paper.",reinforcementlearning,sharafath28,False,/r/reinforcementlearning/comments/ez744m/understanding_ppo/
HER solving robotic push environment (FetchPush),1580767035,"Currently I am able to use HER to solve simple robotic ""Reach"" task. It makes sense that even if the arm doesnt reach desired goal, it still reach some goal and I can use it as arbitrary goal. However, in case of others environment, more complex tasks, I cannot understand how HER can help.

&amp;#x200B;

[OpenAI picture of HER](https://preview.redd.it/ncedyqh75se41.png?width=770&amp;format=png&amp;auto=webp&amp;s=629e67c962b7c7d2a921411358e7cde918fb551e)

Lets talk about this environment. The goal is to push a puck to the desired location. I can imagine that if the arm pushed the puck somewhere, I can use it as virtual goal (I will pretend that the achieved goal was the desired one). BUT how does the arm know that it has to interact with the puck? Why will it not learn only to reach the desired goal? 

Could someone explain this to me? When I use SAC+HER at the different environment it will only learn to reach position and not interacting with objects. 

Thank you all in advance

PS: In the HER publication ""k"" parameter is mentioned . Lets say that k = 0,8. Does it mean that I should apply HER to 80% of episodes?",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/eyesqe/her_solving_robotic_push_environment_fetchpush/
[P] Unity ML-Agents Course new article: Train a curious agent to destroy Pyramids,1580743537,"Hey there,

I published the second article of Unity ML-Agents free course where **we learn about curiosity in deep reinforcement learning and train a curious agent to destroy Pyramids.**

The article: [https://towardsdatascience.com/diving-deeper-into-unity-ml-agents-e1667f869dc3](https://towardsdatascience.com/diving-deeper-into-unity-ml-agents-e1667f869dc3)

The course's syllabus: [http://www.simoninithomas.com/unitymlagentscourse/](http://www.simoninithomas.com/unitymlagentscourse/)

The github repo: [https://github.com/simoninithomas/unity\_ml\_agents\_course](https://github.com/simoninithomas/unity_ml_agents_course)

&amp;#x200B;

https://i.redd.it/amslnderope41.gif

I would love to hear your feedback, what topics should I cover etc,

Thanks for your help! 😃",reinforcementlearning,cranthir_,False,/r/reinforcementlearning/comments/ey8oo1/p_unity_mlagents_course_new_article_train_a/
Questions about batch Actor-Critic,1580732855,"Hi everyone,

I am an RL newbie, so I am sorry if my questions does not make sense. I have a continuing problem and I do not have access to the environment, but only the batch data. After implementing DQN, I was trying to find a policy with Actor-Critic. I am confused about the followings:

1. Since I have a Q-network that converges, I tried Q Actor-Critic. However, since my Q values are extremely close, I don't think it is a good idea to use it as the critic.
2.  Secondly, I tried advantage actor-critic but loss of value network is not going down and policy network's loss seems to diverge to minus infinity. As I read in OpenAI Spinning Up ([https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)) it is usually a bad sign but policy loss does not have to go down.
3.  I don't have any metrics other than loss. As far as I have seen, in most of the RL papers success is evaluated by running (generally episodic) games and observing the reward. Does it make sense to look at OPE (Off-Policy Policy Evaluation) results every couple of epochs?
4. My A2C implementation does not converge even in the CartPole unless I calculate Q-values with rollouts after the episode ends and it is not applicable to my problem. 
5. Finally, I could not find any AC implementation for continuing problems, is there any source I can look at?

My post has been longer than I expected, but I am really stuck with my project. Any pointer would be appreciated!",reinforcementlearning,Aware-Wasabi,False,/r/reinforcementlearning/comments/ey6frc/questions_about_batch_actorcritic/
Reinforcement learning using Transformer seq2seq model,1580715347,"Hello everyone, 

I am trying to apply a policy gradient algorithm to the a sequence to sequence transformer model for abstractive text summarization, in Pytorch. 

So far I have been using RNN sequence to sequence models as examples, and the way they do this is by getting a baseline {greedy} summary and a sampled summary using the Categorical class in Pytorch {with probabilities of those samples} and then use the rouge metric as a reward. 

Anyway, my problem appears when I attempt to get a sampled output from my Transformer model. When using an RNN model, tokens are sampled one by one during inference \[there is no gold label\], which works because RNN's output one token at a time. However, Transformers output the entire summary at one step, even though there still is an inference loop. 

My sampling function so far looks like: 

    def get_distribution(model, batch):
    
        src, (shift_tgt, lbl_tgt), segs, clss, mask_src, mask_tgt, mask_cls = batch
    
        # the mock tgt are just torch.zeros tensors that have the same shape as the tgt
        mock_tgt = get_mock_tgt(shift_tgt)
        mock_return = get_mock_tgt(shift_tgt)
    
        max_length = shift_tgt.shape[1]
        
        log_probs = []
        
        for i in range(0, max_length-1):
            prediction = model(src, mock_tgt, segs, clss, mask_src, mask_tgt, mask_cls)
            prediction = F.softmax(prediction, dim=2)
            
            multi_dist = Categorical(prediction[:, i])
            x_t = multi_dist.sample()
            mock_tgt[:, i+1] = x_t
            mock_return[:, i] = x_t
            log_prob = multi_dist.log_prob(x_t)
            log_probs.append(log_prob)
            
        return mock_return, log_probs

At each timestep during inference I create a distribution using `multi_dist = Categorical(prediction[:, i])` which I believe might not be exactly correct. 

What is your opinion ? Gradient computes successfully but training sessions don't bring much of an improvement. Is there a better way to sample from the Transformer's output?",reinforcementlearning,andelie97,False,/r/reinforcementlearning/comments/ey3ifr/reinforcement_learning_using_transformer_seq2seq/
"How to interpret policy gradient ""loss function"" value when the RL agent successful solves the environment? How to interpret large negative policy gradient algorithm's ""loss function"" and low entropy when policy gradients(A2C) fails to learn?",1580678919,"Hi,

I am working on writing a RL solution(A2C) to a custom openAI gym environment.

The gym environment has flags to control how hard it can be made.

**Success case**

For the easiest possible setting, the RL agent is able to solve the environment.

 I have uploaded the screenshot of tensorboard plots here.

[https://www.dropbox.com/sh/wekczscvhfv5ul5/AABp2x30J1m3JfqNTnVLVj-ma?dl=0](https://www.dropbox.com/sh/wekczscvhfv5ul5/AABp2x30J1m3JfqNTnVLVj-ma?dl=0)

The mean return is high, the value function MSE loss is low and the agent learns to maintain a healthy amount of entropy. I have added an optional section at the end of this post explaining the plots.

As the section on ""Making the Loss Function"" in [https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html),

 the ""loss function"" in A2C is by itself is meaningless and it's gradient is used to get to the policy gradient. 

&gt;How do I interpret the A2C loss function here? As the value is converging to a specific value, can I say that the gradients are small as policy gradient is almost zero. The policy gradient is zero as the RL agent is already following an optimal policy? Also what is the significance of the fact that the value of this loss function is converging to zero?

&amp;#x200B;

**Failure case**

When I make the environment hard, the RL agent isn't able to solve the environment.

 I have uploaded the screenshot of tensorboard plots here.

[https://www.dropbox.com/sh/7oaj5yrvzl9ad7b/AABkJUJbF\_DTL4KI54R6o-nda?dl=0](https://www.dropbox.com/sh/7oaj5yrvzl9ad7b/AABkJUJbF_DTL4KI54R6o-nda?dl=0)

&gt;I am not sure why entropy loss goes to zero eventually and the actor loss just drops to a negative value. When you look at mean returns and success plot, it is as if the RL agent tried for a while and then just gave up :-)

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

OPTIONAL EXPLANATION/INTERPRETATION:

Let me explain and interpret each of the images in the link. Note x-axis refers to number of episodes and y-axis refers to the value of interest as mentioned below.

For Success:

1. InteractionEntropy: This is just the entropy of the learned stochastic policy. I peaked to check how the agent learned to solve the environment perfectly but managed to keep the entropy high. The custom environment I am talking about, the agent has 5 actions(move north, south, west, east and fifth option is stay at the current place).  To reach the goal the agent has to go along a psuedo diagonal direction, so it has learned a policy with 50% probability of going south and 50% probability of going west.
2. ActionRewardMean: It's the mean return agent gets at the end of episode. The episode lasts for 30 steps. The agents takes \~8 steps to reach a goal and 22 steps are spent at the goal. Return = num of steps spent at the goal.
3. InteractionValueLoss: This is the prediction error in value function.
4. InteractionActorLoss: Not sure how to interpret this. [https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) does cover this. But I still have some questions which I have highlighted above.
5. Success: It measure the percent of envs solved fully. 100% means the Rl agent has pretty much aced the environment.",reinforcementlearning,Crazy_Plant,False,/r/reinforcementlearning/comments/exvkdz/how_to_interpret_policy_gradient_loss_function/
Is this training function correct for actor class in ddpg?,1580639183,"&amp;#x200B;

https://preview.redd.it/wpsqfhm8mhe41.png?width=1049&amp;format=png&amp;auto=webp&amp;s=1b04a80551e018c170460df7d2bf4489c9969f89",reinforcementlearning,CrossWayme,False,/r/reinforcementlearning/comments/exml0d/is_this_training_function_correct_for_actor_class/
Is this training function correct for ddpg actor class??,1580639120,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/exmkni/is_this_training_function_correct_for_ddpg_actor/
"""Meta-Learning in 50 Lines of JAX"", Eric Jang",1580613300,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/exhyum/metalearning_in_50_lines_of_jax_eric_jang/
[R] Mimicking Evolution with Reinforcement Learning,1580571221,,reinforcementlearning,jpiabrantes,False,/r/reinforcementlearning/comments/ex7qpb/r_mimicking_evolution_with_reinforcement_learning/
Measure-theoretic foundations of MDPs and RL,1580514778,"Many classic resources for RL are presented in terms of finite state and action spaces. In uncountable spaces, new issues arise about, e.g., the existence of the integrals that define the optimal cost function. Dealing with these without unnecessary loss of generality requires nontrivial measure-theoretic effort.

A classic work on this is
[ Stochastic Optimal Control: The Discrete-Time Case](https://web.mit.edu/dimitrib/www/soc.html) by
Dimitri P. Bertsekas and Steven E. Shreve (1978).
My question is: are there any newer publications that attempt to simplify/streamline the topic at all?

I haven't read Bertsekas's other books, but it seems like these issues are generally avoided (the word ""measure"" does not appear often in the sample chapters).",reinforcementlearning,jurniss,False,/r/reinforcementlearning/comments/ewxdg2/measuretheoretic_foundations_of_mdps_and_rl/
[D] Trying to understand a good approach for hybrid action spaces,1580504916,"Hi!

I'm quite new to deep reinforcement learning, but have toyed around with a few Gym and Atari environments using DDQN and DDPG. I have a project I'm starting on soon in which an agent will need to perform an ability from a discrete set of abilities, but also choose the direction to use that ability in. As such, I am dealing with a hybrid action space.

I've looked up a few papers online, and one that clearly showcases this hybrid approach is Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space (https://arxiv.org/pdf/1810.06394.pdf). However, I've also seen comments regarding PPO and how it is both powerful and could be used for things like this.

Seeing as I don't know a lot about this, I was wondering what approach might be best to take. P-DQN seems intuitively simple to understand, but I don't know if it is the best option for this problem. For PPO, I'm having a hard time grasping how to retrieve both discrete and continuous actions. The way I understand it, PPO has a distribution with the network approximating parameters for it, and then having the actions selected by sampling the distribution. But I would need two distributions of different kinds, so how would that work?

If you have any clarifying information, or links to something useful, please reply to this post. I'm leaning towards P-DQN right now since I think I understand how it works, but if PPO generally is much better then maybe I should focus on using that instead.

Thank you!",reinforcementlearning,Feodalius,False,/r/reinforcementlearning/comments/ewv05l/d_trying_to_understand_a_good_approach_for_hybrid/
Explain me the proof for the reward to go policy gradient,1580483340,"Hey!

I read through the proof presented here:

[https://spinningup.openai.com/en/latest/spinningup/extra\_pg\_proof1.html](https://spinningup.openai.com/en/latest/spinningup/extra_pg_proof1.html)

&amp;#x200B;

Can someone explain me the very end again? I don't quite get the reasoning for **case two** on the very bottom of the page.",reinforcementlearning,TooLazyToWorkout,False,/r/reinforcementlearning/comments/ewpr71/explain_me_the_proof_for_the_reward_to_go_policy/
How to solve bipedalwalker using DDPG?,1580476680,"Hello, I have written DDPG code in tensorflow 2.1. I am executing this on both google colab and my computer. Most I can do is get the agent to learn how to sit down but then it doesnt learn anything after that. I have run this for 1600 episode before resetting thinking it wont learng anymore. Can I get advice on how to solve this problem?",reinforcementlearning,CrossWayme,False,/r/reinforcementlearning/comments/ewocbu/how_to_solve_bipedalwalker_using_ddpg/
Newbie’s Guide to Study Reinforcement Learning,1580462861,,reinforcementlearning,ark_aung,False,/r/reinforcementlearning/comments/ewlxsx/newbies_guide_to_study_reinforcement_learning/
mods don’t be mad,1580455005,,reinforcementlearning,BrahmaTheCreator,False,/r/reinforcementlearning/comments/ewkowk/mods_dont_be_mad/
"""Curriculum for Reinforcement Learning"", Lilian Weng",1580434098,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ewgd4m/curriculum_for_reinforcement_learning_lilian_weng/
Me building a Tic Tac Toe agent,1580431826,,reinforcementlearning,PixxelWizard,False,/r/reinforcementlearning/comments/ewftvl/me_building_a_tic_tac_toe_agent/
"""An Opinionated Guide to ML Research"", John Schulman",1580430931,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ewfm3p/an_opinionated_guide_to_ml_research_john_schulman/
A beginner in RL field,1580405437,"Hi,everyone! I’m a beginner in the field of RL. I want to deal with reinforcement learning to use that knowledge at robotics. However, I can’t be sure about my background to understand what is going on at some lessons such as CS234 (Standord University’s lessons) and David Silver’s videos. Which areas I should know something before starting to RL?",reinforcementlearning,serkancakar,False,/r/reinforcementlearning/comments/ew8zi3/a_beginner_in_rl_field/
"OpenAI restandardizing: TensorFlow → PyTorch [""Spinning Up in Deep RL"" already rewritten for PyTorch]",1580404756,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ew8t0o/openai_restandardizing_tensorflow_pytorch/
Combination of Sparse and Dense reward,1580333871,"Is there any work, in which the environment has a combination of both dense and sparse reward and has been solved using DDPG algorithm?",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/evtrs1/combination_of_sparse_and_dense_reward/
Beginner Question about agent actions (relative/global),1580325608,"Hi everyone,

I'm fairly new to RL with experience from one university class, so I apologize if this question is redundant. 

I've noticed that all basic RL examples involve an agent taking actions from a global perspective, such as up/down/right/left which are always global and consistent directions and really represent north/south/east/west.

I am wondering if there are sources where the agent takes actions relative to its position, such as forwards/backwards/left/right, and not global.

Would there be a big difference between training with relative actions as opposed to global actions? Does anyone have any sources where this is implemented that they can share?

Thanks!",reinforcementlearning,creamLatifah,False,/r/reinforcementlearning/comments/evrlvl/beginner_question_about_agent_actions/
"""Polygames"": another Python3 game framework/library, AlphaZero/expert-iteration self-play-oriented {FB} [Cazenave et al 2020]",1580318858,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/evpyss/polygames_another_python3_game_frameworklibrary/
"Covariant.ai {Abbeel et al} releases warehouse robot details: in Knapp/Obeta warehouse deployments, &gt;95% picker success, ~600 items/hour [imitation+meta-learning+fleet-learning]",1580318075,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/evps5c/covariantai_abbeel_et_al_releases_warehouse_robot/
QLearning with intrinsic reward for exploration,1580286442,"I am trying to implement a simple QLearning algorithm with an intrinsic reward to boost exploration. From what I understood from the count-based papers and MBIE-EB, it should be sufficient to simply add intrisic reward r\_i = beta/sqrt(n(s,a)) to the extrinsic reward to ensure efficient exploration.

However, simply adding r\_i to the reward in tabular QLearning, i.e. using backups Q(s,a) &lt;-- r + r\_i + gamma\*max Q(s', .) yields absolutely no results for any other value of beta than 0, where we come back to QLearning. 

Do you know how to implement this kind of exploration boost, in the true RL setting where the MDP is unknown? Thanks for any help.",reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/evk79z/qlearning_with_intrinsic_reward_for_exploration/
MaxEnt reinforcement learning with policy gradient,1580285894,"I am trying to implement the MaxEnt RL according to this slide from lecture [Connection between Inference and Control](http://rail.eecs.berkeley.edu/deeprlcourse-fa18/static/slides/lec-15.pdf)  of from Sergey Levine's DRL 2018 course, or corresponding lecture ""Reframing Control as an Inference Problem""  2019 course.

https://preview.redd.it/x09gp1dhfod41.png?width=1689&amp;format=png&amp;auto=webp&amp;s=982c06df2346c4bbe1c9f15f4702ece816bac0af

What I don't quite get is: are we going to take the gradient with respect to the entropy term or not with such objective function? Because if we don't  the entropy in my case actually goes down rapidly as long as I don't vastly lower the weight of entropy term (similarly as in paper [https://arxiv.org/abs/1702.08165](https://arxiv.org/abs/1702.08165) eq. 2). But if try the other approach and compute the gradient with respect to entropy, the entropy goes so high (independent of the entropy weight) and kept there that the policy is unable to learn anything meaningful.

Please have a look on the plots of current results. Continuous line represents mean reward, dashed line policy entropy:

[Current results](https://preview.redd.it/opmuiy4ifod41.png?width=640&amp;format=png&amp;auto=webp&amp;s=01812fcaf307be1c717a3257cf13c7bfee6594ca)

What would be then the correct way to introduce entropy term to policy gradient: by taking the gradient with regard to the entropy term or not?",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/evk4dn/maxent_reinforcement_learning_with_policy_gradient/
Efficient Architecture for DQN,1580277489,"Hello,

I've got a convolutional architecture for a DQN, and am wondering what I can do to optimize it for speed/efficiency as I don't have much computational resources to work with. I am looking for  an efficient architecture that performs well. My main issues involve the last layer (I have a large action space.),  which has a lot of parameters (4M), compared to the rest of the network. I'd like to freeze this layer or bottleneck the  layers above it, but I fear losing some performance doing so. Is there any methods for DQNs that can decrease the training time?

&amp;#x200B;

Related question:   [https://ai.stackexchange.com/questions/17715/param-count-in-last-layer-high-how-can-i-decrease](https://ai.stackexchange.com/questions/17715/param-count-in-last-layer-high-how-can-i-decrease)",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/evirw6/efficient_architecture_for_dqn/
[PPO2] Huge loss spikes: sensitivity to action space and exploration?,1580232949,"Hi guys,

recently I encountered **huge policy loss spikes (in order of millions/billions)** in my training runs (self-driving cars). Since I plotted nearly everything, I am pretty sure that the (normalized) advantages are in typical range and the problem should be the ratios. (see graphs below)

To recap, the policy loss catches only large positive ratios but it doesn't care for large negative ones:

https://preview.redd.it/f5jwzst2qjd41.png?width=719&amp;format=png&amp;auto=webp&amp;s=e7b4429b28404f92795494aa197419aa00ddcb93

I asked myself how the ratios can take these large negative values. In all training runs my 2-dimensional action space ranged from \[-5, 5\] (delta velocity) and \[-3.5, 3.5\] (delta lateral position) respectively. Since my training environment is a simple obstacle course where the full action space is not needed to drive without collision, I set the standard deviation to a relatively low level (\~1.0 and \~0.5), sometimes with decay to (0.5, 0.15) or lower. As a consequence the agent is not able to explore the action space in its entirety. (remark: collisions occur quite frequently at the beginning)

In my histograms of the un-clipped ratios, I noticed a few ratios being in the order of hundreds/thousands, which are likely responsible for the loss spike. They always appear at arbitrary update counts.

So there might be a chance that during the training the agent will see anything he has never seen before, if I don't let him explore the entire action space. That might distort the loss, but I cannot tell exactly.

**Has anyone encountered similar problems and/or can help me?**

Example tensorboard with default values from [Ray Rllib](https://ray.readthedocs.io/en/latest/rllib-algorithms.html#proximal-policy-optimization-ppo): Before the huge spike, there is a smaller one with magnitude \~14.5.

Rewards range from 0 to 1 with a collision-terminal of -10.

https://preview.redd.it/bk3gfs9h0kd41.png?width=1373&amp;format=png&amp;auto=webp&amp;s=6d73f4f9c728abeb12672087b87b36af9e0701c9

&amp;#x200B;

https://preview.redd.it/cd5aqh5i0kd41.png?width=1338&amp;format=png&amp;auto=webp&amp;s=5d38212a59b600c339f429736079b88adf2ff934",reinforcementlearning,50svent,False,/r/reinforcementlearning/comments/ev8las/ppo2_huge_loss_spikes_sensitivity_to_action_space/
Procedural Content Generation via Reinforcement Learning,1580189269,,reinforcementlearning,hardmaru,False,/r/reinforcementlearning/comments/ev16g9/procedural_content_generation_via_reinforcement/
RL beginner question: RL with very large action space,1580177939,"Thanks to my PhD work, I am fairly familiar with un/supervised ML but hardly at all with Reinforcement Learning. Sorry for being fairly abstract in the following; happy to disclose more to RL researchers in a PM.  


* I have a problem where **I have a huge labelled dataset** \--&gt; so far absolutely ideal for supervised (deep) learning. 
* But where there the **cost function cannot easily be determined, let alone be differentiated** with respect to the features --&gt; no way I can just use Deep Learning.
* But: Given an input and an output, I am able to compute some score of how good the output is.

So question is: Can I use something like Deep RL for my problem?

Most of the RL problems I have seen are problems where the agent can only tweak a limited number of ""knobs"", e.g. the direction of a car as one ""knob"", and speed as another ""knob"". But I have a problem where there are thousands of ""knobs"" that can be turned.  


Are there RL problems where the action space is as incredibly large as in my case? What is generally (for dummies / ELI5) the approach to deal with this?",reinforcementlearning,sabertoothedhedgehog,False,/r/reinforcementlearning/comments/euyx6c/rl_beginner_question_rl_with_very_large_action/
Learning and Implementing RL for Project,1580173588,"I am a graduate student in engineering doing a thesis project that will likely involve RL. I am trying to piece together a way to learn and implement RL to my project. So far, I have been going through Sutton and Barto's textbook and David Silver's UCL lectures. Are there any other resources you all recommend? I am mainly looking for MOOCs or tutorials that would help me start using RL in Python. Really appreciate any input, thanks!",reinforcementlearning,ubermensch-56,False,/r/reinforcementlearning/comments/euxztc/learning_and_implementing_rl_for_project/
RL environment,1580164147," suppose I want to  build  an agent that can  learn how to do certain task and  there is no environment to train the agent,Please  how is  RL environment  designed. Deepmind, openai   and other big  tech companies do create when they  want to solve some problems.",reinforcementlearning,Osarenomawise,False,/r/reinforcementlearning/comments/euvoxf/rl_environment/
"""why does reinforcement learning not work (for you)?"" - Shie Mannor",1580139616,,reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/eupios/why_does_reinforcement_learning_not_work_for_you/
OpenSpiel,1580125073,"Has anyone used this package and can point me to a tutorial/ more detailed documentation, that is available on GitHub?

I am really interested in using this framework but the lack of documentation makes it really hard.",reinforcementlearning,marboka,False,/r/reinforcementlearning/comments/eumt8m/openspiel/
Looking Backwards,1580094760,"New to RL, so...

&amp;#x200B;

Given the next state using a model of the environment, does it make sense to have the policy act from the future, looking backwards? Would this be easier for the policy net to act from its future states, instead of from the present state? Would it be theoretically much easier to determine cause from effect, and use a prediction of the future states to act?",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/euhwhb/looking_backwards/
Difficulties in Training Swimmer-V2,1580083502,"Hi, I followed the instructions of the parameters setup of TRPO, and both Walker and Hopper are ending up functioning well. However, I am unable to improve the performance of the Swimmer.

I have also tried to run it with a few codes that I checked out from Github. Still, they all failed in Swimmer, but not in the other two.

Can anyone who had succeeded in achieving great improvement in Swimmer give me a piece of advice?

Thanks!

https://preview.redd.it/9ud0m2ldp7d41.png?width=1732&amp;format=png&amp;auto=webp&amp;s=8729ac1fa12e517f35ef1b97fd39273edb2ebc2f",reinforcementlearning,AiLearnerXYF,False,/r/reinforcementlearning/comments/eufjcm/difficulties_in_training_swimmerv2/
Which IDE do you use?,1580083402,"I’m just getting started with RL through OpenAI Gym (which was a nightmare to get it to work with mujoco), but I’ve found that I sometimes encounter IDE specific bugs, eg rendering not working properly in Jupyter, something about the terminal acting up in VSCode etc. And that sometimes it was best to simply run the script from the terminal.

Just wondering if you guys found that one IDE worked better for you than another? Or perhaps it’s simply just because I’ve not setup my libraries correctly. Thanks!",reinforcementlearning,blinkndsmash,False,/r/reinforcementlearning/comments/eufijv/which_ide_do_you_use/
PPO is updating quite weird... anyone firmiliar with this?,1580071769,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/euco9o/ppo_is_updating_quite_weird_anyone_firmiliar_with/
"Advanced readings, courses",1580060194,"Hey,

Im curious if someone can recommend any advanced resources for DRL.   
Besides reading recent papers i cant really find anything that does not mainly touche the basics of DRL.

Im thankful for any suggestion!",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/eu9sq4/advanced_readings_courses/
"""Population-based"" Constant Rebalanced Portfolio Selection",1580000092,"I  was shocked to figure out that any good backtest result, which I have  previously obtained, could simply be converging to a local minima that  performs well just on the test set or in other words, winning the  lottery due to random initialization. In order to prevent this kind of  test-set overfitting;

I have  developed a system that is able to simultaneously train 10K trials,  which are all differently initialized, in GPU and then validated with  the mean portfolio, rather than selecting the best trial. More details  of the methodology and the blind-set results are present in the below  presentation:

[https://www.slideshare.net/KamerAliYuksel/populationbased-portfolio-selection](https://www.slideshare.net/KamerAliYuksel/populationbased-portfolio-selection)

This  is also first method I guess, which optimizes a constant rebalanced  portfolio for a defined risk-adjusted reward and an assumed transaction  cost, using Deep Learning methods (Policy Gradient).",reinforcementlearning,kyuksel,False,/r/reinforcementlearning/comments/etzltn/populationbased_constant_rebalanced_portfolio/
"Population-based"" Constant Rebalanced Portfolio Selection",1579999225,"I  was shocked to figure out that any good backtest result, which I have  previously obtained, could simply be converging to a local minima that  performs well just on the test set or in other words, winning the lottery due to random initialization. In order to prevent this kind of  test-set overfitting.

I have  developed a system that is able to simultaneously train 10K trials,  which are all differently initialized, in GPU and then validated with  the mean portfolio, rather than selecting the best trial. More details  of the methodology and the blind-set results are present in the below  presentation:

[https://www.slideshare.net/KamerAliYuksel/populationbased-portfolio-selection](https://www.slideshare.net/KamerAliYuksel/populationbased-portfolio-selection)

This  is also first method I guess, which optimizes a constant rebalanced  portfolio for a defined risk-adjusted reward and an assumed transaction  cost, using Deep Learning methods (Policy Gradient).",reinforcementlearning,kyuksel,False,/r/reinforcementlearning/comments/etzf6s/populationbased_constant_rebalanced_portfolio/
How to know/check if your environment is pomdp?,1579988731,"Title. I’ve created an environment, however i tried all diffrent kind of parameter/regularization/reward methods, but nothing Worked and usually resolves in either in bad learning or exploding gradient. I am using PPO and I am suspecting that my environment is pompd, its self made. How would one check if their environment is Pomdp? I tried by just looking at it and i can see solutions , but of course i know more about it than the agent... any other tips?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/etx5s6/how_to_knowcheck_if_your_environment_is_pomdp/
Lyceum: An efficient and scalable ecosystem for robot learning,1579975748,,reinforcementlearning,traversaro,False,/r/reinforcementlearning/comments/etu6vl/lyceum_an_efficient_and_scalable_ecosystem_for/
"""AQL: Q-Learning in enormous action spaces via amortized approximate maximization"", Van de Wiele et al 2020 {DM}",1579971912,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ettbg1/aql_qlearning_in_enormous_action_spaces_via/
"""The Incentives that Shape Behaviour"", Carey et al 2020 [causal influence diagrams] {FHI/DM}",1579968877,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/etsnpa/the_incentives_that_shape_behaviour_carey_et_al/
Benchmarks of paper readings for SLAM RL,1579916326,"Hi, 

Does anyone know about what papers to read for studying SLAM based RL?

Thanks!",reinforcementlearning,AiLearnerXYF,False,/r/reinforcementlearning/comments/etk867/benchmarks_of_paper_readings_for_slam_rl/
Reinforcement Learning with Attention Machanism,1579888835,"Hi,

Anyone knows about the key papers to read for studying RL with attention?

Thanks",reinforcementlearning,AiLearnerXYF,False,/r/reinforcementlearning/comments/etdzsx/reinforcement_learning_with_attention_machanism/
Clear illustration of the lack of generalization?,1579870807,"I am gathering resources to explain how hard it is to train an agent that generalizes well in RL. 

I am looking for a specific example I saw a long time ago: how the performance of a given agent changes (collapses!) when some small changes are applied to the environment.

I seem to remember there was an example with Breakout, if you change the paddle size or background color.

Does this ring any bell? I'm unable to find this paper! Or do you know of any similar clear illustration of the problem?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/eta3eh/clear_illustration_of_the_lack_of_generalization/
[D] Looking for continuous control from raw pixel inputs implementations,1579827493,"https://arxiv.org/pdf/1509.02971.pdf

According to the Lillicrap DDPG paper, it is possible to train an agent on simple continuous control tasks such as Reacher, CartPole using only the image pixels as observations. However, I can't get it to train for any of the simple tasks. I copied the architecture / parameters directly from the paper. I looked on Github and am surprised that I can't find a single re-implementation of continuous control on pixel inputs. Anyone have pointers I would be really grateful. Thanks!",reinforcementlearning,tmpberk,False,/r/reinforcementlearning/comments/et2p2b/d_looking_for_continuous_control_from_raw_pixel/
RL in Education,1579821430,"Hi Guys, 
I have recently gotten into RL by mistake (just read the first chapter of sutton and barto for fun and I immediately got hooked on it during a family trip) and I wanted to work on an RL project by myself but I wanted to develop something in the education sector. For context, I am still in University and I am an undergrad but I am pining to create a project so if any of you have any ideas or suggestions regarding the application of RL in Education then please feel free to tell me. 
Additionally, if any of you are actually working on a cool RL project and dont mind collaborating then please let me know as I would love to collaborate and learn even more from any of you lovely RL enthusiasts or experts.",reinforcementlearning,cs_deep_learning,False,/r/reinforcementlearning/comments/et1ciq/rl_in_education/
DDPG actor update function,1579807324,"Hi.  
I'm implementing DDPG for solving some toy-problems.  
One big obstacle that I found is the actor update functions.  
This is the function.  


    def actor_train(self, minibatch):
            s_batch, _, _, _, _= minibatch
            
            with tf.GradientTape() as tape:
                mu = self.actor_network(s_batch)
                q = self.critic_network([s_batch, mu])
                q = functools.reduce(lambda a,b: a+b, q)/self.MINIBATCH_SIZE
                
            grad = tape.gradient(q, self.actor_network.trainable_weights)
            self.actor_optimizer.apply_gradients(zip(-1*grad, self.actor_network.trainable_weights))


Is it correct? I don't know if the problem of the not convergence is caused by an erroneus actor update function or from something else in the code.  
Sampling the functions, the actor sometimes staturates and sometimes get stuck at 0 for every value.  
For completeness here is the critic update function.  


    def critic_train(self, minibatch):
            s_batch, a_batch, r_batch, s_1_batch, t_batch = minibatch
    
            mu_prime = self.actor_target_network(s_1_batch)
            q_prime = self.critic_target_network([s_1_batch, mu_prime])
            
            ys = np.reshape(r_batch, (self.MINIBATCH_SIZE, 1)) + self.GAMMA * (1 - np.reshape(t_batch, (self.MINIBATCH_SIZE, 1))) * q_prime
            
            with tf.GradientTape() as tape:
                predicted_qs = self.critic_network([s_batch, a_batch])
                loss = (predicted_qs - ys)*(predicted_qs - ys) 
                loss = functools.reduce(lambda a,b: a+b, loss)/self.MINIBATCH_SIZE
                dloss = tape.gradient(loss, self.critic_network.trainable_weights)
            
            self.critic_optimizer.apply_gradients(zip(dloss, self.critic_network.trainable_weights))


Does is make sense?  
I followed the openai exaplanation.  


This is the repo if someone is willing to give a look, it would be fantastic.[REPO](https://github.com/bebbo203/DDPG)",reinforcementlearning,bebbo203,False,/r/reinforcementlearning/comments/esxxcs/ddpg_actor_update_function/
Using RL to make pricing decisions,1579803796,"Just wanted to hear your thoughts.

In which context can RL be used to make pricing decisions? (for example, say in an e-commerce platform, do you think we can design an agent that can adjust the pricing of items)

I'm thinking, hypothetically, even if we don't know the global demand, shouldn't a model free method be able to handle the pricing of items in a way that it increases the cumulative profit in the long run? (while supply can be modeled as a state variable?)

What do you all think about it?",reinforcementlearning,PsyRex2011,False,/r/reinforcementlearning/comments/esx3bj/using_rl_to_make_pricing_decisions/
Questions about TRPO and PPO,1579752411,"The introduction of PPO is saying that TRPO is not compatible with the architectures including noise or parameters sharing between policy and value function. 
Can someone explain that why TRPO has such flaws and why PPO can well handel these issues?

Thanks",reinforcementlearning,AiLearnerXYF,False,/r/reinforcementlearning/comments/esnwx4/questions_about_trpo_and_ppo/
torchstruct: use multiple PyTorch Tensors as they would be one,1579708111,,reinforcementlearning,iamhatesz,False,/r/reinforcementlearning/comments/esdtny/torchstruct_use_multiple_pytorch_tensors_as_they/
What is the difference between Bootsrapping and Backup when it comes to Temporal Difference Learning?,1579697713,,reinforcementlearning,duffpaddy,False,/r/reinforcementlearning/comments/esbilg/what_is_the_difference_between_bootsrapping_and/
SOTA on ATARI sample efficientcy?,1579691896,"Couldn't find the answer on papers with code, I'm ideally looking for the SOTA for the minimal number of frames to learn to 80% of human ability, but any sample efficient ATARI results are very very welcome",reinforcementlearning,Billy737MAX,False,/r/reinforcementlearning/comments/esakre/sota_on_atari_sample_efficientcy/
"""DD-PPO: Near-perfect point-goal navigation from 2.5 billion frames of experience"", Wijmans &amp; Kadian 2020 {FB} [PPO scaling w/many-GPU-envs: synchronous model updates, shortcircuit env rollouts]",1579664837,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/es6cze/ddppo_nearperfect_pointgoal_navigation_from_25/
What's the advantage of policy gradient methods over model free q learning methods?,1579653937,I've been following a reinforcement learning course however I struggle to understand how is one method better to the others. I can't see in which scenarios would it be better to use policy gradient methods or model free methods.,reinforcementlearning,BeepaBee,False,/r/reinforcementlearning/comments/es44dm/whats_the_advantage_of_policy_gradient_methods/
"""MCTSPO: Monte-Carlo Tree Search for Policy Optimization"", Ma et al 2019",1579647859,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/es2s6u/mctspo_montecarlo_tree_search_for_policy/
Understanding the Intuition Behind TD(λ),1579644993,"I'd like to better understand Temporal Difference Learning. I'm wondering if it is prudent to think about Temporal-Difference Learning (Lambda) as a type of ""truncated"" Monte Carlo Learning?",reinforcementlearning,nickkunz,False,/r/reinforcementlearning/comments/es22rk/understanding_the_intuition_behind_tdλ/
Help for my frist RL project in graph exploration,1579620104,"Hi everyone,

I'm trying to apply RL to **graph** **exploration**. Let's say I have a grid structure like this

&amp;#x200B;

https://preview.redd.it/72htrrtmc5c41.png?width=939&amp;format=png&amp;auto=webp&amp;s=c64c3b7c1bead360aaf4d3b1d9e485b43d105632

Where green nodes are ***visitable*** nodes while red ones are ***obstacles****.* Edges marked with 1s are then considered valid while the one marked with 0s are not. I want to program an agent that explores the graph in an **efficient** way (i.e. visiting every node with the shortest possible path **or** visiting as more nodes as possible in a time horizon of T hops.

My agents starts from a random (visitable) node and knows nothing about the environment, which he wants to reconstruct. He has at most 4 actions: *North, South, West, East.* Let's say he gets initialized in node (0,0), his knowledge at step 0 is: 

* The current state (already visited)
* The possibility of visiting node (0,1) or (1,0)

I wanted to work with Deep Q-Learning and I thought about using a Graph Neural Network as the  action-value function approximator. My questions are:

1. **Does this make sense** to you guys?
2. How can I handle the **variable-dimension action space**? When the agent is in the (3,6) node, for example, it has only 2 possible actions out of 4 (going *East* or going *South*). Using a Q-table could be a solution but I wanted to experiment with GNNs.
3. How can I handle the **dynamic environment**? At timestep 0 the agent knows a partial graph G\_0, while at timestep *t* he knows another partial graph G\_*t*. 
4. My ultimate goal is to implement a **multi-agent** graph-exploration algorithm: how could I extend this formalization to multiple agents, sharing the same knowledge but taking decentralized and cooperative actions, in order to explore the graph in an efficient manner.

I know there's a lot of stuff going on here, but this is my first project with Reinforcement Learning and I feel quite confused.

Thank you for helping me, have a nice week.",reinforcementlearning,sm_0ne,False,/r/reinforcementlearning/comments/erw14t/help_for_my_frist_rl_project_in_graph_exploration/
"A batched, concurrent OpenAI gym environment.",1579618137,,reinforcementlearning,bardanking,False,/r/reinforcementlearning/comments/ervmfp/a_batched_concurrent_openai_gym_environment/
Direct Access to Actions,1579503324,"Hello,

I'm looking for a deep RL algorithm which takes in states and outputs an action vector which can not be probabilistic. Basically, I want a deep RL algorithm that maps states to actions directly, and not to a probability vector (like a DQN).

I think something like UDRL should theoretically work, but not sure..",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/er9mn1/direct_access_to_actions/
"Traditional reinforcement learning theory claims that expectations of stochastic outcomes are represented as mean values, but new evidence supports artificial intelligence approaches to RL that dopamine neuron populations instead represent the distribution of possible rewards, not just a single mean",1579493388,,reinforcementlearning,Stauce52,False,/r/reinforcementlearning/comments/er7st2/traditional_reinforcement_learning_theory_claims/
Interactive RL Benchmark by CleanRL (Soft Actor-Critic Added),1579475668,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/er441h/interactive_rl_benchmark_by_cleanrl_soft/
RL under high variance returns,1579417785,"This might seem like a basic question, but I'm having trouble coming to grips on how algorithms such as Policy Gradient, and DQN deal with returns that have high variance?

A simple example would be an initial state 0, with two actions, action-1 gives 1 reward landing in state 1, and action-2 gives either 0 reward landing in state 2 or x reward landing in state 3.

The reward x can be set such that the expected return for action-2 is always twice action-1. However, I find that neither PPO, A2C, nor DQN can solve this relatively simple problem if the variance of the outcome of action-2 is set high enough. 

For the record, the stable-baseline implementations break with a positive action-2 reward probability set to:

|Probability|Algorithm|
|:-|:-|
|1/10|PPO|
|1/4|A2C|
|1/3|DQN|

Has anyone come across this before? Is it a limitation with the algorithms? I've seen proofs for policy gradient before, but it seems that clipping the loss function would make this kind of tasks impossible. Most surprising is that DQN didn't work, as I would have thought this would correctly estimate the Q-values for this problem.",reinforcementlearning,VirtualHat,False,/r/reinforcementlearning/comments/eqt6ue/rl_under_high_variance_returns/
[D] Why is the loss function in policy gradient a multiple of its policy history and discounted reward?,1579408530,"While [updating policy gradient](https://medium.com/@ts1829/policy-gradient-reinforcement-learning-in-pytorch-df1383ea0baf#5807) we use a loss function which is a mulitple of its policy history and discounted reward,

image: https://miro.medium.com/max/319/1*VFRng5GHkOzNrx8wG2BlqA.png

-Why is that? 

-Can't we use the same loss function but with discounted reward alone and no policy history?

-Isn't the modified loss function( without policy history) also maximises the expected reward?",reinforcementlearning,begooboi,False,/r/reinforcementlearning/comments/eqrupo/d_why_is_the_loss_function_in_policy_gradient_a/
Terminal state indication and punishment question.,1579385704,"TL;DR | I am trying to figure out whether adding a terminal state indication is efficient if not needed. I also ask the question whether a terminal state punishment should be given.

When using a RL algorithm, would it be useful to indicate when the terminal state will happen? When I talk about a terminal state indication, i mean  an indication in the state that could involve a timer or indication on when the terminal state will happen, this can be in a countdown value in the state that counts down to the terminal state. In environments like Pendelum I can relate to why a terminal reward (and/or indication) woudn’t be to good use, because its not relevant to the problem, holding a pendelum up. 

However in an environment where time is key, for example making a pizza in a defined amount of time, an environment where it’s not all about if it will solve the problem, but rather when it will solve it.

I’ve seen a paper where it discussed the issue about whether to add a terminal state indicator.  This paper tested the scenario where it adds and removes a terminal state indication. This indication was a “counter” that represents a number of the steps left to the terminal state. When it reaches the terminal state it gets a negative reward. The results  where diverse, but the papers’ conclusion was that it could indeed be a benefit. 

Another discussion I had once with a fellow friend of mine was whether the terminal state should even involve a punishment. We tried this on a package delivery environment, an environment where the goal is to deliver a package in a street in a specified amount of steps. We did 2 scenario’s: 1 where the terminal state gave a punishment of -1 and one where the terminal state gave a reward of 0. The results were “more” efficient with the scenario where the agent only received a 0 reward for the terminal state, however both scenario’s were able to solve the environment in the same amount of total tested episodes.

What is your take on this information? I am not sure what to decide, because the results weren’t that reliable, imo. I am sorry that i couldn’t link the original paper, because I am not sure how I got it. Thank you for providing your insights and critisism to this post in advance. Terminal State indication in Steps.

TL;DR | I am trying to figure out whether adding a terminal state indication is efficient if not needed. I also ask the question whether a terminal state punishment should be given.

When using a RL algorithm, would it be useful to indicate when the terminal state will happen? When I talk about a terminal state indication, i mean  an indication in the state that could involve a timer or indication on when the terminal state will happen, this can be in a countdown value in the state that counts down to the terminal state. In environments like Pendelum I can relate to why a terminal reward (and/or indication) woudn’t be to good use, because its not relevant to the problem, holding a pendelum up. 

However in an environment where time is key, for example making a pizza in a defined amount of time, an environment where it’s not all about if it will solve the problem, but rather when it will solve it.

I’ve seen a paper where it discussed the issue about whether to add a terminal state indicator.  This paper tested the scenario where it adds and removes a terminal state indication. This indication was a “counter” that represents a number of the steps left to the terminal state. When it reaches the terminal state it gets a negative reward. The results  where diverse, but the papers’ conclusion was that it could indeed be a benefit. 

Another discussion I had once with a fellow friend of mine was whether the terminal state should even involve a punishment. We tried this on a package delivery environment, an environment where the goal is to deliver a package in a street in a specified amount of steps. We did 2 scenario’s: 1 where the terminal state gave a punishment of -1 and one where the terminal state gave a reward of 0. The results were “more” efficient with the scenario where the agent only received a 0 reward for the terminal state, however both scenario’s were able to solve the environment in the same amount of total tested episodes.

What is your take on this information? I am not sure what to decide, because the results weren’t that reliable, imo. I am sorry that i couldn’t link the original paper, because I am not sure how I got it. Thank you for providing your insights and critisism to this post in advance.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/eqna2k/terminal_state_indication_and_punishment_question/
Terminal State indication in Steps.,1579385587,"TL;DR | I am trying to figure out whether adding a terminal state indication is efficient if not needed. I also ask the question whether a terminal state punishment should be given.

When using a RL algorithm, would it be useful to indicate when the terminal state will happen? When I talk about a terminal state indication, i mean  an indication in the state that could involve a timer or indication on when the terminal state will happen, this can be in a countdown value in the state that counts down to the terminal state. In environments like Pendelum I can relate to why a terminal reward (and/or indication) woudn’t be to good use, because its not relevant to the problem, holding a pendelum up. 

However in an environment where time is key, for example making a pizza in a defined amount of time, an environment where it’s not all about if it will solve the problem, but rather when it will solve it.

I’ve seen a paper where it discussed the issue about whether to add a terminal state indicator.  This paper tested the scenario where it adds and removes a terminal state indication. This indication was a “counter” that represents a number of the steps left to the terminal state. When it reaches the terminal state it gets a negative reward. The results  where diverse, but the papers’ conclusion was that it could indeed be a benefit. 

Another discussion I had once with a fellow friend of mine was whether the terminal state should even involve a punishment. We tried this on a package delivery environment, an environment where the goal is to deliver a package in a street in a specified amount of steps. We did 2 scenario’s: 1 where the terminal state gave a punishment of -1 and one where the terminal state gave a reward of 0. The results were “more” efficient with the scenario where the agent only received a 0 reward for the terminal state, however both scenario’s were able to solve the environment in the same amount of total tested episodes.

What is your take on this information? I am not sure what to decide, because the results weren’t that reliable, imo. I am sorry that i couldn’t link the original paper, because I am not sure how I got it. Thank you for providing your insights and critisism to this post in advance.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/eqn94k/terminal_state_indication_in_steps/
Undergrad frustrated with Industry RL internship search.,1579381340,"I'm a second year undergrad who has been doing Deep RL/Robotics research at my university ever since my first term. Last summer (my first summer as a full-time undergrad) I stayed at school and worked in my research lab full-time. Although this experience was great for my learning in robotics, control, and Deep RL, it was not as intense or well-regarded as a general software engineering internship most computer science students would do. Like most research positions at universities, it also paid very little. 

For the upcoming summer I've been aggressively looking for internships in industry that focus on RL or robotics, but I've had very little success. For starters there aren't that many RL internships to apply to, and of those that are there only one actually got back to me: [NVIDIA's Applied Research Intern - Deep Learning Autonomous Vehicles](https://nvidia.wd5.myworkdayjobs.com/en-US/UniversityJobs/job/Applied-Research-Intern---Deep-Learning-Autonomous-Vehicles_JR1924667). I thought I did great in the interviews for this position, I was eventually declined, perhaps because the position wasn't really intended for undergraduates in the first place.

Now, quite late in recruiting season I feel like I have no choice but to accept a general software engineering internship less related to my core interests, or continue to delve deep into those interests with another summer at my university. I might be way too overconfident in my abilities or too idealistic, but I feel like both of these options mark my abilities to be on par with the sea of CS students that can answer some data structures + algorithms questions or who just want to start exploring research like I did last year.

I want to keep looking for my dream internship, but I'm afraid that it's too late in the recruiting cycle. Should I keep looking, or take the offers that I have right now?",reinforcementlearning,GirlImJustABird,False,/r/reinforcementlearning/comments/eqmalb/undergrad_frustrated_with_industry_rl_internship/
"""Global optimization of quantum dynamics with AlphaZero deep exploration"", Dalgaard et al 2020",1579368217,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/eqj9o2/global_optimization_of_quantum_dynamics_with/
Collecting multiple trajectories in REINFORCE doesn't improve performance over updating after every step,1579288039,"Intuitively, collecting multiple trajectories should result in more robust policies that are able to learn. After all, the update rule is over the expected logprob times the reward. However, collecting a single trajectory and updating consistently outperforms collecting multiple. Any ideas as to why that might be?",reinforcementlearning,da_qstar,False,/r/reinforcementlearning/comments/eq4td4/collecting_multiple_trajectories_in_reinforce/
Can imitation learning/inverse reinforcement learning be used to generate a distribution of trajectories?,1579274986,"I know that it's common in imitation learning for the policy to try to emulate one expert trajectory. However is it possible to get a stochastic policy that emulates a distribution of trajectories?

For example with GAIL, can you use a distribution of trajectories rather than one expert trajectory?",reinforcementlearning,mellow54,False,/r/reinforcementlearning/comments/eq1tlj/can_imitation_learninginverse_reinforcement/
What is the actual state of the art?,1579246835,"There are obviously tons of new algorithms and algorithm variants that come out all the time. But what are the actual state of the art algorithms? For example, there was a lot of hype about OpenAI's RND but they didn't even use it for their dota bots. Why is that? There are seemingly lots of improved versions of basic algorithms like GAIL and ACKTR and whatnot, but at the end of the day it seems Google trained AlphaStar with a slightly modified A2C and OpenAI trained the Dota bots with basic PPO. Is there nothing better than these two algos?

I'm also aware of D4PG, Rainbow DQN, etc",reinforcementlearning,BrahmaTheCreator,False,/r/reinforcementlearning/comments/epxaq5/what_is_the_actual_state_of_the_art/
"""Smooth markets: A basic mechanism for organizing gradient-based learners"", Balduzzi et al 2020 {DM}",1579191000,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/eplcw8/smooth_markets_a_basic_mechanism_for_organizing/
RAM shortage,1579185852,"Dear great community,

I am running the deepmind atari game approach in a server with 126GB of RAM and it seems to be crashing because of RAM shortage (it runs for 4-5 hours and then it crashes). The variable that occupais more space is the memory buffer which stores 1M samples of the tuples (s, a,r,s'). Each state (s) variable is made of 4 greyscale  frames of 84x84. I was wondering if anyone came across the same issue and,  if so, how he/she managed to solve it.

Thanks in advance!",reinforcementlearning,kashemirus,False,/r/reinforcementlearning/comments/epk814/ram_shortage/
"[Q] Noisy-TV, Random Distillation Network and Random Features",1579172773,"Hello,

I'm reading both the Large-Scale Study of Curiosity-Driven Learning (LSSCDL) and Random Distillation Network (RDN) papers by Burda et. al (2018). I have two questions regarding these papers:

1. I have a hard time distinguishing between the RDN and the RF setting of the LSSCDL. They seem to be identical, but they never explicitly refer to it in the RND paper (which came slightly afterwards, if I get it correctly). It seems to be simply a paper to dig into the best-working idea of the Study, but then another question pops up:
2. In the RDN blog post (and only a bit in the paper), they claim to solve the noisy-TV problem, (if I got it correctly) saying that, eventually, the prediction network will ""understand"" the inner workings of the target (e.g. fit the weights). They show this on the room change on Montezuma. However, in the LSSCDL, they show in section 5 that the noisy-TV completely kills the performance of all their agents, including RF. 

What is right then? Is RDN any different to the RF from the study paper? If not, what's going on?

Thanks for any help.",reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/ephwua/q_noisytv_random_distillation_network_and_random/
"""How people decide what they want to know"", Sharot &amp; Sunstein 2020",1579137472,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/epbvtq/how_people_decide_what_they_want_to_know_sharot/
"""A distributional code for value in dopamine-based reinforcement learning"", Dabney et al 2020 {DM}",1579120763,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ep820l/a_distributional_code_for_value_in_dopaminebased/
Need a reliable repository for GAIL/PPO in continuous action space,1578976411,"I am trying to implement GAIL using demonstration data on the Hopper simulation of Pybullet. However, most of the GAIL codes are depending on PPO and they are all in discrete action space. I am unable to find a reliable source code for PPO/GAIL in continuous action space. Also, the only working RL code I have is DDPG for which I didn't find any specific GAIL implementation.

1. Does GAIL require on policy algorithm or can it work on off policy RL also?
2. What is the simplest and most robust RL algo I can implement alongside GAIL to finish my task?
3. Can anyone point me towards a usable repository of continuous action space RL algorithms so I can fish out PPO or GAIL from there?

Help is really appreciated thanks!",reinforcementlearning,ramak27,False,/r/reinforcementlearning/comments/eogbn8/need_a_reliable_repository_for_gailppo_in/
"""Challenges of real-world reinforcement learning"", Dulac-Arnold et al 2019 discussion",1578938965,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/eo7tcn/challenges_of_realworld_reinforcement_learning/
I feel for them....,1578923121,,reinforcementlearning,crazy_lazy_life,False,/r/reinforcementlearning/comments/eo4ak2/i_feel_for_them/
multi-task training problems,1578882024,"Hi all, I'm doing an RL project for chemistry, where the agent (represented by a graph neural network) is tasked with rotating a molecule into certain orientations via an interface and an energy oracle for the reward.

It works fairly well on the single-molecule case, where it learns the best series of orientations for a given random molecule. But the algorithm is not useful unless it's transferrable to other molecules. Testing on similar molecules shows that the performance is better than random due to some degree of generalization probably. 

The next step would be to use it in the ""multi-task"" case, where we randomly select from a set of molecules at train time and task the agent with solving the best sequence of orientations. So I whipped up an implementation of this.

In the multi-task case, the agent I am training either performs no better in a transfer setting for a very small train set, or seemingly does not show improvements in train performance (no better than random) at all for larger train sets.

At first I thought it was reward normalization problems, but after rescaling energies to the 0 to 1 range, batched A2C is making little progress.

Two questions:

1) Are there general guidelines or state of the art writeups for the multi-task RL training setting?

2) Is PPO capable of learning problems that A2C is incapable of? Or is it simply more stable and more sample efficient?",reinforcementlearning,BrahmaTheCreator,False,/r/reinforcementlearning/comments/enxn9x/multitask_training_problems/
Hybrid reinforcement learning,1578837757,"Hi everyone, 

I work on NP-hard problems and multimodal optimization, recently I have been trying to hybrid some meta-heuristics with reinforcement -learning but I can't find any examples of code or application of  machine-learning with meta-heuristics to test my approach, most of the resources are theoretical articles with pseudo-codes without much details and no code publicly available.  I would really appreciate if someone could give me some advice or examples about Hybrid meta-heuristics and reinforcement learning (machine learning) or just share this post 

Thank you for taking the time to read this message,  any advice you offer would be greatly appreciated.

Best regards",reinforcementlearning,z_beldi,False,/r/reinforcementlearning/comments/ennsav/hybrid_reinforcement_learning/
Boltzman sampling and Entropy Loss?,1578756329,"When using boltzman sampling for action choosing, But also using a soft actor critic method with Entropy Loss, doesn’t that create an issue? Entropy Loss Or TAU is a parameter which is usually &gt;0.01 , however putting this on eg 0.00001 would result in the output of a network to be close to one Another, however with boltzman sampling this is not a Good idea, Because of the Way it works, it would result into the agent straight up taking random actions only with a softmax output of eg [0.46,0.54] . I think I am wrong about this somewhere, Because I see Some implementations use boltzman sampling And a super low Entropy Loss, however could Someone explain Why And what this is?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/en8oyd/boltzman_sampling_and_entropy_loss/
What is a 'state' in a k-Armed Bandit problem?,1578742252,"Is it the reward distribution of the bandit problem?

Actually I am trying to understand the Contextual Bandits. Since the Contextual Bandit problem is a collection of random bandit problems having a unique feature value to identify each, I understood the \`state' at each time-step as the reward distribution of each of the randomly picked bandit problem at that time step. So unlike the k-armed bandit problem, the state of the Contextual bandit problem is always changing as the underlying reward distribution keeps changing.

Really appreciate for any input and please correct me if my understanding is wrong.",reinforcementlearning,PsyRex2011,False,/r/reinforcementlearning/comments/en6dqe/what_is_a_state_in_a_karmed_bandit_problem/
Adding noise to the reward to encourage exploration?,1578689260,"I'm using PPO to learn a game where the magnitude of the reward is proportional to the initial value of a parameter p_0 that I get to choose. In essence if p_0 is 10,000 then everything works just fine, but if I lower p_0 to 100 then the magnitude of the reward is too small and the agent basically learns to do nothing. I was thinking of adding Gaussian noise to the reward to help combat this. Any opinions or other suggestions on this?",reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/emwxfa/adding_noise_to_the_reward_to_encourage/
Upside-Down-Reinforcement-Learning Pytorch implementation,1578668460,"Hey there i did a implementation of the ⅂ꓤ Algorithm published by Jürgen Schmidhuber. 

Feel free to check it out! :)

[Upside-Down-RL](https://github.com/BY571/Upside-Down-Reinforcement-Learning) 

Since this implementation is only for discrete action space, im currently working on a continuous action space implementation. But i have some trouble to understand the changes. I surely understand that the behavior function for the continuous version outputs the mean and the std for a gaussian distribution where the actions get sampled from. 

But for the optimization, how do I calculate the loss? I couldnt find anything helpful yet and I hope you guys can help me.  I dont know if calculating the MSE loss between the target actions from the replay buffer and the means as the output from the behavior functions is appropriate.

Any Idea is welcome, thanks! :)",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/ems2r0/upsidedownreinforcementlearning_pytorch/
Looking for help/advices on my first RL project (2D space exploration),1578665770,"Hi everyone,

I'm a RL newbie and I've been given this problem to solve:

A number $N$ of cooperative and information-sharing agents must explore and create a map of a 2D unknown environment using the shortest possible routes. The environment is a $M \\times M$ matrix containing obstacles (walls, objects,...) and the agents must explore the environment starting from an initial known spot, discovering stuff and updating their shared representation (map) of the environment. This should be done in a smart way so that they visit the least amount of cells.

As the title says I'm looking for advices on starting out: I'm trying to understand how to formalize this problem and which strategy to apply in order to solve it. I was thinking about a deep q-learning approach with a CNN as the value function approximator (since the action space has a small cardinality, say 4, and I can decide the environment size, say 20x20).

Does all this does make sense to you? Thank you for you help and time, have a nice day",reinforcementlearning,w00zie23,False,/r/reinforcementlearning/comments/emrhbv/looking_for_helpadvices_on_my_first_rl_project_2d/
Getting Started with RL,1578663284,"What's the quickest way to get started with reinforcement learning and start working on cool projects ? 

I've heard that David Silver's course is one of the best out there. Wanted to know if there are any other good resources which are geared towards the practical side of things. I have roughly a 2 month timeframe to work with.",reinforcementlearning,neelay_shah,False,/r/reinforcementlearning/comments/emqzby/getting_started_with_rl/
RL Beginner need help with my first Project (Hearthstone Battlegrounds),1578648383,"I'm really into RL lately so I thought the best way for me to learn to teach a computer to play Hearthstone Battlegrounds (The new Auto-Chess like mode, not the classic HS).

So for a month, I was working on developing a simulation of the game, aka BGSimulator.

The concept I have in mind is having 8 AI players each with its own implementation of an RL algorithm fight each other and get better and better in time.

The simulation phase of this project is done and is available on GitHub: 

 [https://github.com/yossielimelech/BGSimulator](https://github.com/yossielimelech/BGSimulator)

The next phase is implementing the AI.

I started learning from lectures on Youtube and saw that there are many algorithms I could use many variations of each algorithm. I got lost and I need your help.

So, what's this game is about, what are the rules/rewards and environment the agent will have to deal with?

Background:

Battlegrounds is an imperfect information turn-based, board game consists of 8 players fighting each other, there are two phases the recruiting phase and the battle phase.

Recruiting Phase

The player can only take action in the recruiting phase is the time where you use your income (gold) to buy minions and upgrade your board.

Battle Phase

Players are matched and a copy of their current board is auto-fighting until either player loses all minions or a tie.

The actions that the agent can take:

Buy - buy a minion from the shop for 3 gold.

Sell - sell a minion that is on the board and gain 1 gold.

Play - Play a minion on the board -&gt; 

Target (sub-action of play) target a minion you want to buff.

Freeze - freeze the shop offer for 1 turn at no cost.

Roll - refreshes the shop offer with a new offer, costs 1 gold.

Level-up - spend gold level up the shop to get better tier minions (the cost varies).

Sort - sort the order of minions on the board. (sort board of 7, which might add billion of states as I perceive it).

Hero power - uses the hero power (I omitted heroes, for now, to keep it simple and focus on the cards).

&amp;#x200B;

Side actions from the game mechanics and card effects

Choose Discover - Choose a minion out of 3.

Choose Adapt - Select a buff out of 3.

Make a tripple - When you get 3 copies of the same minion in your board/hand it gets converted to one golden version of that minion that whenever played will give you a reward, the ""minion from next tier card"". (This one is tricky cause it might actually make the player weaker but you get a reward card for it).

Currently, the simulation has a very dumb player class which needs a brain implant :)

The information the agent can see:

1. Whos it's next opponent, how much health he got and if he is dead or alive.
2. The board state at the start of the fight.
3. The board state of the opponent the last time you fought each-other.
4. The level of other players.
5. How many players are alive.
6. The damaging result of each battle last turn.

Rewards:

1. Win the game. The ultimate goal.
2. Winning a battle. -&gt; Damage done to opponent can also be a factor.
3. Take no damage (win/tie).
4. Making golden minions (might count as a reward but not sure).

So now we know the game rules, the actions that the agent can take and the set of rewards it is expected to get. Now I need to translate all of that to an interface that will communicate it all to the AI. Because I am very new to this I'm not even sure where to begin.

I have so many questions, please bear with me.

Which algorithm would fit this project the most?

Is this game too complicated in terms of the number of states and information? Is it too big that I need GPU programming?

In the simulation I have 8 players running, should all of them use the same algorithm? can it even learn by playing itself?

If the algorithm had no gold does it ever know about it? How do I commune to it that it can't do certain actions?

Can I use openAI with my project (C#) or any other libraries to ease my coding?

Any good very similar resources to learn from?",reinforcementlearning,Diabler,False,/r/reinforcementlearning/comments/emomay/rl_beginner_need_help_with_my_first_project/
"""Google Research: Looking Back at 2019, and Forward to 2020 and Beyond"", Jeff Dean",1578616464,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/emiwds/google_research_looking_back_at_2019_and_forward/
"""The Gambler's Problem and Beyond"", Wang et al 2019 [Sutton &amp; Barto's double-or-nothing example is ""fractal, self-similar, derivative 0/∞, not smooth on any interval, not written as elementary functions...one of the generalized Cantor functions""]",1578593201,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/emdf4a/the_gamblers_problem_and_beyond_wang_et_al_2019/
Reinforcement Learning book for beginners?,1578586613,"Hi there, I've been working with neural networks for a while and I'm currently moving into using reinforcement learning for control. I have to use Matlab for this since my model is in Simulink. 
I was wondering if anyone could recommend some interesting and maybe some basic books (for dummies) to learn more about reinforcement learning and the different algorithms there are; and if you have any advice on using the RL toolbox in Matlab/Simulink, it would be appreciated :)

TIA!",reinforcementlearning,langan-7,False,/r/reinforcementlearning/comments/embtnp/reinforcement_learning_book_for_beginners/
Good PPO Keras implementations?,1578585710,"Hi everyone,

&amp;#x200B;

I am looking for good PPO implementations in Keras, which uses Gym. I tried some implementations like the LuEE-C but i wanna try diffrent implementations. Preferred is a one-file algorithm, meaning you have only 1 file/script that you run and that you don't need an environment/run/algo script list. I am sure there are some good algorithmes deep down in for example articles that you may know? :)

&amp;#x200B;

Thanks for reffering in advance",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/embluf/good_ppo_keras_implementations/
Dueling DQN in Keras?,1578585019,"Do you guys know any implementation of Dueling DQN in Keras? I’ve found TensorFlow and pytorch ones, but no luck with Keras.",reinforcementlearning,mortadelass,False,/r/reinforcementlearning/comments/embfz7/dueling_dqn_in_keras/
New to RL and looking for help to solve Mountain Car,1578583091,"Hi everyone,

&amp;#x200B;

This is my first question to this sub-reddit. I am new to RL, and I am trying to play with GYM ([https://gym.openai.com/](https://gym.openai.com/)) to try to apply what I read in the book ""Reinforcement Learning: an Introduction"" ([https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation-ebook/dp/B008H5Q8VA](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation-ebook/dp/B008H5Q8VA)).

I managed to apply the knowledge of this book to the simple example of Cartpole-v0, but I am failing at making any of my implementations of Q-Learning that work for Cartpole work for MountainCar ([https://gym.openai.com/envs/MountainCar-v0/](https://gym.openai.com/envs/MountainCar-v0/)).

I tried different variants, such as with or without replay buffer, with prioritized replay buffer, with double learning, etc. All of my attempts are failing miserably for now: my neural net seems to hit some solutions at some points, but then the losses of my neural net explode, and the neural net seems to ""forget"" everything that it has learned.

I tried other RL approaches as well (SARSA, REINFORCE, REINFORCE with baseline) which should avoid the deadly triad mentioned in the book ""Reinforcement Learning: an introduction"" (approximation, off-policy and bootstrapping) either because they are on-policy (SARSA) or do not bootstrap (REINFORCE). 

I could not make them work on MountainCar either.

My guess is that the reward mechanism of MountainCar, combined with the fact that it is unlikely to find a solution ""by chance"" makes it really hard for a too naive agent to train on this problem.

So basically, I am kind of lost and I am looking for some help to move forward:

* Is there something (some techniques / tutorial) you would recommend me to move forward?
* Would you have any example of working code for learning on Mountain Car?

&amp;#x200B;

**NB**: Before coming to this sub-reddit, I looked for examples of Jupyter notebooks demonstrating agent that successfully manage to train an agent on MountainCar.

Here is one such notebook: [https://github.com/ts1829/RL\_Agent\_Notebooks/blob/master/MountainCarv0/Mountain%20Car%20v0%20-%20Q%20Learning.ipynb](https://github.com/ts1829/RL_Agent_Notebooks/blob/master/MountainCarv0/Mountain%20Car%20v0%20-%20Q%20Learning.ipynb)

I copied the code and it works, but I am unsure how it actually manages to work.

For instance, the Pytorch neural net it features sequences 2 linear layers without activation functions in between. This does not seem correct to me (the composition of two linear functions is just another linear function), but if I add a torch.nn.ReLU() in between, or if I fuse the two linear layer into one single layer, it does not work anymore.

I think I am missing something rather big unfortunately.",reinforcementlearning,quduval,False,/r/reinforcementlearning/comments/emb02o/new_to_rl_and_looking_for_help_to_solve_mountain/
RELATIONSHIP Action vs Future reward,1578537134,"How is the relationship between actions vs future rewards calculated in optimizing Q tables or fitting DQN?

As far as I know Q tables optimize its value by the actual reward and the chosen action... ( does it exists here?

In DQN how is that also possible if in all codes I’ve seen the neural networks weights are optimized according to random replay batches of the states the environment has interacted? What if my last episode reward is the most important one?",reinforcementlearning,d2hf,False,/r/reinforcementlearning/comments/em37kg/relationship_action_vs_future_reward/
SLM Lab: New RL Research Benchmark &amp; Software Framework,1578500892,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/eluz8g/slm_lab_new_rl_research_benchmark_software/
[D] Algorithm suggestions for solving option hedging problem,1578465140,"Hi guys,
I've been trying to implement [this](https://www.researchgate.net/publication/329435926_Dynamic_Replication_and_Hedging_A_Reinforcement_Learning_Approach) for a while but with no success so far. The problem seems pretty straight forward. The state is three dimensional: underlying price, time to maturity, and current stock owned. Action space is continous between 0 and 1. I have tried both A2C and DDPG so far but either have learned nothing. I was wondering if there is something I need to take into account when the envinronment has a significant randonmness in it? Do some algorithms cope better or worse with randonmness?",reinforcementlearning,LetSimTsu,False,/r/reinforcementlearning/comments/eloznx/d_algorithm_suggestions_for_solving_option/
"Sutton &amp; Barto Book, Chapter 6, Example 6.1 Driving Home + slight variant of the problem?",1578376594,"In Sutton &amp; Barto Book, Chapter 6, **Example 6.1 Driving Home**, we see that the author takes the elapsed time during the journey as the reward(for prediction task).  the value of each state is expected time to go. Can anyone explain what would be the action set here? 

Now suppose if we change the problem a bit. we fixed the time at which we have to be in that state. 

Leaving office - 6:00 PM

Reach Car - 6:05 PM

exiting highway - 6:25 PM

2nd road: 6:35 PM

entering home street: 6:47 PM

arrive home: 6:50 PM

this is the schedule one strictly wants to follow, but because of some circumstances, there will be delays. 

I need to compute the average delays in each of the states, We have a sufficient amount of historical data. Can anyone explain how to formulate this problem as an RL instance(if possible)?.

I'm trying to solve this problem using machine learning and results are positive and just wanted to know how to solve this problem using RL.",reinforcementlearning,dangling_pntr,False,/r/reinforcementlearning/comments/el7b27/sutton_barto_book_chapter_6_example_61_driving/
Does anyone have thoughts on autoregressive rl policies?,1578361002,"I read this [paper](https://arxiv.org/abs/1903.11524) on replacing the normal distribution in a stochastic rl policy with an AR(p) process. It seems like this could be useful in scenarios where you don’t won’t your samples actions to jump around a lot. But I’m not sure their experiments really illustrated this as well as they could have. 

Has anyone read about this kind of work before? Does the paper seem promising?",reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/el42p8/does_anyone_have_thoughts_on_autoregressive_rl/
How to set the number of process in A2C using OpenAI baseline ?,1578314573,"    import gym
    from baselines import bench, logger
    from baselines.bench.monitor import Monitor
    from baselines.common.atari_wrappers import make_atari
    from baselines.a2c import a2c
    
    def main():
        logger.configure('./logs/A2C')
        env = make_atari('PongNoFrameskip-v4')
        env = bench.Monitor(env, logger.get_dir())
        
        model = a2c.learn(
            ""cnn_lstm"",
            env,
            seed=1,
            nsteps=5,
            total_timesteps=int(1e7),
            vf_coef=0.5,
            ent_coef=0.01,
            max_grad_norm=0.5,
            lr=7e-4,
            epsilon=1e-5,
            alpha=0.99,
            gamma=0.99,
            log_interval=100,
        )
    
    if __name__ == '__main__':
        main()
    

I read baselines.a2c, but I don't see any  parameter to set the number of process.",reinforcementlearning,m1croyu,False,/r/reinforcementlearning/comments/ekto1o/how_to_set_the_number_of_process_in_a2c_using/
multi-agent selfplay training with ppo ( Pong-Atari2600),1578299618,"when i train pong-atari2600 in Multiplayer Environments mode using selfplay with ppo,i can not get a good policy.i consturct two models for each player.Each player train with its model to compete with others.i also made 10% of environments train with opponents whose models params were sampled from history models params. After this efforts,i eval the trained model with built-in ai，it is hard to defeat the built in-ai. How to refine it ?

i make the environment like this: env = retro.make(game='Pong-Atari2600', players=2)",reinforcementlearning,liuchaoyong,False,/r/reinforcementlearning/comments/ekrap6/multiagent_selfplay_training_with_ppo/
Joomla Website Backup - Javatpoint,1578118419,,reinforcementlearning,nehapandey01,False,/r/reinforcementlearning/comments/ejsu8i/joomla_website_backup_javatpoint/
A Survey for RosettaStone 2020 Roadmap,1578117370,"Hello, everyone! I'm Chris Ohk, the creator of [RosettaStone project](https://github.com/utilForever/RosettaStone): Hearthstone simulator using C++ with some reinforcement learning. Finally, we finished implementing all the original cards last year. Thank you for your interest and understanding.

We have created a new roadmap for 2020 and the list is as follows:

* Implement all standard cards
   * Rise of Shadows
   * Saviors of Uldum
   * Descent of Dragons
* Implement programs for playing game
   * Console-based
   * GUI-based
   * Web-based
* Prepare ""Hearthstone pro gamer"" vs AI match-up
* Implement ""Hearthstone Battlegrounds""
* Support various deep-learning framework for RL environment
   * Tensorflow
   * PyTorch
* Fully support Python API
* Write a paper on Hearthstone's RL environment
* Make architecture documents for contributors
* Make tutorials for programs

If you have any comments on the road map that you would like to add or modify, please feel free to leave them for us. Thank you.",reinforcementlearning,utilForever,False,/r/reinforcementlearning/comments/ejsnms/a_survey_for_rosettastone_2020_roadmap/
Beginner questions for concrete Q-Learning attempt with tensorflow 2.0,1578081837,"Hello .. I created a basic game simulation for the Starcraft2 custom map Direct Strike created by Tya. I already implemented a [brute force attempt](http://www.youtube.com/watch?v=M6noTYbdSp4&amp;t=10m0s) to find the best build/position.

At the moment I am experimenting with a [RL approach](https://github.com/ipax77/paxai/blob/master/paxgame_q.py) (tensorflow 2.0) with [unfortunate success](https://github.com/ipax77/paxai/blob/master/result_10k.png) ..

I am not that familiar with the math behind the RL and it seems my try and error attempts reached its limit in that case.

I tried to adapt the Q-Learning [Tic-Tac-Toe code](https://planspace.org/20191103-q_learning_tic_tac_toe_briefly/) from Aron Schumacher to my game implementation but after 10k games and 11h training my AI has no chance vs a random Player :(

&amp;#x200B;

I have a few concrete questions where I am uncertain, any tips or suggestions are very much appreciated.

&amp;#x200B;

My board size is (40, 120) with each of the two players does have a building area of (20, 60)

At the moment there are 3 different unit types available which I try to map in a 3rd dimension - so my board size is (the 4th is for unit upgrades):

    def new_board(size):
        return np.zeros(shape=(4, 40, 120))

Is this valid or is there a better way to do this? E.g. set the value to a combination of unit.id and player.id (atm the value is the player.id (+1 or -1)).

&amp;#x200B;

Other than in Tic-Tac-Toe both players can build units until they spent a defined number of minerals and then the units fight each other what produces the reward (1, 0, -1).

Does it help if I refine the reward (based on damage done / mineral value killed)?

&amp;#x200B;

The board shape is (4, 40, 120) but my Agent (which is Player 1) is only allowed to build/move in (4, 20, 60).

Is it valid to reshape the board to (4, 20, 60) for the model, predict\_q and move function?

    class Agent(Player):
        def __init__(self, size, seed):
            self.size = size
            self.training = True
            self.model = tf.keras.Sequential()
            self.model.add(tf.keras.layers.Dense(
            4*20*60,
            kernel_initializer=tf.keras.initializers.glorot_uniform(seed=seed)))
            self.model.compile(optimizer='sgd', loss='mean_squared_error')
        def predict_q(self, board):
            return self.model.predict(
                   np.array([board.ravel()])).reshape(4, 20, 60)
        def fit_q(self, board, q_values):
            self.model.fit(
                   np.array([board.ravel()]), np.array([q_values.ravel()]), verbose=0)
        def new_game(self):
            self.last_move = None
            self.board_history = []
            self.q_history = []
            self.minerals = MAXMINS
        def move(self, board):
            # always ask the agent to play the same side
            q_values = self.predict_q(board)
            temp_q = q_values.copy()
            move = np.unravel_index(np.argmax(temp_q), (4, 20, 60))
            value = temp_q.max()
            return move, value, q_values
        def train(self, move, board, value, q_values):
            if self.training and self.last_move is not None:
            self.reward(value)
            self.board_history.append(board.copy())
            self.q_history.append(q_values)
            self.last_move = move
        def reward(self, reward_value):
            if not self.training:
            return
               new_q = self.q_history[-1].copy()
            #new_q[self.last_move] = reward_value
               new_q[self.last_move[0]][self.last_move[1]][self.last_move[2]] = reward_value
            self.fit_q(self.board_history[-1], new_q)",reinforcementlearning,pax_77,False,/r/reinforcementlearning/comments/ejkus6/beginner_questions_for_concrete_qlearning_attempt/
Ideas needed on simulating/training Burglar and Guard Agents,1578058110,"Hi,

for a research project, I want to create a simulation on how burglars and guards behave in a grid world. The idea is to have two different types of agents where the burglar agent needs to get one of the treasures (somewhere in the grid world) and then has to reach the edge of said grid world. The guard needs to both protect these treasures and catch the burglar. 

We want to use the simulation to learn three things:  
1. What strategy does the burglar use?  
2. What strategy is most optimal for the guards?  
3. What types of information do benefit the guards the most from? (e.g. treasure locations, big detection range, imprecise burglar location)

I would love to add terrain types and other things to improve the realism of the simulation, but the problem is probably hard enough as is. Do any of you guys have ideas on how to accomplish this? 

Thanks in advance",reinforcementlearning,6six9stones,False,/r/reinforcementlearning/comments/ejfef8/ideas_needed_on_simulatingtraining_burglar_and/
Causal Discovery with Reinforcement Learning,1578055982,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/ejf101/causal_discovery_with_reinforcement_learning/
Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models,1578015309," [https://arxiv.org/pdf/1805.12114.pdf](https://arxiv.org/pdf/1805.12114.pdf) 

This paper is from Berkeley and they claimed a SOTA model based RL algorithm on par with SAC/TD3.

Model-Based RL usually failed to solve general problems but somehow this paper says otherwise and they gave some concrete examples.  I do have doubts on if it's a general case or it's just performs great in those examples showed.

Share your insights if you happened to read this paper.",reinforcementlearning,Nicolas_Wang,False,/r/reinforcementlearning/comments/ej809c/deep_reinforcement_learning_in_a_handful_of/
How to vectorize environments?,1577997449,"I have a Python class that conforms to OpenAI's environment API, but it's written in non-vectorized form i.e. it receives one input action per step and returns one reward per step. How do I vectorize the environment? I haven't been able to find any clear explanation on GitHub.",reinforcementlearning,RSchaeffer,False,/r/reinforcementlearning/comments/ej3v8t/how_to_vectorize_environments/
DQN/Reinforcement Learning Question,1577985493,"I have created an environment that persists for a very many bot ""actions"" 100,000s. Is it ok to to still use the formula for reward = reward + gamma(max(model.predict(next\_state))). I'm worried that because there will be no hardcoded value for reward ever that it will be harder for the model to accurately predict",reinforcementlearning,DistractedEmployee,False,/r/reinforcementlearning/comments/ej11g2/dqnreinforcement_learning_question/
"Why is DESPOT (POMDP online solver) said ""Determinized""?",1577978315,"I have read **""*****Determinized*** **Sparse Partially Observable Tree"" (****DESPOT****)** by [(Ye, Somani, Hsu &amp; Lee. 2017)](https://arxiv.org/abs/1609.03250). 

I do not undertand why the tree is said **""*****Determinized"".***

Also in [DESPOT-alpha](http://www.roboticsproceedings.org/rss15/p06.pdf): ""Exploration consists of multiple trials to build a **determinized** sparse belief tree incrementally"".

From wiktionary:

* determinize (computer science): To remove non-determinism in an automaton; to convert a non-deterministic automaton to a deterministic one.

What **""non-determinism""** is incrementally removed from the tree? And how?",reinforcementlearning,chauvinSimon,False,/r/reinforcementlearning/comments/eizgyx/why_is_despot_pomdp_online_solver_said/
Beginner resources,1577929557,"I am a beginner in the field of RL (with a good understanding of old-school statistics, linear algebra etc.). I am comfortable coding in C++ but I am not knowledgable about modern tools in python for ML/RL etc. 

I am looking for coding-rich tutorials that will lead me through implementing standard RL algorithms for standard tasks, preferably with neural-networks representing the value functions etc. Do you have suggestions?",reinforcementlearning,ssriramReddit,False,/r/reinforcementlearning/comments/eirc8c/beginner_resources/
Does a good Q network need to be bigger than a good π (policy) network?,1577900043,"This is a bit of a low effort question, but maybe someone has some good insights to offer or can at least confirm my intuition.

Are well behaving Q approximators (neural networks) usually bigger than well behaving π (policy) approximators?

It seems this would be the case, because the policy answers the question ""what should I do?"", while the Q function can answer ""what should I do, and how much reward should I expect?"" which is a more difficult question.",reinforcementlearning,Buttons840,False,/r/reinforcementlearning/comments/eikyh8/does_a_good_q_network_need_to_be_bigger_than_a/
[P] Reinforcement Learning: Monte-Carlo and Temporal-Difference Learning,1577868359, A step-by-step approach to understanding Q-learning  [https://medium.com/ai%C2%B3-theory-practice-business/reinforcement-learning-part-5-monte-carlo-and-temporal-difference-learning-889053aba07d](https://medium.com/ai%C2%B3-theory-practice-business/reinforcement-learning-part-5-monte-carlo-and-temporal-difference-learning-889053aba07d),reinforcementlearning,cdossman,False,/r/reinforcementlearning/comments/eig9ai/p_reinforcement_learning_montecarlo_and/
Is there any work that establishes the convergence *rate* of policy *evaluation*?,1577861493,"Please note that the question is about the convergence rate, not just convergence, and about policy evaluation (i.e., given a fixed policy), not about policy iteration.",reinforcementlearning,conan279,False,/r/reinforcementlearning/comments/eifbw9/is_there_any_work_that_establishes_the/
Early stopping in RL,1577848647,"Is it normal to have an early stopping in training an agent? Say, at the end of the training, the final model will be the model with the highest accuracy during training? I saw this in the implementation of DQN by OpenAI, but I just want to ask if it is the normal case. Also, it seems to me that the agent is not often tested on the test set like in the case of supervised learning, is it correct?",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/eid811/early_stopping_in_rl/
Using RMSProp over ADAM,1577830502,"In the deep learning community I have seen ADAM being used as a default over RMS Prop, and I  understand the improvements in ADAM (momentum and bias correction), when compared to RMS Prop. But I cant ignore the fact that most of the RL papers seems to use RMSProp (like TIDBD) to compare their algorithms. Is there any concrete reasoning as to why RMSProp is often preferred over ADAM.",reinforcementlearning,intergalactic_robot,False,/r/reinforcementlearning/comments/ei9p3y/using_rmsprop_over_adam/
Terminal state in Deep Q-Learning,1577820169,"Hello all,

First of all, happy new year to all. Second of all, I have a question relating to the target value with terminal states in DQN.

For the terminal state, the target value will be r = R(s,a,s'). However, in my custom environment, there is no terminal state. But the environment is still episodic in the sense that the episode will be over under certain conditions with the probability of 1.  

Therefore, for DQN, I assume that the target will always be R(s,a,s') + gamma \* max Q(s',a'), correct? Another thing is that if I use baseline libraries for my environment, will it be wrong for those libs that use ""done"" to change the target? For instance, r = R(s,a,s') + (1-done) \* gamma \* max Q(s',a') or r = R(s,a,s') + (not done) \* gamma \* max Q(s',a')

Best, 

Hai.",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/ei7hnv/terminal_state_in_deep_qlearning/
Implementation of Upside-Down RL in Python,1577752017,,reinforcementlearning,theaicore,False,/r/reinforcementlearning/comments/ehuuzf/implementation_of_upsidedown_rl_in_python/
Python Implementation of Upside Down RL,1577749950,,reinforcementlearning,theaicore,False,/r/reinforcementlearning/comments/ehuf4d/python_implementation_of_upside_down_rl/
Deep Reinforcement Learning is a waste of time,1577740439,,reinforcementlearning,toisanji,False,/r/reinforcementlearning/comments/ehs924/deep_reinforcement_learning_is_a_waste_of_time/
Tencent AI ‘Juewu’ Beats Top MOBA Gamers,1577728976,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/ehphxg/tencent_ai_juewu_beats_top_moba_gamers/
A Rant on Kaggle Competition Code (and Most Research Code),1577639738,,reinforcementlearning,GChe,False,/r/reinforcementlearning/comments/eh8g9u/a_rant_on_kaggle_competition_code_and_most/
Wrote a Blog on Quantum Probability in Operant Conditioning: Behavioral Uncertainty in Reinforcement Learning,1577633608,,reinforcementlearning,Nailer_Owl,False,/r/reinforcementlearning/comments/eh76ou/wrote_a_blog_on_quantum_probability_in_operant/
"Why the “RL as Inference” proposal is wrong, or how to properly connect RL with Bayesian Inference",1577626888,,reinforcementlearning,quaternion,False,/r/reinforcementlearning/comments/eh60yy/why_the_rl_as_inference_proposal_is_wrong_or_how/
Is there any simulation environment that provides humanoid with soccer or baseball??,1577626042,I am currently working on a DRL algorithm that aims to train a low-level control policy(which means output of the network are torques or angles of joints) allowing the humanoid to be able to dribble soccer or throwing a baseball to a specific distance. Does anyone know where can I find a proper simulation environment? Or just customizing one in MuJoCo by myself? Any suggestions?,reinforcementlearning,OhGeeezzz,False,/r/reinforcementlearning/comments/eh5wjs/is_there_any_simulation_environment_that_provides/
Finding the value of a deterministic policy,1577559335,"I am trying to follow a video but I’m stuck with a couple of questions. 

&amp;#x200B;

**1** The bellman equation for a deterministic action would be: 

V(s) = R(s,a) + γ(V(s’)) 

In [here](https://i.imgur.com/fXjoVBG.png) they show V(s’) as being the reward received after the immediate reward instead of some V(s’) obtained by working backwards from the terminal state. 

I’m assuming the reason they do this is because there is no terminal state, is this correct? 

&amp;#x200B;

**2**  The second question is how they arrived at [this](https://i.imgur.com/KDsaOmQ.png) formula for the value of π1.

Using what they did before: 

1 + 0.9 \* (0)   +    0 + (0.9)\^2 \* (1)     +    1 + 0.9\^3 \* (0)….. 

If we start at time step 0 then the value for time step 3 would be 1. 

But using the formula  (0.9)\^2k at time step 3 would yield (0.9)\^2\*3  = 0.53

&amp;#x200B;

**3** I was also curious as to why γ\^k is not included in V(s) = R(s,a) + γ(V(s’)) 

Is it because we are just looking at the current state and we are not summing over anything?",reinforcementlearning,RLAttempt2Many2Count,False,/r/reinforcementlearning/comments/egujzb/finding_the_value_of_a_deterministic_policy/
I used neuroevolution to train an agent that balances a double pendulum,1577557581,,reinforcementlearning,Giacobako,False,/r/reinforcementlearning/comments/egu6fi/i_used_neuroevolution_to_train_an_agent_that/
Changing Action/Feature Spaces,1577549433,"When exploring a new environment, my agent can find new actions and/or environmental features that will be useful for the neural nets used to approximate a value function.  What approaches exist to take what is already known about features and action results into account when building the new neural networks that will incorporate the new features and actions?",reinforcementlearning,isolationtank,False,/r/reinforcementlearning/comments/egsgvk/changing_actionfeature_spaces/
Is Chess a deterministic or stochastic MDP?,1577531501,"Hi, I was watching David Silver's lecture on model-based learning, where he says that chess is of deterministic nature. Perhaps I misunderstood what he meant, but if I'm in a state S and take an action A, I can't deterministically say in which state I will end up, as that depends on my opponent's next move. So isn't the state transition stochastic?

I also don't understand if we model Chess as single-agent or multi-agent in general.",reinforcementlearning,iFra96,False,/r/reinforcementlearning/comments/egpmvu/is_chess_a_deterministic_or_stochastic_mdp/
"""[D]"" How to calculate the normalized agent score in MADDPG",1577474009,"Hi

I was reading the MADDPG paper and I am struggling to understand how the authors have calculated the normalized agent score in figure 3. I will be thankful for your help  [link](https://arxiv.org/pdf/1706.02275)",reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/egfeg9/d_how_to_calculate_the_normalized_agent_score_in/
2019 In Review: 10 Open-Sourced AI Datasets,1577471084,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/egeqty/2019_in_review_10_opensourced_ai_datasets/
Does anyone know why the agent often converges to a fixed action no matter what state is fed to the agent?,1577435010,[removed],reinforcementlearning,mint1219,False,/r/reinforcementlearning/comments/eg8kxs/does_anyone_know_why_the_agent_often_converges_to/
Environment for Recommendation Systems,1577398517,"Hey!
I am Atul and I am new here.
It will be great if anyone could help me out in formulating STATES, ACTIONS &amp; REWARDS for a Reinforcement Learning based Recommendation System. 

Thanks.",reinforcementlearning,atul_1511,False,/r/reinforcementlearning/comments/eg1vju/environment_for_recommendation_systems/
2019 in Review: 10 Essential AI YouTube Channels,1577292852,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/efippf/2019_in_review_10_essential_ai_youtube_channels/
"Reinforcement Learning: Past, Present, and Future Perspectives (Katja Hofmann)",1577241276,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/efat2n/reinforcement_learning_past_present_and_future/
"Best collections of DRL ""Tips &amp; Tricks""",1577240929,"There are large overlap between those, but I'd still recommend going through them all if you're starting to work with RL!

- “Nuts and Bolts of DRL Experimentation” by John Schulman https://www.youtube.com/watch?v=8EcdaCk9KaQ

- “Reinforcement Learning Tips and Tricks” by the Stable Baselines team
https://stable-baselines.readthedocs.io/en/master/guide/rl_tips.html

- ""Spinning Up as a Deep RL Researcher” by @jachiam0
https://spinningup.openai.com/en/latest/spinningup/spinningup.html

- ""Lessons Learned Reproducing a DRL Paper” by Matthew Rahtz
http://amid.fish/reproducing-deep-rl",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/efaqxw/best_collections_of_drl_tips_tricks/
What's the current status on Schmidhuber's Worldmodels?,1577240573,"I don't have any experience with it. Is anyone using it? Is it good?
https://worldmodels.github.io/",reinforcementlearning,NikEy,False,/r/reinforcementlearning/comments/efaorw/whats_the_current_status_on_schmidhubers/
[D] What to do when reward modelling from human preference is too hard?,1577173745,"[https://youtu.be/PYylPRX6z4Q?t=941](https://youtu.be/PYylPRX6z4Q?t=941)

The video is about 'Deep reinforcement learning from human preferences'. Where you have a problem with an unknown reward function and use live human preference to make a supervised learning network to learn the reward function.

At the end he raises the problem of when the task is too complex like evaluating a novel, designing a city plan, etc. where using a human for preferences is too time consuming or too hard. He does give a hint: 'How do you learn when there's nobody who can teach you?'

I love this topic and I'm too impatient to wait 1-3 months for his next video. Does the reddit RL community know what he's hinting to or have some nice papers I can read on that topic?",reinforcementlearning,CptVifen,False,/r/reinforcementlearning/comments/eey9vv/d_what_to_do_when_reward_modelling_from_human/
"""Prioritized Sequence Experience Replay"": a nice improvement over PER",1577153238,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/eeuhfk/prioritized_sequence_experience_replay_a_nice/
"RL Weekly 38: Clipped objective is not why PPO works, and the Trap of Saliency maps",1577149698,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/eetrkc/rl_weekly_38_clipped_objective_is_not_why_ppo/
A simple environment to test and implement reinforcement learning algorithms,1577132099,,reinforcementlearning,atharv24,False,/r/reinforcementlearning/comments/eepveu/a_simple_environment_to_test_and_implement/
A simple environment to implement and reinforcement learning algorithms,1577131443,"Hello everyone!  
I was stumped when OpenAI defeated Dendi in Dota 1v1. After months of youtube tutorials to build self-learning AI lead me nowhere, I dived into the world of Deep Reinforcement learning in these summers. Guided by my awesome peers, I studied the basic theory behind state of the art reinforcement learning algorithms.  
I have written a simple environment based on the classic game of snake and trained a simple agent with PPO. I have tried to keep the workings very modular and easy to use.   
This is nowhere near perfection so I would like people to report the issues and even contribute to the project :)

 [https://github.com/Atharv24/SnakeGym](https://github.com/Atharv24/SnakeGym)",reinforcementlearning,atharv24,False,/r/reinforcementlearning/comments/eepq49/a_simple_environment_to_implement_and/
"""Mastering Complex Control in MOBA Games with Deep Reinforcement Learning"", Ye et al 2019 {Tencent} [~OA5 applied to 'Honor of Kings']",1577119348,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/eemy3t/mastering_complex_control_in_moba_games_with_deep/
Structured / hybrid action space in reinforcement learning,1577101835,"I would like to know what kind of method we should use when the action space is a compound of discrete and continuous space.

For example, an auto-drive car could go three directions: forward, left, right. But not three directions at the same time. And for the chosen direction, it has to output the amount.

I know we could simplify this by output angles etc. But I am only using this example as an illustration.

I found this paper Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space. However, it does not apply to the above scenario.

Could somebody provide some literature on this topic? Many thanks!",reinforcementlearning,spacegoing,False,/r/reinforcementlearning/comments/eejo3h/structured_hybrid_action_space_in_reinforcement/
[R] High-quality multi-agent RL environments,1577099712,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/eejcvr/r_highquality_multiagent_rl_environments/
State-Aggregation in Memory-Buffer,1577095843,"Hi all,

I am currently working on recommendation methods with Reinforcement Learning and it is typical practice to define the state as ""Recently viewed/consumed items"" and accumulate it to a state vector with e.g. a RNN. 

It is not clear to me how to handle this with respect to Memory Buffer in DDPG/DQN settings. Do you put the accumulated state (RNN output) into the buffer and use this representation for training updates, or do you put the raw trajectory in it and use the most recent RNN at update-time for a fresh accumulation? 

It is also typical that the Action-embeddings are learned jointly in the process. 

Can anybody give me a hint on how to process this or concerns on the procedure in general?

Thanks guys and happy holidays",reinforcementlearning,balllamann,False,/r/reinforcementlearning/comments/eeisla/stateaggregation_in_memorybuffer/
Custom environment for RL,1577056994,"Hello all.

             I'm trying to work on image captioning problem and trying to optimize it using DQN so that I can get some realistic captions for the given input image.

             I'm stuck at the point where I'm unable to create a custom environment with my own functions and losses with rewards. Can you help me out on how to create my own custom environment and simulate the image captioning on that?

 Thanks in advance.",reinforcementlearning,raghu_1809,False,/r/reinforcementlearning/comments/eec0ns/custom_environment_for_rl/
First RL for PygameSnake,1577039086,"Assuming the DQN algorithm and the Environment aka Pygame ecetera work fine, what would cause my AI to take the fastest way to kill itself despite getting a -1 reward for it? (After around 600 episodes consistently)",reinforcementlearning,CarryGGan,False,/r/reinforcementlearning/comments/ee895s/first_rl_for_pygamesnake/
Is this like MuZero?,1577016692,,reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/ee493s/is_this_like_muzero/
ICLR 2020 Accepted Papers Announced,1576886047,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/edi6ii/iclr_2020_accepted_papers_announced/
What is a policy activation?,1576860372,"In the new deep RL papers I have noticed a new term used: ""policy network activation"" or ""policy activation"". What is it? Just the output of a policy network? As the reference you can find this term here https://arxiv.org/abs/1802.01557 and here https://arxiv.org/abs/1905.10615",reinforcementlearning,Jendk3r,False,/r/reinforcementlearning/comments/edcm9n/what_is_a_policy_activation/
Educational Resources and Content on RL,1576835391,"Hello,

Not sure if this should go here... I am looking for educational content and resources that focuses on deep reinforcement learning, and AI safety. I understand the basics. but now I want to go deeper. I'm looking for something like [this channel](https://www.youtube.com/channel/UCLB7AzTwc6VFZrBsO2ucBMg). Thanks.",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/ed83ov/educational_resources_and_content_on_rl/
Beginner theoretical questions,1576834359,"Hi guys, I'm just starting to get into RL by watching David Silver's lectures on YouTube. I have a couple of questions to see if I understood some key concepts explained at the beginning: 

&amp;#x200B;

1) Is a game like Breakout played on an Atari emulator a partially observable environment? That is, at each time step the observation we get from the emulator is not the representation of the emulator's internal state (where is the ball going? can't tell with a frame), so we are in a POMDP. If we want to solve the problem, we could instead keep a state that is the last n observations, so that we know the velocity of the ball and its trajectory. To sum up, is this a way to shift from a POMDP to a MDP?

&amp;#x200B;

2) How can we classify a game like Candy Crush? I would say it's a POMDP because some cells are hidden to the agent, and moreover we have stochastic rewards and state transitions due to random candies falling from above the grid. Is this correct?",reinforcementlearning,iFra96,False,/r/reinforcementlearning/comments/ed7ymi/beginner_theoretical_questions/
NeurIPS 2019 Deep RL Workshop on TalkRL: Reinforcement Learning Interviews,1576783200,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/ecxxce/neurips_2019_deep_rl_workshop_on_talkrl/
[R] Back from NeurIPS: quick summaries of RL talks &amp; posters,1576781474,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/ecxiy7/r_back_from_neurips_quick_summaries_of_rl_talks/
[R] Back from NeurIPS: quick summaries of RL talks and posters that caught my attention,1576776207,https://twitter.com/MasterScrat/status/1207710189526429697,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/ecwaxy/r_back_from_neurips_quick_summaries_of_rl_talks/
"""Generative Teaching Networks: Accelerating Neural Architecture Search by Learning to Generate Synthetic Training Data""",1576767918,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ecudso/generative_teaching_networks_accelerating_neural/
MuZero implementation,1576716314,"Hi, [I've implemented MuZero in Python/Tensorflow](https://github.com/johan-gras/MuZero).

You can train MuZero on [CartPole-v1](https://gym.openai.com/envs/CartPole-v1/) and usually solve the environment in about 250 episodes.

My implementation **differs** from the original paper in the following manners:

* [I used fully connected layers](https://github.com/johan-gras/MuZero/blob/master/muzero/networks/cartpole_network.py) instead of convolutional ones. This is due to the nature of the environment (Cartpole-v1) which as no spatial correlation in the observation vector.
* Training is not implemented using any multiprocessing: self-play and model optimization are performed alternatively.
* The hidden state is not scaled between 0 and 1 using min-max normalization. But, instead with a tanh function that maps any values in a range between -1 and 1.
* The invertible transform of the value is slightly simpler: the linear term as been removed.
* During training, samples are drawn from a uniform distribution instead of using prioritized replay.
* The loss of each head is also scaled by 1/K (with K the number of unrolled steps). But, K is always considered constant in this implementation (even if it is not always true).

I do have **a few doubts** concerning the network architecture (this is not clear to me in the paper, Appendix F):

* Does the value and policy function have some shared layers given an input hidden state? (I'm not talking about the representation and dynamic function)
* Similarly, how is the dynamic function composed? It is unclear if there is a shared layer between the hidden state and the reward output.

In the future, I'm looking forward to try MuZero on [a bit more complex environment](https://gym.openai.com/envs/LunarLander-v2/) and after that moving onto [visual based](https://gym.openai.com/envs/CarRacing-v0/) [ones](https://gym.openai.com/envs/#atari).

*However, this is not an easy task to perform a replication of* [*a fresh RL paper*](https://arxiv.org/abs/1911.08265). *I would appreciate any feedback from you guys :)*

Link to the repo:  [https://github.com/johan-gras/MuZero](https://github.com/johan-gras/MuZero)",reinforcementlearning,Johan_Gras,False,/r/reinforcementlearning/comments/eclg7t/muzero_implementation/
[1912.03905] ChainerRL: A Deep Reinforcement Learning Library,1576702286,,reinforcementlearning,tihokan,False,/r/reinforcementlearning/comments/ecifmx/191203905_chainerrl_a_deep_reinforcement_learning/
How to feed an image into your RL agent without actually saving the image anywhere?,1576657090,"Hi everyone,

&amp;#x200B;

I am looking for a way  to input an image to my PPO network. I know i will be needing some CNN layers for processing the image to an useful input for the RL agent to act on, right? However i made my maze environment with matplotlib, a python library which allowes you to make graphs and plots. With this library you could save the plot/graph to your pc with a save function and then use that image to input the image into the network, for my case. This is possible, however this would require a lot of image saving and deleting, which doesn't sound very efficient... Are there methods for doing this without actually the image.

&amp;#x200B;

Thank you for responding in advance.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/ec9ub1/how_to_feed_an_image_into_your_rl_agent_without/
Discounted Reinforcement Learning Is Not an Optimization Problem,1576628014,,reinforcementlearning,hardmaru,False,/r/reinforcementlearning/comments/ec4k43/discounted_reinforcement_learning_is_not_an/
Is policy updated only with the most recent data or with all data available?,1576618240,Reinforcement Learning is about an agent creating its own dataset and learning from it. But do I only let the agent learn from the most recent data by updating the current policy with it or do I update the policy using the whole dataset? The former might cause the agent to forget what he learned long ago while the latter might create huge amounts of data and training time. The context is an artificial neural network learning to play a game. I can't even find general remarks on how exactly to use new data in the literature.,reinforcementlearning,id428,False,/r/reinforcementlearning/comments/ec2cc3/is_policy_updated_only_with_the_most_recent_data/
"A16z invests in Anyscale, developer of the Ray RL library",1576617657,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ec2797/a16z_invests_in_anyscale_developer_of_the_ray_rl/
Open AI Dota 2 Bots Get Leaner &amp; Meaner,1576617198,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/ec239z/open_ai_dota_2_bots_get_leaner_meaner/
The communication speed for RL,1576595499,"What is the communication speed between Simulator(like OpenAI Gym) and Python? 
In other words, how long does it take for agent to receive state and reward?",reinforcementlearning,9JjJjJj,False,/r/reinforcementlearning/comments/ebwzsq/the_communication_speed_for_rl/
DQN has problems with unseen levels,1576583460,"This is my first implementation of Deep Q-Learning. I am working with a custom gridworld environment. It is currently 8x8 and there are 6 objects including the agent and walls, I render a bit-matrix for each object so my state is pretty sparse and it is \[6, 8, 8\]. The agent has to solve simple tasks like ""go to this object"" or ""pick up that object""

I am using a vanilla Deep Q-Learning approach (similar to the pytorch implementation). My DQN is a simple feedforward network, consisting of 4 linear layers. This was enough to solve multiple tasks and converge relatively fast, but my approach fails to generalize:  
I train the agent on 8 different levels, that only differ in the position of the objects and the agent. Testing the Agent on the same levels has perfect results, but when testing on 2 unseen levels, the agent fails completely. This means my agent is overfitting.

Is there something I could do, to improve generalization?

My assumptions:  
\- 8 different levels are not enough, the agent should be trained on more to avoid overfitting  
\- By flattening the \[6, 8, 8\] state the agent loses information (e.g. the second bit-matrix represents the walls in the environment)  
\- Bit-matrices are a bad idea, I should try rendering the state as RGB matrices",reinforcementlearning,Turaa,False,/r/reinforcementlearning/comments/ebuvay/dqn_has_problems_with_unseen_levels/
what is the nonstationary optimal value distribution and bellman operator in C51-DQN (A Distributional Perspective on Reinforcement Learning),1576580286,[removed],reinforcementlearning,170928,False,/r/reinforcementlearning/comments/ebuep1/what_is_the_nonstationary_optimal_value/
How to reimplement Safe RL,1576503028,"I believe that Safe RL is effective for the situations which need safety. I think this paper is well done [A Lyapunov-based Approach to Safe Reinforcement Learning](https://arxiv.org/abs/1805.07708)
and I want to apply this RL to robotics.
What do I have to learn? I studied Model free RL. What's next?",reinforcementlearning,9JjJjJj,False,/r/reinforcementlearning/comments/ebf5pg/how_to_reimplement_safe_rl/
"In the DDPG, How to decide the number of neruals for both actor and critic ?",1576464493,"Hello, everyone. I am new to reinforcement learning and I have a question about ddpg. In supervised learning ,we can evaluation the network by its loss to see the network is overfitting ,fitting or less-fitting, but how can we do these with reinforcement learning ?",reinforcementlearning,JoeyChia,False,/r/reinforcementlearning/comments/eb901g/in_the_ddpg_how_to_decide_the_number_of_neruals/
"""NeurIPS 2019 Notes"", David Abel",1576453276,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/eb6qd5/neurips_2019_notes_david_abel/
Deriving Bellman Equation,1576439504,"https://www.quora.com/In-Sutton-Bartos-book-on-reinforcement-learning-what-is-the-derivation-of-the-Bellman-equation-for-value-functions

𝑣𝜋(𝑆𝑡=𝑠)=𝐸𝜋[𝐺𝑡|𝑆𝑡=𝑠]=𝐸𝜋[(𝑅𝑡+1+𝛾𝑅𝑡+2+𝛾2𝑅𝑡+3+...)|𝑆𝑡=𝑠] (1)
=𝐸𝜋[(𝑅𝑡+1+𝛾(𝑅𝑡+2+𝛾𝑅𝑡+3+...))|𝑆𝑡=𝑠] (2)
=𝐸𝜋[(𝑅𝑡+1+𝛾(𝐺𝑡+1))|𝑆𝑡=𝑠] (3)
=𝐸𝜋[𝑅𝑡+1|𝑆𝑡=𝑠]+𝛾𝐸𝜋[𝐺𝑡+1|𝑆𝑡=𝑠] (4)
=𝐸𝜋[𝑅𝑡+1|𝑆𝑡=𝑠]+𝛾𝐸𝜋[𝐸𝜋(𝐺𝑡+1|𝑆𝑡+1=𝑠′)|𝑆𝑡=𝑠] (5)

I am following the derivation in this quora post. I understand everything up to step 4. In step 5 however, I am not sure how to argue that this 𝐸𝜋[𝐺𝑡+1|𝑆𝑡=𝑠] turns into this 𝐸𝜋[𝐸𝜋(𝐺𝑡+1|𝑆𝑡+1=𝑠′)|𝑆𝑡=𝑠].",reinforcementlearning,tsailfc,False,/r/reinforcementlearning/comments/eb3nol/deriving_bellman_equation/
Why AlphaZero doesn't need opponent diversity?,1576317290,"As I read through some self-play RL papers, I notice that to prevent overfitting or knowledge collapsing,  it needs some variety during self-play. This was done in AlphaStar, OpenAI Five and Doom CTF[https://deepmind.com/blog/article/capture-the-flag-science].

So I wonder how can AlphaZero get away without opponent diversity? Is it because of MCTS and UCT?",reinforcementlearning,51616,False,/r/reinforcementlearning/comments/eahy6r/why_alphazero_doesnt_need_opponent_diversity/
"""Dota 2 with Large Scale Deep Reinforcement Learning"", Berner et al 2019 {OA}",1576277998,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/eabfgb/dota_2_with_large_scale_deep_reinforcement/
"""e-prop: A solution to the learning dilemma for recurrent networks of spiking neurons"", Bellec et al 2019",1576267388,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ea918z/eprop_a_solution_to_the_learning_dilemma_for/
Question on Inverse Reinforcement Learning,1576259113,"I have a question regarding the loss function of the Guided Cost Learning algorithm.

The Loss L, is:

E _ { \tau \sim p } [ c _ { \theta } ( \tau ) ] + log Z

The goal is to optimise theta or c _ { \theta } so that it gives low cost( or high reward) to an optimal trajectory.

My question is about making sense of the loss function : if we set c _ { \theta } to all states - wouldn't that give you the minimal loss? - if so that doesn't sound right.

What are the intuition behind both of the two terms in trying to maximize the likelihood of an optimal trajectory?",reinforcementlearning,mellow54,False,/r/reinforcementlearning/comments/ea777q/question_on_inverse_reinforcement_learning/
"""AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos"", Smith et al 2019 {BAIR}",1576209744,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e9ytd0/avid_learning_multistage_tasks_via_pixellevel/
"""Model-Based Reinforcement Learning: Theory and Practice"", Michael Janner {BAIR} [why MBPO?]",1576182087,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e9t0rl/modelbased_reinforcement_learning_theory_and/
Trust Your Model: Model-Based Policy Optimization [Paper and Code included],1576180270,"**Codes:** [https://github.com/JannerM/mbpo](https://github.com/JannerM/mbpo) 

**Paper:** https://arxiv.org/pdf/1906.08253.pdf 

&amp;#x200B;

*Processing gif 08vo0v5kb9441...*",reinforcementlearning,ai-lover,False,/r/reinforcementlearning/comments/e9sli6/trust_your_model_modelbased_policy_optimization/
Is there a subreddit for reinforcement learning or deep reinforcement learning research papers?,1576155566,,reinforcementlearning,rajat_k_saini,False,/r/reinforcementlearning/comments/e9n8jm/is_there_a_subreddit_for_reinforcement_learning/
What reward to choose for generalized A to B problem?,1576142618,"Hello,

Right now my A to B problem is: generate a random starting point A and a random destination point B, both in \[-10,10\]\^2, and let the agent that starts at A try to reach point B. Actions are in \[-1,1\]\^2 and state space is unbounded R\^2, everything is continuous.

My current reward is -norm(state-destination) where state is the position of the agent, and destination the point B. I first tried to use A2C and it worked for the bounded, discrete version of the problem, then switched to PPO to solve the continuous unbounded problem.

Now I am trying to generalize it to more than 1 point: I generate n independant A to B positions and solve them all simultaneously, ie. state space is R\^2n, action space is \[-1,1\]\^2n, and states and destinations are the stacked positions of the n agents.

It still works for n=2, works okay for n=3 but then start struggling when n gets higher, and doesn't work at all for n=10 for instance. I assume it is normal since the reward is not very informative for the agent when n points move at the same time with high n.

How could I improve my reward? Is there any way I can do it single-agent with maybe a larger neural network, or do I have to go multi-agent if I want to solve it for much larger values of n (n=100..)?

Thanks for the insights.",reinforcementlearning,sparkyhusky,False,/r/reinforcementlearning/comments/e9ley4/what_reward_to_choose_for_generalized_a_to_b/
"RL Weekly 37: Observational Overfitting, Hindsight Credit Assignment, and Procedurally Generated Environment Suite",1576076698,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/e987n8/rl_weekly_37_observational_overfitting_hindsight/
Reinforcement learning for Autonomous Driving,1576047415,How do you see the future of self driving cars using reinforcement learning? I am super excited by the recent developments in the field of reinforcement learning and I feel though self driving cars has been making rapid progress but there has been very efforts using reinforcement learning. It's true that RL are hard to train but at the same time RL can learn policies which are close to human like working and I feel it can be a huge step towards achieving level 5 autonomy.,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/e93o5f/reinforcement_learning_for_autonomous_driving/
Is my assumption correct? REINFORCE and A2C are on policy because the gradient is an expectation wrt trajectories sampled from the policy,1575999273,"I am trying to understand IMPALA and all the ideas that it builds upon - if you can recommend a really good resource on A3C or IMPALA, that would also be helpful. But for now, I am mostly concerned with the question in the title.",reinforcementlearning,nielsrolf,False,/r/reinforcementlearning/comments/e8trdb/is_my_assumption_correct_reinforce_and_a2c_are_on/
Help understanding the reasoning behind the frequent update rule,1575996134,"I am trying to understand the intuition behind the frequent update rule that occurs in RL by looking at how the incremental estimated reward was derived. [This](https://i.imgur.com/LgqUDZe.png) is the update rule. 

My three questions are 

* Why use Qn+1? 
* Why take out Rn?
* why simplify it to  Qn + 1/n \[Rn + Qn\] ? 

I came up with my own reasoning but I do not know if it's right since there is no one else I can talk to about RL. 

* In order to solve this you need to make sure that it is true for all future values n+1, For that reason you take the current formula and look at Qn+1. 
* From here your goal is to come up with a recursive solution, you can accomplish this by taking Rn out. Now what is left can be multiplied and divided by (n-1) as to include Qn into the equation while keeping the equation of Qn+1 intact. 
* Simplifying if further to the form Qn + 1/n \[Rn + Qn\] makes it easier to understand what is going on in the equation. 

Am I right or am I going completely off track?",reinforcementlearning,RLAttempt2Many2Count,False,/r/reinforcementlearning/comments/e8t13k/help_understanding_the_reasoning_behind_the/
"Become ready to facilitate your process, make your decisions faster and hedge fraud losses with a hands-on AI training course for managers.",1575983586,,reinforcementlearning,benjamin_brook,False,/r/reinforcementlearning/comments/e8qg04/become_ready_to_facilitate_your_process_make_your/
Expected error,1575965271,Has any research been done in DRL on using a neural network to predict how reliable another neural network prediction (Q function for example) is?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/e8ntr8/expected_error/
My first pc building for RL,1575942807,,reinforcementlearning,Rowing0914,False,/r/reinforcementlearning/comments/e8jty3/my_first_pc_building_for_rl/
My first pc building for RL,1575942736,,reinforcementlearning,Rowing0914,False,/r/reinforcementlearning/comments/e8jte5/my_first_pc_building_for_rl/
PhD in RL - need advice,1575921040,"Hi guys,

I am doing my integrated masters in mathematics at the University of Manchester. I have some background in ML and DL. I did an internship in DL, did graduate level ML - DL course, and have some projects (not super fancy) on github. I am currently writing my double semester project(dissertation) about Reinforcement Learning. The project starts with the basics - I am mostly following David Silvers RL course on youtube. In the next semester we will focus more on deep reinforcement learning and games with imperfect information. I am really confident in Python and confident in C++.

I would like to apply for a phd in RL somewhere in Europe. 

Do you know any phd programs? Where should I start looking for a PhD in RL?

Thanks!",reinforcementlearning,marboka,False,/r/reinforcementlearning/comments/e8eyzk/phd_in_rl_need_advice/
Training Agents using Upside-Down Reinforcement Learning,1575913936,,reinforcementlearning,koolaidman123,False,/r/reinforcementlearning/comments/e8dbww/training_agents_using_upsidedown_reinforcement/
Solving Pacman using Q-learning,1575897820,"Hi, 
Is it possible to solve Pacman problem using only Q-learning.

I got the pacman script from http://ai.berkeley.edu/reinforcement.html.

I'd like to write Q-learning algo as given in the book of Sutton-Barto and want my pacman agent to play the game and want to show the same for winning condition.

Any advices about how I should proceed with that.

Thanks,
Nikhil",reinforcementlearning,singh_nikhil,False,/r/reinforcementlearning/comments/e89wj1/solving_pacman_using_qlearning/
Need suggestions regarding Projects which uses Computer Vision and Deep Reinforcement Learning.,1575893959,"Hi Guys,

I needed some help in choosing projects which uses CV and Deep RL. I've been learning Deep Learning, CV since, past 9-10 months now and got a grip on the basic concepts. I've just started to learn Deep RL. so, I'm a Rookie here. 

For past couple of months I've gained knowledge from the online courses(Andrew Ng's Deep Learning Specialization on Coursera and Udacity's CV Nanodegree program)and completed their projects and wrote a medium article about a CV project which got featured on Towards Data Science. 

I'm really interested in combining CV with Deep RL and experiment with some projects which uses the same.
Right now I'm blank and don't have any idea about what sort of projects I can implement using those.

It would really helpful if you guys can tell me to implement certain project ideas or atleast guide me to do something which will help me in gaining more knowledge.

Thank you.",reinforcementlearning,Nailer_Owl,False,/r/reinforcementlearning/comments/e8991b/need_suggestions_regarding_projects_which_uses/
"""Combining Q-Learning and Search with Amortized Value Estimates"", Hamrick et al 2019 {DM}",1575771533,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e7npus/combining_qlearning_and_search_with_amortized/
My latest blog post - A mathematical introduction to Policy Gradient,1575745558,"Hi All,  
I have written a new introductory blog post called ""[A mathematical introduction to Policy Gradient](http://machinelearningmechanic.com/deep_learning/reinforcement_learning/2019/12/06/a_mathematical_introduction_to_policy_gradient.html)"". I would really appreciate your thoughts and feedback.",reinforcementlearning,rbahumi,False,/r/reinforcementlearning/comments/e7icp6/my_latest_blog_post_a_mathematical_introduction/
"[R] ""Reusable neural skill embeddings for vision-guided whole body movement and object manipulation""",1575680675,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/e77k6p/r_reusable_neural_skill_embeddings_for/
"[R] ""Hindsight Credit Assignment""",1575680469,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/e77iun/r_hindsight_credit_assignment/
[R] Building AI that can master complex cooperative games with hidden information,1575663019,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/e73tcy/r_building_ai_that_can_master_complex_cooperative/
"Sunil Mallya, on AWS Deepracer &amp; Sagemaker RL",1575623812,"Hi everyone, 

Would like to share the latest episode of our Podcast “the Humans of Ai”. The latest episode features Sunil Mallya, who is the Principal Deep Learning Scientist at Amazon Web Services Machine Learning Lab &amp; is one of the key people behind AWS Deepracer &amp; Sagemaker RL. 

In this episode we explore the rich interaction that Sunil has had with machine learning over the course of his career including talking about what drew him to the field, his experience as an early phase machine learning entrepreneur, his knowledge in building distributed machine learning systems and finally the two biggest projects Sunil has taken on recently - Sagemaker RL &amp; AWS Deepracer!

We also discuss many other topics including what the Machine Learning scene is like in San Francisco, some real world applications of Reinforcement Learning &amp; what it takes to be a part of the Amazon's ML Lab.

You can find The Humans of Ai on: 

iTunes here:

[https://podcasts.apple.com/au/podcast/the-humans-of-ai/id1464995550](https://podcasts.apple.com/au/podcast/the-humans-of-ai/id1464995550)

Spotify here:

[https://open.spotify.com/show/2RY5mcNl0iAs8HUTNbwT0J](https://open.spotify.com/show/2RY5mcNl0iAs8HUTNbwT0J)

Sticher here:

[https://www.stitcher.com/s?fid=414486&amp;refid=stpr](https://www.stitcher.com/s?fid=414486&amp;refid=stpr)

Moderators, if you feel that this is not the right place to be posting, feel free to remove &amp; completely understand, but we do feel that this episode in particular has a lot of information that could benefit the Reinforcement Learning community. Cheers &amp; best",reinforcementlearning,sigmoidp,False,/r/reinforcementlearning/comments/e6wfth/sunil_mallya_on_aws_deepracer_sagemaker_rl/
NeurIPS 2019 Livestream,1575602215,"aideeptalk will livestream the expo and posters at NeurIPS 2019 on Twitch on channel [aideeptalk](https://www.twitch.tv/aideeptalk/)

To receive a notification when we go live, please follow us and enable notifications on our [Twitch channel](https://www.twitch.tv/aideeptalk/).

Follow us on Twitter at [aideeptalk](https://twitter.com/aideeptalk) for our schedule.

Please pass this on to those who can't make it to NeurIPS For more details see our website [aideeptalk.com](http://www.aideeptalk.com/)",reinforcementlearning,aideeptalk,False,/r/reinforcementlearning/comments/e6sqg5/neurips_2019_livestream/
"""Dream to Control: Learning Behaviors by Latent Imagination"", Hafner et al 2019",1575585067,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e6ow1r/dream_to_control_learning_behaviors_by_latent/
3D environments suitable to learn from pixels only?,1575568358,"Hi, does anyone have experience with this and can recommend environments where the agent actually converges and is able to learn the given task?

What I found so far is a paper (""Deep Reinforcement Learning for Vision-Based Robotic Grasping: A Simulated Comparative Evaluation of Off-Policy Methods"", couldn't link it because of reddit spam bot) where Quillen et al. apply deep RL algos to a robotic grasping scenario. The agent's inputs are pixels only and the results look promising. The task/env doesn't have to be as complex as the one used in the paper. Something like Cartpole 3D would be fine as well.

To summarize what I'm looking for:

* 3D environment such as Mujoco / Pybullet / DM Control Suite / RLBench envs
* Observation space consists of pixel values only
* Algos used are not of importance (preferably simple ones such as DQN)

I also managed to find pixel observation wrappers for Gym and DM Control Suite. Does anyone have experience using these to train agents with pixels as sole input?

Any help is appreciated, thanks in advance!",reinforcementlearning,Happy-Canary,False,/r/reinforcementlearning/comments/e6krqy/3d_environments_suitable_to_learn_from_pixels_only/
3D environments suitable to learn from pixels only?,1575567645,[removed],reinforcementlearning,jodel51,False,/r/reinforcementlearning/comments/e6klnb/3d_environments_suitable_to_learn_from_pixels_only/
Question regarding Q_Value calculation and Q_Learning for non deterministic cases,1575564786,"Hi folks! I have a couple of questions.
What's the difference between these two equations that I find for when computing the Q Value of a new state  
new_value = (1-learning_rate)*old_value + learning_rate(*reward + discount_factor * future_q_value)  
vs  
new_value = old_value + learning_rate * (reward +discount_factor * future_q_value) - old_value  

And also, how would you go about adapting a basic Q learning algorithm for a multitude of environments that can be either deterministic or non deterministic? This question comes because I'm working on a project where I have to develop an agent that will be tested in both cases randomly",reinforcementlearning,throwaway1738696969,False,/r/reinforcementlearning/comments/e6jwq0/question_regarding_q_value_calculation_and_q/
3D environments suitable to learn from pixels only?,1575560370,[removed],reinforcementlearning,jodel51,False,/r/reinforcementlearning/comments/e6iutn/3d_environments_suitable_to_learn_from_pixels_only/
3D environments suitable to learn from pixels only?,1575559767,[removed],reinforcementlearning,jodel51,False,/r/reinforcementlearning/comments/e6ipse/3d_environments_suitable_to_learn_from_pixels_only/
3D environments suitable to learn from pixels only?,1575559158,[removed],reinforcementlearning,jodel51,False,/r/reinforcementlearning/comments/e6ikqc/3d_environments_suitable_to_learn_from_pixels_only/
Multiagent environment state and actions encoding,1575548700,Hello I'm trying to make multiagent environment for a card game with imperfect information. The goal is to learn policy/model (with custom-strength by applying random noise to enable difficulty selection and develop human-like play). How do you encode states and actions in such multiplayer game for model to understand? I'm looking at actor-critic now. Can you recommend to read something on this topic?,reinforcementlearning,The_kingk,False,/r/reinforcementlearning/comments/e6ghdz/multiagent_environment_state_and_actions_encoding/
DRQN from scratch,1575477354,"Hi, 

I'd like to share my tensorflow implementation of Deep Recurrent Q Learning.

https://github.com/marctuscher/DRQN-tensorflow

Mainly intended for learning purposes, it gave my some good insights on how to train Deep RL agents. If someone's just starting in DRL I recommend writing one of the State-of-the-art Algorithms from scratch (I also did A3C, VPG and started PPO; should be somewhere in my GitHub repos). However, since then I trained a few Agents for Real-World Robotics scenarios and I mostly rely on really good implementations like stable-baselines (https://stable-baselines.readthedocs.io/en/master/). Also, there are a few improments to be made in my DRQN.

Another thing: I recently switched from tf to pytorch and really do love it. It also seems to be well suited for production scenarios with things like TorchScript.",reinforcementlearning,g4mplex,False,/r/reinforcementlearning/comments/e61sr4/drqn_from_scratch/
"Using DQN in reinforcement learning, what's the best initialization scheme (specifically, for Atari games)?",1575427551,"I would like to generate many (hundreds) of local optima/saddle point solutions using DQN algorithm with Atari games (and convolutional neural network for the model). I would like these weight solutions to produce as varied as possible outcomes the solution space (meaning they allow the agent to explore as many varying scenarios in the game as possible). So my question is: Is random initialization for the weights in the DQN algorithm the best way to go about this? Or is there a more informed way (which would still likely involve some sort of randomness since these solutions should be different) to do this? 

tldr; I want as many ""good"" solutions (local/saddle pts) for the weights as possible which also explore as much of the game as possible instead of congregating to similar regions in solution space.",reinforcementlearning,debussyxx,False,/r/reinforcementlearning/comments/e5sicr/using_dqn_in_reinforcement_learning_whats_the/
[Q] How is the policy updated in PPO when the epsilon + advantage term is used?,1575412117,,reinforcementlearning,Carcaso,False,/r/reinforcementlearning/comments/e5owll/q_how_is_the_policy_updated_in_ppo_when_the/
"""Procgen Benchmark: 16 simple-to-use procedurally-generated environments which provide a direct measure of how quickly a reinforcement learning agent learns generalizable skills"" {OA}",1575405281,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e5n6e4/procgen_benchmark_16_simpletouse/
Solving free-endpoint problems with RL,1575384529,"How can you use reinforcement learning to deal with free-endpoint dynamic optimization problems? In other words, problems where the terminal time is also a decision variable.",reinforcementlearning,Antonioe89,False,/r/reinforcementlearning/comments/e5i4sf/solving_freeendpoint_problems_with_rl/
Auxillary Loss in DDPG,1575366103,Is it possible to have an auxiliary loss along with the original loss for Actor in DDPG?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/e5eu1h/auxillary_loss_in_ddpg/
System Architecture in rl,1575327670,Is it not acceptable for reinforcement learning to have a millisecond time difference between state and reward?,reinforcementlearning,9JjJjJj,False,/r/reinforcementlearning/comments/e577w3/system_architecture_in_rl/
"""Procedural Content Generation: From Automatically Generating Game Levels to Increasing Generality in Machine Learning"", Risi &amp; Togelius 2019",1575306133,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e51wfp/procedural_content_generation_from_automatically/
Dynamical system in Mujoco,1575305821,,reinforcementlearning,alish_212,False,/r/reinforcementlearning/comments/e51tpg/dynamical_system_in_mujoco/
Keeping up with RL research,1575289929,How do you keep yourself notified of recent RL developments (before looking them up on arxiv),reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e4yg9p/keeping_up_with_rl_research/
DQNfrom scratch,1575281137,"I'm trying to implement a variey of RL algorithms fro scratch. Here I've made DQN :

[https://github.com/MakisKans/Reinforcement\_Learning/tree/master/DQN](https://github.com/MakisKans/Reinforcement_Learning/tree/master/DQN)

and just like my PPO implementation (and every other one) it works for CartPole, Lunar Lander perfectly well. The thing is it doesn't work for atari. I add certain code for that  and more specifically :

i) Convolution Layers and Flatten

ii) image preprocessing and stacking

iii) no-ops, same action for 4 steps(though this one is not important)

I've been trying for the past month to figure out what I'm doing wrong. Any help would be appreciated , if you could review parts of my dqn.py

The code that I add for atari and don't use when training on the ""simpler ones"" is wrapped arround in hashtags (while the commented code is the inverse)",reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e4x0vj/dqnfrom_scratch/
Escaping difficult explorable environments?,1575236603,"I am highly wondering what the techniques are for escaping difficult exploravle environments, meaning eg a maze environment Where if you hit the wall you die And restart. Chances are that you Will hit the wall a lot of times And barely exploring the environment. Both boltzman And epsilon greedy (atleast for me) failed “escaping” this situation.

Does anyone know what possible approaches are for this?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/e4oawy/escaping_difficult_explorable_environments/
I'm looking for a good library to use when creating a basic reinforcement learning ai,1575232256,I'm a highschooler who knows some python and Java and I'm looking to create a basic reinforcement AI what program and library should I use to get started.,reinforcementlearning,tornadoluna,False,/r/reinforcementlearning/comments/e4n6w1/im_looking_for_a_good_library_to_use_when/
Can anyone explain how solving Bellman equation through Linear Equations work?,1575146591,"In David Silver's lecture here [https://youtu.be/lfHX2hHRMVQ?t=2349](https://youtu.be/lfHX2hHRMVQ?t=2349) 

I am trying to grasp what the linear algebra is solving here. What is the initial v ? My linear algebra is weak, so apologizes if this seems obvious. I don't understand what is being solved when we're not using past rollout rewards to calculate our state value function. 

Earlier in the lecture, we have rollouts where we have make moves and gain some reward and we can use that to get an idea of how value each state is so it's a Monte-Carlo method.

But again, with this linear algebra thing, I don't see how it incorporates that past rewards. What is it doing? 

Thanks!",reinforcementlearning,117throw,False,/r/reinforcementlearning/comments/e4347c/can_anyone_explain_how_solving_bellman_equation/
Using Soft Actor-Critic in OpenAI robotics environments,1575138224,Have anyone successfully used SAC for solving any of the Fetch Environments?,reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/e40y58/using_soft_actorcritic_in_openai_robotics/
OpenAI releases Safety Gym for reinforcement learning,1575094406,,reinforcementlearning,imthiyagarajan,False,/r/reinforcementlearning/comments/e3rqk5/openai_releases_safety_gym_for_reinforcement/
ICM Implementation,1575067005,"I'm implementing the \[Intrinsic Curiosity Module\]([https://pathak22.github.io/noreward-rl/](https://pathak22.github.io/noreward-rl/) (IDM) as part of my research, and I'm finding that the ICM features often collapses to 0 during training. Presumably, this is because the \*\*mean squared error (MSE) of the forward dynamics model (FDM) is lowest when the learned representation is smallest. Just wondering if anyone has any experience with this? I've tried adjusting the learning between the IDM and the FDM but it has not helped yet.

My current strategy is to add a small amount of autoencoder loss, which stops the collapse but isn't ideal as the features then include parts of the environment that the agent does not directly interact with.

Just wondering if anyone else has tried implementing this and if they've bumped into the same probleM?

Some notes

* On Pong, the IDM only gets to around 50% accuracy. Given that there are only two useful actions, this seems not that impressive.
* Things worked really well when I feed 4 stacks into the IDM, I now feed just the most recent frame and the predictions are much worse. Presumably in Pong 4 stacks would be enough for the IDM to simply learn the policy from the initial state, ignoring the final state.
* I've paired this with PPO, so my policy is stochastic (if that matters).
* Deepak very kindly has made the \[source\]([https://github.com/pathak22/noreward-rl](https://github.com/pathak22/noreward-rl)) available, so I'll probably look through that. Still, love to know what the IDM accuracy was on their tests though, and how they stopped the representation collapsing.",reinforcementlearning,VirtualHat,False,/r/reinforcementlearning/comments/e3mamt/icm_implementation/
[R] What do you use to manage your RL experiments?,1575059139,"Currently, I am using wandb.com and make sure to commit my code before each run with a descriptive commit message.

How are you handling things? As I am running more and more experiments, and sometimes would want to run automated grid search, my current method is reaching its limits.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/e3kbz9/r_what_do_you_use_to_manage_your_rl_experiments/
HER principle,1575057384,"I am trying to implement HER algorithm and I have found this amazing article: [https://deeprobotics.wordpress.com/2018/03/07/bitflipper-herdqn/](https://deeprobotics.wordpress.com/2018/03/07/bitflipper-herdqn/)

In the article only the last achieved goal is used as additional goal (from the four interactions). Using HER the original replay buffer, consisting of four trajectories, now consists of eight trajectories (the first four are original using original desired goal, the other four use additional goal (the last achieved goal) as desired goal). According the original paper this strategy is called *final* . 

However, it is recommended to use the strategy *future:* replay with k random states which come from the same episode as the transition being replayed and were observed after it. The recommended value of k is four. The problem is that I find it hard to image, how should I implement this strategy.

Lets say that the length of the episode is **40**, so our replay buffer has size **40** as well at the moment. Consequently, we sample **4** random transitions from the episode lets say **t\_10**, **t\_20**, **t\_30** and **t\_40**. So each achieved goal from these timesteps would serve as desired goal as follows:

1. for timestep **t\_0** \- **t\_10** the desired goal will be achieved goal from **t\_10** and the reward in the timestep **t\_10** will be **+1** (or using reward function again, but with the approach that desired goal was achieved)
2. for timestep **t\_11** \- **t\_20** the desired goal will be achieved goal from **t\_20** and the reward in the timestep **t\_20** will be **+1**.
3. for timestep **t\_21** \- **t\_30** the desired goal will be achieved goal from **t\_30** and the reward in the timestep **t\_30** will be **+1**.
4. for timestep **t\_31** \- **t\_40** the desired goal will be achieved goal from **t\_40** and the reward in the timestep **t\_40** will be **+1**.

So our original replay buffer will increase its size by **40**, so its size is **80** now. Do I understand it correctly so far? Is this whole implementation or have I missed something?

Thank you in advance",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/e3jwkl/her_principle/
Query regarding research prospects in RL,1575043799,"What are the research prospects at the intersection of multi-agent systems, causality, and learning theory? I found this area to be interesting

Are any of you working (or used to work) in a research capacity in industry or academia? What do you think are some of the pressing questions? Could you give a quick overview of the field?

Also, do you happen to know research groups or professors who work on these fields, either in the US or Europe?

Finally, how should I go about exploring the interdisciplinary field?

Thanks!",reinforcementlearning,kbnewreddit,False,/r/reinforcementlearning/comments/e3gnrg/query_regarding_research_prospects_in_rl/
Where do you test your code?,1575040471,,reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e3fy3l/where_do_you_test_your_code/
List of input-driven Reinforcement Learning Environments?,1575034784,"Hi all, I'm working on a project that requires the agent to learn effectively in an (exogenous) input-driven environment. For those who don't know, the state of input-driven environments could be divided into 2 components: exogenous i.e. these parts of the state would not be affected by agent actions but other factors, and endogenous i.e. these parts would be transited according to agent actions. In some problems, the exogenous inputs don't have much impact on reward and therefore could be considered as some sort of noise. However, in my problem, they will affect the rewards and cause high variance in state value or Q value. For instance, managing in a restaurant where the customers arbitrarily visit or controlling the network system where the packets arrive randomly. Could anyone give me the name of existed RL environments with these properties?",reinforcementlearning,iaelitaxx,False,/r/reinforcementlearning/comments/e3euzv/list_of_inputdriven_reinforcement_learning/
What is SOTA in RL applied to robotics?,1575014794,"What are good, sample-efficient (model-based) RL algos that have been successfully applied in robotics? Last time I checked PILCO was considered state of the art (or at least referred to a lot in Levine's lectures), has anything changed since then?",reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/e3bvki/what_is_sota_in_rl_applied_to_robotics/
Solving the FrozenLake environment from OpenAI gym using Value Iteration,1574968072,[https://medium.com/@digankate26/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438](https://medium.com/@digankate26/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438),reinforcementlearning,aidiganta,False,/r/reinforcementlearning/comments/e32lnj/solving_the_frozenlake_environment_from_openai/
Solving the FrozenLake8x8 environment from OpenAI gym using Value Iteration,1574966182,,reinforcementlearning,aidiganta,False,/r/reinforcementlearning/comments/e325eq/solving_the_frozenlake8x8_environment_from_openai/
Using RNNs for NAS,1574962991,"Why are RNN controllers fed the previous configuration they outputted? 

Reference: https://ai.google/research/pubs/pub45826

This seems really...awkward. Is it because there's no clear input to the RNN?",reinforcementlearning,Turings_Ego,False,/r/reinforcementlearning/comments/e31bys/using_rnns_for_nas/
"""Contrastive Learning of Structured World Models"", Kipf et al 2019",1574962418,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e31745/contrastive_learning_of_structured_world_models/
List of Deep Reinforcement Learning algos for continuous tasks?,1574955428,One of the continuous algorithms I looked at is a DDPG.  I'm still having a hard time finding other other non discrete algorithms and how they differ.  Is there a good resource or a list that you know show casing the difference between continuous algorithms ?,reinforcementlearning,thinking_computer,False,/r/reinforcementlearning/comments/e2zke2/list_of_deep_reinforcement_learning_algos_for/
Vladimir Kramnik on using AlphaZero to test chess rule modifications like no-castling,1574953078,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/e2z2lj/vladimir_kramnik_on_using_alphazero_to_test_chess/
How much background knowledge is required for implementation of the code,1574930145,"Hello! I am working on a quadruped project. Our goal is to perform gait  planning for this robot. Until now we have been doing this with modular controllers and non-linear optimizers. We are planning to move to learning methods for performing the gait planning. I am pretty new to this concept and my question is how much theoretical knowledge on Reinforcement learning one needs to know in order to use packages like openAI, tensorflow etc.. for performing specific tasks like motion planning, gait generation etc... I talked to a friend of mine who's working on this field already. He said if I want to use these RL packages as a tool and perform specific applications I can do it pretty easily without the background knowledge in RL. But if I need to improve on the existing RL algorithms then I definitely need to spend indefinite amount of time for getting the necessary perception. Is this indeed the case? My another question is, if this is indeed the case i.e if people can use the existing packages as it is without knowing the A, B, Cs of RL, then why so much people invest their time in learning the theory behind RL? I am looking forward to have an open discussion on this and can't think of anyplace better than this community to post this question.  


On the side note, I did try to learn the concept by watching the reinforcement learning course by Emma Brunskill and reading the book by Sutton and Barto but could not follow it, as I don't have the necessary prerequisite background knowledge in ML. And I fell that learning all the prerequisites and then learning RL will take a lot of time.",reinforcementlearning,hari191197,False,/r/reinforcementlearning/comments/e2v9wg/how_much_background_knowledge_is_required_for/
2 in 1: Computer Vision + Data Science in one 5-Day Course in Washington D.C.,1574861641,,reinforcementlearning,benjamin_brook,False,/r/reinforcementlearning/comments/e2g2ep/2_in_1_computer_vision_data_science_in_one_5day/
Is this a good course?,1574837104,"Hello! I am currently learning reinforcement learning but most of the online resources have only the theoretical aspects of the subject and I want to learn more about the implementation part of it. I came across this course in udemy called [Artificial Intelligence: Reinforcement Learning in Python](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/) and it has a lot of positive reviews. People who have taken this course or know about this, Is this a good place to learn the implementation aspect of RL?",reinforcementlearning,hari191197,False,/r/reinforcementlearning/comments/e2c38g/is_this_a_good_course/
AI takes on popular Minecraft game in machine-learning contest,1574819094,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/e28kie/ai_takes_on_popular_minecraft_game_in/
What's your job?,1574819056,"As an RL enthusiast/professional, what's your career like? I'm currently a student but I imagine ml engineer as the next step",reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e28k7s/whats_your_job/
DQN environment to test,1574797713,I've implemented DQN from scratch and now I want to test it on Atari given that it works on mountain car and cart pole. Which game convergences fast enough and how long should I expect it to do so (in frames / days / episodes). Ive seen different implentations and papers and they all say something different with steps ranging from 5 mil to 200 mil,reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e23bnw/dqn_environment_to_test/
Interactive RL demo - RL_Soccer,1574794865," [https://baimoro.itch.io/rl-soccer](https://baimoro.itch.io/rl-soccer) 

This is a reinforcement learning based project. You play a 1v1 with an AI agent which has been trained for 10 hours to master this simple game. 

This game is based in an environment created by me in order to test self-playing and agents with an continuous action space. I have used the TD3 algorithm to train the agents. I thought a small game where you could interact with the agents and experience there superhuman level of play would be the best way to share this project.

I hope you guys enjoy this small game.",reinforcementlearning,baimoro,False,/r/reinforcementlearning/comments/e22lu9/interactive_rl_demo_rl_soccer/
How to Solve Board Games,1574790166,,reinforcementlearning,formalsystem,False,/r/reinforcementlearning/comments/e21fq4/how_to_solve_board_games/
Clearing out some definitions,1574779289,"More specifically, iterations and episodes,  , batches and minibatches. And their usefulness in the context of RL. Is it possible for an algorithm like ppo or DQN to work extremely better episodic compared to one step learning?",reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e1ytlm/clearing_out_some_definitions/
AI Basics and Machine Learning with Python. Download cheat sheet and get a collection of ready-to-use algorithms,1574768339,,reinforcementlearning,benjamin_brook,False,/r/reinforcementlearning/comments/e1wsuj/ai_basics_and_machine_learning_with_python/
Mujoco alternatives,1574760170,"Hi guys, does any of you know of an open source simulator that has environments similar to the [OpenAi robotics ones](https://gym.openai.com/envs/#robotics)?

I hate the fact that MuJoCo is licensed and closed source, and as a researcher I do not understand how the community relies so heavily on a closed source simulator, so I would like to avoid using it for my research.",reinforcementlearning,Fragore,False,/r/reinforcementlearning/comments/e1vku3/mujoco_alternatives/
Is SARSA used in applications?,1574744715,"It seems that the SARSA RL algorithm (on-policy, value-based) is widely discussed in textbooks, but not widely used in applications.

For discrete action spaces, more papers mention Q-learning - understandable, since off-policy learning with a replay buffer can improve sample efficiency.

For continuous action spaces, more papers mention policy gradient methods - understandable, since taking the argmax of the value function learned by SARSA is no longer trivial.

Is there some area where SARSA shines that I've missed?",reinforcementlearning,jurniss,False,/r/reinforcementlearning/comments/e1t4iw/is_sarsa_used_in_applications/
Can you guys see if my concept abouts the RL taxonomy is correct or not.,1574715550,"Hi everyone, have been trying and reading about reinforcement learning from many sources (including the Sutton &amp; Burto book, David Silver's slides, Spinning Up, etc). I have also gone through different discussions here and r/MachineLearning on people getting confused between why a particular RL approach is off/on policy and workarounds to make the approach flexible. Clearly people seem sometimes at confusion due reading from multiple sources. I have my own share in that. So I would like to mention a few things about classifying basic RL approaches based on literature and books/ articles I have read. Point it out if my thought process is clear or not or wrong. 

=================================================

1. 1st Classification: Model based vs Model Free
   1. If I can have a complete Markov Description (`S,A,P,r`) of the environment `E,` then only can I use model based RL approaches to solve it. In fact I can do planning even before I go ahead and take actions in the environment. Much like MPC. Existence of the MDP information  is the only deciding factor to segregate model based and model free problems. One way to solve/plan it is via Generalized Policy Iteration 
   2. If the problem requires us to sample actions from the environment/ do trial and error on it to know about it, then we cannot say it can be solved by model based RL approach.
   3. The above **points and only the above points** can **objectively** tell us whether a problem requires model based or model free approach.
   4. My confusion: DYNAQ. What does it mean for it to learn about the environment as shown in [here](https://www.google.com/url?sa=i&amp;url=https%3A%2F%2Fmedium.com%2F%40ranko.mosic%2Fonline-planning-agent-dyna-q-algorithm-and-dyna-maze-example-sutton-and-barto-2016-7ad84a6dc52b&amp;psig=AOvVaw0nBAdATStFr2g1Odq0ED3Y&amp;ust=1574800931061000&amp;source=images&amp;cd=vfe&amp;ved=0CAIQjRxqFwoTCLiLs-GchuYCFQAAAAAdAAAAABAD). 
2. 2nd Classification: On-policy vs Off-policy
   1. The main difference is the updating the value/ action value function  
*ie* On policy: update Q value using action `a'` chosen by the target policy pi\_theta (usually an epsilon greedy approach shown in examples on ""on policy"" algorithms)  
Off-policy: update Q value using the Bellman optimality assumption. ie `a'=max_a(Q,s')`(could very well be a different way to select an action than max\_a) Here `s'` is the next state. But the action actually chosen by the target policy when doing the trial and error is according to its own policy.
   2. Second way, to classify on vs off policy (personally I find this  a little subjective): In ""on policy"" we are trying to find the value/q-value of the current policy pi\_theta. But in off policy we are using any policy with the hope that the bellman optimality will help us learn the Q\*. (But in that case what is the use of keeping the final policy nework if we know we are going to do max\_a Q\* in testing the learned agent. Recall DDPG learns a separate actor network)
3. General question: Policy gradient is on-policy because we are applying the gradient at the particular value of theta which was used to generate trajectories -&gt; which in turn were used to relearn the Q network -&gt; which is part of the policy gradient expectation equation?",reinforcementlearning,AvisekEECS,False,/r/reinforcementlearning/comments/e1mlqe/can_you_guys_see_if_my_concept_abouts_the_rl/
Job opportunities,1574709315,Please is there job  opportunities  for  reinforcement learning  engineer  or research  scientist?,reinforcementlearning,Osarenomawise,False,/r/reinforcementlearning/comments/e1kz66/job_opportunities/
Book: Foundations of Deep Reinforcement Learning,1574706280,"I'm a co-author of **Foundations of Deep Reinforcement Learning***.* We wrote this book with the aim of providing a comprehensive introduction to the field of deep RL, both in theory and in practice. It stemmed from our experience giving deep RL tutorial sessions, and it uses our SLM Lab as a companion library.

\- [Amazon link](https://www.amazon.com/dp/0135172381)

\- [Pearson (publisher) link](http://www.informit.com/promotions/black-friday-2019-buy-2-save-55-142106?utm_source=Author&amp;utm_medium=web_social&amp;utm_campaign=bf2019) (with a discount: Black Friday sale through Dec 3)

Some of the book reviews:

&gt;“This book provides an accessible introduction to deep reinforcement learning covering the mathematical concepts behind popular algorithms as well as their practical implementation. I think the book will be a valuable resource for anyone looking to apply deep reinforcement learning in practice.”  
*–Volodymyr Mnih, lead developer of DQN*  
&gt;  
&gt;  
&gt;  
&gt;“An excellent book to quickly develop expertise in the theory, language, and practical implementation of deep reinforcement learning algorithms. A limpid exposition which uses familiar notation; all the most recent techniques explained with concise, readable code, and not a page wasted in irrelevant detours: it is the perfect way to develop a solid foundation on the topic.”  
*–Vincent Vanhoucke, principal scientist, Google* 

If you have questions I can help answer, feel free to ask below.

&amp;#x200B;

*p/s. I feel this is a useful resource to share with people here, but will respect the moderator's decision to keep/flag it since it is not a free resource.*",reinforcementlearning,kengzwl,False,/r/reinforcementlearning/comments/e1k7o9/book_foundations_of_deep_reinforcement_learning/
Data Science training with real-life cases in Washington D.C. (Dec 2),1574672595,,reinforcementlearning,benjamin_brook,False,/r/reinforcementlearning/comments/e1dflr/data_science_training_with_reallife_cases_in/
Is there a library out there to easily check an environment you've made for gym compliance?,1574641967,,reinforcementlearning,MockingBird421,False,/r/reinforcementlearning/comments/e17qxs/is_there_a_library_out_there_to_easily_check_an/
LSTM And ER storing,1574543117,"I’ve implemented LSTM in a PPO implementation. I made the LSTM structure so that the State gets reshaped in the reset/step function. So that has the shape of (20,5) one batch of 20 sequences with 5 datapoints each. I do this in the env function, custom env No problem. So it gets in the LSTM which requires this shape And it works, however When it comes to the ER I am not storing in the ER for s And s_ a (20,5) shape instead of a 1d list.

 It doesn’t give me any errors about it Though, however should I only store the latest state/the actual state at t1 instead of storing t1,t2,t3,t4,t5 as it does now for 20 sequences? I am a bit confused on what to give the ER in this scenario, anyone that could help?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/e0o430/lstm_and_er_storing/
Anyone have resources discussing MPO (Maximum a posteriori policy optimization) By Abdolmaleki et. al?,1574533863,"Link to paper: https://arxiv.org/abs/1806.06920

I'm trying to wrap my head around some math concepts in the paper. Usually in these situations there is a treasure trove of online articles and discussion which present the math in different explanations, but I haven't been able to find them for this one. Anyone know any good articles / implementations I could look into?",reinforcementlearning,gamer_alien,False,/r/reinforcementlearning/comments/e0lve0/anyone_have_resources_discussing_mpo_maximum_a/
Continuous Action Spaces,1574515381,"New to RL so...

I don't get continuous action spaces. I understand DQN uses a discrete action space, and a softmaxed output to represent the probabilities of a given action. However, I fail to understand how continuous action spaces work in a neural network, and I don't really understand continuous action spaces in general. Can someone explain this to me?",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/e0hs8p/continuous_action_spaces/
HER tutorials,1574453692,Can anyone recommend the great tutorial with code for applying HER? The theory connected with practice would be great. Thank you in advance,reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/e06lxu/her_tutorials/
reviewing two PPO implentations,1574442256,"I have uploaded here my PPO implentation from scratch:

[https://github.com/MakisKans/Reinforcement\_Learning/tree/master/PPO](https://github.com/MakisKans/Reinforcement_Learning/tree/master/PPO)

In the [PPO.py](https://PPO.py) you can find two functions : learn  and   train. 

The former is used in a one step manner. The other in an episodic with GAEs.

 The former works  (tested with CartPole and LunarLander) the latter doesnt.

 I can't seem to figure out why? I don't know whether something is wrong with my GAE implentation or the rest of the algorithm is inconsistent with episodic learning. Some help would be much appreciated",reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/e03tf0/reviewing_two_ppo_implentations/
How does one train an RL agent to imitate a hardcoded policy/rules engine before allowing it to explore further and develop a better policy?,1574393525,"I'm reading the Hands on ML book SKLearn/TF and came across this ""Tip"" in the reinforcement learning section

&gt; Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should not hesitate to inject prior knowledge into the agent, as it will speed up training dramatically. For example, since you know that the pole should be as vertical as possible, you could add negative rewards proportional to the pole’s angle. This will make the rewards much less sparse and speed up training. Also, if you already have a reasonably good policy (e.g., hardcoded), you may want to train the neural network to imitate it before using policy gradients to improve it.

So now I'm curious - how would someone ""train the neural network to imitate it before using policy gradients to improve it.""?",reinforcementlearning,edmguru,False,/r/reinforcementlearning/comments/dzv7gw/how_does_one_train_an_rl_agent_to_imitate_a/
Confused on calculating the value using the Sample Average Method for the K-armed Bandit,1574369853,"I am in section 2.2 of Sutton and Barto and I am confused on how he is calculating the Action-Value using the sample average method. I am also following along with the course on Coursera Fundamentals of Reinforcement Learning and I am trying to replicate the values that are shown in the week one video Learning Action Values. 

[This](https://i.imgur.com/Ucs2WZz.png) is the example that I am trying to replicate

If I wanted to find the value for taking action Y at timestep 12 then I would do 1 + 0 + 1 + 1 / 4 = 0.75 which is the value that is given for timestep twelve and also happens to be q\*(a), However If I follow the equation then I would do    

1 + 0 + 1 + 1 / 11 which would give me 0.23. 

He also says that the reason for using t-1 in the bottom is because ""The value at time t is based on action prior to time t"". That makes sense but I don't see the relevance of that in calculating the value.",reinforcementlearning,RLAttempt2Many2Count,False,/r/reinforcementlearning/comments/dzptvo/confused_on_calculating_the_value_using_the/
"""Alphabet's Dream of an 'Everyday Robot' Is Just Out of Reach: Google's parent is infusing robots with artificial intelligence so they can help with tasks like lending a supporting arm to the elderly, or sorting trash"" [profile of Google X's trash-sorting robots/grasping arms]",1574366506,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dzoyan/alphabets_dream_of_an_everyday_robot_is_just_out/
RL Weekly 35: Escaping Local Optimas in Distance-based Rewards and Choosing the Best Teacher,1574357749,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/dzmr9c/rl_weekly_35_escaping_local_optimas_in/
Under-explored Game Genres for Reinforcement Learning Research,1574357164,,reinforcementlearning,formalsystem,False,/r/reinforcementlearning/comments/dzmm5c/underexplored_game_genres_for_reinforcement/
Mushroom - Python Reinforcement Learning library update,1574350547,"A few months ago I posted about the RL library I developed during my Ph.D.: [https://www.reddit.com/r/MachineLearning/comments/an3cqz/p\_mushroom\_reinforcement\_learning\_library/](https://www.reddit.com/r/MachineLearning/comments/an3cqz/p_mushroom_reinforcement_learning_library/). We have released a major update of Mushroom that now includes most of Deep RL algorithms (e.g. DDPG, PPO, TRPO, A2C, SAC, TD3), updated documentation, test coverage, and other improvements.

Check it out at: [https://github.com/AIRLab-POLIMI/mushroom](https://github.com/AIRLab-POLIMI/mushroom). Also, star the project if you like it since we need visibility.

Cheers!",reinforcementlearning,carloderamo,False,/r/reinforcementlearning/comments/dzkyxj/mushroom_python_reinforcement_learning_library/
How to implement LSTM in keras?,1574348266,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/dzkfxu/how_to_implement_lstm_in_keras/
"""MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model"", Schrittwieser et al 2019 {DM} [tree search over learned latent-dynamics model reaches AlphaZero level; plus beating R2D2 &amp; SimPLe ALE SOTAs]",1574295359,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dzaui6/muzero_mastering_atari_go_chess_and_shogi_by/
Why does experience replay work with q values but not policy gradients? Both produce transitions/returns that become outdated as the function approximator changes.,1574294608,,reinforcementlearning,samlerman,False,/r/reinforcementlearning/comments/dzap48/why_does_experience_replay_work_with_q_values_but/
Struggling to get REINFORCE / policy gradients working for Atari Breakout.,1574284753,"**Self-contained code**: https://colab.research.google.com/drive/1HYEXMpicymPUySkhGOaCJdJ3pN4RzXYd

The problem: I am trying to use a CNN to play Atari breakout from pixels (Breakout-V0 OpenAI gym). I am trying to use the simple policy gradients algorithm, implemented in PyTorch, to do this. There are four possible actions in this game `[&lt;NO-OP&gt;, &lt;FIRE&gt; (play), &lt;LEFT&gt;, &lt;RIGHT&gt;]`.

**Expected results**: I would expect the policy to learn to play &lt;NO-OP&gt;, &lt;LEFT&gt;, &lt;RIGHT&gt; with about equal probability, and only play &lt;FIRE&gt; on the first frame of the game.

**Actual results**: After ~4 weight updates, the network predicts ONE action with nearly 100% probability. This means the gradient vanishes and the policy never recovers.

**What I've tried:**

1. Only play random actions at the start of the game (take increasingly greedy actions that follow the policy). You can fiddle around with this in the notebook.
2. I have rewarded only one action, just to prove to myself that it learns to only play that one action. It does, so I think I can rule out any PyTorch specific implementation errors.
3. Introducing an entropy penalty to the loss to discourage high confidence in actions.

My understanding is that this should not be necessary to explicitly introduce random actions because `action = categorical.sample()` does this. And if one action becomes dominant, but does not lead to reward, it should be subsequently discouraged.

My thought was that my training batches were skewed so that taking the action &lt;RIGHT&gt;, for example, lead to more reward per episode than punishment, and hence it's likelihood kept increasing. I would have thought that taking totally random actions at the start of the game and only slowly beginning to listen to the policy would have fixed this, but in my experiments, it didn't.

I'm really very confused as to why this doesn't work. BIG thanks to anyone who can help. I tried to debug this with a PhD in RL for 5 hours yesterday and made no progress.

**Extra Questions**:

1. Is it common to play e.g. 1000 rollouts, then sample a batch randomly from this rollout buffer to learn from? My understanding of policy gradients was that the policy is updated after every episode.
2. Am I correct in thinking that in policy gradients, you only backprop through the neuron that you selected your action from, but the gradient gets distributed to ALL the network weights through the softmax?",reinforcementlearning,ynmidk,False,/r/reinforcementlearning/comments/dz8fgh/struggling_to_get_reinforce_policy_gradients/
On-policy distribution in episodic tasks with discounting,1574280399,"Hi everyone, 

I am reading Chapter 9, SB book. How to understand the last sentences when there is discounting as gamma is often used to discount feature rewards? What would be 9.2 when there is discounting.

https://preview.redd.it/r6vb4ko4ewz31.png?width=670&amp;format=png&amp;auto=webp&amp;s=ac2a077e7e1ff7121c6dae946cd134f770427dea",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/dz7iia/onpolicy_distribution_in_episodic_tasks_with/
On-policy distribution in episodic tasks with discounting,1574280224,,reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/dz7gws/onpolicy_distribution_in_episodic_tasks_with/
On-policy distribution in episodic tasks with discounting,1574279996,,reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/dz7epf/onpolicy_distribution_in_episodic_tasks_with/
Data-Efficient Hierarchical Reinforcement Learning,1574199979,,reinforcementlearning,timtody,False,/r/reinforcementlearning/comments/dyrejr/dataefficient_hierarchical_reinforcement_learning/
Data-Efficient Hierarchical Reinforcement Learning,1574193932," [https://arxiv.org/pdf/1805.08296.pdf](https://arxiv.org/pdf/1805.08296.pdf) 

  
Does anyone care to discuss?",reinforcementlearning,timtody,False,/r/reinforcementlearning/comments/dypvqc/dataefficient_hierarchical_reinforcement_learning/
Scott Fujimoto on TalkRL: Reinforcement Learning Interviews,1574178412,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/dym4qy/scott_fujimoto_on_talkrl_reinforcement_learning/
Simple Q table based RL game with GUI,1574169790,,reinforcementlearning,heisenbergq,False,/r/reinforcementlearning/comments/dykcr8/simple_q_table_based_rl_game_with_gui/
Population Based Training implementation,1574157126,"Hi, All,

  I'm wondering if anyone has implemented PBT with OpenAI's Gym and/or Baseline. In my idea, I can  customize one env and add explore/exploit functions  to it so it will work. 

  In case my idea is wrong, please give some hint. It's hard to check if this idea is correct or not until I implement and test on some golden tasks.

Thanks,",reinforcementlearning,Nicolas_Wang,False,/r/reinforcementlearning/comments/dyidns/population_based_training_implementation/
Performance of Deep Learning Frameworks,1574144414,[removed],reinforcementlearning,benjamin_brook,False,/r/reinforcementlearning/comments/dyglb0/performance_of_deep_learning_frameworks/
"Great Coursera Course on RL, Goes Through Sutton's Book",1574116293,,reinforcementlearning,RLClub,False,/r/reinforcementlearning/comments/dyarru/great_coursera_course_on_rl_goes_through_suttons/
Having trouble understanding Gaussian Policy parameterization (Sutton Barto Chapter 13),1574116029,"Hi,

I don't really follow section 13.7 of the 2nd edition of Sutton Barto book. I'm trying to do exercise 13.4, but I think I'm missing something here.

I applied ln() of both sides, to try and separate the RHS into two terms (using ln(x*y) = ln(x) + ln(y), then removing the ln(e^y) of the right term). I applied gradient wrt theta of all three terms. Now I am unsure where to go next. Any pointers or advice would be much appreciated.",reinforcementlearning,throwingaway1203,False,/r/reinforcementlearning/comments/dyapcp/having_trouble_understanding_gaussian_policy/
Sutton&amp;Barto book: I get this result for Exercise 12.1 on Eligibility traces but the final middle term might be wrong,1574081975,,reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/dy2va5/suttonbarto_book_i_get_this_result_for_exercise/
Deep Learning &amp; Reinforcement Learning Training,1574076419,,reinforcementlearning,benjamin_brook,False,/r/reinforcementlearning/comments/dy1z4l/deep_learning_reinforcement_learning_training/
Reinforcement Learning: A Declarative Paradigm for Game AI,1574020275,,reinforcementlearning,formalsystem,False,/r/reinforcementlearning/comments/dxrhj1/reinforcement_learning_a_declarative_paradigm_for/
I made a gym-like environment for the board game Santorini with accompanying Kivy App and video! Let me know what you think!,1574015246,"Hi there!

I'm the same guy as  [https://www.reddit.com/r/reinforcementlearning/comments/dkxbtl/by\_dabbling\_in\_reinforcement\_learning\_i\_made\_a/](https://www.reddit.com/r/reinforcementlearning/comments/dkxbtl/by_dabbling_in_reinforcement_learning_i_made_a/) 

I made a Gym-environment like code for a simplified version of the board game Santorini to train agents. I also made a fancier Kivy app this time to play.

This time I decided to focus on Santorini due to its simpler implementation. I also cut my project in 2 parts: the environment/app and the AI. The first part is now done so I'm curious to your opinions! If you have any tips in creating environments, also don't be shy.

Video Link:  [https://www.youtube.com/watch?v=k52yZdrnxxQ](https://www.youtube.com/watch?v=k52yZdrnxxQ) 

Github:  [https://github.com/TheGameBotYT/SantoriniAI](https://github.com/TheGameBotYT/SantoriniAI)",reinforcementlearning,TheGameBotYT,False,/r/reinforcementlearning/comments/dxqa71/i_made_a_gymlike_environment_for_the_board_game/
Confusion on evaluating a policy using the bellman expectation equation,1574009481,"I am watching David Silver RL course and I am trying to compare the Bellman equation he mentions on the board with the one he provided in the homework assignment to do DP policy evaluation. 

The equation he provides on the board is the Bellman expectation equation which is: 

&amp;#x200B;

https://preview.redd.it/jflpzk4yv9z31.png?width=468&amp;format=png&amp;auto=webp&amp;s=05fa9ad0a41018261db502c17e49a2b870112e1a

I was able to solve the assignment doing: 

v += action\_prop \* (reward + (discount\_factor \* (prop \* V\[next\_state\])))

However, when I looked at the solution I noticed that the answer has the probability (prop) of arriving at the next state outside instead of having it inside multiplying by the value function like the equation on the board suggests. 

v += action\_prob \* prob \* (reward + discount\_factor \* V\[next\_state\])

In this case it does not matter because the prop is 1, however, if the prop was another number then the results would be different. I was wondering why he did this. 

I did see that he referenced equation 4.6 in the Sutton book which does follow the convention he used of putting the prop outside when finding the q(s, a) value of a given policy. 

I am still wondering why the equation on the board puts the prop \* value function together inside if in fact in order to calculate the equation it would need to go on the outside.",reinforcementlearning,FireStory,False,/r/reinforcementlearning/comments/dxoxmf/confusion_on_evaluating_a_policy_using_the/
Softmax never crossing?,1573992939,"&amp;#x200B;

*Processing img qx62zf9cn8z31...*

&amp;#x200B;

In PPO i am trying to converge on a gym environment, however as you can see the softmax output never croses even though rewards change, same for the state. I tried diffrent settings such as higher LR, which would give me somewhat more noise, but still never crossing, how comes?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/dxlv5n/softmax_never_crossing/
"Should I use a 3D Engine to simulate an environment for agents, and if so which one?",1573986517,"I am trying to implement a reinforcement learning environment such as [this](https://youtu.be/NNbyiXTAOkM) one (sorry, the video is German) or [this](https://youtu.be/r_It_X7v-1E) (this one is English).  

The goal for the agents would be to survive and have offspring in a nature like environment. 

I already have experience with Java, a bit C++ and Python.

How would you tackle this problem, should I use an engine such as Unity or should I try to implement it in something like Monogame or Libgdx.

Thank you.",reinforcementlearning,wdjpng,False,/r/reinforcementlearning/comments/dxl0hz/should_i_use_a_3d_engine_to_simulate_an/
RL Algorithms Implementation Study Group,1573945666,"Hello everyone, hope all of you are having a great day. My name is Costa Huang and I am a computer science Ph.D. student at Drexel University. I was wondering if anyone would be interested in joining my RL Algorithms Implementation Study Group. I am interested in implementing some SOTA RL algorithms such as TD3, SAC, and Rainbow; and I hope to find people with similar interests to work together.

For anyone who has implemented an RL algorithm from scratch, they probably would have found that the algorithm always doesn't work right off the bat. There are so many crucial components of RL that if any of them went wrong, the algorithm will just fail, leaving very little explanation. And when they try to find resources online to help with the debugging process, those resources are always not directly helpful because their codes are structured differently or using different tricks; and there are left with their buggy algorithm and trying to smash their head against the wall...

It probably would be wonderful if a group of people works on the same algorithm at the same time, using the same code base and structure. Because everyone is working on the same problem, it will be easier to communicate and help each other debug. Here I propose an RL implementation study group that will focus on implementing one SOTA RL algorithm every month. Here is a rough schedule of our monthly tasks:

Week 0
    * Logistics
        * Have our first-week meeting on Google Hangouts and introduce ourselves to each other.
        * Set up a slack channel that will help communicate.
        * I will be demoing the common code base CleanRL(https://github.com/vwxyzjn/cleanrl) that we will be using as a base for our implementation. The demo will showcase its excellent hackability, readability, and great experiment management.

Every Month Forward:
    * Week 1 (Literature week)
        * We will announce our interested/voted SOTA RL algorithm on slack for everyone to read. For the first week, let's say TD3. Everyone is welcome to suggest interested SOTA RL algorithms.
        * If you want, you could also start with the implementation
        * Have our weekly discussion leader to host the conversation on Google Hangouts. He/she will do a presentation on the algorithm and everyone can ask questions and discuss.
    * Week 2 (Implementation week)
        * A list of gym environments will be announced; half of which have discrete action space and the other half continuous action space. People can sign up for implementing one or more of those envs, but preferably one.
        * Have our weekly discussion on Google Hangouts and people who made the algorithm work will be invited to do a presentation to show their source code and talk about the difficulty that faced and how they overcame it.
    * Week 3 (Implementation week continued)
        * People will be asked to make the algorithm work for *discrete and continuous action spaces*, generalizing the algorithm. 
        * Have our weekly discussion on Google Hangouts and people who made the algorithm work will be invited to do a presentation.
    * Week 4 (Integration week)
        * We will benchmark these implementations (much like http://cleanrl.costa.sh/) on a variety of gym environments (20+, continuous and discrete action spaces) and integrate the best and most stable to the CleanRL library.
        
Some notes:
* You should probably be dedicating a solid 3 hours+ per week to gain something out of this.
* It's fine if you just want to join to see what is going on, but if you are willing to commit 3 hours+ per week with this group, please PM me and I will add you to a private channel for core members.
* If you have any feedback to make this a better study group, please let me know and I will adjust accordingly.

Finally, if you are interested, please join through https://join.slack.com/t/rlimplementation/shared_invite/enQtODI0NzczNjIyMTE0LTc4MTljYjg5NzFmZWM0ZjJhNDJjZDc4ZjVmYTJmZGRmOTU1MDk3MDY0ZTVlZjQ3NjA2YjFjMTQwMTdiOTdjYjA

Thanks.",reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/dxebbp/rl_algorithms_implementation_study_group/
Contextual Bandits and Supervised Classification,1573877093,"If I don't need a online model and if I am willing to wait to collect training data and then train a Supervised classifier, Do I need a contextual Bandit (CB)

Is a CB formulation equivalent to supervised classifier if trained offline? 

When a CB formulation should be used? When it is an overkill and just a classifier will do good.",reinforcementlearning,bluedunnock,False,/r/reinforcementlearning/comments/dx1ylf/contextual_bandits_and_supervised_classification/
Policy Gradient code review,1573860164,"I've been trying to implement classic polcy gradient and the variant with the baseline. However I've had certain diffculties evaluating my code with the CartPole environment. PG vanilla seems to converge after much too many episodes(compared to DQN at least) and even then it's with plenty tuning. On the other hand the baseline variant doesn't converge at all. (200 is max score in cartpole). Could someone please review my code-either one will do- so as to make sure that at least my implementation is correct or not, maybe it's the parameters? My code is here:

[https://github.com/MakisKans/Reinforcement\_Learning/tree/master/PGwbaseline](https://github.com/MakisKans/Reinforcement_Learning/tree/master/PGwbaseline)

If this isn't the place for this could you please point me to a relevant hub?",reinforcementlearning,MaximKan,False,/r/reinforcementlearning/comments/dwyqdp/policy_gradient_code_review/
DeepMind Research Lead Doina Precup On Reinforcement Learning,1573847668,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/dwvy8o/deepmind_research_lead_doina_precup_on/
RTX 2070 SUPER is slower than GTX 860M,1573837598,"I am sorry to bother you again with GPU problem but I am desperate. I have installed Tensorflow requirements according this: [https://www.tensorflow.org/install/gpu#ubuntu\_1804\_cuda\_10](https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_10)

I ve bought new PC with I7-9700 and Geforce RTX 2070 SUPER, but after executing same program (SAC from SpinningUP) the old laptop (geforce 860 &amp; i7-4xxx) is faster than my new PC, which is totally frustrated for me. What am I doing wrong? I use CUDA 10.1, could this be a problem?",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/dwto86/rtx_2070_super_is_slower_than_gtx_860m/
Episodes in Reinforcement Learning,1573830714,What is the best way to start a new episode after reaching a terminal state in an RL environment? Is it a good idea to start from the same observation space or location for each episode?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dws2ij/episodes_in_reinforcement_learning/
State space configuration,1573809119,Im currently using sac to balance a ball on a plate. First I added the position and target location to the state space. After training with this setup I tried with adding the deviation from the target (Xball - Xtarget) instead. This seems to speed up training a lot and it seems that the training curve is a lot smoother. Does anybody know why this is the case? Is the representation of the data better which makes it easier?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/dwo90d/state_space_configuration/
Training a pong agent with 2 actions,1573777774,"So, I recently read well known Karpathy's blog where he explains basics of RL and he posted link to his code there. He uses a convolutional neural network as a function approximator with 1 output node with sigmoid activation function. He uses that output to decide whether the action should be ""go up"" or ""go down"" . I tried to implement similar thing but instead of 1 output node, I have 2(one for each action) and softmax activation function. The thing is that my agent diverges and I always end up with agent that learned nothing and just stands in the bottom or top corner doing nothing. Did anyone tried to do it this way? Any link to git repo with implementation in python 3 is welcome.",reinforcementlearning,nspasic96,False,/r/reinforcementlearning/comments/dwiohw/training_a_pong_agent_with_2_actions/
Replay Buffer in off Policy Reinforcement Learning,1573764853,"In the off-policy RL replay buffer acts as a core parameter in order to train the RL agent. There have been several recent research that has tried several different kinds of replay buffer like cyclic replay buffer, priority replay buffer, hindsight replay buffer. Which kind of replay buffer is the most prefered and leads to the best optimal training of the RL agent?",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dwfod9/replay_buffer_in_off_policy_reinforcement_learning/
Deep Reinforcement Learning Portfolio Management,1573763476,"I have trained a risk-sensitive Deep Reinforcement Learning agent for Financial Portfolio-Management on Forex and US-Stock markets. The following presentation investigates how the agent learnt an expert-like market-neutral pair-trading strategy and applied that on a large-scale (using all stocks in S&amp;P 500) successfully on a blind test-set.

[https://www.slideshare.net/KamerAliYuksel/deep-reinforcement-learning-portfolio-management](https://www.slideshare.net/KamerAliYuksel/deep-reinforcement-learning-portfolio-management)",reinforcementlearning,kyuksel,False,/r/reinforcementlearning/comments/dwfcoz/deep_reinforcement_learning_portfolio_management/
"Nov 21, Free Talk on PyTorch with Its Co-Author and Maintainer, Adam Paszke",1573741578,,reinforcementlearning,ACMLearning,False,/r/reinforcementlearning/comments/dwa85n/nov_21_free_talk_on_pytorch_with_its_coauthor_and/
Reinforcement Learning Slides November 2019,1573741353,"Reinforcement Learning Slides November 2019

By Nando de Freitas, @DeepMind 

Courtesy: khipu.ai 

Slides:

https://drive.google.com/file/d/1kPc3fyOzt0I3Sdwt5EgHH5Bsn1Ng-h11/view",reinforcementlearning,aiforworld2,False,/r/reinforcementlearning/comments/dwa6hn/reinforcement_learning_slides_november_2019/
"I'm doing my undergrad thesis on RL (under a certain constraint), what's the ""standard"" set of environments used to benchmark in the literature, is it still atari?",1573722337,,reinforcementlearning,PlymouthPolyHecknic,False,/r/reinforcementlearning/comments/dw6xr5/im_doing_my_undergrad_thesis_on_rl_under_a/
"""Compressive Transformers for Long-Range Sequence Modelling"", Rae et al 2019 {DM}",1573704511,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dw3zbt/compressive_transformers_for_longrange_sequence/
Reward Shaping,1573682146,How does low reward affect the performance and training of an RL Agent? I have seen works that normalize reward to 0-1 and there are some works that use discrete large rewards for each step.  I wonder how this difference affects the training?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dvz25n/reward_shaping/
GPU Tensorflow configuration,1573675699,"I ve bought a new computer with Geforce RTX 2080 Super but I am afraid that I have not configure Tensorflow-gpu correctly. I used this example from tensorflow-models ([https://stackoverflow.com/questions/35703201/speed-benchmark-for-testing-tensorflow-install](https://stackoverflow.com/questions/35703201/speed-benchmark-for-testing-tensorflow-install)) and I got 6ms. Then I have selected in *Nvidia X Server Settings -&gt; powermixer settings =* Prefer Maximum Performance and because of that I have got 4ms. However, as you can see at stackoverflow link in the comments, people with worse GPU got performance under 3ms. I have installed all things (except pip3 install tensorflow-gpu) according this: [https://www.tensorflow.org/install/gpu#install\_cuda\_with\_apt](https://www.tensorflow.org/install/gpu#install_cuda_with_apt)

 What can be wrong? Thank you in advance",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/dvxfxo/gpu_tensorflow_configuration/
"""RoboTurk: Human Reasoning and Dexterity for Large-Scale Dataset Creation"" [smartphone app for crowdsourcing robot demonstrations in manipulation tasks]",1573675388,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dvxczx/roboturk_human_reasoning_and_dexterity_for/
Reinforcement Learning for Autonomous Driving,1573656306,Why is it difficult to learn end to end driving using only visual inputs for an RL agent?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dvsqm2/reinforcement_learning_for_autonomous_driving/
Searching for paper learning multiple value functions for different return depths/ number of steps,1573641007,"Hi, a few weeks ago I saw a really interesting video from MILA about an RL approach, that learn multiple value functions, for different return-lengths, i.e. one learns the expected one step reward, the next the expected two step reward etc.

However I can not find the paper nor the video anymore.

Does anybody know what I'm talking about and can send provide a link?

I've been searching for a while and would be very grateful for some help.",reinforcementlearning,Mefaso,False,/r/reinforcementlearning/comments/dvpwwn/searching_for_paper_learning_multiple_value/
How to choose an action a* such that the chances of it inducing a given state change s* is maximum,1573637049,"I am considering only a discrete action and state space. And during training, I create a matrix where the rows represent the action, and columns represent all possible values of state change. By state change, I mean if action a takes agent from s1 to s2, then state change is s2-s1.   


So, from this matrix I kind of have a joint probability distribution between action and state change. Now, I wish to induce a certain state change s\*, which action should I choose?  


Intuitively, I guess I choose a\* by maximizing P(s\*|i) for all i in the set of actions. Can this be proved somehow? Can you point me to some rigorous theory that explains this bit. Anything else I could look into? My probability is a bit rusty, any help would be great. Essentially, I am looking for 2 things ---(1) is my intuition right? (2) If yes, how can I prove it?",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/dvpaod/how_to_choose_an_action_a_such_that_the_chances/
Citation needed,1573636148,Can anybody cite me some article from around 2005 talking about how value approximation with nonlinear function approximation doesn't work yet.,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/dvp5qp/citation_needed/
[AI application with source code] Let your machine play Street Fighter!,1573600425,,reinforcementlearning,1991viet,False,/r/reinforcementlearning/comments/dvid24/ai_application_with_source_code_let_your_machine/
Jessica Hamrick of Deepmind on TalkRL: Reinforcement Learning Interviews,1573591538,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/dvg1ol/jessica_hamrick_of_deepmind_on_talkrl/
Online group that focuses on learning reinforcement learning?,1573583075,"Hey there, I'm wondering if there's a group for people who are learning reinforcement learning? I was trying to implement deep Q-learning the other day and I was having trouble with it. I thought it would be nice if someone can look at my code and see what's wrong with it, and that makes me wonder if there's already a group out there for this kind of stuff. Since I'm also a beginner in this field, it can also be a good opportunity to make friends with other people who are working toward the same goal. Sorry if this isn't the right place to post this kind of thing, I really have no idea where to even begin haha.",reinforcementlearning,polaricebear,False,/r/reinforcementlearning/comments/dvdzka/online_group_that_focuses_on_learning/
"""Multiplayer AlphaZero"", Petosa &amp; Balch 2019",1573517821,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dv1olw/multiplayer_alphazero_petosa_balch_2019/
Creating a DQN for a game and want to know the best way of getting a screenshot image of the screen.,1573435756,So I have a game I created and want a screen shot of the game screen for my cnn. What is the best way of doing it. I am probably going to be taking a continuous stream of  screen shots for the input but I don't know the best way to approach this.,reinforcementlearning,pitin753,False,/r/reinforcementlearning/comments/dukyc6/creating_a_dqn_for_a_game_and_want_to_know_the/
State embeddings for reinforcement learning,1573428374,"Hi all, I’ve been learning deep reinforcement learning for about a year now with just a laptop (i7 with nvidia GeForce 650M) and I have had some success but still mixed results. I’ve been trying to implement a simple Neural Episodic Controller, and according to the paper they use state embeddings to make observations smaller. I have trained convolutional autoencoders before. Do these use the same type of model where state_t is the input and state_t+1 is the target data?",reinforcementlearning,wiltors42,False,/r/reinforcementlearning/comments/dujbub/state_embeddings_for_reinforcement_learning/
Reinforcement Learning Club,1573414268,I am a 5th year undergrad physics major with prior machine learning experience taking an extra year and trying to pursue research in reinforcement learning. I have been having a hard time though because I have no one to talk to about rl or people to collaborate with on ideas. I want to solve this by making an rl club for people serious about rl and who have a strong background in math/stats/etc. needed to understand the papers. Please message me if you are interested with some quick info about your background.,reinforcementlearning,RLClub,False,/r/reinforcementlearning/comments/dug2ey/reinforcement_learning_club/
Anyone know what could have happened to cause my agent to stop learning so rapidly?,1573409215,,reinforcementlearning,floridianfisher,False,/r/reinforcementlearning/comments/duevc3/anyone_know_what_could_have_happened_to_cause_my/
Should I buy new PC?,1573394373,"Currently I am working on RL project within robotics. I would like to use SAC algorithm to learn robotic arm. I am currently using laptop (GTX 860m and I7-4710 CPU). For example the solving HalfCheetah (from openai gym) takes cca 30 minutes (no parallel agents, no rendering). However, for my project I will need rendering and robotics environments are much more complex than HalfCheetah (sparse rewards, etc). Thats why I am thinking about buying PC for 2k euros (I7-9700K + RTX 2080). Because the fact that it cost 2000 euros, I am worried that I am making bad decision. Will the new computer with these parameters solves RL problems much faster or will I just waste a lot of money?",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/dubqs2/should_i_buy_new_pc/
TensorLayer Team Released Reinforcement Learning Algorithm Baseline-RLzoo,1573340828,"Recently, in order to enable the industry to better use the cutting-edge reinforcement learning algorithms, the TensorLayer Reinforcement  Learning Team has released a complete library of reinforcement learning  baseline algorithms for the industry — RLzoo. TensorLayer is an extended  library based on TensorFlow for better supports of basic neural network  construction and diverse neural network applications. The RLzoo project  is the first comprehensive open source algorithm library with  TensorLayer 2.0 and TensorFlow 2.0 since the release of TensorFlow 2.0.  The library currently supports OpenAI Gym, DeepMind Control Suite and  other large-scale simulation environments, such as the robotic learning  environment RLBench, etc.

Link of full post: [https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904](https://medium.com/@zhding96/tensorlayer-team-released-reinforcement-learning-algorithm-baseline-rlzoo-2663cfb77904)

Link of RLzoo: [https://github.com/tensorlayer/RLzoo](https://github.com/tensorlayer/RLzoo)

Link of RL tutorial: [https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement\_learning](https://github.com/tensorlayer/tensorlayer/tree/master/examples/reinforcement_learning)

Slack group: [https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7](https://app.slack.com/client/T5SHUUKNJ/D5SJDERU7)",reinforcementlearning,quantumiracle,False,/r/reinforcementlearning/comments/du3bl5/tensorlayer_team_released_reinforcement_learning/
RTX 2080 and i7-900 for reinforcement learning?,1573332659,"Does anyone have experience with using RTX2080 for Tensorflow purposes? Will it work with this GPU? Currently I am using GTX 860m and with using more complex algorithms and environments, tensorflow is not computing. Is this normal behaviour? Thank you in advance",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/du1jdy/rtx_2080_and_i7900_for_reinforcement_learning/
Masking invalid actions in Stable Baselines PPO model,1573330295,"I'm trying to use RL to play a card game (currently Spades but other games as well). I've got the environment/rules of the game set up, but a major problem is that I can't mask invalid actions before they come out of the model. If I declare my action space as Discrete(52) (one action for each card in the deck), obviously I can't play a card if it isn't in my hand, but the model simply outputs a single action rather than the probabilities. So I can't mask the actions while using the simple model.learn() method. Has anyone dealt with this and what would you recommend?",reinforcementlearning,MasterExchange6,False,/r/reinforcementlearning/comments/du11b4/masking_invalid_actions_in_stable_baselines_ppo/
"How to precisely compute the training curve of the ""average score"" of an RL algorithm?",1573268289,"Hi all, 

To report the performance of a RL algorithm, one common metric is to plot the training (learning) curve of the algorithm's per-episode average score, for example, the following two figures. My question is whether this per-episode average score reported in these figures is computed from ""training"" episode or from validation episode? By training episode, I mean that during the process of training, we collect the score for each episode, average these score over say the last 100 training episodes and report this score as one point in the graph. By validation episode, I mean that at a training epoch, we frozen the training, evaluate the trained agent at this point for say 100 episodes, average the scores and report it as one point in the graph.  I guess the former is more likely the case by looking at the OpenAI baseline code but have never actually officially confirmed it because I did not find any text explicitly indicating this; all I could find is something along the line of the descriptions in the captions of the two figures below.  

&amp;#x200B;

![img](ttx8l8ixkkx31 ""A screenshot of Figure 2 from [1] ."")

&amp;#x200B;

![img](6cwljkozkkx31 ""A screenshot of Figure 3 from [2]. "")

Could someone help clarify this for me? Many thanks.  

\-Best 

**References**:

\[1\] Mnih et al., Human-level control through deep reinforcement learning, 2015. 

\[2\] Bellemare et al., A Distributional Perspective on Reinforcement Learning, 2017.",reinforcementlearning,conan279,False,/r/reinforcementlearning/comments/dtqajq/how_to_precisely_compute_the_training_curve_of/
Anyone interested in remote collaboration on RL theory problems?,1573266186,"I am a junior PhD student interested in RL theory and bandits. Things like provable convergence, sample complexity, regret bound etc. anyone interested in remote collaboration/reading group or something?",reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/dtpxa1/anyone_interested_in_remote_collaboration_on_rl/
SAC looking for theory in SpinningUp code,1573243578,"Hello, currently I am working on SAC implementation based on SpinningUp. However, I do not understand few things about the code in terms of looking for theoretical equations inside the code. Hope someone can help me.

1. **q1\_pi and q2\_pi:** I know that we use 2 Q-functions as it is in case of TD3 and thats why I have **q1** and **q2**. But I do not know what exactly mean **q1\_pi** and **q2\_pi**. In the code there is an explanation which I do not understand:

\`Gives the composition of \`\`q1\`\` and \`\`pi\`\` for states in \`\`x\_ph\`\`:  q1(x, pi(x)).\`

What exactly is the difference between **q1** and **q1\_pi**? I have found out that we use min(q1\_pi, q2\_pi)  for computing y(v) what I believe is target ( ""desired value function"" that is used for updating target value function)

[Compute targets for Q and V](https://preview.redd.it/cc2c8ir8pix31.png?width=749&amp;format=png&amp;auto=webp&amp;s=0a7329adfc0817e7f090496b1c9e0a9311cda030)

2. Why do we multiply q1\_loss, q2\_loss and v\_loss by 0.5? And why do we put all losses into one, although we have 3 different neural network for each?

\`q1\_loss = 0.5 \* tf.reduce\_mean((q\_backup - q1)\*\*2)

q2\_loss = 0.5 \* tf.reduce\_mean((q\_backup - q2)\*\*2)

v\_loss = 0.5 \* tf.reduce\_mean((v\_backup - v)\*\*2)

value\_loss = q1\_loss + q2\_loss + v\_loss\`

3. In learning process I should use stochastic behavior (so action = pi) and after training switching it to deterministic behavior where action = mean (gaussian policy)

Thank you in advance and feel free to correct me, I would really like to understand this algorithm in details.",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/dtky90/sac_looking_for_theory_in_spinningup_code/
Reinforcement Learning and Optimal Control Volume 1 by D. Bertsekas,1573217461,"As part of my curriculum I am taking a class this semester using D. Bertsekas' book Reinforcement Learning and Optimal Control. Unfortunately in Greece it tends to take months until we are given books, and even then many books are irresponsibly translated, to the point where the English version is preferable. Usually it isn't too hard to find the English version of book on the internet, and although I've found the summary-like draft version of the book, getting my hands on the full version (volume 1) seems to be impossible. 

Could someone be so kind as to give steps or link it here?",reinforcementlearning,computo2000,False,/r/reinforcementlearning/comments/dtf023/reinforcement_learning_and_optimal_control_volume/
Loss-To-Domain,1573214302,Would it be possible to determine the difficulty of states and create new ones based on a higher loss value to retrain on by feeding the loss of a policy network into another model which attempts to map losses to states in ascending difficulty?,reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/dtefil/losstodomain/
How to assign reward when it has to be multiplied by itself rather than summed,1573208614,"How should I assign reward when it has to be multipied by itself rather than summed?

Normally, in all environments I used of OpenAI Gym the total reward can be calculated as

`tot_reward = tot_reward + reward`

where `_, reward, _, _ = env.step(action)`. Now I'm defining a custom environment where

`tot_reward = tot_reward * reward`

In particular, my reward is the next-step portfolio value after a trading action, so it is &gt; 1 if we have a positive returns, &lt; 1 otherwise. How should I pass the returns to the training algorithm? Currently I'm returning `1 - reward` so that we have a positive number in case of a gain, a negative one in case of a loss. Is this the correct way to tackle the problem? How it is treated normally in the literature? Thank you",reinforcementlearning,basso1995,False,/r/reinforcementlearning/comments/dtdikn/how_to_assign_reward_when_it_has_to_be_multiplied/
How to assign reward when it has to be multiplyed by itself rather than summed,1573208008,"How should I assign reward when it has to be multipied by itself rather than summed?

Normally, in all environments I used of OpenAI Gym the total reward can be calculated as

`tot_reward = tot_reward + reward`

where `_, reward, _, _ = env.step(action)`. Now I'm defining a custom environment where

`tot_reward = tot_reward * reward`

In particular, my reward is the next-step portfolio value after a trading action, so it is &gt; 1 if we have a positive returns, &lt; 1 otherwise. How should I pass the returns to the training algorithm? Currently I'm returning `1 - reward` so that we have a positive number in case of a gain, a negative one in case of a loss. Is this the correct way to tackle the problem? How it is treated normally in the literature? Thank you",reinforcementlearning,basso1995,False,/r/reinforcementlearning/comments/dtdf5q/how_to_assign_reward_when_it_has_to_be_multiplyed/
Non-determinism in Atari,1573159681,"I've noticed a switch away from non-determinism in Atari. For example, OpenAIs baselines use a [fixed frameskip](https://github.com/openai/baselines/blob/7c520852d9cf4eaaad326a3d548efc915dc60c10/baselines/common/atari_wrappers.py#L97-L103) of 4 instead of sticky actions, and only a random 30-NOOP reset for non-determinism. Uber got a lot of [heat](https://www.alexirpan.com/2018/11/27/go-explore.html) in the past for disabling sticky actions, so I'm just wondering what current best practices is for Atari. Is SOTA still using sticky actions, or have we moved past this, and if so why?",reinforcementlearning,VirtualHat,False,/r/reinforcementlearning/comments/dt40s1/nondeterminism_in_atari/
Credit assignment problem,1573125453,Can anything concrete be said about how modern model free algorithms deal with the credit assignment problem?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/dswbpe/credit_assignment_problem/
Learning methods for controlling a quadruped robot,1573112790,"I am working on a quadruped project. Our goal is to perform gait planning for this robot. Until now we have been doing this with solvers and non-linear optimizer. We are planning to move to learning methods for performing the gait planning. Many projects nowadays are moving to reinforcement learning for motion planning and I am new to reinforcement learning. I want to know what would be the general advantage in using Reinforcement learning in performing motion planning than the conventional methods that are available now. 

I am currently working in opeanai\_ros package to get started with the interface of RL algorithms with ROS for the robot. But during the later stages of the project I feel that openai won't be capable of performing complex gait planning tasks. Is tensorflow a better alternative for this. If it is, then is there an existing interface between ros and tensorflow like that of openai\_ros package.

And also, I want to train the robot in the gazebo environment, but many suggested Pybullet or MuJoCo. I have never worked in either of these two environments. Is the physics engine faster or more accurate or both when compared to Gazebo?",reinforcementlearning,hari191197,False,/r/reinforcementlearning/comments/dsuiun/learning_methods_for_controlling_a_quadruped_robot/
"""DADS: Dynamics-Aware Unsupervised Discovery of Skills"", Sharma et al 2019 {GB}",1573090953,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dsqc9l/dads_dynamicsaware_unsupervised_discovery_of/
"""Computers Evolve a New Path Toward Human Intelligence: Neural networks that borrow strategies from biology are making profound leaps in their abilities. Is ignoring a goal the best way to make truly intelligent machines?"", Quanta [novelty search &amp; PBT]",1573075338,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dsmt6l/computers_evolve_a_new_path_toward_human/
Problem on tracking agents trajectory in CoinRun as a video,1573068527,"Hello,
currently I'm implementing PPO and I'd love to see the agent playing in the environment. Sadly I'm using Windows as OS and CoinRun only supports linux thus I have to use google collab. I've attempted various things to record a video of the agents gameplay i.e. using wrappers for the environment like VecVideoRecorder, Monitor or collecting a trajectory of states in a list of RGB images and convert them as a video. All in all I failed miserably. I appreciate any help.

links:
coinrun: https://github.com/openai/coinrun
google collab coinrun: https://colab.research.google.com/drive/1e2Eyl8HANzcqPheVBMbdwi3wqDv41kZt
example code for an agent: https://github.com/openai/coinrun/blob/master/coinrun/random_agent.py
Monitor: https://github.com/openai/gym/blob/master/gym/wrappers/monitor.py
VecVideoRecorder: https://github.com/openai/baselines/blob/6e607efa905a5d5aedd8260afaecb5ad981d713c/baselines/common/vec_env/vec_video_recorder.py",reinforcementlearning,Horrible22232,False,/r/reinforcementlearning/comments/dsl8j2/problem_on_tracking_agents_trajectory_in_coinrun/
Which policy lead to large norm of Fisher Information Matrix in MDP?,1573048244,"Does deterministic policy lead to large norm of Fisher Information matrix or it leads to singular FIM? 

Reasoning 1: 

Deterministic policy means that entropy is close to zero. It means that gradient of log policy converges to 0, thus Fisher information matrix is almost singular matrix.

Kakade. Natural Policy gradient. page 5

Reasoning 2:

For deterministic policy we have that small changes in parameter will lead to larger changes in the KL-divergence. Also we know that Fisher matrix measures the curvature of the KL term, then Fisher matrix norm will be large. 

DeepMind, Progress &amp; Compress. page 5

Thus, I have two statements that contradict each other. Where do I mistake?",reinforcementlearning,shagadatov,False,/r/reinforcementlearning/comments/dsgkfv/which_policy_lead_to_large_norm_of_fisher/
Which policy lead to greater Fisher information matrix?,1573047540,"Does deterministic policy lead to greater norm of Fisher Information matrix? 

Reasoning 1: 

Deterministic policy means that $H(\pi (\cdot \mid s_t, \theta))$ close to zero. It means that $\nabla_{\theta} \log \pi(a_t \mid s_t, \theta) \rightarrow 0$, thus Fisher information matrix is almost singular matrix.

Reasoning 2:

For deterministic policy we have that small changes if parameter $\theta$ will lead to larger changes in the KL-divergence: $KL(\pi (\cdot mid s_t, \theta) \| \pi (\cdot \mid s_t, \theta + \Delta \theta))$, we know that Fisher matrix measures the KL curvature of the KL term, then Fisher matrix norm will be large. 

Where do I mistake?",reinforcementlearning,shagadatov,False,/r/reinforcementlearning/comments/dsgfbz/which_policy_lead_to_greater_fisher_information/
State defined as change in image of game between frames?,1573006672,"In the PyTorch DQN example code ([https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)) it has this line:

&amp;#x200B;

`state = current_screen - last_screen`

&amp;#x200B;

Why is that?",reinforcementlearning,MockingBird421,False,/r/reinforcementlearning/comments/ds9nfh/state_defined_as_change_in_image_of_game_between/
Is summing the Q values the same as finding the value for a state?,1573001053,"I had had two questions that I’m not 100% sure about: 

&amp;#x200B;

The first is if the bellman expectation equation

v(s) = E\[Rt+1 + γV(st+1) | st = s\]

Can be rewritten as: 

V(s) = \[R(s,a) + γΣp(s, a, s’)v(s’)\]

So is it pretty much is if the Expectation just doing Σp(s, a, s’)v(s’)?  If so does that mean in order to get the average I need to divide by that number of values as well? 

&amp;#x200B;

And the second is if summing all the q values for a state the same as finding the value v(s’)? 

Can the Bellman equation be rewritten as : 

V(s) = \[R(s,a) + γΣp(s, a, s’)q(s,a)\]",reinforcementlearning,FireStory,False,/r/reinforcementlearning/comments/ds8hga/is_summing_the_q_values_the_same_as_finding_the/
I put my reinforcement learning resources on GitHub,1572988994,,reinforcementlearning,ADGEfficiency,False,/r/reinforcementlearning/comments/ds5t1d/i_put_my_reinforcement_learning_resources_on/
Looking at the Fundamentals of Reinforcement Learning,1572931614,"Hi, if you're interested in reading up on the fundamentals of RL, check out my new blog post, Looking at the Fundamentals of Reinforcement Learning! It covers MDPs, policies and value functions, Bellman equations and optimality, and a couple of classical RL methods. Cheers.

 [https://jfpettit.github.io/blog/2019/11/03/fundamentals-of-reinforcement-learning](https://jfpettit.github.io/blog/2019/11/03/fundamentals-of-reinforcement-learning)",reinforcementlearning,jcobp,False,/r/reinforcementlearning/comments/drureh/looking_at_the_fundamentals_of_reinforcement/
"""Generalization of Reinforcement Learners with Working and Episodic Memory"", Fortunato et al 2019",1572909665,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/drq5pv/generalization_of_reinforcement_learners_with/
Why is everyone using shallow neural networks?,1572906851,"Hello everyone,

I am a RL research student and I am from a Deep Learning theoretical background. In all other fields of DL, we usually find that ""deeper is better"", as depth allows for much higher function expressiveness than width.

However, for as long as I have been studying Deep Reinforcement Learning, I have always found people using shallow networks (usually 2 layers) instead.

Do you have explanations or intuitions for this, please?",reinforcementlearning,yannbouteiller,False,/r/reinforcementlearning/comments/drph2m/why_is_everyone_using_shallow_neural_networks/
"""Meta-World: A Benchmark and Evaluation for Multi-Task and Meta Reinforcement Learning"", Yu et al 2019",1572894206,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/drmb5x/metaworld_a_benchmark_and_evaluation_for/
Image Captioning based on Deep Reinforcement Learning,1572893301,"Hello guys. I'm currently working on a project titled ""Image Captioning based on Deep Reinforcement Learning"" by Haichio Shi et al 2018. Here is the link to the paper. Can anybody help me out in finding the source code of that project please? 

https://arxiv.org/abs/1809.04835",reinforcementlearning,raghu_1809,False,/r/reinforcementlearning/comments/drm371/image_captioning_based_on_deep_reinforcement/
Dopamine author Pablo Samuel Castro on TalkRL: Reinforcement Learning Interviews,1572882479,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/drjehy/dopamine_author_pablo_samuel_castro_on_talkrl/
Policy gradient algorithm with multiple actions?,1572872800,"Is there a policy gradient method where agent performs multiple actions?

As far as I know a policy network's output in a policy gradient method is normally distributed and from that distribution the agent takes best action. I want to know whether  a policy gradient agent can learn and simultaneously perform multiple actions (sigmoid output instead of softmax) and how?",reinforcementlearning,begooboi,False,/r/reinforcementlearning/comments/drhd11/policy_gradient_algorithm_with_multiple_actions/
Exploration steps,1572863619,In a lot of implementations of off policy algorithms you first start with a certain amount of exploration steps to fill your memory buffer with. What amount of exploration steps is generally recommended? Are there any rules of thumb?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/drfu8r/exploration_steps/
"Implementation of distributed reinforcement learning algorithms with Ray and PyTorch (Ape-X, D4PG)",1572848412,"I would like to share my implementation of distributed RL algorithms, currently including Ape-X DPG, Ape-X DQN, and D4PG. The implementations use [Ray](https://github.com/ray-project/ray) instead of python-multiprocessing for parallelization, which significantly simplified the implementation process for me.

[https://github.com/cyoon1729/Distributed-Reinforcement-Learning](https://github.com/cyoon1729/Distributed-Reinforcement-Learning)

Hope this can be helpful for anyone.",reinforcementlearning,cyoon1729,False,/r/reinforcementlearning/comments/drdoa7/implementation_of_distributed_reinforcement/
Action Saturation in DDPG and TD3,1572813590,"Why do RL algorithms like DDPG and TD3 always predict maximum/minimum values after only a few training steps and the performance is extremely bad? This seems a common issue and still, there is no perfect reason or technique to solve it?  [https://github.com/yanpanlau/DDPG-Keras-Torcs/issues/16](https://github.com/yanpanlau/DDPG-Keras-Torcs/issues/16)",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dr6d0m/action_saturation_in_ddpg_and_td3/
RL Algorithm for Continuous Action Space,1572811954,"Which is the best algorithm for continuous action space in an extremely complex environment? I worked on DQN, DDPG, TD3 but the performance of each one of them was extremely bad.",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dr5z32/rl_algorithm_for_continuous_action_space/
PER Under performing for DQN,1572805641,"I am experimenting with PER (Priority Experience Replay) right now, and testing CartPole-v1 with a batch size of 32. PER params are: \`e=0.01, alpha=0.6, beta=0.4, b\_inc=-0.001\`.

So for the basic ER DQN over 5 runs I get:

[DQN with ER for 450 episodes for OpenAI CartPole-v1](https://preview.redd.it/2jsp0bwujiw31.png?width=902&amp;format=png&amp;auto=webp&amp;s=d3ddb57360cf5a0d7be28df868be421feae1a8aa)

For the PER DQN over 5 runs I get:

[DQN with PER for 450 episodes for OpenAI CartPole-v1](https://preview.redd.it/tmu62x53kiw31.png?width=908&amp;format=png&amp;auto=webp&amp;s=52c5bf2364d3db3913e5f78f8e02af8f5f8853dd)

So both these graphs show the max and min rewards at each episode over 5 runs. PER seems to result is greater variance... I would know if this is incorrect if the original paper \`(Schaul ., et al 2016) Priority Experience Replay\` had cartpole / classic controls envs but it doesn't. 

Does anyone have any papers / graphs of a basic DQN rewards for Cartpole? I feel like maybe PER is only good for longer spanning env's? Because the original paper only has atari games for benchmarking.",reinforcementlearning,jlaivins,False,/r/reinforcementlearning/comments/dr4fu2/per_under_performing_for_dqn/
Soft Actor Critic,1572803021,Is there any TensorFlow implementation of the Soft Actor-Critic Algorithm on a simple openai gym environment?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dr3u13/soft_actor_critic/
Strategy for understanding a complex code such as SAC,1572789364,"Does anyone have any strategy or advice for python novice how to read/understand [https://github.com/haarnoja/sac](https://github.com/haarnoja/sac) ? Or in general, how to start with a difficult code such as OpenAI baseline etc.

Thank you in advance",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/dr0olm/strategy_for_understanding_a_complex_code_such_as/
Alternative view on Value Based Methods for RL,1572758414,"Hello guys ! I've been into RL for maybe 3 or 4 years now and I try to have a good theoretical profile for understanding the analytical issues that we often encounter in practice. I recently dug  into the instability of value-based methods in conjunction with function approximators. From what I understand, typically what happens is that when we train a function approximator on some sort of bootstrapped objective, we implicitly try to enforce a temporal difference consistency. This idea of temporal difference consistency is inherited from the literature of dynamic programming and takes the form of a Bellman equation. But empirically, algorithms like Q-learning, or any critic trained on some sort of bootstrapped objective in conjunction with neural nets tend to be highly unstable. Although I am aware of practical tricks to make it work more often (target networks, replay buffer and so on) this is not entirely satisfactory from a theoretical perspective.

The question I faced was the following: why do we continue to use these methods? I mean, when fitting a Q-function, we are essentially learning a model of the reward function (I am not entirely confortable with the distinction between model-based and model free learning since for me, fitting a Q-function is already learning a model but anyway...). Why not going all-in and learn the full generative model of rewards with a graphical model instead of sticking with unstable methods? Is there some research on replacing Q-functions with for instance probabilistic generative models and integrating these with an actor critic architecture? Some connection with distributional RL maybe?

Your help and references are much appreciated!",reinforcementlearning,bOmrani,False,/r/reinforcementlearning/comments/dqw9b0/alternative_view_on_value_based_methods_for_rl/
Is there too much hype in RL?,1572726605,"A few days ago a post on the Machine Learning subreddit appeared ""I'm so sick of the hype"":

[https://www.reddit.com/r/MachineLearning/comments/donbz7/d\_im\_so\_sick\_of\_the\_hype/](https://www.reddit.com/r/MachineLearning/comments/donbz7/d_im_so_sick_of_the_hype/)

Which pointed out that ML in general has a lot of hype (which I agree with). Despite this, supervised learning has delivered on many different fronts like NLP and CV.

Then a user pointed out that a RL robotics lab shut down due to no progress in robotics ([https://www.reddit.com/r/MachineLearning/comments/donbz7/d\_im\_so\_sick\_of\_the\_hype/f5p2k2h?utm\_source=share&amp;utm\_medium=web2x](https://www.reddit.com/r/MachineLearning/comments/donbz7/d_im_so_sick_of_the_hype/f5p2k2h?utm_source=share&amp;utm_medium=web2x)).

&amp;#x200B;

My question is, while other areas of Machine Learning, have delivered, one of the most hyped ones is Reinforcement Learning and apart from some cool but not applicable results (videogames and GO) and some niche projects (drug discovery, server energy efficiency, robotics which seems that is not superior to traditional techniques) there are no known applications of Reinforcement Learning, does it need more research? Or it will not find any interesting applicability?

&amp;#x200B;

If you know any other interesting RL applications I would love to know.",reinforcementlearning,LazyButAmbitious,False,/r/reinforcementlearning/comments/dqpya8/is_there_too_much_hype_in_rl/
Training Off Policy RL Algorithms,1572725885,"In off-policy RL like DDPG or TD3 for each simulation-step training is performed once on a single batch of data.  Is this process of training optimal? Why not train it for 5 to 10 epochs after the end of each episode? In almost all the implementations of algorithms like DQN, DDPG, DRQN on GitHub the above training process is followed.

&amp;#x200B;

\*\*By a single epoch, I mean multiple batches of data that cover the entire replay buffer.",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dqps33/training_off_policy_rl_algorithms/
Action smoothing in early exploration phases (Soft Actor-Critic Paper),1572720066,"Hello people,

I'm currently reading the paper *Learning to Walk via Deep Reinforcement Learning*. They mention that they smooth out actions for the first 50 episodes to prevent too jerky motions but never provide any hint how they do that \[1\].

As I'm working with self driving cars and SAC I'm struggling in the exploration phase since the actions are sampled in a high frequency and are canceling each other out. This leads to the replay buffer not being filled with useful learning signals I believe.

Does anyone know how they do that or has a good idea how to approach my problem? I was thinking about some exponential moving average..

\[1\] - Haarnoja, Tuomas, et al. ""Learning to walk via deep reinforcement learning."" *arXiv preprint arXiv:1812.11103*(2018).",reinforcementlearning,GaiMor,False,/r/reinforcementlearning/comments/dqoes1/action_smoothing_in_early_exploration_phases_soft/
"Trying to reproducing ""IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures""",1572672769,"I have been to reproducing IMPALA by deepmind.

In this link, some results with atari(breakout, pong, star-gunner, boxing, space-invader) are introduced in [README.md](https://README.md).

If there are any fault in implementation, please be willing to comment. Thank you.

[https://github.com/RLOpensource/IMPALA-Distributed-Tensorflow](https://github.com/RLOpensource/IMPALA-Distributed-Tensorflow)",reinforcementlearning,ioricha,False,/r/reinforcementlearning/comments/dqg49x/trying_to_reproducing_impala_scalable_distributed/
"""MetaGenRL: Improving Generalization in Meta Reinforcement Learning"", Kirsch et al 2019",1572659982,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dqe1or/metagenrl_improving_generalization_in_meta/
Learning to Predict Without Looking Ahead: World Models Without Forward Prediction,1572652070,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/dqcijy/learning_to_predict_without_looking_ahead_world/
Reinforcement Learning Algorithms - Book,1572626044,,reinforcementlearning,andri27-ts,False,/r/reinforcementlearning/comments/dq6oc9/reinforcement_learning_algorithms_book/
Best book on Reinforcement Learning,1572622382,,reinforcementlearning,andri27-ts,False,/r/reinforcementlearning/comments/dq5tgj/best_book_on_reinforcement_learning/
What is the purpose of torch.distributions when implementing certain RL algorithms ?,1572597949,"I was going through [this implementation of PPO](https://github.com/higgsfield/RL-Adventure-2/blob/master/3.ppo.ipynb) in PyTorch when I came across the usage of `torch.distributions` (see `forward()` of class `ActorCritic`). The output of actor network is used to construct a normal distribution which is used to sample actions. But I'm having difficulty understanding why this is necessary. This is probably a stupid question but, why not just use regular softmax for the last layer of the policy network and use that to pick actions ?

P.S I also found that the docs for `torch.distributions` use REINFORCE algorithm as a use-case",reinforcementlearning,SecondEpoch,False,/r/reinforcementlearning/comments/dq1jht/what_is_the_purpose_of_torchdistributions_when/
"[N] First results of MineRL competition: hierarchical RL + imitation learning = agents exploring, crafting, and mining in Minecraft!",1572546283,,reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/dprmvc/n_first_results_of_minerl_competition/
[Project] OpenAI Gym PyBullet 3D printed legged robot,1572543395,"Hey  guys! I'm a software engineer starting playing with OpenAI Gym and I'd  like to collect genuine feedback/thoughts on my project: [https://github.com/nicrusso7/rex-gym](https://github.com/nicrusso7/rex-gym)

I've successfully trained a quadruped 3d printed robot editing the PyBullet Minitaur example ([https://arxiv.org/pdf/1804.10332.pdf](https://arxiv.org/pdf/1804.10332.pdf))

Any contribution is welcomed!! Thanks!!",reinforcementlearning,nicrusso7,False,/r/reinforcementlearning/comments/dpqyc5/project_openai_gym_pybullet_3d_printed_legged/
"""How We Know What Not To Think"", Phillips et al 2019",1572538879,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dppvmz/how_we_know_what_not_to_think_phillips_et_al_2019/
Need help fitting value function to a dataset,1572532958,"I have a set of transitions, and i want to fit my state value estimator (neural network) on this set in a bootstrapped way.

Because of the bootstrapping, the fitting process never seems to converge (there is always a nonzero gradient when i apply full batch gradient descent), even when i use a target network.

I tried various stuff, different optimizers (e.g. the fitted q iteration algorithm applies Rprop optimizer) but it doesnt seem to help.

Any advice?",reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/dpoi48/need_help_fitting_value_function_to_a_dataset/
Getting same results with half the number of time-steps as in original Hindsight Experience Replay Paper?,1572505370,"I am reproducing the results from \[Hindsight Experience Replay\]\[1\] by Andrychowicz et. al. In the original paper they present the results below, where the agent is trained for 200 epochs.

200 epochs \* 800 episodes \* 50 time steps = 8,000,000 total time steps.

\[!\[enter image description here\]\[2\]\]\[2\]

I try to reproduce the resutls but instead of using 8 cpu cores, I am using 19 CPU cores.

I train the FetchPickAndPlace for 120 epochs, but with only 10 episodes per epoch. Therefore 120 \* 50 \* 50 = 300,000 iterations. I present the curve below:

￼\[!\[enter image description here\]\[3\]\]\[3\]

&amp;#x200B;

and logger output for the first two epochs:

&amp;#x200B;

￼\[!\[enter image description here\]\[4\]\]\[4\]

&amp;#x200B;

Now, as can be seen from my tensorboard plot, after 30 epochs we get a steady success rate very close to 1. 30 epochs \* 50 episodes \* 50 time steps = 75,000 iterations. Therefore it took the algorithm 75,000 time steps to learn this environment.

&amp;#x200B;

The original paper took approximately 50 \* 800 \* 50 -2,000,000 time steps to achieve the same goal.

&amp;#x200B;

How is it that in my case the environment was solved nearly 30 times faster? Are there any flaws in my workings above?

&amp;#x200B;

NB: This was not a one off case. I tested again and got the same results.

&amp;#x200B;

\[1\]: [https://arxiv.org/abs/1707.01495](https://arxiv.org/abs/1707.01495)

\[2\]: [https://i.stack.imgur.com/xw447.png](https://i.stack.imgur.com/xw447.png)

\[3\]: [https://i.stack.imgur.com/OraSY.png](https://i.stack.imgur.com/OraSY.png)

\[4\]: [https://i.stack.imgur.com/rtxyP.png](https://i.stack.imgur.com/rtxyP.png)",reinforcementlearning,rrz0,False,/r/reinforcementlearning/comments/dpjwfu/getting_same_results_with_half_the_number_of/
Overestimation in Q-learning,1572497807,"The Double DQN paper [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf) claims about Q-learning that ""it is known to sometimes learn un-realistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values."". Can someone explain how does taking a max overestimates the action values? I can't understand that with respect to what are the action values overestimated.",reinforcementlearning,faal_dovahkiin,False,/r/reinforcementlearning/comments/dpitc9/overestimation_in_qlearning/
Is there any “exploration in RL” analog in control theory?,1572483367,Is there anything in optimal control theory literature or optimization which can be loosely thought of similar to exploration issue in RL?,reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/dpg182/is_there_any_exploration_in_rl_analog_in_control/
AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning,1572463553,,reinforcementlearning,ReinforcedMan,False,/r/reinforcementlearning/comments/dpbfwx/alphastar_grandmaster_level_in_starcraft_ii_using/
Build your own RL environments w/ Unity ML agents,1572451872,,reinforcementlearning,formalsystem,False,/r/reinforcementlearning/comments/dp8vvt/build_your_own_rl_environments_w_unity_ml_agents/
[D] ICML 2019 Reinforcement Learning talks,1572391674,"---
Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning

Presented by Chelsea Finn and Sergey Levine

https://www.facebook.com/icml.imls/videos/400619163874853/

https://www.facebook.com/icml.imls/videos/2970931166257998/

---
Recent Advances in Population-Based Search for Deep Neural Networks: Quality Diversity, Indirect Encodings, and Open-Ended Algorithms

Presented by Jeff Clune, Joel Lehman and Kenneth Stanley

https://www.facebook.com/icml.imls/videos/481758745967365/

---
Session on Deep Reinforcement Learning

• ELF OpenGo: an analysis and open reimplementation of AlphaZero

• Making Deep Q-learning methods robust to time discretization

• Nonlinear Distributional Gradient Temporal-Difference Learning

• Composing Entropic Policies using Divergence Correction

• TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning

• Multi-Agent Adversarial Inverse Reinforcement Learning

• Policy Consolidation for Continual Reinforcement Learning

• Off-Policy Deep Reinforcement Learning without Exploration

• Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation

• Revisiting the Softmax Bellman Operator: New Benefits and New Perspective

https://www.facebook.com/icml.imls/videos/1577337105730518/

---
Session on Deep Reinforcement Learning

• An Investigation of Model-Free Planning

• CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning

• Task-Agnostic Dynamics Priors for Deep Reinforcement Learning

• Collaborative Evolutionary Reinforcement Learning

• EMI: Exploration with Mutual Information

• Imitation Learning from Imperfect Demonstration

• Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty

• Dynamic Weights in Multi-Objective Deep Reinforcement Learning

• Fingerprint Policy Optimisation for Robust Reinforcement Learning

https://www.facebook.com/icml.imls/videos/298536957693171/

---
Session on Deep Reinforcement Learning

• Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning

• Maximum Entropy-Regularized Multi-Goal Reinforcement Learning

• Imitating Latent Policies from Observation

• SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning

• Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement 
Learning

• Structured agents for physical construction

• Learning Novel Policies For Tasks

• Taming MAML: Efficient unbiased meta-reinforcement learning

• Self-Supervised Exploration via Disagreement

• Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables

https://www.facebook.com/icml.imls/videos/355035025132741/

---
Session on Deep Reinforcement Learning

• The Natural Language of Actions

• Control Regularization for Reduced Variance Reinforcement Learning

• On the Generalization Gap in Reparameterizable Reinforcement Learning

• Trajectory-Based Off-Policy Deep Reinforcement Learning

• A Deep Reinforcement Learning Perspective on Internet Congestion Control

• Model-Based Active Exploration

• Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from 
Observations

• Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN

• A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs

• Remember and Forget for Experience Replay

https://www.facebook.com/icml.imls/videos/674476986298614/

---
Session on Reinforcement Learning

• Batch Policy Learning under Constraints

• Quantifying Generalization in Reinforcement Learning

• Learning Latent Dynamics for Planning from Pixels

• Projections for Approximate Policy Iteration Algorithms

• Learning Structured Decision Problems with Unawareness

• Calibrated Model-Based Deep Reinforcement Learning

• Reinforcement Learning in Configurable Continuous Environments

• Target-Based Temporal-Difference Learning

• Iterative Linearized Control: Stable Algorithms and Complexity Guarantees

• Finding Options that Minimize Planning Time

https://www.facebook.com/icml.imls/videos/2547484245262588/

---
Session on Bandits and Multiagent Learning

• Decentralized Exploration in Multi-Armed Bandits

• Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback

• Exploiting structure of uncertainty for efficient matroid semi-bandits

• PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits

• Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model

• Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning

• TarMAC: Targeted Multi-Agent Communication

• QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement 
Learning

• Actor-Attention-Critic for Multi-Agent Reinforcement Learning

• Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent 
Reinforcement Learning

https://www.facebook.com/icml.imls/videos/444326646299556/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

""Self Supervised Learning""
invited talk by Yann LeCun 

""Mental Simulation, Imagination, and Model-Based Deep RL""
invited talk by Jessica B. Hamrick

• Bayesian Inference to Identify the Cause of Human Errors

• Data-Efficient Model-Based RL through Unsupervised Discovery and Curiosity-Driven 
Exploration

• A Top-Down Bottom-Up Approach to Learning Hierarchical Physics Models for Manipulation

• Discovering, Predicting, and Planning with Objects

• FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and 
Discovery

• Generalized Hidden Parameter MDPs for Model-based Meta-reinforcement Learning

• HEDGE: Hierarchical Event-Driven Generation

• Improved Conditional VRNNs for Video Prediction

• Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual 
Foresight

• Learning Feedback Linearization by MF RL

• ""Learning High Level Representations from Continuous Experience""

• Deep Knowledge-Based Agents

https://www.facebook.com/icml.imls/videos/394896141118878/

https://www.facebook.com/icml.imls/videos/2084133498380491/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

""What should be Learned?""
Invited talk by Stefan Schaal

• When to Trust Your Model: Model-Based Policy Optimization

• Model Based Planning with Energy Based Models

• A Perspective on Objects and Systematic Generalization in Model-Based RL

https://www.facebook.com/icml.imls/videos/1286528018196347/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

Value Focused Models, Invited Talk by David Silver

• Manipulation by Feel: Touch-Based Control with Deep Predictive Models

• Model-based Policy Gradients with Entropy Exploration through Sampling

• Model-based Reinforcement Learning for Atari

• Learning to Predict Without Looking Ahead: World Models Without Forward Prediction

• Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video

• Planning to Explore Visual Environments without Rewards

• PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent settings

• Regularizing Trajectory Optimization with Denoising Autoencoders

• Towards Jumpy Planning

• Variational Temporal Abstraction

• Visual Planning with Semi-Supervised Stochastic Action Representations

• World Programs for Model-Based Learning and Planning in Compositional State and Action 
Spaces

• Online Learning and Planning without Prior Knowledge

https://www.facebook.com/icml.imls/videos/2366831430268790/

---
Workshop on Generative Modeling and Model-Based Reasoning for robotics and AI

• ""Online Learning for Adaptive Robotic Systems"" - Byron Boots

• ""An inference perspective on model-based reinforcement learning""

• ""Reducing Noise in GAN Training with Variance Reduced Extragradient""

• ""Complexity without Losing Generality: The Role of Supervision and Composition"" - Chelsea Finn
• ""Self-supervised Learning for Exploration &amp; Representation"" - Abhinav Gupta

• Panel Discussion

https://www.facebook.com/icml.imls/videos/449245405622423/

---
Workshop on Exploration in Reinforcement Learning

• ""Exploration: The Final Frontier"" - Doina Precup

• ""Overcoming Exploration with Play"" - Corey Lynch

• ""Optimistic Exploration with Pessimistic Initialisation"" - Tabish Rashid

• ""Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration"" - Nicolai Dorka

• ""Generative Exploration and Exploitation"" (Missing)

• ""The Journey is the Reward: Unsupervised Learning of Influential Trajectories"" - Jonathan Binas

https://www.facebook.com/icml.imls/videos/2236060723167801/

---
Workshop on Exploration in Reinforcement Learning

• ""Sampling and exploration for control of physical systems"" - Emo Todorov

• ""Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment"" - 
Adrien Taiga

• ""Simple Regret Minimization for Contextual Bandits"" - Aniket Deshmukh

• ""Some Explorations of Exploration in Reinforcement Learning"" - Pieter Abbeel

https://www.facebook.com/icml.imls/videos/2265408103721327/

---
Workshop on Exploration in Reinforcement Learning

• ""Exploration... in a dangerous world"" - Raia Hadsell

Lightning Talks:

• ""Curious iLQR: Resolving Uncertainty in Model-based RL"" - Sarah Bechtle

• ""An Empirical and Conceptual Categorization of Value-based Exploration Methods"" - Niko Yasui

• ""Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"" - Vitchyr H. Pong

• ""Optimistic Proximal Policy Optimization"" - Takahisa Imagawa

• ""Exploration with Unreliable Intrinsic reward in Multi-Agent reinforcement Learning"" - Tabish 
Rashid

• ""Parameterized Exploration"" - Lili Wu

• ""Efficient Exploration in Side-scrolling VIdeo Games with Trajectory Replay"" - I-Huan Chiang

• ""Hypothesis Driven Exploration for Deep Reinforcement Learning"" - Caleb Chuck

• ""Epistemic Risk-Sensitive Reinforcement Learning"" - Hannes Eriksson

• ""Near-optimal Optimistic Reinforcement Learning using Empriical Bernstein Inequalities"" - 
Aristide Tossou

• ""Improved Tree Search for Automatic Program Synthesis"" - Lior Wolf

• ""MuleX: Disentangling Exploration and Exploitation in Deep Reinforcement Learning"" - Olivier Teboul

https://www.facebook.com/icml.imls/videos/2324338441219681/

---
Workshop on Exploration in Reinforcement Learning

• ""Adapting Behavior via Intrinsic Rewards to Learn Predictions"" - Martha White

• Panel Discussion: Martha White, Jeff Clune, Pulkit Agrawal, and Pieter Abbeel. Moderated by Doina Precup

https://www.facebook.com/icml.imls/videos/1094687407344868/

---
I thought I would put together a list of the reinforcement learning talks from ICML 2019 since I found they were kind of difficult to look through on facebook, and I figured I would share it here. I believe they are mostly available on the ICML website too, but I was just looking through the livestreams: https://icml.cc/Conferences/2019/Videos",reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/doy9ni/d_icml_2019_reinforcement_learning_talks/
[D]ICML 2019 Reinfocrment Learning Talks,1572391489,"
---
Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning

Presented by Chelsea Finn and Sergey Levine

https://www.facebook.com/icml.imls/videos/400619163874853/

https://www.facebook.com/icml.imls/videos/2970931166257998/

---
Recent Advances in Population-Based Search for Deep Neural Networks: Quality Diversity, Indirect Encodings, and Open-Ended Algorithms

Presented by Jeff Clune, Joel Lehman and Kenneth Stanley

https://www.facebook.com/icml.imls/videos/481758745967365/

---
Session on Deep Reinforcement Learning

• ELF OpenGo: an analysis and open reimplementation of AlphaZero

• Making Deep Q-learning methods robust to time discretization

• Nonlinear Distributional Gradient Temporal-Difference Learning

• Composing Entropic Policies using Divergence Correction

• TibGM: A Transferable and Information-Based Graphical Model Approach for Reinforcement Learning

• Multi-Agent Adversarial Inverse Reinforcement Learning

• Policy Consolidation for Continual Reinforcement Learning

• Off-Policy Deep Reinforcement Learning without Exploration

• Random Expert Distillation: Imitation Learning via Expert Policy Support Estimation

• Revisiting the Softmax Bellman Operator: New Benefits and New Perspective

https://www.facebook.com/icml.imls/videos/1577337105730518/

---
Session on Deep Reinforcement Learning

• An Investigation of Model-Free Planning

• CURIOUS: Intrinsically Motivated Modular Multi-Goal Reinforcement Learning

• Task-Agnostic Dynamics Priors for Deep Reinforcement Learning

• Collaborative Evolutionary Reinforcement Learning

• EMI: Exploration with Mutual Information

• Imitation Learning from Imperfect Demonstration

• Curiosity-Bottleneck: Exploration By Distilling Task-Specific Novelty

• Dynamic Weights in Multi-Objective Deep Reinforcement Learning

• Fingerprint Policy Optimisation for Robust Reinforcement Learning

https://www.facebook.com/icml.imls/videos/298536957693171/

---
Session on Deep Reinforcement Learning

• Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning

• Maximum Entropy-Regularized Multi-Goal Reinforcement Learning

• Imitating Latent Policies from Observation

• SOLAR: Deep Structured Representations for Model-Based Reinforcement Learning

• Dimension-Wise Importance Sampling Weight Clipping for Sample-Efficient Reinforcement 
Learning

• Structured agents for physical construction

• Learning Novel Policies For Tasks

• Taming MAML: Efficient unbiased meta-reinforcement learning

• Self-Supervised Exploration via Disagreement

• Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables

https://www.facebook.com/icml.imls/videos/355035025132741/

---
Session on Deep Reinforcement Learning

• The Natural Language of Actions

• Control Regularization for Reduced Variance Reinforcement Learning

• On the Generalization Gap in Reparameterizable Reinforcement Learning

• Trajectory-Based Off-Policy Deep Reinforcement Learning

• A Deep Reinforcement Learning Perspective on Internet Congestion Control

• Model-Based Active Exploration

• Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from 
Observations

• Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN

• A Baseline for Any Order Gradient Estimation in Stochastic Computation Graphs

• Remember and Forget for Experience Replay

https://www.facebook.com/icml.imls/videos/674476986298614/

---
Session on Reinforcement Learning

• Batch Policy Learning under Constraints

• Quantifying Generalization in Reinforcement Learning

• Learning Latent Dynamics for Planning from Pixels

• Projections for Approximate Policy Iteration Algorithms

• Learning Structured Decision Problems with Unawareness

• Calibrated Model-Based Deep Reinforcement Learning

• Reinforcement Learning in Configurable Continuous Environments

• Target-Based Temporal-Difference Learning

• Iterative Linearized Control: Stable Algorithms and Complexity Guarantees

• Finding Options that Minimize Planning Time

https://www.facebook.com/icml.imls/videos/2547484245262588/

---
Session on Bandits and Multiagent Learning

• Decentralized Exploration in Multi-Armed Bandits

• Warm-starting Contextual Bandits: Robustly Combining Supervised and Bandit Feedback

• Exploiting structure of uncertainty for efficient matroid semi-bandits

• PAC Identification of Many Good Arms in Stochastic Multi-Armed Bandits

• Contextual Multi-armed Bandit Algorithm for Semiparametric Reward Model

• Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning

• TarMAC: Targeted Multi-Agent Communication

• QTRAN: Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement 
Learning

• Actor-Attention-Critic for Multi-Agent Reinforcement Learning

• Finite-Time Analysis of Distributed TD(0) with Linear Function Approximation on Multi-Agent 
Reinforcement Learning

https://www.facebook.com/icml.imls/videos/444326646299556/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

""Self Supervised Learning""
invited talk by Yann LeCun 

""Mental Simulation, Imagination, and Model-Based Deep RL""
invited talk by Jessica B. Hamrick

• Bayesian Inference to Identify the Cause of Human Errors

• Data-Efficient Model-Based RL through Unsupervised Discovery and Curiosity-Driven 
Exploration

• A Top-Down Bottom-Up Approach to Learning Hierarchical Physics Models for Manipulation

• Discovering, Predicting, and Planning with Objects

• FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and 
Discovery

• Generalized Hidden Parameter MDPs for Model-based Meta-reinforcement Learning

• HEDGE: Hierarchical Event-Driven Generation

• Improved Conditional VRNNs for Video Prediction

• Improvisation through Physical Understanding: Using Novel Objects as Tools with Visual 
Foresight

• Learning Feedback Linearization by MF RL

• ""Learning High Level Representations from Continuous Experience""

• Deep Knowledge-Based Agents

https://www.facebook.com/icml.imls/videos/394896141118878/

https://www.facebook.com/icml.imls/videos/2084133498380491/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

""What should be Learned?""
Invited talk by Stefan Schaal

• When to Trust Your Model: Model-Based Policy Optimization

• Model Based Planning with Energy Based Models

• A Perspective on Objects and Systematic Generalization in Model-Based RL

https://www.facebook.com/icml.imls/videos/1286528018196347/

---
Workshop on Generative Modeling and Model-Based Reasoning for Robotics and AI

Value Focused Models, Invited Talk by David Silver

• Manipulation by Feel: Touch-Based Control with Deep Predictive Models

• Model-based Policy Gradients with Entropy Exploration through Sampling

• Model-based Reinforcement Learning for Atari

• Learning to Predict Without Looking Ahead: World Models Without Forward Prediction

• Physics-as-Inverse-Graphics: Joint Unsupervised Learning of Objects and Physics from Video

• Planning to Explore Visual Environments without Rewards

• PRECOG: PREdiction Conditioned On Goals in Visual Multi-Agent settings

• Regularizing Trajectory Optimization with Denoising Autoencoders

• Towards Jumpy Planning

• Variational Temporal Abstraction

• Visual Planning with Semi-Supervised Stochastic Action Representations

• World Programs for Model-Based Learning and Planning in Compositional State and Action 
Spaces

• Online Learning and Planning without Prior Knowledge

https://www.facebook.com/icml.imls/videos/2366831430268790/

---
Workshop on Generative Modeling and Model-Based Reasoning for robotics and AI

• ""Online Learning for Adaptive Robotic Systems"" - Byron Boots

• ""An inference perspective on model-based reinforcement learning""

• ""Reducing Noise in GAN Training with Variance Reduced Extragradient""

• ""Complexity without Losing Generality: The Role of Supervision and Composition"" - Chelsea Finn
• ""Self-supervised Learning for Exploration &amp; Representation"" - Abhinav Gupta

• Panel Discussion

https://www.facebook.com/icml.imls/videos/449245405622423/

---
Workshop on Exploration in Reinforcement Learning

• ""Exploration: The Final Frontier"" - Doina Precup

• ""Overcoming Exploration with Play"" - Corey Lynch

• ""Optimistic Exploration with Pessimistic Initialisation"" - Tabish Rashid

• ""Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration"" - Nicolai Dorka

• ""Generative Exploration and Exploitation"" (Missing)

• ""The Journey is the Reward: Unsupervised Learning of Influential Trajectories"" - Jonathan Binas

https://www.facebook.com/icml.imls/videos/2236060723167801/

---
Workshop on Exploration in Reinforcement Learning

• ""Sampling and exploration for control of physical systems"" - Emo Todorov

• ""Benchmarking Bonus-Based Exploration Methods on the Arcade Learning Environment"" - 
Adrien Taiga

• ""Simple Regret Minimization for Contextual Bandits"" - Aniket Deshmukh

• ""Some Explorations of Exploration in Reinforcement Learning"" - Pieter Abbeel

https://www.facebook.com/icml.imls/videos/2265408103721327/

---
Workshop on Exploration in Reinforcement Learning

• ""Exploration... in a dangerous world"" - Raia Hadsell

Lightning Talks:

• ""Curious iLQR: Resolving Uncertainty in Model-based RL"" - Sarah Bechtle

• ""An Empirical and Conceptual Categorization of Value-based Exploration Methods"" - Niko Yasui

• ""Skew-Fit: State-Covering Self-Supervised Reinforcement Learning"" - Vitchyr H. Pong

• ""Optimistic Proximal Policy Optimization"" - Takahisa Imagawa

• ""Exploration with Unreliable Intrinsic reward in Multi-Agent reinforcement Learning"" - Tabish 
Rashid

• ""Parameterized Exploration"" - Lili Wu

• ""Efficient Exploration in Side-scrolling VIdeo Games with Trajectory Replay"" - I-Huan Chiang

• ""Hypothesis Driven Exploration for Deep Reinforcement Learning"" - Caleb Chuck

• ""Epistemic Risk-Sensitive Reinforcement Learning"" - Hannes Eriksson

• ""Near-optimal Optimistic Reinforcement Learning using Empriical Bernstein Inequalities"" - 
Aristide Tossou

• ""Improved Tree Search for Automatic Program Synthesis"" - Lior Wolf

• ""MuleX: Disentangling Exploration and Exploitation in Deep Reinforcement Learning"" - Olivier Teboul

https://www.facebook.com/icml.imls/videos/2324338441219681/

---
Workshop on Exploration in Reinforcement Learning

• ""Adapting Behavior via Intrinsic Rewards to Learn Predictions"" - Martha White

• Panel Discussion: Martha White, Jeff Clune, Pulkit Agrawal, and Pieter Abbeel. Moderated by Doina Precup

https://www.facebook.com/icml.imls/videos/1094687407344868/

---
I thought I would put together a list of the reinforcement learning talks from ICML 2019 since I found they were kind of difficult to look through on facebook, and I figured I would share it here. I believe they are mostly available on the ICML website too, but I was just looking through the livestreams: https://icml.cc/Conferences/2019/Videos",reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/doy83m/dicml_2019_reinfocrment_learning_talks/
"""Relay Policy Learning: Solving Long-Horizon Tasks via Imitation and Reinforcement Learning"", Gupta et al 2019 {BAIR/G}",1572379555,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dovd3q/relay_policy_learning_solving_longhorizon_tasks/
Theory behind a Simple Policy Gradient Algorithm vs. Implementation,1572366567,"I am struggling  to bring together the theoretical remarks on a simple policy gradient algorithm on SpinningUp with their actual implementation. The derivation on Spinning Up ([here](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#implementing-the-simplest-policy-gradient)) states the following formula for the gradient:

&amp;#x200B;

https://preview.redd.it/u2u90di69iv31.png?width=287&amp;format=png&amp;auto=webp&amp;s=05e5dc59186a3fa61c3b93103b8594560b2094d0

To my understanding, this requires the algorithm to 

1. Sample multiple (D) complete trajectories from the environment
2. Calculate the mean of the summed up (gradients \* total rewards) of the trajectories
3. Apply the gradient

However, if I look at SpinningUp's implementation on GitHub ([here](https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py)), they do not sample multiple complete trajectories. Instead, they apply the mean of the gradients from each timestep multiplied by the total reward of that time step.

It would be great, if somebody could help me to understand how the derivation is fitting the implementation.",reinforcementlearning,J1810Z,False,/r/reinforcementlearning/comments/dos97n/theory_behind_a_simple_policy_gradient_algorithm/
Update REINFORCE algorithm: step-wise or episode-wise?,1572340410,"Here's original question I uploaded on Stack Exchange AI : [https://ai.stackexchange.com/questions/16136/update-in-reinforce-algorithm-step-wise-or-episode-wise](https://ai.stackexchange.com/questions/16136/update-in-reinforce-algorithm-step-wise-or-episode-wise)

Today, I'm learning about gradient descent and here's a question about update of REINFORCE algorithm. The following is the pseudo-code in Sutton's book:

[REINFORCE in Sutton](https://preview.redd.it/b8vbojys3gv31.png?width=1628&amp;format=png&amp;auto=webp&amp;s=31563c4fcf5f58d74263d1d1e684a70d13574f76)

As I understand, the last line of the algorithm tells us that the parameter theta is updated for each step. However, most of the implementations seem different. Here's Pytorch's implementation in their GitHub: 

&amp;#x200B;

[Pytorch implementation of REINFORCE update](https://preview.redd.it/zrkcw3s75gv31.png?width=1186&amp;format=png&amp;auto=webp&amp;s=71480a9e8671fb993ce446197a03d2b9a2af64c4)

This seems that we first compute the total loss by summing over all steps, \*then\* weight theta is updated, i.e. update is done for each episode. I think two algorithms are different, and I'm so confused now. I googled a lot and most of the implementations use the second form. I also tried to change the second code (Pytorch one) by updating weight for each \*step\* and the result was different, so there's a difference between them. Could you explain what is going on here?",reinforcementlearning,seewoo5,False,/r/reinforcementlearning/comments/don9ux/update_reinforce_algorithm_stepwise_or_episodewise/
Full Atari and Roboschool benchmark results available in SLM Lab,1572326629,,reinforcementlearning,kengzwl,False,/r/reinforcementlearning/comments/dolg3k/full_atari_and_roboschool_benchmark_results/
DDQN vs DQN,1572300214,"I am wondering if there is any reason to not use DDQN over DQN? If you for example want to try an algorithm with a DQN method, is there than any reason not to use DDQN? I could see that computing power may be diffrent, But overall is DDQN “better” than DQN. An improvement at the minimum...

Could anyone inform me about the diffrence And why you could/would choose DQN over DDQN? 

Thank you",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/dog8hc/ddqn_vs_dqn/
How to implement a Reinforcement Learning library from Scratch — A Deep dive into Reinforce.jl,1572280595,,reinforcementlearning,formalsystem,False,/r/reinforcementlearning/comments/dobbip/how_to_implement_a_reinforcement_learning_library/
"What's best rl algorithm for gpu utilization, for continuous action space? Can ACER be updated in synchronously to achieve better gpu utilization.",1572274288,"I'm trying to train reinforcement learning agent for non standard environment. The environment uses big dataset for calculating rewards and (s, a)-&gt; s' transitions. As rule of thumb I started with a3c, but I noticed that it poorly utilities my gpu and (probably) because of dependency on large dataset it cannot utilize my cpu efficiently.  


How should i try to tackle this problem, what is the most suited rl algorithm for this case?  


As side question at first I thought try memory replay algorithm like acer [https://arxiv.org/abs/1611.01224](https://arxiv.org/abs/1611.01224), but while starting to implement it i noticed it does global network updates in the same manner, as a3c algorithm and the only difference is the experience source.   
In a2c updates of the global network are made in sync way and if i understand it correctly it makes possible to calculate gradients from agent's experience all at once. Can I use this on acer algorithm in the way that i would get several replay memory experiences to compute gradients at one?",reinforcementlearning,RailgunPat,False,/r/reinforcementlearning/comments/do9vpt/whats_best_rl_algorithm_for_gpu_utilization_for/
Paper: Neural networks with motivation,1572216348,"Just found this paper and wanted to ask your guys' opinion on it: [https://arxiv.org/abs/1906.09528](https://arxiv.org/abs/1906.09528)

What do you think about the definition and implementation of motivation here and do you think it scales to more complex environments like StarCraft or Dota?",reinforcementlearning,bengal7ion,False,/r/reinforcementlearning/comments/do0dsk/paper_neural_networks_with_motivation/
Automating Entropy Adjustment for Maximum Entropy RL,1572199273,"Hey,   
im currently implementing the newest version of SAC ([https://arxiv.org/pdf/1812.05905.pdf](https://www.youtube.com/redirect?v=CLZkpo8rEGg&amp;event=video_description&amp;redir_token=jADzGgcUWLo2CQNExoTElv1sE5d8MTU3MjE3MzI3MkAxNTcyMDg2ODcy&amp;q=https%3A%2F%2Farxiv.org%2Fpdf%2F1812.05905.pdf)) in the TD3 style with 2 Q-Nets as a critic. The only part I am struggling with is the entropy adjustment / temperature adjustment step (Part 5 and 6 of the paper)

I know they are using a NN and SGD to minimize alpha. And the gradients are computed like:

&amp;#x200B;

https://i.redd.it/uz1p70mmo1v31.png

H is a minimal expected entropy. as they define it with the target entropy: -dim(A)  
What I dont get is how do they calculate alpha. on the one hand alpha is the weights of the network J(a) and on the other hand it is a coefficient that is needed to update the critics:

&amp;#x200B;

https://i.redd.it/xipk74blh4v31.png

and the actor: 

&amp;#x200B;

https://i.redd.it/4598sylnh4v31.png

Am I missing out on something? Hope someone implemented it already and can help me.

Thanks in advance!",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/dnx23a/automating_entropy_adjustment_for_maximum_entropy/
Suggestions on Action-Space definition,1572175596,"I'm currently revisiting action-space definitions in an RL project, much like the microRTS environment ([https://github.com/santiontanon/microrts](https://github.com/santiontanon/microrts)).The game I'm working on is a 2-dimensional (grid-based) RTS game. The game supports direct API access to function such as left click, right-click, build ...etc. Also the game has mouse and keyboard support where the game has several keybindings for actions in the game. The game is fully-visible at all times (full-state is available).My question is the following:

1. Is there a generalised recommendation on how the action-space should be presented to RL-algorithms?
2. How many actions are TOO many actions? 
3. Which algorithm would be the choice for such an environment? Q-based?
4. Any clever and concrete examples of how an action-space should look like?

Considerations:For example, how to handle a dynamic amount of units and buildings. One way would be to give the AI access to moving the mouse and requiring it to click a unit and then perform an action, but again, this would increase the action-space significantly. Any common way of doing this?In total, I estimate that the game could have up to 10 000 actions if the AI had to move mouse manually. Is this feasible?",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/dns1dv/suggestions_on_actionspace_definition/
RL for Autonomous Driving,1572174393,I need suggestions for research papers that involve using Reinforcement Learning for Autonomous Driving. It would be great if there are resources that give a very good intuitive explanation of the reward function to use and how they affect the training of the RL agent. I am aware of Self Driving Car Specialization on Coursera but they are much more focused on the Non-Deep Learning-based approach.,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dnrvj6/rl_for_autonomous_driving/
Good keras PPO implementation?,1572122600,"Hi everyone,

I am looking for a PPO implementation which i can use with keras python 3.6, i’ve found Some however  They Don’t use/adjust their q target with the done/terminal variable which is crucial... 

Furthermore it can be Just basic as long as it has Everything standard like ER.

Hope someone could help me out!

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/dnjh4b/good_keras_ppo_implementation/
Thought this belonged here. It looks like the one in OpenAI's Gym library environment,1572118783,,reinforcementlearning,vignesh_shankar,False,/r/reinforcementlearning/comments/dninvx/thought_this_belonged_here_it_looks_like_the_one/
[Discussion] Why does the performance of POP-ART worse than clip-reward in almost half of Atari Environments?,1572105384,,reinforcementlearning,xiong-hui-chen,False,/r/reinforcementlearning/comments/dnfo7o/discussion_why_does_the_performance_of_popart/
[Discussion] Why does the performance of POP-ART worse than clip-reward in almost half of Atari Environments?,1572104409,"According to the author’s experiments, the magnitude problem has been solved well. If the effect of clip-reward is just to reduce the variance of Q-value target, POP-ART should have as least similar performance to clip-reward setting since POP-ART also reduces the variance of Q-value target. However, results show half of performance in Atari are significantly worse than clip-reward setting. ",reinforcementlearning,zpcxh95,False,/r/reinforcementlearning/comments/dnfga7/discussion_why_does_the_performance_of_popart/
How do people deal with episodes ending in Model-Based RL?,1572031439,"The standard ""(State/Action -&gt; State) and (State/Action -&gt; Reward)"" model that one learns doesn't have anything in it to deal with episodic tasks explicitly.

Generally, the assumption is that you turn an episodic MDP into a regular MDP by saying the ""goal state"" is self-absorbing forever, with zero reward.

And that's fine for learning a tabular model, but if you were learning a linear/deep model, and you present roughly infinite self-transitions at the goal state, the model is going to bias its learning 100% to that state at the expense of all others. I know that's not how people do it, but it illustrates why it's a tricky thing to deal with.

But if you ignore the problem, your model loses a super important piece of information. For example, if it was a goal state, and you learned that you get a big reward for being to the right of the goal, and your model doesn't ""terminate"" when it hits the goal, you'll keep getting huge reward by just staying to the left. That's not great either.

Is there a standard solution?",reinforcementlearning,asdfwaevc,False,/r/reinforcementlearning/comments/dn2e41/how_do_people_deal_with_episodes_ending_in/
Target Network Updates in TD3 algorithm,1572006405,"In the TD3 algorithm three target networks are used for one actor and two critics. According to the paper, each of the target networks is updated according to the original network after every one training iterations but the paper mentions the less frequent the updates are the better it is. I tried with the same one update step as mentioned in the paper for a complex problem(harder than gym environment) and the learning was very bad with the critic loss going to as high as 10\^11 within just 10k Steps.

I wonder what should be the optimal update rate and how does it affect the performance?",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dmwqdl/target_network_updates_in_td3_algorithm/
1 NN for every output,1571987985,In the early days of deep learning people used 1 neural network for every output only to later realise this wasnt so efficient. Doesn't this have some advantages which could also be useful to DRL?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/dmtuyx/1_nn_for_every_output/
Reinforcement Learning online course,1571948464,"Hi there, I've created deep reinforcement learning course in Udemy addressed Q-learning, DQN and other RL algorithms and concepts in an easy way with hands-on projects: [https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?referralCode=6EFFEF951DDE7B78FBA0](https://www.udemy.com/course/deep-reinforcement-learning-a-hands-on-tutorial-in-python/?referralCode=6EFFEF951DDE7B78FBA0)",reinforcementlearning,mehdi_mka,False,/r/reinforcementlearning/comments/dmm50h/reinforcement_learning_online_course/
Replay Memory in Reinforcement Learning,1571924768,How does the size of replay memory affect the training of an RL agent? How long the agent should be allowed to have random exploration before starting the training? There are works that start the training once the replay memory size reaches their selected batch size which usually varies from  16 to 128 but there are also works that start the training once they have collected 1000s of steps. I wonder which is the correct way to move forward in a highly complex environment?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dmgmpk/replay_memory_in_reinforcement_learning/
Reward distribution with multiple neural networks in a single Agent,1571924602,"I have a model where there are multiple neural networks to take decisions.

Something like this.  
Neural Network A, B, C and D.

Neural network A gives an output based on which one of the neural networks B, C and D is chosen for the next step. Like a Decision tree.  
Then the Agent takes action based on the output of the chosen neural network.

After a sequence of similar decision, the reward is given. How would be I apply a reinforcement learning algorithm to something like this?  
If i assign rewards to the NN that had not been chosen at all then it wouldn't train correctly. Or if one of the NN gave the correct outputs and other gave wrong output then the NN giving correct outputs would be affect by the results even if they are correct. If I train them by the same result.  


I could distribute the rewards based on how many times each of the NN had been choose but I don't know if its the right method to do something like this.",reinforcementlearning,Envenger,False,/r/reinforcementlearning/comments/dmglgs/reward_distribution_with_multiple_neural_networks/
Autonomous Driving using Reinforcement Learning,1571910750,"When training an RL agent using DDPG algorithm to learn to drive with the help of a reward function, the agent after a few 1000 time steps start rotating at the same spot without moving forward. Is this a common problem as I faced the same issue when I used a different algorithm? I modified the CNN architecture but nothing seems to work. On observing the predicted steer throttle values it seems the network is always predicting the extreme values that is full throttle with left or right steer always.",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dme4ns/autonomous_driving_using_reinforcement_learning/
New Reinforcement Learning framework for Researchers,1571907746,,reinforcementlearning,_djab_,False,/r/reinforcementlearning/comments/dmdpj4/new_reinforcement_learning_framework_for/
Stable-Baselines Reinforcement Learning Tutorial,1571857706,"A tutorial on reinforcement learning with stable-baselines ([https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)).

Based on colab notebooks, it covers: 

*  getting started with Stable Baselines 
*  gym wrappers, saving/loading 
* multiprocessing
* callbacks and hyperparameter tuning
* creating a custom gym environment

Repo: [https://github.com/araffin/rl-tutorial-jnrr19](https://github.com/araffin/rl-tutorial-jnrr19)

Slides: [https://araffin.github.io/#talks](https://araffin.github.io/#talks)

This tutorial was created by [Edward Beeching](https://github.com/edbeeching), [Ashley Hill](https://github.com/hill-a) and [Antonin Raffin](https://araffin.github.io/) for the Journées Nationales de la Recherche en Robotique 2019 ([https://jnrr2019.loria.fr/](https://jnrr2019.loria.fr/)).",reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/dm40oo/stablebaselines_reinforcement_learning_tutorial/
Calculate Empowerment in Python,1571854393,"Hi,

New to RL and I'm trying to calculate empowerment as the sklearn.metrics.mutual\_info\_score between the action probs and next state. My states are 2D  though, and sklearn refuses to use them. Is there something I can do to fix this,  or is there something I did that's wrong?",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/dm381k/calculate_empowerment_in_python/
How do you implement off-policy policy gradients ?,1571812834,"&amp;#x200B;

I found this equation from [https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#off-policy-policy-gradient](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#off-policy-policy-gradient)

https://i.redd.it/q1tmkfsri8u31.png

I have a hard time understanding how this objective is actually implemented. If I'm not wrong π is the target policy and β is the behavior policy and the behavior policy is used to generate the trajectories. But how are the policies updated ?

Any help would be greatly appreciated. Thank you",reinforcementlearning,SecondEpoch,False,/r/reinforcementlearning/comments/dlvc7j/how_do_you_implement_offpolicy_policy_gradients/
Coding the GridWorld Example from DeepMind’s Reinforcement Learning Course in Python,1571811947,,reinforcementlearning,aidiganta,False,/r/reinforcementlearning/comments/dlv7k4/coding_the_gridworld_example_from_deepminds/
Coding the GridWorld Example from DeepMind’s Reinforcement Learning Course in Python,1571811133,,reinforcementlearning,aidiganta,False,/r/reinforcementlearning/comments/dlv36y/coding_the_gridworld_example_from_deepminds/
PPO A to B not working (RLlib),1571799066,"Hello,

After having learned a lot of theory in my RL class, I wanted to try it in practice. I got the RLlib library and tried to solve a simple A to B problem: agent starts at point A and must go to point B (both fixed through the whole experiment). (Points are in the R\^2 plan, I am trying to do approximate RL).

So I made a very simple code that attempts to use the PPO algorithm to solve this problem ([https://pastebin.com/pKwZTKvL](https://pastebin.com/pKwZTKvL)) and let it run for some time but it does not seem to be working. The best mean episode reward I got was around -1000 while it can go up to 0 (reward is opposite of squared distance between agent and B).

I am not sure what I did wrong and would appreciate some help to get started and solve more difficult problems (random A and B for instance). My code is based on an RLlib example: [https://github.com/ray-project/ray/blob/master/rllib/examples/custom\_env.py](https://github.com/ray-project/ray/blob/master/rllib/examples/custom_env.py) that is similar to my A to B, except on only one axis and discrete. Thank you for your help!!",reinforcementlearning,sparkyhusky,False,/r/reinforcementlearning/comments/dlt2pz/ppo_a_to_b_not_working_rllib/
"""The evolution of intelligence in robots: Part 1"": current challenges and progress in robotics, Simon Kalouche",1571791718,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dlrhib/the_evolution_of_intelligence_in_robots_part_1/
Diagnosing my SAC not learning issue,1571752133,"I am currently working on an deep rl project for my thesis, and at the moment, the training is not working, and I wondered if there may be advice on how to find what's wrong, or perhaps I've made a foolish mistake/assumption somewhere. 

The setup: 

I'm attempting to learn in a multi-discrete action space with SAC. The space consists of actions (of a space of 16) and coordinates ( 64 x 64, which some, but not all, actions require).  Both action components are sampled independently using straight-through gumbelsoftmax, in order to be differentiable (and thus are one-hot). I add the log\_probs of the components together to form the full action\_log prob, and both the coord log\_prob and the one-hot coordinates are multiplied by zero in the event that the action selected does not use coordinates. My network architectures are based off a combined architecture for this problem, but separated into actor and critic, and the critic network as made considerably larger (following the convention I have seen of using double the units). 

 My SAC implementation is very similar to that of  [https://github.com/kengz/SLM-Lab/blob/master/slm\_lab/agent/algorithm/sac.py](https://github.com/kengz/SLM-Lab/blob/master/slm_lab/agent/algorithm/sac.py) . At the moment, I am treating it exactly as I would with discrete action space. The Q function takes in a state, a one-hot action and a one-hot coordinate (which is fully zero as mentioned above if the action doesn't use coordinates). 

&amp;#x200B;

Observation:

The loss of the critics  (after the first few volatile iterations) seems to act as you would expect, converging towards zero and staying low (as it's MSE between the critics and targets this should make sense). The policy loss tends to decrease and then stagnate. It appears that the action component of the policy converges rapidly (even to actions that achieve nothing) and will quite quickly reach a point where it only chooses 1/2 actions. The coord component seems almost unchanged (or at least acts fairly uniformly). This then rapidly fills the replay buffer with useless information (and in many cases since the actions achieve nothing the agent remains still and no exploration is done)

Ideas for what is going wrong?:

So the first thing may be hyperparameters, and while I've experimented a little I'm not the most clued up about how to tune these. If anyone could give me advice or a link to a good guide on this that would be great.

The second thing is to verify the weights generating the coordinates are actually changing as they should, and I'm running some checks for that as i write this.

The third thing is whether I've successfully adapted SAC for a multidiscrete space or if it can be? I can't see why having a Q function on (state, (action, coords)) should not work but I could be wrong. Should I be separating the component log\_probs and using a separate entropy reg term for the action and coord components?

Another thing I have considered is one-hot coordinates may be losing the fact that coordinates are not categorical, even if they are given discretely (so the detail of coordinates being close or far apart may be being lost. If this is the problem, how do I represent the coordinates in a way that is still differentiable and can also handle the needing to say that they are sometimes irrelevant (if i were to use the coords as inputs, simply zeroing them will just bias the model towards the 0,0 coordinates).

&amp;#x200B;

Would love it if anyone could provide any advice, if you have any questions or need something clarifying just ask.",reinforcementlearning,Intelligent_Aardvark,False,/r/reinforcementlearning/comments/dli723/diagnosing_my_sac_not_learning_issue/
perlexity instead of entropy for incentivizing exploration?,1571751560,"Often we see entropy terms added to objective function in order to encourage exploration. My question is: why not use perplexity instead? Perplexity reflects the more intuitive concept of ""the number of options"" the agent can choose from. Whereas entropy, being the average number of bits for optimal encoding, is less intuitive.

P.S. I am not talking about SAC, as in SAC entropy is rigorously derived and incorporated into state value...",reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/dli2sk/perlexity_instead_of_entropy_for_incentivizing/
DDPG Convergence with Recurrent Networks,1571749395,"I'm using DDPG for training a modified network with a recurrence to it. Essentially, the network consists of a ""memory"", similar to a hidden state in an RNN.

Referencing work from recent DDPG w/ RNN, I see that the Critic is conditioned on the (hidden state, actions) instead of the (observation, actions). Similarly, there has also been some work on sampling entire trajectories of an episode, instead of conditioning on the hidden unit.

Does anyone have any experience/insights on why that makes it work?

I can see where the problem lies in general with my updates:
*1. The memory is generated/updated at each time step as a function of parameters of the policy. However, since the target policy has different parameters, the target policy wouldn't necessarily have that same ""memory"" given the observation, and which is likely causing the non-convergence in the training.

Is my reasoning correct here?",reinforcementlearning,FatherCannotYell,False,/r/reinforcementlearning/comments/dlhmt4/ddpg_convergence_with_recurrent_networks/
Self-play by next-state imagination from current action,1571689753,,reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/dl6f2w/selfplay_by_nextstate_imagination_from_current/
"""Collaborating with Humans Requires Understanding Them""",1571689335,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dl6bgt/collaborating_with_humans_requires_understanding/
Flipped DQNs and MCTS?,1571686196,"New to RL...

Would it be possible to flip a deep-q network to estimate the next state based on the action space. and then use a regular actor, and then basically perform MCTS using the flipped DQN without actually simulating the future (just using an 'imagination' of all possible future states)? I'm trying this method, and it seems possible, but quite limiting for the flipped DQN in my opinion.",reinforcementlearning,ZeroMaxinumXZ,False,/r/reinforcementlearning/comments/dl5kt2/flipped_dqns_and_mcts/
"What is the best approach to find the optimal policy when the MDP is known, a.k.a. SOTA planning algorithm?",1571684257,"I'm wondering what's the SOTA approach when the model of the MDP is known? Let's keep things general by assuming stochastic transition and unsmooth reward function.   


Some pointers will be great!",reinforcementlearning,zhangxz1123,False,/r/reinforcementlearning/comments/dl53y9/what_is_the_best_approach_to_find_the_optimal/
Reinforcement Learning Agent from scratch pt.9: Letting Agent Play,1571681230,,reinforcementlearning,indi0508,False,/r/reinforcementlearning/comments/dl4edr/reinforcement_learning_agent_from_scratch_pt9/
SAC balancing,1571662234,"Im currently implementing the SAC algorithm on a plate which needs to balance a ball but I feel like the algorithm has a bit of trouble learning. In my env there are situations where the agent has no chance of recovering the ball from falling off the platform, could this make training more difficult considering it will try exploring a lot when it is unlucky and spawns in a bad location on the plate therefore destabalising the current decent policy? Or shouldnt this matter?",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/dl02ry/sac_balancing/
How to rigorously compute (or at least estimate) the inference time of a trained RL algorithm?,1571661379,"I've already [asked](https://www.reddit.com/r/MLQuestions/comments/dklj4e/what_is_the_computational_cost_of_implementing_an/) in another sub but my doubts are still here.

Taking for example AlphaStar, how could one arrive with pen and paper to that ""50 ms on a normal desktop"" [mentioned](https://www.youtube.com/watch?v=cUTMhmVh1qs&amp;feature=youtu.be&amp;t=5202) by the Deepmind people?",reinforcementlearning,Fab527,False,/r/reinforcementlearning/comments/dkzwmm/how_to_rigorously_compute_or_at_least_estimate/
Holland Manufacturing | Reinforced Tape,1571647426,,reinforcementlearning,Holland_MFG,False,/r/reinforcementlearning/comments/dkxk1a/holland_manufacturing_reinforced_tape/
"By dabbling in Reinforcement Learning, I made a simplified version of the board game Stone Age with accompanying video and Kivy app! Please let me know what you think!",1571645814,"Hello! 

A few months ago I decided Reinforcement Learning piqued my interest and since I have been long at work for making a game environment, an app in Kivy that runs on it and an AI that I can train! I summarised my result in this YouTube video, mostly for entertainment purposes.

I also would like reinforcement learning and its implementation to become more known!

 I am still learning so please let me know where to improve and let me know what you think of the video!

Video link:   [https://www.youtube.com/watch?v=GlZGcviUsOY](https://www.youtube.com/watch?v=GlZGcviUsOY) 

Github link:  [https://github.com/TheGameBotYT/StoneAgeAI](https://github.com/TheGameBotYT/StoneAgeAI)",reinforcementlearning,TheGameBotYT,False,/r/reinforcementlearning/comments/dkxbtl/by_dabbling_in_reinforcement_learning_i_made_a/
Random State change in MDP,1571623684,Suppose there is a MDP where the environment's state changes randomly over time. Is this a valid MDP?,reinforcementlearning,MachLearningEnthu,False,/r/reinforcementlearning/comments/dktn75/random_state_change_in_mdp/
Reinforcement learning for discrete observation space?,1571596999,"I am tackling a problem where agents are moving in a discrete grid. 

I realise now that have I never worked on a DRL problem with discrete observations. Can I just feed them into the usual methods like Rainbow/DDPG/PPO and it would work? Or should I expect stability issues? Are there specific methods/tricks for this situation?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/dkns2q/reinforcement_learning_for_discrete_observation/
Memory in RL and suitable environment,1571568488,"Hi all,

I am looking for a RL environment similar to the following problem:  
In each episode a box and a target appear in a random location. The agent have local sensors. The goal is to bring the box to the target location. 

So basically the agent should find the target location and then find the box and remember where the target was.

Is there a game in atari similar to that or any other high performance environment? the randomization is important because I don't want the policy to just remember a specific location.

maybe deepmind maze environment? 

Thanks",reinforcementlearning,What_Did_It_Cost_E_T,False,/r/reinforcementlearning/comments/dki30f/memory_in_rl_and_suitable_environment/
I recently submitted over 5 ETH in code bounties on Gitcoin for Python developers to claim,1571536155,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/dkd7bz/i_recently_submitted_over_5_eth_in_code_bounties/
[Question] Step size decay - is this a thing in Deep RL?,1571505852,"While theoretical RL (stochastic approximation) requires the step size to decay at a certain rate, and a common approach in supervised learning is to decay the LR every so often - this doesn't seem to be the practice in DRL.

Have any works tried to see what happens with various LR regimes? how it affects performance, stability etc...?

\*\* While we use ADAM / RMSProp, there is an effect minimal learning rate which can be attained - decaying the base LR will enable it to continue decrease.",reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/dk6s6t/question_step_size_decay_is_this_a_thing_in_deep/
Hey Guys check out part 8 of my Reinforcement Learning series : Making agent of Game,1571501704,,reinforcementlearning,indi0508,False,/r/reinforcementlearning/comments/dk5u6c/hey_guys_check_out_part_8_of_my_reinforcement/
"[RL Question] Why does Soft Actor Critic not use a target network, but DDPG does?",1571463059,,reinforcementlearning,_llucid_,False,/r/reinforcementlearning/comments/djzimb/rl_question_why_does_soft_actor_critic_not_use_a/
Any tips on how to deal with PPO overfitting?,1571461019,,reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/djz7qu/any_tips_on_how_to_deal_with_ppo_overfitting/
RLCard: A Toolkit for Reinforcement Learning in Card Games,1571426047,"Hi,

We've recently worked on imperfect information games and reinforcement learning, and we would like to share our toolkit to everyone. RLCard supports various popular card games such as UNO, blackjack, Leduc Hold'em and Texas Hold'em. It also has some examples of basic reinforcement learning algorithms, such as Deep Q-learning, Neural Fictitious Self-Play (NFSP) and Counter Factual Regret Minimization (CFR). Also, it has a simple interface to play with the pre-trained agent. Any generous comments will be appreciated. Have fun!

Github: [https://github.com/datamllab/rlcard](https://github.com/datamllab/rlcard)",reinforcementlearning,lhenry15,False,/r/reinforcementlearning/comments/djsapp/rlcard_a_toolkit_for_reinforcement_learning_in/
Why unsupervised learning objective can have good performance in RL?,1571383117,"Hello, everyone. I'm a new hand in this area. If there are some misunderstanding, please correct me, thank you!!!

Recently, I surveyed some RL papers with VAE objective. I found that they don't directly maximize reward function, but only optimize variational lower bound and derive MPC (model predictive control). These methods are called model-based RL due to the understanding of latent representation from environment. With MPC, they called their method planning algorithm, because model predictive control can find best simulated trajectory then do the corresponding action.

The question I faced is that they don't ""directly maximize reward function"" but have better performance (reward) than TRPO, PPO, etc. I know some objective encourage entropy search, but have a better reward still confuse me. 

Here's the paper link:

\[ Self-Consistent Trajectory Autoencoder \] [https://arxiv.org/pdf/1806.02813.pdf](https://arxiv.org/pdf/1806.02813.pdf)

\[ LEARNING IMPROVED DYNAMICS MODEL IN REINFORCEMENT LEARNING BY INCORPORATING THE LONG TERM FUTURE \]  [https://openreview.net/pdf?id=SkgQBn0cF7](https://openreview.net/pdf?id=SkgQBn0cF7)

\[ Learning Latent Dynamics for Planning from Pixels \]  [https://arxiv.org/pdf/1811.04551.pdf](https://arxiv.org/pdf/1811.04551.pdf)",reinforcementlearning,raychiuOuO,False,/r/reinforcementlearning/comments/djjxhq/why_unsupervised_learning_objective_can_have_good/
why unsupervised learning objective can have good performance in RL,1571374459,"Hello, everyone. I'm a new hand in this area. If there are some misunderstanding, please correct me, thank you!!!  
Recently, I surveyed some RL papers with VAE objective. I found that they don't directly maximize reward function, but only optimize variational lower bound and derive MPC (model predictive control). These methods are called model-based RL due to the understanding of latent representation from environment. With MPC, they called their method planning algorithm, because model predictive control can find best simulated trajectory then do the corresponding action.   


The question I faced is that they don't directly maximize reward function but have better performance (reward) than TRPO, PPO, etc.   


Here's the paper link:  
\[ Self-Consistent Trajectory Autoencoder \] [https://arxiv.org/pdf/1806.02813.pdf](https://arxiv.org/pdf/1806.02813.pdf) 

\[ LEARNING IMPROVED DYNAMICS MODEL IN REINFORCEMENT LEARNING BY INCORPORATING THE LONG TERM FUTURE \]  [https://openreview.net/pdf?id=SkgQBn0cF7](https://openreview.net/pdf?id=SkgQBn0cF7) 

\[ Learning Latent Dynamics for Planning from Pixels \]  [https://arxiv.org/pdf/1811.04551.pdf](https://arxiv.org/pdf/1811.04551.pdf)",reinforcementlearning,raychiuOuO,False,/r/reinforcementlearning/comments/djiktq/why_unsupervised_learning_objective_can_have_good/
Needed time for training agent solving robotic tasks,1571344576,"Does anyone have experience/knowledge how much time it takes to train agent to solve for example OpenAI Fetch robotic environments? At least some reliable estimation.  I am using GeForce 1660

Thanks in advance",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/djcbeb/needed_time_for_training_agent_solving_robotic/
[ off topic] Thinking of training an RL agent that would learn to swipe left or right (Tinder) based on training data from me.,1571341073,,reinforcementlearning,username-cs231n,False,/r/reinforcementlearning/comments/djbgi2/off_topic_thinking_of_training_an_rl_agent_that/
Hey guys check out part 7 of Reinforcement Learning Tutorial Series,1571282373,,reinforcementlearning,indi0508,False,/r/reinforcementlearning/comments/dj0j6e/hey_guys_check_out_part_7_of_reinforcement/
SAC for Autonomous Driving no smooth trajectory,1571259948," Hi. I am new to deep reinforcement learning and I was hoping if someone here could help me.  
I trained a model RC car using Softlearning([https://github.com/rail-berkeley/softlearning](https://github.com/rail-berkeley/softlearning?fbclid=IwAR2-7KdyWfJdkI9BcSeLK57NyXH_a0I02UAz4e4GfJhEH-DNhHvBVJf_9-M)) a state-of-the art approach. This is the resulting evaluation run.

 As you can see the car learns to avoid collisions but does not employ a smooth trajectory. I want to understand why that is the case. It can even avoid complex obstacles such as boxes placed in the way or obstacles moving at a constant velocity.

  
Action\_space = \[-0.6,0.6\] for steering and \[0,1\] for velocity

Observation\_space = \[laser array,270' Field of View plus a history of past 20 actions\]

I have tried a variety of reward functions. 

Is it even possible for this algorithm to take a smooth trajectory? Should I work more on a reward function or look towards something else? I would really appreciate any insights into this such as your comments, research papers etc. Thanks.

📷",reinforcementlearning,mlakhani221,False,/r/reinforcementlearning/comments/divsqj/sac_for_autonomous_driving_no_smooth_trajectory/
"""GTrXL: Stabilizing Transformers [XL] for Reinforcement Learning"", Parisotto et al 2019 {DM}",1571237384,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/diqclm/gtrxl_stabilizing_transformers_xl_for/
Trouble reproducing episodic controller results on CartPole-v0.,1571220417,"What could account for my inability to reproduce the results on [OpenAI Gym's CartPole-v0](https://gym.openai.com/envs/CartPole-v0/), as shown by [Karpathy's episodic controller](https://gym.openai.com/evaluations/eval_lEi8I8v2QLqEgzBxcvRIaA/).  Highest 100 consecutive episode average I could get is 180 \[195 is considered success\]. (Have tried over 10 runs.)

I have made minimal [changes](http://mergely.com/mToB0mN0/) to make the code python 3 compatible and add Tensorboard logging.   
Any clue as to what could be going wrong here ?   


Thanks in advance.",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/din5qn/trouble_reproducing_episodic_controller_results/
Trade and Invest Smarter — The Reinforcement Learning Way,1571189012,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/dihwpq/trade_and_invest_smarter_the_reinforcement/
Cool reinforcement learning research ideas?,1571175055,I am a current undergraduate student and have an opportunity to work on a research project. I am interested in robotics specially with regards to reinforcement learning. Any ideas? I have about 4-5 months for this research,reinforcementlearning,Norton6_4,False,/r/reinforcementlearning/comments/dietgr/cool_reinforcement_learning_research_ideas/
"""Solving Rubik’s Cube with a Robot Hand"", on Akkaya et al 2019 {OA} [Dactyl followup w/improved curriculum-learning domain randomization; emergent meta-learning]",1571167795,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/did0cu/solving_rubiks_cube_with_a_robot_hand_on_akkaya/
Workshop on Theory of Deep Learning at IAS from 15-18 Oct. Reinforcement learning on day 3.,1571157587,,reinforcementlearning,adssidhu86,False,/r/reinforcementlearning/comments/diai8f/workshop_on_theory_of_deep_learning_at_ias_from/
Trade Smarter w/ Reinforcement Learning,1571157450,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/diagz5/trade_smarter_w_reinforcement_learning/
"RL Weekly 33: Action Grammar, the Squashing Exploration Problem, and Task-relevant GAIL",1571152057,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/di97ob/rl_weekly_33_action_grammar_the_squashing/
How could i possibly speed up this Learn/Replay Function?,1571149389,"Hi everyone,

&amp;#x200B;

The code below can be used in a basic DQN with ER, however the code below takes ca 5 seconds to compute (normal step 0.5 second). Its the learn/replay function where it is doing a for loop, which i suspect is drastically slowing down the code. Is there a way to have this code run faster without that loop but still function the same? Please let me know!

Batch\_Size = 128, self.memory = 2500

        def replay(self, batch_size):
            minibatch = random.sample(self.memory, batch_size)
            for state, action, reward, next_state, done in minibatch:
                target = reward
                if not done:
                    target = (reward + self.gamma *
                              np.amax(self.model.predict(next_state)[0]))
                target_f = self.model.predict(state)
                target_f[0][action] = target
                self.model.fit(state, target_f, epochs=1, verbose=0)",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/di8m5k/how_could_i_possibly_speed_up_this_learnreplay/
[D] RL Line Follower,1571147698,,reinforcementlearning,hemiwoyi,False,/r/reinforcementlearning/comments/di88wi/d_rl_line_follower/
Off-Policy Actor-Critic with Shared Experience Replay,1571147124,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/di84lp/offpolicy_actorcritic_with_shared_experience/
3 questions,1571141003,"Am I correct in saying that in supervised learning you are guaranteed to get into a local optimum and with deep reinforcement learning it is a possibility to get into a local optimum.

My second question is if good exploration always should guarantee a nearly global optimum instead of only a local one in DRL considering people always say that exploration is the answer to local optima.

Is it common to use dropout for DRL to avoid local optima or is exploration to DRL as dropout is to SL to avoid local optima?",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/di6wm5/3_questions/
Facebook's Pytorch based Reinforcement Learning platform Mvfst-rl open sourced,1571126965,,reinforcementlearning,adssidhu86,False,/r/reinforcementlearning/comments/di4orx/facebooks_pytorch_based_reinforcement_learning/
Need help implementing multi-head DQN with PyTorch,1571092356,"I'm experimenting with multi-head DQN, such as Ensemble-DQN, Bootstrapped-DQN etc.

I have a working implementation, however I am not sure it is using resources efficiently.

My network has the following architecture:

    input -&gt; 128x (separate fully connected layers) -&gt; output averaging

I am using a ModuleList to hold the list of fully connected layers. Here's how it looks at this point:

    class MultiHead(nn.Module):
        def __init__(self, dim_state, dim_action, hidden_size=32, nb_heads=1):
            super(MultiHead, self).__init__()
    
            self.networks = nn.ModuleList()
            for _ in range(nb_heads):
                network = nn.Sequential(
                    nn.Linear(dim_state, hidden_size),
                    nn.Tanh(),
                    nn.Linear(hidden_size, dim_action)
                )
                self.networks.append(network)
    
            self.cuda()
            self.optimizer = optim.Adam(self.parameters())

Then, when I need to calculate the output, I use a for ... in construct to perform the forward and backward pass through all the layers:

    q_values = torch.cat([net(observations) for net in self.networks])
    
    # skipped code to compute TD-loss
    
    self.optimizer.zero_grad()
    loss.backward()
    self.optimizer.step()

**This works!** But I am wondering if I couldn't do this more efficiently. I feel like by doing a for...in, I am actually going through each separate FC layer one by one, while I'd expect this operation could be done in parallel.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/dhybku/need_help_implementing_multihead_dqn_with_pytorch/
"_Readings on the Principles and Applications of Decision Analysis, v2_, ed Howard &amp; Matheson 1983",1571087877,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dhx9u6/readings_on_the_principles_and_applications_of/
"""Functional RL with Keras and Tensorflow Eager"": designing the RLlib API",1571073849,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dhttwm/functional_rl_with_keras_and_tensorflow_eager/
Is max pooling useful for feature extraction in reinforcement learning?,1571058521,Though there have been works which do not use max-pooling as it leads to loss of spatial information and would not be good for training agent in RL  but on the other hand since pooling to some extent prevents overfitting could lead to faster training. Which one is a good analogy to move forward with?,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dhqbyh/is_max_pooling_useful_for_feature_extraction_in/
DQN for Classification tasks,1571053971,"Hello. I am a newbie to RL and just had to ask if DQN can be applied to classification tasks(let's say MNIST dataset)?

Thanks in advance.",reinforcementlearning,raghu_1809,False,/r/reinforcementlearning/comments/dhph4q/dqn_for_classification_tasks/
Studying RL in EU.,1571028118,"Can anyone suggest some colleges in Europe that have good RL programs as part of computer science masters?

Some personal insight and experience will be extra appreciated. 

Thanks in advance.",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/dhlo7z/studying_rl_in_eu/
What are m.log_prob and running_reward from the official pytorch reinforce script?,1571026132,"From the [official pytorch reinforce python script](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py) what is m.log_prob and running_reward?

    probs = policy(state)
    m = Categorical(probs)
    action = m.sample()
    policy.saved_log_probs.append(m.log_prob(action))

After sampling an action scalar value, how m.log_prob converts (ie math logic) it into log probability. In the script running reward is only used for printing output. Why use running reward rather than total rewards per episode?",reinforcementlearning,begooboi,False,/r/reinforcementlearning/comments/dhlcqi/what_are_mlog_prob_and_running_reward_from_the/
[D] How to explain the variances of return of Deep Reinforcement Learning policies?,1570968544,"In the context of Deep Reinforcement Learning (DRL), one typically evaluates a model during training (after some iterations) by calculating the empirical mean of return over a number of evaluation episodes. Usually, the empirical variances of return are also reported. For example, see Figure 2 in this paper [https://arxiv.org/pdf/1703.02702.pdf](https://arxiv.org/pdf/1703.02702.pdf) (the topic of the paper is irrelevant to the discussion here).

The theoretical origin of those variances probably consists of the stochastic transition of the MDP (unless it's deterministic) and the stochastic nature of neural network structure and neural network training (e.g. in cases of TRPO, PPO). However, I suspect that there are some missing parts of the explanation. So I guess my overall question is ""What are those parts?"".

P.S: I'm pretty sure there is a huge literature about this in the context of ""Classic"" Reinforcement Learning, where MDP is deterministic and the variances are mainly contributed by the RL algorithm. Please share it, because there may be some follow-up works in the context of DRL. Thanks!",reinforcementlearning,anvinhnd,False,/r/reinforcementlearning/comments/dh9rql/d_how_to_explain_the_variances_of_return_of_deep/
Looking for dynamic RL implementations,1570961405,"Hi everyone,

&amp;#x200B;

I got an environment setup in gym and now i wanna try it on diffrent RL algorithmes, however i found that most implementations i found online are to foccused on a specific environment and wouldn't work with a diffrent environment. The ones that were dynamic didn't use done = True for the target calculation (if done is true then target = R else target = r\*gamma\*q\_) which is a feature i really need in an implementation. 

&amp;#x200B;

I am just looking for a dynamic implementation where i can just pass my action and state shape to and it will basically work. sounds stupid, but i couldn't find any really... What makes my env diffrent from others is that it takes the raw action probabilities into the env.step, because i proccess it myself in order for it to work proper with my environment with randomness etc, so preferably just with softmax output in raw values...

&amp;#x200B;

Looking for DQN, but things like SAC and A3C would be nice to try. All discrete action spaces.

If anyone could help suggest a good algorithm, thanks,",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/dh8q2n/looking_for_dynamic_rl_implementations/
What is the reason for early saturation of values in case of CNN before the last activation layer?,1570951288,I have been trying to implement a CNN which is a combination of 3 convolution layers followed by 3 fully connected layers for feature extraction for an actor in an RL algorithm. The last activation is tanh. But I observed that for all the action prediction the tanh always predicts values close to the extreme i.e -1 or 1 leading to the very bad performance of the agent and no learning on its part. My implementation is TensorFlow based.,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dh7e63/what_is_the_reason_for_early_saturation_of_values/
"Reinforcement learning in different ""environments""",1570906670,"Hi fellows Redditors! Biologist here. 

I'm familiarizing with reinforcement learning to solve some of my questions regarding the evolution of learning. In particular, I would like to study the exploration-exploitation trade-off of learning which can approximate some biological reality. 

However, at the moment I still miss much of the technical knowledge to understand if reinforcement learning is a good approach.

Here are two doubts:

\- Let's consider that I train a neural network for solving a maze trough RL. Now that maze will generate a specific policy for that particular maze. What happens when I'll change it? Will it need to start from scratch?

\-My simple assumption is that more complex (or diverse) the environment is, more exploration is needed for the agent to exploit optimally that environment. But is RL a good approximation for these properties or should I look at something different?

&amp;#x200B;

Any source or help is very much appreciated!",reinforcementlearning,Underthatree,False,/r/reinforcementlearning/comments/dgzaig/reinforcement_learning_in_different_environments/
Combine PPO with deterministic policy?,1570892262,"Hi everyone, I'm doing a research project this semester. We are trying to use PPO to train an agent (car) to do motion planning in a simulation environment. In order to improve exploration, we'd like to combine the PPO with some deterministic motion planning algorithm. However, when we use both strategies to sample (for example use action from PPO policy and motion planning alternatively), it becomes automatically off-policy and the estimated advantages and gradients are incorrect. I'm looking for a method to combine both policies or update the PPO pi network reasonably.

Does anyone have experience with idea like this or could suggest me some papers to read? Up to now I've tried keywords like “off-policy policy gradient”, “policy gradient with demonstrations”, “deterministic policy” with no luck :/ 

Many thanks in advance.",reinforcementlearning,IserlohnArchmage,False,/r/reinforcementlearning/comments/dgw7db/combine_ppo_with_deterministic_policy/
Illegal Moves with DQNs and Tic Tac Toe,1570858891,"Hello, fellow Redditors.

I wanted to use a Deep Q-Network (not a Q-table) to learn to play Tic Tac Toe. However, one problem that stopped me was that not all moves are always valid.

For example, if the bot is learning to play as O and the top left square is already taken by X, how do you prevent the network from ""cheating"" and picking the top left square?

I was using this repo as a reference:  [https://github.com/yanji84/tic-tac-toe-rl](https://github.com/yanji84/tic-tac-toe-rl) 

That repo's approach to this problem was to assign a negative reward to the AI making an illegal move, even lower than losing. From the readme:

&gt;**Rewards**  
&gt;  
&gt;Won: 100  
&gt;  
&gt;Draw: 10  
&gt;  
&gt;Lost: -1  
&gt;  
&gt;Cheating (placing a move on a taken spot): -10

However, when I downloaded and ran the codebase on my computer, even after 180k episodes, it still played illegal moves.

My question therefore is, is there a way to completely prevent a DQN from picking an illegal move? I assume that any Chess AI using a DQN would face the same problem on a much larger scale.

Thank you!",reinforcementlearning,preyneyv,False,/r/reinforcementlearning/comments/dgr2a2/illegal_moves_with_dqns_and_tic_tac_toe/
Looking for an algorithm with these properties,1570833516,"I’ve been reading a lot about advancements in reinforcement learning over the last few years and I’ve gathered that the following properties are important:

Stochastic policies: for most tasks deterministic policies are too likely to get into a suboptimal state. Ddpg is very sensitive to initial conditions and doesn’t always perform well. This is because it can learn a stage that is suboptimal, because of its deterministic nature it can’t do any exploration around the state once training is finished. 

Prioritized replay buffers: rl is fundamentally a non stationary learning task. To keep up with the changing environment rl algorithms need to constantly learn every time they make an action. If they don’t they risk becoming out of date. On the other hand catastrophic forgetting is an issue where if the algorithm keeps learning it may forget valuable experience from its past. Prioritized replay buffers are a way for an algorithm to keep learning while mitigating the risk of catastrophic forgetting. 

Distributed actors: since rl algorithms are so sample inefficient one of the best ways to increase performance is through having many actors gather experience from their own environments. Making the actors distributed allows your experience gatherers to not be restricted by the resources of a single machine. 

Asynchronous actors: there’s no inherent reason why an algorithm needs to switch between acting and learning. Those two things can be done simultaneously and asynchronously. Doing so increases convergence speed and throughput. 

I’m not sure but it seems like existing algorithms have many but not all of these properties. Here’s my summary:

Ppo: stochastic, distributed, asynchronous 

Impala: stochastic, distributed, asynchronous

Ape-x: distributed, asynchronous, prioritized replay

Sac: maybe all 4? I’m not sure. 

Does this all seem correct? Is there anything I’m missing? Is there an algorithm I’m missing?",reinforcementlearning,mattfirstorderlabs,False,/r/reinforcementlearning/comments/dgmgt6/looking_for_an_algorithm_with_these_properties/
Kamyar Azizzadenesheli on TalkRL: Reinforcement Learning Interviews,1570809058,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/dgh333/kamyar_azizzadenesheli_on_talkrl_reinforcement/
Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models,1570787188,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/dgcszu/batch_renormalization_towards_reducing_minibatch/
[R] Benchmarking Batch Deep Reinforcement Learning Algorithms,1570770181,,reinforcementlearning,MrDoOO,False,/r/reinforcementlearning/comments/dgaadz/r_benchmarking_batch_deep_reinforcement_learning/
Running a DQN policy on a nano drone,1570758650,,reinforcementlearning,crazyflie,False,/r/reinforcementlearning/comments/dg87au/running_a_dqn_policy_on_a_nano_drone/
Automated testing of RL algorithm,1570708668,"Hi,

I've implemented the reinforcement algorithm from [this paper](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) using a custom DL framework. The underlying framework is likely to change over time, but the algorithm itself should never break. 

Therefore I'm looking for input on how to set up testing for the algorithm itself. I've been considering something along the lines of letting it train for a certain number of episodes on a very simple and deterministic environment where the algorithm will converge quickly. 

Any suggestions or general thoughts?",reinforcementlearning,antonkollmats,False,/r/reinforcementlearning/comments/dfwxug/automated_testing_of_rl_algorithm/
DDQN performance drop after N episodes in robotics path planning,1570695980,"I'm currently testing an easy environment (relatively small with fixed obstacles) to train a neural network to perform path planning (navigating from a starting position to a random target position, collision free) with Double DQN.

The fact is that the setup is well defined and it works (it is an easy task overall) but let's say after 3/5k episodes where the network maintains over 95% of success rate (in terms of how many targets the robot reaches), it starts to drop  in performance (the exploration phase is already done far before the drop) and it stabilizes (more or less) around 60/70% of success rate. 

I tested various network dimension, memory size, batch dimension and other hyperparameters to try to fix this...

The question is simple.. this kind of behavior is normal or I am missing something?",reinforcementlearning,emarche,False,/r/reinforcementlearning/comments/dfuyta/ddqn_performance_drop_after_n_episodes_in/
When to use reward clipping?,1570669342,Let’s say I’m playing a game that has vastly different reward scales. A good move can give me 10e3 points while a bad move will give next to nothing. Will reward clipping help or hurt in this situation? What would the drawbacks of not using reward clipping be?,reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/dfqj1l/when_to_use_reward_clipping/
"""ROBEL: Robotics Benchmarks for Learning with Low-Cost Robots"" {G}",1570649770,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dfm58w/robel_robotics_benchmarks_for_learning_with/
How does Self-Play on adversarial games avoid converging to full cooperation?,1570649022,"I'm currently working on my Masters Degree project. It involves applying RL to a game where two teams of agents compete against each other somewhat like soccer. The game is symmetric, so I can use Self-Play in training and acquire twice the data from each episode (from both teams).   


I've read about self-play in a lot of papers and some put the agent against its last version. Others put it against a fixed older version, in which case I can understand the convergence, because the data from the opponent isn't collected for training. My question being,  how come the agents training against the last version of themselves don't end up cooperating? Just the fact that the rewards are different is enough? Or is there something about Self-Play that I am missing?",reinforcementlearning,Bruno_Br,False,/r/reinforcementlearning/comments/dflz0p/how_does_selfplay_on_adversarial_games_avoid/
"""TorchBeast: A PyTorch Platform for Distributed RL"", Küttler et al 2019 {FB} [PyTorch Impala implementation]",1570643045,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dfkk5d/torchbeast_a_pytorch_platform_for_distributed_rl/
ClearnRL: RL library that focuses on easy experimental research with cloud logging,1570636449,,reinforcementlearning,vwxyzjn,False,/r/reinforcementlearning/comments/dfizxd/clearnrl_rl_library_that_focuses_on_easy/
[Question][OpenAI Gym][PyTorch] GPU vs CPU in RL - how to optimize research?,1570618170,"Hi guys, 

I 've started playing around with the OpenAI Gym and I started wonder if there is some way to make learning faster.

So do you have any idea how to optimize (in the faster computing meaning), for example, the REINFORCE algorithm in OpenAI Gym [https://github.com/pytorch/examples/blob/master/reinforcement\_learning/reinforce.py](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py)  ?

I'm looking for any tips how to make OpenAI run faster ;)

If this is important at the moment I'm playing with classical problems and I have i7 9700K + RTX 2070 Super.

Thanks!",reinforcementlearning,JacekPlocharczyk,False,/r/reinforcementlearning/comments/dffav0/questionopenai_gympytorch_gpu_vs_cpu_in_rl_how_to/
Reinforcement Learning with Monte carlo method - Exploring start,1570616844,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/dff3xe/reinforcement_learning_with_monte_carlo_method/
Sticking with the same policy during iterative policy evaluation,1570614937,"In this grid world example from David Silver's lecture at about 25:07, [https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;t=1507s](https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;t=1507s), my understanding is that we keep following the same policy, which is randomly picking top/down/left/right at each given position (each 25% chance). Even without updating the policy during this iterative policy evaluation process, we can still see it converges (25:27) as the greedy policy stabilizes. Then, at 34:00, he introduces policy iteration, which to my understanding is that, as we perform policy evaluation, we also update the policy by acting greedily, explaining the zig-zag diagram. My question here is, if it is guaranteed we can get the optimal policy without having to improve the policy during our iterative process, then what is the motivation for it? Can we not wait til the greedy policy stabilizes then use that as our optimal policy? 

To make it clear:

what policy iteration does: evaluate pi -&gt; improve pi by acting greedily -&gt; evaluate pi -&gt; improve pi by acting greedily ...

but why can't we just: evaluate pi-&gt; check the policy if act greedily(don't update) -&gt; evaluate pi -&gt; check the policy if act greedily(don't update) -&gt; ... -&gt; check the policy if act greedily and found out it is identical as last policy -&gt; obtained optimal policy

Other reference: [http://kvfrans.com/planning-policy-evaluation-policy-iteration-value-iteration/](http://kvfrans.com/planning-policy-evaluation-policy-iteration-value-iteration/)",reinforcementlearning,the_real_adi,False,/r/reinforcementlearning/comments/dfetw7/sticking_with_the_same_policy_during_iterative/
How are the observation spaces generated in OpenAI's Gym?,1570592769,"Hey all!

I'm currently working on a tool that is very similar to OpenAI's Gym. I want to give an experience to developers that is very similar to Gym, but got stuck creating observation spaces. Let's look at the [Atari Breakout](http://gym.openai.com/envs/Breakout-v0/) environment as an example. 

The observation space is \`(210, 160, 3)\`. I looked at the [source code for Gym](https://github.com/openai/gym/blob/1d31c12437e8bd7f466139a479705819fff8c111/gym/envs/atari/atari_env.py#L79) and it looks like \`210\` is the pixel width, \`160\` is the pixel height, but what is the \`3\`? Its hard coded at 3 for every environment. Does it have something to do with score?",reinforcementlearning,zollandd,False,/r/reinforcementlearning/comments/dfbinb/how_are_the_observation_spaces_generated_in/
[Question] Recommended resources of Control Theory,1570592084,"Hi! I am a new coming in RL, and currently following Berkeley [CS285](http://rail.eecs.berkeley.edu/deeprlcourse/) Deep RL this year. The most recent lectures talks about Model-base RL. And model-based RL seems to borrow a lot of things from Control Theory, for example, **LQR**, **iLQR**, **DDP**, etc. Since I have zero background in control theory, is makes me hard to catch up the lectures. Is there some recommended resources covering such topics in control theory? Thanks!",reinforcementlearning,zbqv,False,/r/reinforcementlearning/comments/dfbeam/question_recommended_resources_of_control_theory/
How can I tell if one algorithm is faster than another? (A statistics question.),1570577883,"This is more of a statistics question than a RL question, but I most often want this question answered while doing RL, so maybe someone here has some advice for me.

So, lets say I have some RL algorithm, and I run it on CartPole and it reaches a reward goal after X episodes. Then I modify the algorithm and run it again and it reaches the reward goal after Y episodes. How can I tell which algorithm is better, and how confident can I be that it is better?

(I'm assuming random seeds throughout.)

This is impossible to answer with only 2 runs, so let say I run each algorithm 5 times.

Algorithm A reaches the goal after `[544, 592, 464, 528, 693]` episodes.

Algorithm B reaches the goal after `[801, 278, 596, 411, 358]` episodes.

Which algorithm is better? This is somewhat subjective, as there are multiple definitions of ""better"". This is probably a question we've all had to answer though, so what do you define as ""better""?

Let's say I want the algorithm with the best *average* time to reach the goal. I can calculate averages and determine which of two numbers is lower, but how confident can I be that I have selected the algorithm with the better average?

So, like I said, more of a statistics question, but curious what you think.",reinforcementlearning,Buttons840,False,/r/reinforcementlearning/comments/df8gju/how_can_i_tell_if_one_algorithm_is_faster_than/
continuous actor critic,1570569753,I am using a Implementation (normal distribution with mu and std) and it is learning okay but the standard deviation does Not depents on the states. So for wach step in an Episode it stays on one value until the Network Update is done. Thats how itw should Work right? Because in some states the Policy should be more reliable and therefore have a lower std?,reinforcementlearning,ey_pF9_9,False,/r/reinforcementlearning/comments/df6iva/continuous_actor_critic/
The Paths Perspective on Value Learning,1570567150,,reinforcementlearning,a1b1e1k1,False,/r/reinforcementlearning/comments/df5v8g/the_paths_perspective_on_value_learning/
"""I'm sorry Dave, I'm afraid I can't do that"" Deep Q-learning from forbidden action",1570560450,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/df4ax7/im_sorry_dave_im_afraid_i_cant_do_that_deep/
RL parameters,1570535529,How can you know which variables need to be put into the state space in order to succesfully apply RL. Do you have to assume that the problem only depends on these variables and assume that the situation remains the same?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/deyver/rl_parameters/
Trying to find Open AI Gym RL resources,1570517129,"Hello. I am trying to learn reinforced ML for a project. I have a simple game made with pygame and pymunk with a paddle and a ball. I want to train an AI to play the game. Each time the paddle hits the ball, a point will be added to the score. I want my ai to learn to play and maximize this score. Now, I tried to find information and tutorials about open ai gym, but resources and examples are scarce and those available are pretty complex and hard to wrap my head around them. Can you point me to some decent resources, or where should I start?",reinforcementlearning,Extentho,False,/r/reinforcementlearning/comments/dewbgo/trying_to_find_open_ai_gym_rl_resources/
Does it make sense using ReLU in the fully connected layers before the last fully connected where the last fully connected layer uses tanh in the DDPG network?,1570513476,"I was confused with the use of ReLU in the layers before the last fully connected in DDPG. Since my action space varies from -1 to 1 why not use tanh in all the preceding layers, as well as I, feel using ReLU in the preceding layers leads to a loss of all the negative values which might be useful in predicting the relevant action.",reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/devsz2/does_it_make_sense_using_relu_in_the_fully/
How does weight initialization of the last fully connected layer in DDPG network affect the performance?,1570458690,,reinforcementlearning,pranav2109,False,/r/reinforcementlearning/comments/dek87u/how_does_weight_initialization_of_the_last_fully/
"""Sample Efficient Evolutionary Algorithm for Analog Circuit Design"": ""BagNet: Berkeley Analog Generator with Layout Optimizer Boosted with Deep Neural Networks"", Hakhamaneshi et al 2019 {BAIR}",1570410268,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/decd0k/sample_efficient_evolutionary_algorithm_for/
Google Football Research Environment Compatibilty?,1570369283,Is there any workaround to run [Google Football Research Environment](https://github.com/google-research/football) on Windows? What about on Colab? Running the installation command of `pip3 install gfootball` gives [this error](https://del.dog/ecoxilayuy) on Colab,reinforcementlearning,Syzygianinfern0,False,/r/reinforcementlearning/comments/de3drs/google_football_research_environment_compatibilty/
PPO - number of policy update iterations in one update,1570301494,"Hi guys, I ve been trying to implement PPO for couple of weeks now. However, according my opinion I think that not every important thing (important for beginner at least) is mentioned in original paper and also in tutorials. I ve spent a lots of days by correcting my code and today I have realised that the thing, which was wrong, was number of policy update iterations. SpinningUp tutorial has train\_pi\_iters = 80. With this variable I was unable to solve LunarLander. In one code Ive found the number 4 and surprisingly this was the key correction to solve the LunarLander environment. How many iterations should I use for updating policy (after collecting experience) ?

Because of this reason I would like to ask you to check this ""pseudocode"" if its correct or not. I believe this might be helpful also for others. 

**PSEUDOCODE of PPO algorithm:**

1. initial policy parameters (actor) , initial value function parameters (critic)
2. for number in range(epochs):
   1. Collect set of trajectories by running policy in the environment
   2. Compute rewards-to-go
   3. Compute advantage estimates based on the current value function
   4. for number in range(4):
      1. update policy using PPO-Clip objective (pi\_old is always equal to pi\_new in the first iteration of every update in each epoch)
      2. if (KL divergence &lt; 0.015): break
   5. for number in range(80):
      1. fit value function by regression on mean-squared error

Feel free to correct those numbers, I would really appreciate it because I am not utterly sure about them. However, with this numbers Ive finally solved LunarLander. I should probably mention that I do not want to do any parallelism within agents/collecting experience etc. I have only one agent.

Thank you all for your help. I would really appreciate your opinions.",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/ddrhxu/ppo_number_of_policy_update_iterations_in_one/
"""Stabilizing Off Policy Reinforcement Learning with Conservative Policy Gradients"", Tessler et. al 2019",1570297301,[removed],reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/ddqhbt/stabilizing_off_policy_reinforcement_learning/
Deep RL project ideas?,1570293007,"Reposting from r/MachineLearning

I am taking 2 grad level courses this fall (Deep Learning and Reinforcement Learning) and both of them have a significant project implementation part. Instead of two small projects, I am planning to club both to make something meaningful. The team size will be 2 at max. One idea I have is to explore Deep RL techniques in language modeling. But then this is not the most interesting thing. I have some previous experience in pure RL, but new to DL and Deep RL. I am open to solving games or anything else. Share your thoughts!!!",reinforcementlearning,CoevolvingAgent,False,/r/reinforcementlearning/comments/ddpi42/deep_rl_project_ideas/
Discussion about Pytorch vs Tensorflow,1570269774,"Ive mostly seen at forums that Pytorch is amazing and much easy to learn than Tensorflow. If that is true, why companies like OpenAI, Deepmind etc (at least the requirements in job offers is written) use/require Tensorflow? What are your opinions?",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/ddlazs/discussion_about_pytorch_vs_tensorflow/
"""Unsupervised Doodling and Painting with Improved SPIRAL"", Mellor et al 2019 {DM}",1570240314,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ddgzmq/unsupervised_doodling_and_painting_with_improved/
Is there a way to know if you set up a reinforcement learning environment correctly?,1570236369,"I made my own environment and implemented DQN. It has been about 350 episodes and I don’t see a noticeable reward change. I know that it takes a long time, I saw that space invaders take 25 hours to train on four Titan X GPU’s. 

I don’t have the money to train an agent for 24 hours and then find out that it did not work. 

I know the final outcome is really decided by the observations you picked and the reward you decided to give the agent. 

Is there a way to know that it is, in fact, learning something?",reinforcementlearning,FireStory,False,/r/reinforcementlearning/comments/ddgasa/is_there_a_way_to_know_if_you_set_up_a/
"Is there anything as RL learning rate optimizer (RL equivalent of Adam, RMSprop, SGD etc.)?",1570197798,"Hi folks, quick question - are you aware of any work on RL optimizers? What I mean specifically is that in NNs there is a plethora of optimizers such as Adam, RMSprop, SGD etc. encompassing aspects like momentum, the sparsity of the gradients and many others influencing the learning rate, thus improving the performance of gradient descent. My question is - whether there is anything like that, optimizing the learning rate specifically for RL. I know of heuristic techniques such as linearly decaying the learning rate or more advanced Bowling's WoLF ([http://www.cs.cmu.edu/\~mmv/papers/02aij-mike.pdf](http://www.cs.cmu.edu/~mmv/papers/02aij-mike.pdf), as well as its extensions including GIGA-WOLF etc.)

&amp;#x200B;

Let me know if you knew of anything in this area!",reinforcementlearning,mw_molino,False,/r/reinforcementlearning/comments/dd7sb9/is_there_anything_as_rl_learning_rate_optimizer/
"""TRAIL: Task-Relevant Adversarial Imitation Learning"", Zolna et al 2019 {DM}",1570153027,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dd0md4/trail_taskrelevant_adversarial_imitation_learning/
Best way to have centralized replay buffer for large scale learning,1570138961,"This is more of an implementation question. Can anyone suggest me what is the best way to store a centralized replay buffer on a separate machine. I have multiple Non-GPU machines running multiple workers to gather data, i want a centralized replay buffer from where my training machine can sample data and compute gradients, currently I am using Redis but there has to be something better.",reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/dcxptr/best_way_to_have_centralized_replay_buffer_for/
"""Universal quantum control through deep reinforcement learning"", Niu et al 2019 {G}",1570133950,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dcwivh/universal_quantum_control_through_deep/
Hindsight Experience Replay results which are not based on MuJoCo environments?,1570127221,"I'm currently implementing Hindsight Experience Replay (HER) and was wondering if there were any papers out there which do not depend on MuJoCo. Most  papers use [https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics) which need MuJoCo to run.

These are two of the original papers which implement HER:

[https://arxiv.org/abs/1707.01495](https://arxiv.org/abs/1707.01495)

[https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf](https://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf)

It's such a hassle to setup MuJoCo on an AWS instance and you can't even use it in Colab.

Are there any reputable papers which use different physics engines to test HER?",reinforcementlearning,rrz0,False,/r/reinforcementlearning/comments/dcux9g/hindsight_experience_replay_results_which_are_not/
RL Weekly 32: New SotA Sample Efficiency on Atari and an Analysis of the Benefits of Hierarchical RL,1570111915,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/dcrg1g/rl_weekly_32_new_sota_sample_efficiency_on_atari/
State Representation Question,1570086658,"If you have a for example a Taxi Drive Environment where you either transport animals or humans. I have that the actions switch once the  taxi has picked up a passenger. So when the taxi is carrying a animal or human it needs to of course know that, knowing that it has either a human or animal is important. So what i'd initially did was 

&amp;#x200B;

Animal = \[0,1\]

Human = \[1,0\]

State\_ = \[0,0,otherstatevalues\]

&amp;#x200B;

So the action switches on carrying an animal or human, therefore it needs to know wheter he is carrying on, it is specified in the state by 2 state features, however i am thinking that the agent is not completly getting that if either one of these state features animal/human is active that it knows to switch actions, completly. i've seen some behaviour of him knowing what to do, however that was only for him carrying humans and then completly flopping carrying animals because he doesn't really seem to know how to carry animals even though thats the same inputs required. So i am thinking its because the agent is not getting that  \[0,1\] == \[1,0\]  therefore i was wondering if it would be logic to have:

&amp;#x200B;

Animal               = \[0,1,0\]

Human              =  \[1,0,0\]

Has Passenger = \[0,0,1\]

State\_ = \[0,0,0,otherstatevalues\]

&amp;#x200B;

To instruct him that he is carrying a passenger in global, then the other state features illustrate what kind of passenger?

Could anyone tell me if this is a thing or what you think about it?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/dcne3i/state_representation_question/
When to use a deterministic policy vs a stochastic policy?,1570062291,"Are there tasks where one outperforms the other? It seems like the latest research (sac, ppo) focuses on stochastic policies. Are they always appropriate? Can they be made deterministic?",reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/dcj7ze/when_to_use_a_deterministic_policy_vs_a/
Handling off-policy critic networks for state-action values,1570045618,"I've been trying (and failing) to find some answers about how to best encode and use an action for a critic network in practice.

Let me explain what I'm looking for with some examples. Let's say I have an actor network that suggests an action given a state, chosen from a list of 15 actions (so an integer between 0 and 14). I then want to asses the value of this action in this state by inputting the pair into my critic network. The basic way to do this and what I seem to see as most common is to concatenate this action with the state, or to input this action at some later part of the network. So, as I'm working in PyTorch I might call `critic.forward(state, action)` where action = 5, and 5 may be input in the first layer with the state, or it may be concatenated several layers in.

Now question 1 is, if these actions are discrete and categorical (i.e action 5 is not &lt; than action 6 nor &gt; than action 4), should this action not be one-hot encoded? Most of the implementations I have seen do not bother. 

Now my actual problem is more complicated. An action is described by 4 values, the action, an x and a y coordinate, and a 0 or 1 denoting whether the coordinates are used by that action (some actions require coordinates, some do not). It is worth noting that the action and the coordinates are generated near independently (they rely on the same underlying network but are otherwise not connected), and that an action that requires coordinates always will, and vice versa. 

Since this is to be valued by one network, the action input needs to always be the same shape, however if i simply use this as an input like (5, 23, 33, 0) or (7, 45, 11, 1) , I am surely relying on the network to somehow learn that the 0 or 1 gives meaning to the x, y coordinates, which seems a difficult thing to learn, given that the value is trained using the reward, and has no way of knowing which part of its input made it get that reward. It also would not be reasonable to set the x y values to particular numbers, since that will bias the value estimation from the coordinates.

The best way I can see to do this is independently value the action and the coordinates within the network in a similar fashion to their generation in the actor, and simply give a value of zero to the coordinates when the action taken does not use coordinates.

Does this sound like a reasonable design choice? Feel free to ask questions for clarification if needed. Any direction people have to resources on this would be greatly appreciated.",reinforcementlearning,Intelligent_Aardvark,False,/r/reinforcementlearning/comments/dcfdql/handling_offpolicy_critic_networks_for/
Rule of thumb for epsilon relation to replay memory size,1570036035,"In the DQN variations of algorithms (double, rainbow, etc..), what is the rule of thumb when it comes to choosing the memory size in relation to the epsilon schedule? 

If I have a replay memory of size 1 million examples and in one case I choose my epsilon decay so that I would have 2 million exploration steps, versus another case where the decay results in 500k exploration steps. Did people look into the effect of such parameters?

I have heard that Richard Sutton recently published a paper about buffer size (small buffer sizes in particular), but I honestly haven't had the chance to read it yet.",reinforcementlearning,mahjoobi,False,/r/reinforcementlearning/comments/dcd4tg/rule_of_thumb_for_epsilon_relation_to_replay/
"""Emergent Systematic Generalization in a Situated Agent"", Hill et al 2019 {DM}",1570034499,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dccs7n/emergent_systematic_generalization_in_a_situated/
New to RL: How do I handle rewards in a turn-based game with multiple decisions per in-game turn?,1570031609,"I've been exploring reinforcement learning using Unity's ml-agents. I spent some time putting together a very basic RTS and got some interesting gameplay/behavior with my model making 10 decisions per second.

Now, I want to try making a simplified turn-based 4x game (similar to the Civilization series of games) and train a model that can play it. My first thought was that I should just let the model make one decision per unit/city per turn. But then it struck me that when the number of units changed, the number of steps per in-game turn would change as well. So if a structure is providing 5 wood per turn, and I give 0.0001 reward per wood gained from any source, the discounted future reward will be higher if there are fewer units.

A naive solution that I thought of would be to store the cumulative reward earned from all sources in the previous turn, and add that value as a reward for every decision in the current turn. I have not tested this yet.

I'm hoping that this might already be a solved issue. I have not touched ml-agents on the python side yet, but I have some experience with tensorflow and would be fine with making structural changes to the models that are auto-generated if necessary.

Is anyone aware of how this problem can be solved? If someone could point me to any type of resource that would be helpful.

I have looked through turn based game papers, but so far all of them appear to be one decision per turn, rather than one decision per unit per turn.",reinforcementlearning,PolygonMan,False,/r/reinforcementlearning/comments/dcc48i/new_to_rl_how_do_i_handle_rewards_in_a_turnbased/
How to Visualize Performance in a RL experiment,1570031305,"Hi everyone,

I am doing a project about Reinforcement Learning applying SAC algorithm. I am just wondering what is the canonical way to visualize and compute the performance of the training at a given step. I usually plot reward for each episode and mean reward of the previous 100 episodes, with all the plots about the loss of the critic networks and the policy. But I noticed that in recent paper such that the one of SAC ([https://arxiv.org/abs/1812.05905](https://arxiv.org/abs/1812.05905)) and this page ([https://spinningup.openai.com/en/latest/spinningup/bench.html](https://spinningup.openai.com/en/latest/spinningup/bench.html)), the plots has ""steps"" in *ascissa* and ""average return"" in *ordinata*. As I read in the spinning up page:

&gt;Performance for the on-policy algorithms is measured as the average  trajectory return across the batch collected at each epoch. Performance  for the **off-policy** algorithms is measured once every 10,000 steps by  running the deterministic policy (or, in the case of SAC, the mean  policy) without action noise for ten trajectories, and reporting the  average return over those test trajectories.

In my case the algorithm is **off-policy**, this means that I have to test my algorithm every X steps? (I was doing it every 20 episodes)

Thanks to anyone, I'm waiting for your enlightenment.",reinforcementlearning,PieroMack,False,/r/reinforcementlearning/comments/dcc1lb/how_to_visualize_performance_in_a_rl_experiment/
"""Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning"", Peng et al 2019",1570029172,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dcbjxw/advantageweighted_regression_simple_and_scalable/
Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning,1570009765,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/dc7zwr/advantageweighted_regression_simple_and_scalable/
sample efficiency,1570009132,"Following question uses DQN as example, but can be applied to many other RL algorithms.

In vanilla DQN, between two subsequent environment steps, we do exactly 1 stochastic gradient descent update on the network. Question: Would the algorithm be more sample efficient if it applied multiple minibatch updates between each environment step?

If i wanted to make the algorithm as sample efficient as possible, i would make the agent infer the most likely value function before it carries out next step. Sure, if the number of minibatch updates is too high, it may start overfitting to the experience in the pool. But i would be willing to pay that price if this reflects the agent's best guess on true value function given all previous experience..?

Question 2: Has anyone made an empirical research on this? E.g. how fast the agent learns depending on number of update steps?",reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/dc7wtz/sample_efficiency/
Deep Q-Network (DQN) implementation to play Atari Pong,1570000923,"Hi Everyone,

I recently just completed and open sourced my Pytorch implementation of a Deep Q-Network(DQN) to play Atari Pong. The implementation follows from the paper - Playing Atari with Deep Reinforcement Learning (DQN_neurips implementation) and Human-level control through deep reinforcement learning (DQN_nature implementation).

You can train your agent from scarch or load a trained policy from a checkpoint file and see videos as your agent is training.

Here is a [video](https://www.youtube.com/watch?v=DcyMFIKsVNI) of one the agent's games once it had learned a policy.

Source Code: [GitHub - KaleabTessera/DQN-Atari: Deep Q-Learning (DQN) implementation for Atari pong.](https://github.com/KaleabTessera/DQN-Atari)

Any comments or points are welcome. :)",reinforcementlearning,ktessera,False,/r/reinforcementlearning/comments/dc6uqq/deep_qnetwork_dqn_implementation_to_play_atari/
"""PDDM: Deep Dynamics Models for Learning Dexterous Manipulation"", Nagabandi et al 2019 {GB}",1569978233,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dc319m/pddm_deep_dynamics_models_for_learning_dexterous/
"""Learning to Seek: Autonomous Source Seeking with Deep Reinforcement Learning Onboard a Nano Drone Microcontroller"", Duisterhof et al 2019",1569977145,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dc2t4m/learning_to_seek_autonomous_source_seeking_with/
Exploration in Policy Gradient,1569969526,I am looking for papers on exploration in policy gradient (reinforce). Could someone suggest me some?,reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/dc15ts/exploration_in_policy_gradient/
Ornstein Uhlenbeck Process,1569961360,"Hello, I have a problem with my ddpg agent as a lot of persons here.

Mine have to control a biped robot in a 3d env to make it walk. The agent seems to learn at the beginning but then quickly fall into one specific solution such as throwing himself as far as he can. My reward function seems solid and I have tried a lot of changes on it, the same comportment  appears all the time. 

I would like to be sure the OU function that I use to randomize the process have good params since I have just copy pasted them from a random code of internet and kept it.

Can someone tell me if theta=0.15, mu=0.0, sigma=0.3 seems fair ? Even better, can someone \[re\]explain how those parameters affects the randomization over time, I am not even sure of my good understanding now",reinforcementlearning,Youuuuugoooooo,False,/r/reinforcementlearning/comments/dbz7mc/ornstein_uhlenbeck_process/
Slides/Course notes to follow along with Suton &amp; Barto's book ?,1569956013,,reinforcementlearning,Hot_Ices,False,/r/reinforcementlearning/comments/dbxw8j/slidescourse_notes_to_follow_along_with_suton/
Going it alone with RL in grad school?,1569954797,"Hi all,

I  am a PhD student at a somewhat-prestigious-but-not-top-tier school.  I  want to get into deep RL (specifically model-based DRL).  However, my  advisor isn't familiar with RL (his background is in ML for computer  vision), and there aren't any other professors in the department who  are.  Am I making a mistake by trying to do research in RL?

Some  additional context:  I am starting my third year, and haven't submitted  any papers.  It took me a while to get my priorities straight, build  the organizational skills and self-discipline needed to get things done  in grad school, etc.  But now I'm ready to build expertise, go deep, and  grind out papers.  If it takes three-four more years to get that  degree, that's OK.  But I want to make sure I'm not going to waste my  time working in a field that keeps me isolated.

Thanks!

(Note: I previously asked this in /r/cscareerquestions, and they recommended I ask here instead.)",reinforcementlearning,aeolis_mons,False,/r/reinforcementlearning/comments/dbxk7u/going_it_alone_with_rl_in_grad_school/
Soft-Actor critic with offline training on a real system?,1569944606,"Hello.

As the title suggests, I am trying to implement Soft Actor Critic for continuous control of a real system, I've been trying to base myself on the implementation for pytorch described in the following article: [https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665](https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665). 

However, I noticed that training is done in an ""on-line manner"", that is the best way to describe it I guess; The neural nets are trained after each time step as the agent is interacting with the environment, which is OK for a simulated environment because the next timestep can be simulated after training is done, but not for a real environment where training the neural nets might take so much time the sampling rate of the environment would become really low. 

So I guess training has to be done right after a full episode, sampling the replay buffer, training and then doing it again several times.

What I'd like to ask is if doing this defeats the whole purpose of SAC or if this is the right approach.

Suggestions are also welcome.

Thanks",reinforcementlearning,DaMLearner,False,/r/reinforcementlearning/comments/dbv7zf/softactor_critic_with_offline_training_on_a_real/
DQN for MNIST using GANs,1569924555,"1. Can i apply Deep Q-learning to MNIST data set using GANs where i apply a Q layer on top of Generator and Discriminator?
2. Tuning/Optimizing the hyper-parameters using Deep Q-learning ?",reinforcementlearning,LOfP,False,/r/reinforcementlearning/comments/dbr7e2/dqn_for_mnist_using_gans/
"""The Paths Perspective on Value Learning: A closer look at how Temporal Difference learning merges paths of experience for greater statistical efficiency"", Greydanus &amp; Olah 2019 {GB/OA} [Distill.pub]",1569890760,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/dblb91/the_paths_perspective_on_value_learning_a_closer/
Agent with human experience replay buffer not learning - HELP! :-),1569852338,"I am trying to train an agent in a sparse rewards scenario, and I have tons of human replays with all the required information (i.e. what actions is the human taking, what is the state, etc).

I've plugged it into an implementation of SAC, where the agent learns from two replay buffers, one with his own history and another with the  human history. However, the agent does not learn. It's quite strange, because although the rewards are sparse, they are not soooo sparse (let's say in every batch of 32 there are 1-5 rewards with the human experience). I believe the Q functions should be able to infer that certain actions provide some rewards, and hence direct the agent to that actions. However, this is not happening. 

After a lot of debugging I cannot find further errors, and I wonder if this is because I am mixing both experiences, one from the agent itself, and the other from humans, which the agent is not yet able to reproduce. Do you believe, from a mathematical/modeling pov, is it correct to mix both experiences in SAC or in DQN? 

I cannot think about any reason why this may be a mistake... but I may be wrong.",reinforcementlearning,__me_again__,False,/r/reinforcementlearning/comments/dbc1mb/agent_with_human_experience_replay_buffer_not/
How to check if my new method improves things?,1569829480,"I think I have found a number of nice tricks to make Prioritized Experience Replay more efficient. But I am not sure how to ""prove"" that this is indeed the case.

For now, I use my own implementation of DQN with PER, which works well. Then I add my modifications and it works better.

What would I need to make sure that my tricks are actual improvements? say, if I wanted to write a paper about it? (I have 0 experience with this process)

- Should my own codebase reach the same results than those published in [the original PER paper](https://arxiv.org/pdf/1511.05952.pdf), before I apply my modifications? This is not trivial, as small code changes can have large performance impact.

- On this topic, is there a well-recognised **PyTorch** codebase I could use for this purpose? would the newly released [rlpyt](https://github.com/astooke/rlpyt) be acceptable?

- Is PER considered as ""obsolete"" now, and so to prove that my result is interesting, I would need to attack a more recent approach eg Rainbow or even a distributed approach like Ape-X? In general, should you try to beat the published result closest to what you are doing, or beating the current state of the art? My problem with trying to beat SotA is that I would need a much more complicated system, with tons more hyperparameters, which would make the effect of my ""tricks"" less clear.

- Last but not least, how many games and seeds should I use? This gets expensive quick...",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/db862q/how_to_check_if_my_new_method_improves_things/
Why don't people compute the proper gradient in DQN gradient updates,1569810405,"In DQN, the gradient update is performed as
$$\nabla_\theta L(\theta) = \mathbb{E}[r + \gamma \max_{a'} Q_\theta(s',a') - Q_\theta(s,a)]\nabla_\theta Q_\theta(s,a)$$
in which the gradient is only taken with respect to the $Q_\theta(s,a)$ term and not to the $\max_{a'} Q_\theta(s',a')$. Is there an intuitive reason why this design choice is made? It seems mathematically possible to take the gradient with respect to both terms, which is a more proper gradient of the TD loss, as desired.",reinforcementlearning,zhangxz1123,False,/r/reinforcementlearning/comments/db52yj/why_dont_people_compute_the_proper_gradient_in/
Quick question on PPO - no clipping for actions with negative rewards?,1569778370,"I see that the objective function for PPO uses the minimum value from either (ratio * advantage) or (clipped_ratio * advantage).

Doesn't that mean that the objective function very much prefers to move away from actions with negative reward, while only conservatively moving towards actions with positive reward?

I see that this might help to avoid local minima, but am I right in assuming this might be the reason that a PPO algorithm does not converge, particular for sparse rewards where it's difficult to follow a positive reward signal?",reinforcementlearning,richard248,False,/r/reinforcementlearning/comments/dayahl/quick_question_on_ppo_no_clipping_for_actions/
Neuroevolution vs Reinforcement Learning,1569748045,"Hi guys! So I used PPO to train Breakout and the wall-clock time came out pretty large. For games specifically, I read that deep neuroevolution has shorter training time, only needs sparse rewards and can complete Atari games really well. ( [https://eng.uber.com/deep-neuroevolution/](https://eng.uber.com/deep-neuroevolution/) )

I've never used genetic algorithms so can someone provide some insight into the tradeoff of these things vs RL algorithms like A2C? Hypothetically, if my game goal is just to move somewhere, what would the rewards be? If genetic algorithms are faster, why doesn't everyone just always use them?

Thank you guys in advance!",reinforcementlearning,ronaldo_gao,False,/r/reinforcementlearning/comments/dasu3a/neuroevolution_vs_reinforcement_learning/
"""Fixed-Horizon Temporal Difference Methods for Stable Reinforcement Learning"", De Asis et al 2019",1569633411,,reinforcementlearning,Owlina,False,/r/reinforcementlearning/comments/da9ic7/fixedhorizon_temporal_difference_methods_for/
I'm developing an online competitive environment for RL and am looking for feedback. What do you want?,1569624263,"**Background**

I’m currently working on a project called [Terrarium.ai](http://terrarium.ai/). It's an online (persistent) environment where you can develop models that control agents remotely. I’ve been developing the environment but recently got into a bit of an argument with myself about what direction to take the environment.  At the moment cells can observe the cells around with 2 senses, **smell** and **vision**. They then have a choice between 3 actions: **move**, **eat**, and **attack**.

[Here is a link to documentation about observations with more info.](https://docs.terrarium.ai/how-it-works/agents/observations)

My initial goal with this project was to simulate the life of single celled organisms who have decision making brains. This meant there is no defined objective at the moment other than survival. The agents aren’t tasked with simple goals such as collecting anything or achieving a specific score. The goal is the same as our observable nature: survive and reproduce given your environment.

Now I'm wondering if this is a bit too open ended. The fact that these agents can make decisions is already pretty far off from any naturally occurring singe celled organisms.

**What do you think would make the environment more interesting and challenging? What do you want in an online competitive environment?** 

*Note: If you'd like to join the community and discuss this kind of stuff on Discord,* [*here's the link*](https://discord.gg/gQt8Ayc)*!*",reinforcementlearning,zollandd,False,/r/reinforcementlearning/comments/da7s9k/im_developing_an_online_competitive_environment/
Stable Baselines authors Antonin Raffin and Ashley Hill on TalkRL: Reinforcement Learning Interviews,1569499563,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/d9ipcc/stable_baselines_authors_antonin_raffin_and/
Research project idea suggestions in RL,1569495918,"Hello everyone,

Long time lurker here - posting for the first time.

I'm a DS masters student who's stepping into the 2nd year of studies this October.

In my program, I'm supposed to work on a research module, which is something like a 'small - thesis' and for that, I'm thinking of doing a project which involves RL.

I've always wanted to get into RL as I feel it's one of the areas which has a huge potential to have a major impact across many industries as well as on people's lives. I personally believe there's so much left to discover and comparing with the other sub fields of ML / AI, I feel RL is still bit behind, but rapidly growing. Even though I have some experience in the supervised and unsupervised learning domains, my knowledge in RL is still very new / little, thus my plan is to work on this project as an introductory work towards transitioning into the RL field. 

Afterwards, if all goes well, I plan on doing my masters thesis on a similar topic (utilizing the experience and knowledge that I sincerely hope to gather by working on this module) and finally, figure out some problem that I can continue to work on for a Ph.D.

Having the above plan in mind, I thought it's best to seek advice from this community since I'm pretty sure almost everyone here is more knowledgeable than me. I do have few ideas in mind, but frankly, they are based on the intuition that I have about RL, thus feel they aren't the best candidate topics for a mini thesis project. 

Therefore, I would really appreciate if you can provide some ideas / topics or any sort of tips to identify a good enough topic which is not too broad, but can be used to introduce myself to the basics of RL and gain enough experience to call myself at least a novice in this field.

If all goes well, I promise to share my experience from this point onward until the end, which would be either me stepping down from the idea of pursing a PhD in RL or see to the end of the above laid out plan. 

Thank you!

P.",reinforcementlearning,PsyRex2011,False,/r/reinforcementlearning/comments/d9i35s/research_project_idea_suggestions_in_rl/
"""Why Does Hierarchy (Sometimes) Work So Well in Reinforcement Learning?"", Nachum et al 2019",1569466069,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d9dngq/why_does_hierarchy_sometimes_work_so_well_in/
"RL Weekly 31: How Agents Play Hide and Seek, Attraction-Repulsion Actor Critic, and Efficient Learning from Demonstrations",1569426030,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/d951b1/rl_weekly_31_how_agents_play_hide_and_seek/
I released a simple tool for running a bunch of algorithms on a bunch of environments and plotting results,1569414870,,reinforcementlearning,John-R-Cooper,False,/r/reinforcementlearning/comments/d92n2x/i_released_a_simple_tool_for_running_a_bunch_of/
What hyper parameters for a DDPG would give me better results?,1569413794,"My DDPG keeps achieving a high score the first few hundred episodes but always drops back to 0 near 1000 episodes.  Can a lower learning rate solve this problem?

Current settings.....

* NUM\_UPDATES = 1
* UPDATE\_EVERY = 4
* BUFFER\_SIZE = int(1e6)  # replay buffer size
* BATCH\_SIZE = 64  # minibatch size
* GAMMA = 0.99  # discount factor
* TAU = 1e-3  # for soft update of target parameters
* LR\_ACTOR = 0.0001  # learning rate of the actor
* LR\_CRITIC = 0.001  # learning rate of the critic
* WEIGHT\_DECAY = 0  # L2 weight decay
* SIGMA = 0.05",reinforcementlearning,thinking_computer,False,/r/reinforcementlearning/comments/d92fzv/what_hyper_parameters_for_a_ddpg_would_give_me/
باربری ولنجک تهران با خدمات عالی,1569397038,[باربری ولنجک](https://www.barbari-tehran.com/%d8%a8%d8%a7%d8%b1%d8%a8%d8%b1%db%8c-%d9%88%d9%84%d9%86%d8%ac%da%a9-%d8%aa%d9%87%d8%b1%d8%a7%d9%86-%d8%a8%d9%87%d8%aa%d8%b1%db%8c%d9%86-%d8%ae%d8%af%d9%85%d8%a7%d8%aa-%d8%a7%d8%ab%d8%a7%d8%ab-%da%a9/) متخصص در زمینه های بسته بندی اثاثیه منزل- جابجایی هرگونه وسایل به کلیه منطق تهران در تمامی روزهای هفته حتی در تعطیلات . قیمت های باربری و اسباب کشی کاملا مطابق با نرخ اتحادیه می باشد و #باربری ولنجک با داشتن مجوز رسمی از اتحادیه اتوبار کشور جزو معتبرترین شرکت های حمل و نقل درون شهری می باشد.,reinforcementlearning,barbari_tehran,False,/r/reinforcementlearning/comments/d9006u/باربری_ولنجک_تهران_با_خدمات_عالی/
"In actor-critic, does it matter in which order you train π and q?",1569370495,"In policy gradient ([https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html#other-forms-of-the-policy-gradient](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#other-forms-of-the-policy-gradient)) you often have a policy network π and a q network. Does it matter in which order you train π and q?

For example, I'm currently doing:

    ...
    Generate trajectories by running the policy in the (simulated) environment
    Improve π
    Improve q
    Generate trajectories by running the policy in the (simulated) environment
    Improve π
    Improve q
    ...

Is this more or less common than the following slight modification?

    ...
    Generate trajectories by running the policy in the (simulated) environment
    Improve q
    Improve π
    Generate trajectories by running the policy in the (simulated) environment
    Improve q
    Improve π
    ...

Note that the improving π involves q, but q and be improved independent of π.

I'm going to try it both ways and see if I notice a difference, but my own experiments will be limited, so I'm wondering if anyone else has some experience with this or knows of some research.

(Also, I think this is ""actor-critic"" but I'm not actually sure that's the correct term.)",reinforcementlearning,Buttons840,False,/r/reinforcementlearning/comments/d8vrg4/in_actorcritic_does_it_matter_in_which_order_you/
Discounted State Distribution,1569353320,"Can someone tell me what is the intuition behind discounting the state distribution? I understand the intuition behind discounting the rewards, but not the state distribution.

This term (discounted state distribution or discounted state visitation distribution) is mentioned in the DPG ([http://proceedings.mlr.press/v32/silver14.pdf](http://proceedings.mlr.press/v32/silver14.pdf)) and DDPG ([https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)) papers, in page 3 on both papers. It is also mentioned in this summary paper ([https://arxiv.org/pdf/1706.06643.pdf](https://arxiv.org/pdf/1706.06643.pdf)).",reinforcementlearning,papidant,False,/r/reinforcementlearning/comments/d8s33q/discounted_state_distribution/
AI Learns to Park - Deep Reinforcement Learning,1569344582,,reinforcementlearning,blogueandoatope,False,/r/reinforcementlearning/comments/d8q2ex/ai_learns_to_park_deep_reinforcement_learning/
multi-agent reinforcement,1569334887,Is there any entry-level resources for learning multi-agent reinforcement with example code?,reinforcementlearning,sabumjung,False,/r/reinforcementlearning/comments/d8nvkr/multiagent_reinforcement/
"All Implementations I found of a Soft Actor Critic never use the discounted return. Only the immediate reward, is there a reason for this?",1569332572,,reinforcementlearning,ronsap123,False,/r/reinforcementlearning/comments/d8ndr8/all_implementations_i_found_of_a_soft_actor/
Building Rainbow step by step with TensorFlow 2.0,1569311091,"[Blog: Building Rainbow step by step with Tensorflow 2.0](https://github.com/Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning/blob/master/tutorial_blogs/Building_Rainbow_Step_by_Step_with_TensorFlow2.0.md) 

[Github code](https://github.com/Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning) 

It's not perfect now, I'm also an rl beginner now, maybe need someone's help on the distribution rl and noisy network implementations, but I will also try my best to make it work soon. 

I hope it can be helpful to someone, and any suggestions are more than welcome and thankful. Have a good day\~",reinforcementlearning,H_uuu,False,/r/reinforcementlearning/comments/d8jww0/building_rainbow_step_by_step_with_tensorflow_20/
[P] RLtime: PyTorch RL library for DQN/IQN/C51/R2D2/PPO/A2C [SOTA Atari learning with Recurrent IQN],1569284273,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d8fe9k/p_rltime_pytorch_rl_library_for/
Best Way to indicate an invalid action?,1569281059,"Hi everyone,

In my environment the agent can be in environment state 1 Where the action space is 3 And environment state 2 Where the action space is 2. See it like Mario in 2D platformer When he gets a Fire flower power up, his possible actions increase, so what i did is have a action space of 3 And have, When Its in environment state 2 A punishment for taking the third invalid action, which does nothing. However I came to notice that it Really struggles with that, aswell for finding out When it is an invalid action Or not, even though indicated in states, onehotencoded. But besides the results, is there Another method besides Just simply punishing it to indicate an “invalid” action? 

Thanks for responding in advance,

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/d8eqj6/best_way_to_indicate_an_invalid_action/
Trying to find PyTorch DQN repo reproducing results from Mnih et al (2015) in Atari envs other than Pong,1569272915,"My DQN implementation successfully reproduces the result for *Pong* but achieves subpar scores for other environments (Tested on *Space Invaders, Enduro, Breakout, Seaquest, Beam Rider*). Thus, I wanted to find a PyTorch implementation that successfully reproduced results from Mnih et al. (2015) ""without any modification"", but surprisingly, all the repositories I found did not meet my needs! 

If anyone happens to know one, I would be very grateful if you could share it! :)

By ""without any modification"", I mean

* All the hyperparameters in Extended Data Tables 1 used as-is (ex. RMSprop, not Adam)
* No additional tricks to improve result (DDQN, DuDQN, PER, Soft target update, etc.)",reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/d8cyf0/trying_to_find_pytorch_dqn_repo_reproducing/
Number of hidden layers and nodes in neural networks,1569266086,"What is the rule for deciding the number of hidden layers and nodes in neural networks? I was thinking that I had problem in my PPO implementation because it had been always stacked at 190 return (max is 200 for CartPole). Now Ive changed the structure of the neural network and the results are a bit better (at least it approximates). What can I do with it to make the results better? I ve done GAE, changed learning rate and now the structure of the neural network but it has still not been better than VPG.

[The structure of the neural network is 2 hidden layers \(16 nodes in each\) , one ouput layer \(2\)](https://i.redd.it/9wf3xfhf7eo31.png)

&amp;#x200B;

[The structure of the neural network is 2 hidden layers \(32 nodes in each\) , one ouput layer \(2\)](https://i.redd.it/cea5siap7eo31.png)",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/d8bc75/number_of_hidden_layers_and_nodes_in_neural/
Learning reinforcement learning,1569245761,I am currently a graduate student and i want to focus my studies on reinforcement learning. Any advice for me to go from newbie to expert( i mean have at least a good understanding of RL). I want to be able to write RL algorithms. thanks,reinforcementlearning,mingchuandaxue,False,/r/reinforcementlearning/comments/d86mod/learning_reinforcement_learning/
TD3 in realworld robotics,1569244166,Are there examples of TD3 being used in robotics?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d86bdy/td3_in_realworld_robotics/
Spinningup algorithms,1569235709,I do not how to say it exactly but are the algorithms from OpenAI Spinning-up same as OpenAI baseline in terms of learning performance?  Can their implementation be used for research/paper where I would compare it with OpenAI DDPG paper?,reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/d84tqz/spinningup_algorithms/
motivation behind ACER,1569231105,"ACER was developed as a stable, sample efficient, off-policy policy optimization algorithm. The inherent instability comes from importance weights that can have huge variance. Authors tackled this problem by truncating importance weights and applying bias correction.

My question is: why not just use prioritized experience replay? One could set the weighting for sampling proportional to importance weight, for example.",reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/d845vw/motivation_behind_acer/
Batch updating for DQNs versus two neural networks for a DQN algo?,1569214941,"The tutorial in this post explains the concept well: [Blog\_post\_link](https://towardsdatascience.com/reinforcement-learning-w-keras-openai-dqns-1eed3a5338c). However, is it really necessary to have a target model to estimate the Q function while another model for predicting actions?

I see the point of batch updating but it is totally unnecessary to have two neural network models for this purpose. Instead, we can simply use a single model to predict the actions and after ***accumulating enough*** samples then we can simply update the weights of the single model i.e., updates the model weights occasionally. This gets the job done right?",reinforcementlearning,lolster007,False,/r/reinforcementlearning/comments/d81xm6/batch_updating_for_dqns_versus_two_neural/
Question,1569182034,"Why is it that model-free reinforcement learning is so popular compared to model-based RL? 

Is it that it requires less computational power? Works better?",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d7vkte/question/
Looking people interested in RL to join our Drone challenge team,1569095510,"Hey RL lovers , we have created a RL team from our reddit channel about Microsoft NeurIPS2019 Airsim challenge which involves planning and perception challenges. We have lots of  people already working with some of us just tutoring and helping while some of us actually developing it. We are always interested in more people since its very fun to meet people and learn different things about RL  
[https://github.com/microsoft/AirSim-NeurIPS2019-Drone-Racing](https://github.com/microsoft/AirSim-NeurIPS2019-Drone-Racing)  
Here's challenge  
If you are interested in RL and done some work already even though they are basic or tutorials and really passionte to work about this project and RL in general(like learning new algorithms etc)  


We have a group on Whatsapp , a weekly discord meetings and repo on Github which you will join if you want to meet. We are kinda multi diverse and very different parts of worlds(USA,japan,jamaica,turkey,france)  so we try to make a hour to fit all people in Saturdays mostly. In week you can talk from whatsapp group etc.   


If you are interested hit me dm !",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/d7fbxy/looking_people_interested_in_rl_to_join_our_drone/
Discrete action space for SAC,1569093249,"Hi, I am trying to see if SAC performs better than PPO on discrete action spaces on Retro or Atari env (openai's gym). But from what I know, is that SAC only outputs actions that are meant for continuous action space, Should I even attempt this experiment, or just stick to PPO? it seems like PPO and rainbow are SOTA for atari games and what not, with the exception of a few games. 

&amp;#x200B;

Thanks!",reinforcementlearning,hlsafin,False,/r/reinforcementlearning/comments/d7ett3/discrete_action_space_for_sac/
DeepMind Has Quietly Open Sourced Three New Impressive Reinforcement Learning Frameworks,1569083749,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/d7crvh/deepmind_has_quietly_open_sourced_three_new/
Computational resources for replicating DQN results,1569074324,Hi I want to replicate DQN and its variant UBE-DQN on Atari 57 games. What computational specs are recommended?,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/d7atfi/computational_resources_for_replicating_dqn/
Ideas for implementation RL within Delta Robot,1569058178,"Hi RL community, I need help. I would like to use RL within robotic applications for my Master thesis. My topic can be about separating potatoes with ABB delta robot (Fig. 1), because the client is complaining about commercial separators  that often smash potatoes. I think that idea is nice, because after that it can be use for even complicated tasks as manipulation with softer objects etc (tomatoes, meat, etc). However, do you think about an idea how can I use RL for this task? I do not want it to be in style of: ""if all you have is a hammer,  everything looks like a nail"", but I would really like to focus on RL within robotics. 

One idea crossed my mind, which is: neural networks but also RL algorithms have problem with very frequent actions. If I would find the solution in case of this super fast robot, it could be usable for much slower robots. (Ive seen this delta robot being used in pancakes factory. It moves 450 pancakes per minute).

The ""problem"" with delta robot is that it moves only in 3 axis (x,y,z) and can also rotates with gripper. Because of this fact, I am not sure if it is the correct robot for manipulating with objects of different shapes.  


&amp;#x200B;

[Fig. 1: The ABB delta robot ](https://i.redd.it/vtar3ylx1xn31.png)

I welcome suggestions, 

Thank you",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/d78d15/ideas_for_implementation_rl_within_delta_robot/
Does anyone have implementation examples of tabular Bayesian RL algorithms?,1568988708,"I first saw tabular Bayesian RL in [Ian Osband](https://iosband.github.io/research.html)'s thesis nearly a year ago and it has caught my imagination ever since. Right now I am looking for implementations and am not having much luck Googling for it. From what I remember of it back when I first read the thesis, it seems that unlike regular tabular RL which just propagates scalar values through nodes, the Bayesian way of doing tabular RL would be to propagate and update conjugate distributions. The actually algorithm was introduced more than a decade ago, probably in the [Bayesian Q learning paper](https://www.aaai.org/Papers/AAAI/1998/AAAI98-108.pdf) and I haven't seen this ever covered in any of the courses.",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/d6vhdx/does_anyone_have_implementation_examples_of/
Confusing about the PPO2 implementation in OpenAI baselines,1568979696,"Hi everyone, i'm a little bit confused with the PPO implementation in OpenAI baselines. Should PPO2 update central model by applying the average gradient of all minibatchs (just like SGD)? But it seems that the model will update at each minibatch, instead of calculating the average gradient and then applying the average of them.

&amp;#x200B;

https://i.redd.it/6zu4q54hkqn31.png

Can anyone explain it for me? Thanks a lot!",reinforcementlearning,Ruixiao-Zhang,False,/r/reinforcementlearning/comments/d6tnyq/confusing_about_the_ppo2_implementation_in_openai/
Has anyone implemented a common replay buffer for two different RL algorithms?,1568953794,"Replay buffer is used by most state of the art algorithms like SAC, TD3, etc. Has there been any attempt to use a common buffer for two algorithms like both SAC and TD3 actors create tuples and append them to the same buffer, and for learning phase both the algos sample from that buffer. Stuff like PER can’t be used but I think random sampling should work. And if there’s a study on this did the algos perform much better than normal implementations?",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/d6pqek/has_anyone_implemented_a_common_replay_buffer_for/
Implementation of A Deep Q-Network for the Beer Game,1568952756,"After going through the paper on ""A Deep Q-Network for the Beer Game: Reinforcement Learning for Inventory Optimization"", I am looking for its implementation. I have understood most of the concepts and methodologies used in the paper. But, I am having a problem implementing those algorithms and pseudocode into the code. Any help would be appreciated. Thanks.

P.S. I am looking for collaborations to work on a Supply Chain Management project as well.",reinforcementlearning,rojans,False,/r/reinforcementlearning/comments/d6pjrg/implementation_of_a_deep_qnetwork_for_the_beer/
In what order should I learn RL algorithms?,1568921093,"I've been working through ""Spinning Up"" from OpenAI: [https://spinningup.openai.com/en/latest/user/algorithms.html](https://spinningup.openai.com/en/latest/user/algorithms.html)

    Vanilla Policy Gradient (VPG)
    Trust Region Policy Optimization (TRPO)
    Proximal Policy Optimization (PPO)
    Deep Deterministic Policy Gradient (DDPG)
    Twin Delayed DDPG (TD3)
    Soft Actor-Critic (SAC)

I was wondering which order I should learn the algorithms in? Any opinions?

I am already comfortable with VPG. According to the first link, the Spinning Up course is trying to build towards PPO and SAC, which they say are two start of the art algorithms. Based on that, it looks likes there's two paths building up to these two algorithms: VPG -&gt; TRPO -&gt; PPO, and VPG -&gt; DDPG -&gt; TD3 -&gt; SAC. Is this correct? If that is the case I suppose I can work through either of the two paths?

I tried to learn them in order, but am currently intimidated by all the math notation in TRPO.",reinforcementlearning,Buttons840,False,/r/reinforcementlearning/comments/d6ixx5/in_what_order_should_i_learn_rl_algorithms/
"""Better Rewards Yield Better Summaries: Learning to Summarise Without References"", Böhm et al 2019",1568911923,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d6gtd2/better_rewards_yield_better_summaries_learning_to/
"""Fine-Tuning GPT-2 from Human Preferences"" [training text generation using human ratings of quality]",1568909287,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d6g7js/finetuning_gpt2_from_human_preferences_training/
Applications are open for the Open Phil AI Fellowship,1568904296,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d6f1qg/applications_are_open_for_the_open_phil_ai/
PPO oscillates around max return value,1568896753,"I think that maybe Ive done a simple PPO algorithm for solving CartPole-v0 environment (max score is 200). I compared results with my Vanilla Policy Gradient (VPG) implementation and the results are as follows:   


1. With PPO agent learns cca 2 times faster as in VPG
2. However, after getting to the range 150-200 (return) PPO oscillates in this range and it seems that it will not approximate to stable 200. This is not a problem in VPG

[PPO performance, x\_ax is number of epochs, y\_ax is a mean of returns per epoch](https://i.redd.it/42f41vboojn31.png)

[VPG performance, x\_ax is number of epochs, y\_ax is a mean of returns per epoch](https://i.redd.it/xbzk8734pjn31.png)

My code is based on SpinningUp implementation of PPO. My questions are:

 1) Are those oscillations caused by faults in my code or it is caused by wrong values of hyperparameters/structure of neural networks (small amount of nodes etc)?  
 2) If everything is alright, how can I reduce those oscillations? As the advantage function I used A = r + V(st+1) - V(st)",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/d6dg8a/ppo_oscillates_around_max_return_value/
[Question] Question in PyTorch's REINFORCE example,1568885769,"In [PyTorch's example of REINFORCE](https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py).

There is a line as following: [link to code](https://github.com/pytorch/examples/blob/ee964a2eeb41e1712fe719b83645c79bcbd0ba1a/reinforcement_learning/reinforce.py#L70)

    returns = (returns - returns.mean()) / (returns.std() + eps)

*(*`eps` *is just a small number to prevent divided-by-zero)* In which, `returns` is the discounted total returns at each timestep t in an episode. Why does it do the standardization on `returns`, is it a kind of implementation of baseline?

Thanks!",reinforcementlearning,zbqv,False,/r/reinforcementlearning/comments/d6bmwn/question_question_in_pytorchs_reinforce_example/
"""Meta-Learning with Implicit Gradients"", Rajeswaran et al 2019",1568852778,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d667ax/metalearning_with_implicit_gradients_rajeswaran/
Recommendations on algorithms to train a (real) cartpole system,1568848969,"Hello, and I hope you're having a wonderful time learning! 

I have a real (as in physical, implemented with motors and sensors and all that) cartpole system (or as some of you control systems guys call it, inverted pendulum).

I'd like to use it to go a step beyond gym and train an  agent o nthis environment, which is very different from the one in gym since the agent has to learn to swing up the pendulum and keep it up,  taking action in a continuous action space. 

I'm torn between DDPG and A2C, I'd like a pointer on which should be more effective in my environment, or maybe some other suggestion. 

Thanks",reinforcementlearning,DaMLearner,False,/r/reinforcementlearning/comments/d65ep7/recommendations_on_algorithms_to_train_a_real/
Approaches for restless and arm-acquiring Multi-Armed Bandits,1568835190,"I understand basic Multi-Armed Bandit problems and have read basic stuff on approaches like gittins index and thompson sampling. From what i understand these only work for the basic Multi-Armed bandit problem though. Whittle (to my knowledge) introduced the notion of restless Multi-Armed Bandits, which change their mean payout, and the notion of arm-acquiring Multi-Armed Bandits, i.e. Agents Adding more and more 'levers' - that is an expanding action space.   


Are there any good ressources (maybe even understandable) on approaches for Multi-Armed Bandits, which combine these two properties? That is, approaches for restless, arm-acquiring Muti-Armed Bandits?",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/d62wek/approaches_for_restless_and_armacquiring/
policy optimization with experience replay?,1568820316,Could someone point me towards a policy optimization algorithm that utilizes experience replay?,reinforcementlearning,stevethesteve2,False,/r/reinforcementlearning/comments/d5zdej/policy_optimization_with_experience_replay/
[P] I used A2C and DDPG to solve Numberphile's cat and mouse game!,1568816346,,reinforcementlearning,diddilydiddilyhey,False,/r/reinforcementlearning/comments/d5ygjh/p_i_used_a2c_and_ddpg_to_solve_numberphiles_cat/
"""The unexpected difficulty of comparing AlphaStar to humans"", AI Impacts",1568814674,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d5y35x/the_unexpected_difficulty_of_comparing_alphastar/
SAC applications,1568800496,Does anybody know of an application in robotics using SAC which isnt from from the developers who made it?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d5vgjv/sac_applications/
[1909.07373] Policy Prediction Network: Model-Free Behavior Policy with Model-Based Learning in Continuous Action Space,1568799712,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/d5vcmf/190907373_policy_prediction_network_modelfree/
"Play Hide and Seek , Artificial Intelligence Style",1568745192,,reinforcementlearning,adssidhu86,False,/r/reinforcementlearning/comments/d5l48w/play_hide_and_seek_artificial_intelligence_style/
Why Playing Hide-and-Seek Could Lead AI to Humanlike Intelligence,1568740011,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/d5jx3x/why_playing_hideandseek_could_lead_ai_to/
PPO principle,1568731017,"I am trying to implement PPO algorithm. However, there is a problem with that ratio, in which I am comparing a probability of taking certain action (using a new policy) with probability of taking (same?) action in (same??) state (using old policy).  
1. The first question is: Am I comparing actions, which were taken under same conditions (same states) but which were output from different policies? Do those actions have to be also same, or just states in which they were taken have to be same? I think that just states have to be same.  
2. The second question is about the aforementioned ratio. According to the equation for policy update, I need new and old policy. Does it mean, that I need two neural networks (one represents new policy, the other one old policy)? Also how should I do that algorithm? After collecting experience for the first time, should I update the second network with Vanilla Policy Gradient ( log pi(at|st) \* A )? Then feed the updated second network with the same states, which were collected during agent's interaction and then I should have log\_probs of new and old policy for the same inputs(states) but the difference is in actions. The final step is that the second network == the first network and then the first network will be updated with PPO update. Do I understand it right? In Spinningup tutorial and also in PPO publication this isn't explained. In code they use completely different approach, which is not equal as pseudocode. It is completely unknown how should someone get a new policy, which I need for getting new policy??? (it's like being in a magic circle).  


Thank you all for your help",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/d5hw19/ppo_principle/
"OpenAI spinning up: difference between ""variables""",1568717507,"1) I am trying to write PPO algorithm based on OpenAI spinning up tutorial/code. However, I am confused because of the meaning of variables: logp and logp\_pi. Below is description of these variables but can someone give me a clear explanation what is the difference? For computing the ratio they use:

    ratio = tf.exp(logp - logp_old_ph)          # pi(a|s) / pi_old(a|s)

[Legend of variables](https://i.redd.it/kg2i0xake4n31.png)

The placeholder ""logp\_old\_ph"" is fed with collected values of log\_pi during agent's interaction in environment. Why dont they collect values of logp instead?

&amp;#x200B;

2) PPO uses logp(new)/logp(old). According to my understanding, I would save a logp at each timestep and then use it as logp(new). After the first policy update, I would save that buffer of logp as logp(old). What should I use as the first values for logp(old)?

&amp;#x200B;

Thank you for your help",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/d5f8mo/openai_spinning_up_difference_between_variables/
[RL+NLP] Why there are only a few papers about using RL to solve dependency parsing?,1568683430,"Directly optimize the UAS score of dependency parsing seems to be an intuitive idea, but there are only a few papers talk about this. I mean RL + dependency parsing by design a transition system, predict the next step transition by using policy, and update this policy by using the final UAS score as a reward.

But to my knowledge, only (*Yogatama* et al., 2016) Has tried this, but their reward is from downstream tasks, not directly from the UAS score. Why people don't do this easy-to-think idea? because it is hard to train? or because the performance is not better than supervised parser?

If there are some papers I missed, please let me know.",reinforcementlearning,speedcell4,False,/r/reinforcementlearning/comments/d59yp0/rlnlp_why_there_are_only_a_few_papers_about_using/
A2C for Hanabi underfits,1568663565,"Hello everyone. 

I am trying to solve the game of Hanabi ([paper describing game](https://arxiv.org/pdf/1902.00506.pdf)) with actor-critic algorithm. I took code for the environment from the Deepmind's [repository](https://github.com/deepmind/hanabi-learning-environment) and implemented a2c algorithm myself. From the tests on simple gym environments (lunar lander, cartpole) my implementation takes about the same time to converge as A2C from OpenAI/baselines. So, I suppose that algorithm is implemented correctly and problem is somewhere else.

In the paper, actor critic algorithm used is IMPALA, but I am currently using classic A2C, because I thought that difference should not be that crucial.  Also, I do not use population based training yet, but in Hanabi paper there is plot showing that even with out PBT their algorithm shows decent results.

Basically, problem is that I can not even perfectly solve so called ""Hanabi-Very-Small"" which is a simplification (state space of dim.100 instead of 600). It hits 3 points out of 5 after some training and then learning curve saturates and on the full environment learning stops at \~5.8 pts/25  

I have been playing with hyper parameters, such as learning rate, gradient clipping and weight of entropy term in the loss function but it did not help. Architecture is fc256-lstm256-lstm256, same as in Hanabi paper.

Since I do not have much experience in RL, I am confused with this behaviour and do not know what is the reason, so I am asking for hints on where the problem could be.

Firstly, can it be simple because IMPALA is better algorithm in general? I expected A2C to work worse, but 5.8 vs 24 seems to be to much of a difference.

Another question is how to search for optimal hyper-parameters effectively? Learning, even for very small version of the game, takes several hours until it saturates and doing greed search would take to long? Are there any common heuristics that could help? Also, is the choice of hyper-parameters that important? I mean can it really change behaviour from ""largely undefits"" to ""nearly perfect"" ?

Thanks in advance?",reinforcementlearning,gr1pro,False,/r/reinforcementlearning/comments/d55m12/a2c_for_hanabi_underfits/
Model-based applications in robotics,1568641562,Im looking for modelbased RL applications using real robotics but im having trouble finding more than 2 examples. Anybody knows any applications?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d50vob/modelbased_applications_in_robotics/
"""Hierarchical Foresight: Self-Supervised Learning of Long-Horizon Tasks via Visual Subgoal Generation"", Nair &amp; Finn 2019 {GB}",1568585888,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d4rjum/hierarchical_foresight_selfsupervised_learning_of/
Tricks and adaptions for PPO,1568583589,"In spinningup OpenAI claim, that there exist several tricks to optimize training in PPO, which can be found in different implementations: [""there are a bunch of tricks used by different PPO implementations to stave this off""](https://spinningup.openai.com/en/latest/algorithms/ppo.html)

Except for early stopping, which is explained there, can you give other examples?",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/d4r0o7/tricks_and_adaptions_for_ppo/
Super low Critic Loss?,1568576324,"Hi in my DDPG I Got the Critic loss Where the loss is like Really Really low...

It would look like https://media.discordapp.net/attachments/481815689405923329/564457320613150720/unknown.png

Very low And rapidly decreasing. You could Say that this is “Good” however the network Just performs worse than bad. I am highly suspecting that the critic loss has something to do with it Because i’ve never seen such low losses. I have my rewards normalized to the -1,1 range. It is a sparse reward space however other sparse environments Don’t show such behaviour. Anyone has an idea Why this is happening And what the solution could be?

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/d4pciu/super_low_critic_loss/
PyTorch implementation of 17 Deep RL algorithms,1568558414,"For anyone trying to learn or practice RL, here's a repo with working PyTorch implementations of 17 RL algorithms including DQN, DQN-HER, Double DQN, REINFORCE, DDPG, DDPG-HER, PPO, SAC, SAC Discrete, A3C, A2C etc..

Let me know what you think!

[https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch](https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch)",reinforcementlearning,__data_science__,False,/r/reinforcementlearning/comments/d4l9vf/pytorch_implementation_of_17_deep_rl_algorithms/
intrinsic curiosity modul (icm) continuous action space,1568477104,I have problems to understand the intrinsic curiosity modul. When i use a normal distribution (with mean = proposed action and varianz = uncertainty of that action) which action should i calculate the intrinsic reward with. The sample or the mean. And which should i use to calculate the inverse\_loss with?,reinforcementlearning,Piyt1,False,/r/reinforcementlearning/comments/d473f8/intrinsic_curiosity_modul_icm_continuous_action/
"Catastrophic ""Un-Learning"" in PPO: A plausible Cause and Solution?",1568414956,"Greetings all! Many of you may have heard about or encountered the phenomenon of sudden training instability, where in a single (or a few) passes of the optimizer, the entire policy is ruined ([here's another thread describing it](https://www.reddit.com/r/reinforcementlearning/comments/bqh01v/having_trouble_with_ppo_rewards_crashing/?utm_source=share&amp;utm_medium=web2x))

I've been experimenting, and I think I might know what causes it to occur in some cases, but I'm not exactly qualified to confirm or prove it mathematically (I'm still a machine learning novice).

So here's the concept: advantage non-linearity; *surprise.*

When an agent is refining a particular policy, the mean reward tends to increase, and as the policy stabilizes, the standard deviation starts to decrease. When reward deviation is reliably low, I am guessing that the PPO advantage estimator starts looking to consider smaller and smaller reward deviations as indicating a ""trust zone"" of X size. In other words, when reward is very stable, the network starts taking larger *gradient steps as a proportion of reward variance.* In layman's terms (my own :D),  if an agent has been receiving the same reward for 1000's of episodes, and suddenly the reward changes drastically without significant deviation in observations or actions, the gradient step that accounts for that episode will be relatively larger because the math assumes some specific action change *must* have caused the massive deviation.

So, in complex problems and environments there are often hard-to-foresee ""consequences"" which cannot be accounted for in advance. For example, imagine a walker agent that is unconstrained in a 3d space. If we train it to run in a specific direction using the classic directional velocity reward, it may begin to develop a successful running policy. But what if, in the course of learning to walk, the agent learns an action which is good for walking, but also promotes accidental \***turning\*. As the agent's max reward starts to increase, it may then accidentally turn around and walk in the wrong direction, which would result in a massively anomalous negative reward, which subsequently min/maxes the weights of the entire network in a single back-propagation phase.**

As the agent is closing in on what it thinks is a very good strategy, it suddenly discovers an emergent pitfall that the advantage estimator just cannot deal with.

I have managed to overcome the catastrophic unlearning with two main strategies: First, reducing the learning rate is an obvious way to deal with this. It's also possible to pause training at peak performance (before the crash) and reduce the learning rate before continuing. This is an unsatisfying solution though, because it wrecks training times.  


The second strategy is to **INCREASE the observation dimensionality**. If the observational inputs that reflect why the reward is suddenly negative are too few among many other observations, then it will be much harder for the network to correlate reward deviation with observational deviation (and therefore the actions which caused them). Obviously, it is important to add the right kinds of observations for this strategy to be effective (they mus be observations which relate directly to the variables that the rewards are derived from, so that the advantage estimator can *see the difference* between the old successful actions/observations/rewards and the now suddenly bad new set of actions/observations and rewards.

A possible third strategy might be to increase the buffer and batch sizes, but in experimenting this did not actually solve the issue. I'm skeptical that this would fundamentally address the problem because the negative delta reward can still emerge just as suddenly, and in consistent proportions. Intuitively the problem looks like a contradiction: a previously excellent policy is now suddenly extremely bad, for no apparent reason (hence the larger gradient descent step?), which is why I'm not sure if merely increasing sample size would address the issue.

If I am right about this as a cause of catastrophic instability, then I can say that anticipating when and where it will occur will be an ongoing problem in reinforcement learning projects. Due to the nature of non-linear emergence in complex systems, we basically have to just *wait and see what happens* when exploring new environments and problems. That said, this also means that it will be very useful to develop different sets of best practices which apply to like-problems and like-environments.",reinforcementlearning,Flimflamm,False,/r/reinforcementlearning/comments/d3wym2/catastrophic_unlearning_in_ppo_a_plausible_cause/
"""Hierarchical Decision Making by Generating and Following Natural Language Instructions"", Hu et al 2019 {FB} [imitating pairs of humans playing a text-coordination version of MiniRTS]",1568394245,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d3s9i7/hierarchical_decision_making_by_generating_and/
[Question] Multitask Replay Buffer,1568368035,"Hello,
When doing multitask learning via an off-policy algorithm like DDPG, is it crucial to use a replay buffer for each task ? Why ?
Note that the tasks are similar in a sense that action and state spaces are the same. Rewards are sparse and dynamics don't differ that much. For example let's consider the 4 mujoco robotics Fetch tasks.",reinforcementlearning,ahmed-akakzia,False,/r/reinforcementlearning/comments/d3mv2b/question_multitask_replay_buffer/
"Practical, problem solving models for different types of tasks?",1568364114,"Hey guys! I’ve been in the ML game for a while but haven’t really deep dived into RL and what’s going on with it for a while. I’m wondering if you guys could possibly share some practical examples of using RL to solve problems for different types of tasks that doesn’t necessarily involve putting an agent into an environment. For example, fluid flow, recommendation, search, user behavior prediction etc. From my understanding, RL is super applicable to robotics for example, but is there a way to get the agent to learn how to, for example, what the user’s course of action should be in a given type of situation? How would you go along coding this? It’s possible with feed forward networks (multi-class classification with features and labels), but is it possible for RL? Any examples? Cheers!",reinforcementlearning,chemkick,False,/r/reinforcementlearning/comments/d3mbwr/practical_problem_solving_models_for_different/
Need help understanding the implementation of Differentiable Inter-Agent Learning (DIAL),1568340212,"I am currently reading [Learning to Communicate with Deep Multi-Agent Reinforcement Learning](https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf). I understand the way RIAL works.

For DIAL, the paper states that each agent receives a feedback from other agents about the message it sends. However, in the [Pytorch implementation](https://github.com/minqi/learning-to-communicate-pytorch/blob/master/agent.py) of the project, the loss for DIAL looks like the standard TD-error to me. Am I missing something? How would the agent receive the feedback about the sent messages with just DQN error? 

Also, the code loops over the rewards for every agent, which makes me question whether the reward for an agent at a given time step is a local reward.",reinforcementlearning,rohitbokade94,False,/r/reinforcementlearning/comments/d3iayb/need_help_understanding_the_implementation_of/
[R] Using Fractal Neural Networks to Play SimCity 1 and Conway’s Game of Life at Variable Scales,1568339403,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d3i53z/r_using_fractal_neural_networks_to_play_simcity_1/
[Probabilistic &amp; Statistic course for RL],1568302439,"Does anyone know a good course in Probabilistic &amp; Statistics commonly used in RL. When I read papers, I often feel lost when I meet keywords such as ""Gaussian Process, Gaussian Mixture Models, Bayesian Models, Probabilistic/Deterministic Neural Networks, Probabilistic Dynamics Models, etc.""... Thanks in advance.",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/d39m9c/probabilistic_statistic_course_for_rl/
Modelfree DRL robotics applications,1568280820,Is anyone aware of proven modelfree DRL applications in robotics?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d35fep/modelfree_drl_robotics_applications/
Inconsistant Results in DDPG,1568277373,"In my DDPG Algorithm that i am using the results are very inconsistant. The rewards show usually a sideways trend and the network is suffering from exploding gradient, this is weird, because i have my State and Rewards all normalized to the \[-1,1\] range, so exploding gradient is really weird why that happens. The problem is that sometimes i may get a good run where it doesn't explode, but i could also get a run where it doesn't explode and actually works. I got informed that this is just due to random weight init causing the runs to be diffrent everytime, which i know. So it said when it works, i get lucky and the task is to get lucky more often than unlucky, but i can't resolve it because the network explodes inconsistant without any signs of what the issue is... Again, i normalized the State and Rewards and my LR is like super low, i tried lower and then it just saturates slower. Anyone has any idea or tips?

&amp;#x200B;

Network specs: LR\_A = 0.0001, LR\_C = 0.0002, GAMMA = 0.99, BATCH-SIZE = 64, MEMORYR = 3000, Hidden Layers Critic = Leaky ReLu (128 Units), Hidden Layers Actor = Leaky ReLu(128) + Leaky ReLu 128. I tried normal relu aswell so as diffrent batch sizes such as 128,96,48,256. Epsilon = 0.1

&amp;#x200B;

Thank you for helping in advance!",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/d34yme/inconsistant_results_in_ddpg/
"PG methods are ""high variance"". Can I measure that variance?",1568248891,"I've been working through [https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html).

I understand that one of the primary difficulties with policy gradient methods is their ""high variance"". I have an intuitive understanding of this, but can the variance be measured / quantified?

Some personal background: I posed a few months ago about having trouble solving the Lunar Landar OpenAI Gym environment, and got some good advice. I kept trying to add more ""tricks"" to my algorithms, and eventually they sorta-kinda worked sometimes. Then I implemented a minimal vanilla PG algorithm and it turned out to be less sample efficient, but more reliable. It was less likely to get stuck in local maximums. Lesson learned: keep it simple to begin with.

My vanilla PG algorithm was working, but I wanted to add a baseline, and every baseline I've added seems to lead to local maximums and much worse performance. Since the purpose of the baseline is to reduce variance, I wanted to measure if this was actually the case.",reinforcementlearning,Buttons840,False,/r/reinforcementlearning/comments/d2zzbg/pg_methods_are_high_variance_can_i_measure_that/
Looking for machine learning Master/PhD students for internship,1568217091,"Hi,

This is somewhat unconventional, but I gonna give it a try over reddit, since many of you frequent this sub. 

We're looking for a machine learning Master/PhD student (ideally with a focus on reinforcement learning) for an internship in 2020 (or potentially earlier). This would be in a financial domain with a high chance of getting directly hired full-time should things work out, with extremely competitive compensation. Research revolves around analyzing large data sets in order to find profitable trading signals, reducing transaction costs, and other similar ideas that have positive impact on the bottom line. 

Please ONLY pm me if you're interested as I will not answer any more questions on this post and make sure to include your github profile - it's by far the best way to get a good overview on your skills.",reinforcementlearning,HiringResearchers,False,/r/reinforcementlearning/comments/d2s9f2/looking_for_machine_learning_masterphd_students/
[1902.02186] Distilling Policy Distillation,1568211096,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/d2qsa2/190202186_distilling_policy_distillation/
RL Hardware question,1568208068,Would I need to have a desktop with a gpu if I would like to implement SAC on a ball balancing system and train it in realtime? Could I do with a desktop without gpu or simply use a laptop? Do I not need powerful hardware considering you have to wait on the environment a bit? What hardware is generally recommended for modelfree deep RL in realtime robotics considering I wont be using CNN's?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d2q2lm/rl_hardware_question/
Question,1568193956,When using function approximation for value estimation it is said that there are no garantuees for convergence. Could someone maybe give reasons for why the training can destabilize in some situations.,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d2ne53/question/
Looking for RL project idea involving autonomous driving or drones,1568186426,"Hello ,  I am looking for project ideas (for my graduation project ) involving autonomous driving  or drones which i will able to simulate in gazebo , unity , unreal engine or any other virtual simulation platform. I was thinking autonomous driving on heavy traffic on some Unity city / unreal city with only frame data used or maybe parking problem ?  for drones i though maybe making quadrocopter steady or something. I really didnt use drones that much so i dont know what problems they have.  Would love suggestions , thanks",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/d2mbhp/looking_for_rl_project_idea_involving_autonomous/
"""AI-GAs: AI-generating algorithms, an alternate paradigm for producing general artificial intelligence"", Clune 2019 {Uber}",1568148257,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d2e5b1/aigas_aigenerating_algorithms_an_alternate/
"""DipNet for 'No Press Diplomacy': Modeling Multi-Agent Gameplay"", Paquette et al 2019 [agent &amp; library for playing _Diplomacy_ without communication]",1568134363,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d2agjm/dipnet_for_no_press_diplomacy_modeling_multiagent/
ACM Fellow Michael Littman on TalkRL: Reinforcement Learning Interviews,1568129587,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/d297rg/acm_fellow_michael_littman_on_talkrl/
"""Automated deep learning design for medical image classification by health-care professionals with no coding experience: a feasibility study"", Faes et al 2019 [AutoML case study for medical images]",1568058190,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d1vxl0/automated_deep_learning_design_for_medical_image/
[SB Book Excercise - Newbie],1568035914,"Ex 3.8 - Page 44,45: Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

My answer: Is it because we give a reward of zero at all other times making the robot to wander around. Instead we should give negative reward for each step that the robot runs so that it should try to escape the maze as fast as it can.",reinforcementlearning,hhn1n15,False,/r/reinforcementlearning/comments/d1qy0g/sb_book_excercise_newbie/
How should I do to implement a multi-step TD learning within a Deep Q-Network?,1568032411,"Recently, I was trying to implement a DQN which combines several other methods like, (prioritized) experience replay, multi-step learning, etc. When I tried to implement the multi-step learning, I felt confused that, I tried to build it before storing an experience, i.e. normally, one-step td learning, we just store a one step reward into the replay buffer, that \`(s, a, one-step\_r, next\_s, done)\`, but when it comes to multi-step, I want to save the sum of n-step discounted rewards directly to replace the one-step reward, that \`(s, a, sum\_of\_n-step\_discounted\_r, n\_step\_later\_s, done)\` into the replay buffer, but I don't know why this kind of implementation performs really bad.(maybe something wrong).   

Another method is still store \`(s, a, r, ns, done)\` into the replay buffer, where different is when I sample from the replay buffer to let the agent learn, I will take n-1 more continue experiences after each sampled experience by their index. But there is a problem, if it is prioritized experience replay, the experiences are not a sequential storage by order, so ... I don't know how, and hope someone can give some advice here, Thanks a lot.",reinforcementlearning,H_uuu,False,/r/reinforcementlearning/comments/d1qah2/how_should_i_do_to_implement_a_multistep_td/
Residual algorithms,1568031139,Why arent algorithms being used that have convergence guarantees. I read a bit about residual algorithms that are proven to converge. Whats holding them back from being used?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d1q2im/residual_algorithms/
List of Reinforcement Learning Papers Accepted to NeurIPS 2019,1567963131,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/d1dthz/list_of_reinforcement_learning_papers_accepted_to/
[R] DeepMind Starcraft 2 Update: AlphaStar is getting wrecked by professionals players,1567956805,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d1ce40/r_deepmind_starcraft_2_update_alphastar_is/
How to set a openai-gym environment a specific initial state not the `env.reset()`?,1567926258,"Today, when I was trying to implement an rl-agent under the environment openai-gym, I found a problem that it seemed that all agents are trained from the most initial state: \`env.reset()\`, i.e.  

\`\`\`python

import gym

env = gym.make(""CartPole-v0"")

initial\_observation = env.reset()  # &lt;-- Note

done = False

while not done:

action = env.action\_space.sample()  

next\_observation, reward, done, info = env.step(action)

env.close()  # close the environment

\`\`\`

So it is natural that the agent can behave down the route \`env.reset() -(action)-&gt; next\_state -(action)-&gt; next\_state -(action)-&gt; ... -(action)-&gt; done\`, this is an episode. But how can an agent start from a sepecific state like a middle state, then take an action from that state? For example, I sample an experience from the replay buffer, i.e. \`(s, a, r, ns, done)\`, what if I want train the agent start directly from the state \`ns\`, and get an action with a \`Q-Network\`, then for an \`n-step\` steps forward. Something like that:  

\`\`\`python

import gym

env = gym.make(""CartPole-v0"")

initial\_observation = ns  # not env.reset() 

done = False

while not done:

action = DQN(ns) 

next\_observation, reward, done, info = env.step(action)

\# n-step later or done is true, break

env.close()  # close the environment

\`\`\`

But even though I set a variable \`initial\_observation\` as \`ns\`, I think the agent or the \`env\` will not aware it at all. How can I tell the \`gym.env\` that I want set the initial observation as \`ns\` and let the agent know the specific start state, get continue train directly from that specific observation(get start with that specific environment)?",reinforcementlearning,H_uuu,False,/r/reinforcementlearning/comments/d17s3n/how_to_set_a_openaigym_environment_a_specific/
"Looking for collaborations, cool projects ideas .. !",1567900549,"Hello !

I just took a gap year after my Bachelor in Computer Science. I now have the time to work on projects I enjoy ! I am looking for maybe collaborations or any ""cool"" projects ideas.

&amp;#x200B;

""Cool"" ideas is pretty personnal but to give you a feeling of what I enjoy can be creating an agent that solves this golbin-lake puzzle ([https://www.youtube.com/watch?v=V0V3LMK40iI](https://www.youtube.com/watch?v=V0V3LMK40iI)). I do love when an AI solves a problem in an unexpected way !

&amp;#x200B;

*Background* : BSc in Computer Science / Bachelor thesis in AI Safety (robustness in DeepMind gridworlds) for the tabular case. I currently have no experience with **deep** RL but I can learn on the fly with projects !

&amp;#x200B;

Enough about me, I am happy to look forward for your ideas ! **Propose anything**, even an idea that might be off-topic, this is open :)

&amp;#x200B;

Thank you !",reinforcementlearning,fruits_et_legumes,False,/r/reinforcementlearning/comments/d13osk/looking_for_collaborations_cool_projects_ideas/
Deep Reinforcement Learning Roadmap,1567869559,"What is the roadmap I need to follow to learn deep reinforcement learning?

Thank you for guiding me",reinforcementlearning,milad_farzalizadeh,False,/r/reinforcementlearning/comments/d0x8y6/deep_reinforcement_learning_roadmap/
Why does the gradient in the DQN paper not have a negative sign?,1567863660,"Grad theta_i on the loss: (r + max a' Q(s, a'; theta_{i-1})  - Q(s, a; theta_{i}))^2 should have a negative sign, shouldn't it?",reinforcementlearning,ada_td,False,/r/reinforcementlearning/comments/d0w3q3/why_does_the_gradient_in_the_dqn_paper_not_have_a/
"[D] ""Evolution Strategies"", Lilian Weng",1567797723,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d0l900/d_evolution_strategies_lilian_weng/
[D] Looking for a collaborator for an Inverse Reinforcement Learning project idea,1567793634,,reinforcementlearning,banksyb00mb00m,False,/r/reinforcementlearning/comments/d0kccr/d_looking_for_a_collaborator_for_an_inverse/
"""rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch"", Stooke &amp; Abbeel 2019 [agent &amp; training framework, previously 'accel_rl'; agents: R2D2/A2C/PPO/DQN/DDQN/CDQN/DDPG/TD3/SAC]",1567791181,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d0jsrr/rlpyt_a_research_code_base_for_deep_reinforcement/
"If DeepMind makes DQN as patent, it is also illegal to use Rainbow DQN, DRQN, DDQN, Dueling DQN commercially?",1567774302,,reinforcementlearning,fantastic_dog,False,/r/reinforcementlearning/comments/d0g8xf/if_deepmind_makes_dqn_as_patent_it_is_also/
I am testing LunarLanderContinuous-v2 and LunarLander-v2 with DQN/DDPG. What is Continous version of LunarLander? What is difference compared to original LunarLander-v2?,1567770989,,reinforcementlearning,fantastic_dog,False,/r/reinforcementlearning/comments/d0fnvw/i_am_testing_lunarlandercontinuousv2_and/
Early stopping in Deep rl models?,1567768132,We generally stop the training of a deep learning model by looking at when the validation accuracy is least so as to prevent overfitting of the model. This is usually the premise of early stopping. However I wanted to ask whether there is any advantage in using early stopping for training Deep rl models (by looking at the average rewards over few test episodes) makes any sense. Rl models should ideally overfit over the state action space in order to give good rewards right? Correct me if I'm wrong. Any thoughts?,reinforcementlearning,ramak27,False,/r/reinforcementlearning/comments/d0f6zu/early_stopping_in_deep_rl_models/
Simple Random Search Provides A Competitive Approach To Reinforcement Learning,1567758866,,reinforcementlearning,mystikaldanger,False,/r/reinforcementlearning/comments/d0dwpx/simple_random_search_provides_a_competitive/
Notation of value functions,1567758206,In the notation of value functions they use R_t+1 but in common envs like they use in gym they add R_t to the replay buffer if im not mistaken. Does this matter? Why is  this done? Anybody has good resources on this?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/d0dtz6/notation_of_value_functions/
What are some good articles about RL in robotics?,1567741144,,reinforcementlearning,dimem16,False,/r/reinforcementlearning/comments/d0bcmd/what_are_some_good_articles_about_rl_in_robotics/
"""R2D3: Making Efficient Use of Demonstrations to Solve Hard Exploration Problems"", Le Paine et al 2019 {DM} [R2D2 augmented with expert replay buffer]",1567717587,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/d06ufb/r2d3_making_efficient_use_of_demonstrations_to/
[D] optimizing clipping functions,1567715490,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/d06d9w/d_optimizing_clipping_functions/
AI Learns to Park - Deep Reinforcement Learning with Unity ML-Agents,1567682630,,reinforcementlearning,SamuelArzt,False,/r/reinforcementlearning/comments/czzhkw/ai_learns_to_park_deep_reinforcement_learning/
SOTA Q-Learning in PyTorch,1567678237,"Hi,

I've posted an RL library focused on state-of-the-art q-learning features. It supports IQN and most rainbow and R2D2 features. In particular combining IQN with an LSTM and additional features gives promising (sample efficient) results on Atari.

[GitHub](https://github.com/opherlieber/rltime)

[Recurrent IQN details and results](https://github.com/opherlieber/rltime/blob/master/docs/atari_iqn_lstm.md)",reinforcementlearning,olieber,False,/r/reinforcementlearning/comments/czytth/sota_qlearning_in_pytorch/
How does one literately update the stable baselines agents?,1567664923,"Hi, I'm looking to do a step wise update on the stable baselines PPO2 agent after having trained it i.e. after having run `model.learn(10000)` I would like to update the agent from within a for loop on a step by step basis:

    agent = PPO2()
    env = gym.make('LunarLander-v2')
    while True:
        action = agent.predict(obs)
        observation, reward, done, info = env.step(action)
        agent.update(observation, reward)

This might seem redundant, however it proves useful for fine tuning an agent's weights on real-world tasks.

Some advice on how I might effectively achieve this would be truly helpful!",reinforcementlearning,AlphaExchange,False,/r/reinforcementlearning/comments/czx2g5/how_does_one_literately_update_the_stable/
What are the best RL algorithms for robotics e.g. controlling a robotic arm?,1567655687,,reinforcementlearning,dimem16,False,/r/reinforcementlearning/comments/czvnya/what_are_the_best_rl_algorithms_for_robotics_eg/
Need TD3 performance diagnostic help in custom environment.,1567610865,,reinforcementlearning,bridats,False,/r/reinforcementlearning/comments/czmdms/need_td3_performance_diagnostic_help_in_custom/
Need TD3 performance diagnostic help in custom environment.,1567610168,"Hi all, 

I'm fairly new to DRL and was hoping to get a bit of help in interpreting the TD3's performance on my custom environment as I lack a lot of the intuition needed to change the parameters in a way that could remedy the problem. 

As you can see in the image, the critic loss converges too quickly to learn anything meaningful and the actor loss actually trends upwards almost right out of the gate. If I let it run well beyond this point, the losses eventually diverge exponentially (followed by converging, diverging again, and so on). The environment requires 15 consecutive actions before getting a reward. I can't give partial rewards intermittently before the end of the episode, it is only realizable at the end. It is a problem with an unknown optimal policy, so the parameters have to allow for adequate exploration.

I have kept all parameters virtually the same as the original paper 
 for a baseline (I used the Pytorch implementation), except for changing the actor's final layer activation function to sigmoid (my environment requires an action between 0 and 1). The dim of my observation space is (54,) and input data is normalized (although I'm aware this might not be necessary, I don't think it could be causing the weird behaviour but perhaps I'm wrong).

I'm trying to understand what is intuitively happening in the graphs and field the sub for suggestions about next best remedial steps. I really appreciate your help!

Please let me know if more information is needed, although I thought this would be a good start.",reinforcementlearning,bridats,False,/r/reinforcementlearning/comments/czm803/need_td3_performance_diagnostic_help_in_custom/
Stanford cs234 - Assignment 1 - problem 1 - Optimal Policy for Simple MDP,1567589959,"Hi! I have just begun studying reinforcement learning by taking cs -234 lectures.  I was going through written assignments. I am kind on stuck in problem one.  


The question is given here : [https://web.stanford.edu/class/cs234/assignment1/assignment1.pdf](https://web.stanford.edu/class/cs234/assignment1/assignment1.pdf)

&amp;#x200B;

The question asks to compute optimal value function for all states.  


I was able to compute optimal value function for goal state and which matches with the solution (solution link : [https://web.stanford.edu/class/cs234/assignment1/assignment1\_sol.pdf](https://web.stanford.edu/class/cs234/assignment1/assignment1_sol.pdf))

&amp;#x200B;

I didn't quite understand the solution mentioned for optimal value function for other states.  
Can someone give more detailed explanation to the solution for computing optimal value function for other states (with proper steps)?  


May be I am missing something.  


Thanks.",reinforcementlearning,swagh1611,False,/r/reinforcementlearning/comments/czilbo/stanford_cs234_assignment_1_problem_1_optimal/
How to tackle 'Class imbalance' in RL?,1567583192,"In Deep Learning we have methods to take care of datasets where 99.9% of the dataset comprises of one class and 0.01% comprises of the other. How do we take care of a similar problem in RL? 

To explain further, 

Consider a 10x10 gridworld where there is a door in one grid and everywhere else is empty. The agent and door are spawned randomly in this world. 

Agent State = (Agent Location) + (Door location) 

Agent Actions = Up + Down + Left + Right + 'Go through door'

Rewards = +ve for going through door (and episode ends), -ve otherwise (episode continues)

Say I'm using an off-policy learning method like DDPG or Soft Actor-Critic, where I collect experiences and train using those experiences. Now, MOST of these experiences would consist of the first four actions, and very few experiences would consist of the 'go through door' action (which lead to actually going through door. Of course the agent might've tried to go through door when it's not at the door which is not helpful), meaning there's a massive imbalance and it doesn't get enough experience to learn when to correctly use the 'go through door' action.

  
Does it make sense to create artificial situations where the agent is very close to the door, thus creating more experiences where the 'go through door' action leads to positive reward? Is there any other way to go about this? 

(Note: This is slightly different from the 'sparse reward' problem which papers light Hindsight Experience Replay tackle. This is more of a 'Sparse correct action' problem)",reinforcementlearning,shura04,False,/r/reinforcementlearning/comments/czhqga/how_to_tackle_class_imbalance_in_rl/
Policy - other ways of making/representing policy,1567545270,"Hi, I would like to ask you what are the other ways of making/representing policy than neural networks?",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/czaze3/policy_other_ways_of_makingrepresenting_policy/
[Research] Tensor-train-based VAE model generates a novel drug in 21 days,1567538983,,reinforcementlearning,aiismorethanml,False,/r/reinforcementlearning/comments/cz9l0b/research_tensortrainbased_vae_model_generates_a/
"""GENTRL: Deep learning enables rapid identification of potent DDR1 kinase inhibitors"", Zhavoronkov et al 2019",1567535386,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cz8rvy/gentrl_deep_learning_enables_rapid_identification/
Natasha Jaques on TalkRL: Reinforcement Learning Interviews,1567528303,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/cz77gd/natasha_jaques_on_talkrl_reinforcement_learning/
"RL Weekly 30: Learning State and Action Embeddings, a New Framework for RL in Games, and an Interactive Variant of Question Answering",1567527927,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/cz74mt/rl_weekly_30_learning_state_and_action_embeddings/
My class project this semester - LIDAR based obstacle avoidance with (not deep) reinforcement learning. Feedback and stars appreciated :P,1567501111,,reinforcementlearning,uakbar,False,/r/reinforcementlearning/comments/cz29k5/my_class_project_this_semester_lidar_based/
"how to save , load this dddqn model?",1567457951,"i am beginner ,    how to save/load this dddqn mode ?  thanks

[https://www.kaggle.com/itoeiji/deep-reinforcement-learning-on-stock-data/notebook](https://www.kaggle.com/itoeiji/deep-reinforcement-learning-on-stock-data/notebook)",reinforcementlearning,asda43asdf23423,False,/r/reinforcementlearning/comments/cyuy98/how_to_save_load_this_dddqn_model/
Learn deep learning,1567454959,"Actually i am a unity game developer and want to learn how deep learning is used in game development. 
Can anybody helps me where i have to start ?",reinforcementlearning,moaazafzal,False,/r/reinforcementlearning/comments/cyua3c/learn_deep_learning/
Assessing the performance of an agent in a customized environment,1567442817,"Hello community.

I am currently training an agent to solve a customized environment I have created in gym but I was wondering: 

- first, how should I define the reward above which the environment is considered solved?

- second, how the training/testing must be done in terms of the seeding to assess the performance and more importantly the robustness of the agent?

Many thanks in advance.",reinforcementlearning,white_noise212,False,/r/reinforcementlearning/comments/cyrhb1/assessing_the_performance_of_an_agent_in_a/
Using Reinforcement Learning to solve Gridworld,1567439410,,reinforcementlearning,tvganesh,False,/r/reinforcementlearning/comments/cyqpn1/using_reinforcement_learning_to_solve_gridworld/
Intuition about CNN in games like Go ?,1567434944,"Hi everyone,

I'm a computer science student and I've started to take an interest in reinforcement learning, trying to learn how I could create an agent that could learn to play adversarial, turn-based, perfect information games like Go (or tic-tac-toe at least ?). And one thing that kind of bothers me is that, in every implementation of such games, when the state space becomes too large (like the 19x19 Goban grid), people tend to use convolutional layers in their neural networks. This bothers me because, to me, CNN are here to generalize the information contained in the input (think of atari games: if my character is on the left of the screen, and my inputs are the pixel values, my character moving one pixel to the left or right is negligible), but it feels highly counter intuitive to use in a game where a move on a certain space is completely different than a move on an adjacent space (think of 5 in a row, you don't want to blunder juste because your position was ""close enough"" to winning)... So, could you guys help me understand the intuition behind this please ?",reinforcementlearning,db00n,False,/r/reinforcementlearning/comments/cypqw5/intuition_about_cnn_in_games_like_go/
Implementation Recommendation system based Deep Reinforcement Learning,1567413951,"I'm going to implement a recommendation system based Deep Reinforcement Learning for a research job, What do you think is the best library for implementation? TF-Agent or PyTorch or etc?

Thank you for guiding us",reinforcementlearning,milad_farzalizadeh,False,/r/reinforcementlearning/comments/cymga7/implementation_recommendation_system_based_deep/
Inverse Dynamics in Mujoco,1567364241,"I am trying to understand how to use mujoco's \`mj\_inverse\` function? (using mujoco-py)  
Ideally I'd like to be able to write a minimal code snippet where I run forward dynamics on an object using some fixed arbitrary control, then recover the control inputs using \`mj\_inverse\`.  I am having trouble doing this using only mujoco's official documentation as I find it quite concise and tough to understand. Is there a better tutorial out there?   


Thank you.",reinforcementlearning,ziii93,False,/r/reinforcementlearning/comments/cydyi9/inverse_dynamics_in_mujoco/
Q Density Plotting for RL agent debugging,1567276557,"In Lillicrap et al., 2016 \[1\], they had this great visualization of a q density plot:

[Q Density Plot: y is the model's estimation, x is the actual returns.](https://i.redd.it/te7mi0yzutj31.png)

As a quote from their paper ""***Density plot showing estimated Q values versus observed returns sampled from test episodes on 5 replicas.*** *In simple domains such as pendulum and cartpole the Q values are quite accurate. In more complex tasks, the Q estimates are less accurate, but can still be used to learn competent policies. Dotted line indicates unity, units are arbitrary*"" \[1\]

I thought ""wow finally a good visualization for a multidimensional state space"". At the moment the only good debugging plots for rl agents are a heatmap over a simple discrete domain (2d mazes) and maybe loss plots.

What I am curious about is how the ""actual returns"" are calculated? Originally I was going to just sum Q via rolling rewards are over an episode but then I realized that Q is comparative across episodes thus increasing the complexity of calculating Q.

Am I over complicating this?

How did they calculate the actual returns over continuous domains?

Is it possible the ""actual returns"" are like naive q values where q is only for that episode?

\[1\] Lillicrap, Timothy P., et al. ""Continuous control with deep reinforcement learning."" *arXiv preprint arXiv:1509.02971*(2015). [https://arxiv.org/pdf/1509.02971.pdf](https://arxiv.org/pdf/1509.02971.pdf)",reinforcementlearning,jlaivins,False,/r/reinforcementlearning/comments/cxyz6a/q_density_plotting_for_rl_agent_debugging/
Experiment Managers for RL?,1567255031,"I'm thinking of starting to use an experiment manager for my RL research, but I am not sure which tools are available and which one to choose. I've looked at [allegro.ai](https://allegro.ai) 's TRAINS framework but could not find any alternative.

Does anyone have other alternatives or recommendations? 

I would very much like to see something that features visualisation of saved variables and to be TF 2.0 compatible 

&amp;#x200B;

Thanks in advance!",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/cxvjdk/experiment_managers_for_rl/
YouTube using RL for Recommendations?,1567242849,"Recently, YouTube has started to ask me to rate recommended videos - ""Is this a good video recommendation for you?"".  
I can't help but wonder if they have started to use Reinforcement Learning for recommendations? The ratings seem to be their way of getting immediate rewards for the agent.  


Any thoughts on this?

https://i.redd.it/cmnt3t5s3rj31.jpg",reinforcementlearning,theAB316,False,/r/reinforcementlearning/comments/cxtx04/youtube_using_rl_for_recommendations/
Ideas on training how to play a futile game?,1567214466,"I am an amateur learning reinforcement learning on my spare time and I  was working on building a go engine suitable for playing handicapped games (for non-go-players, imagine playing chess with your opponent having an extra queen) and the problem is that the net eventually figures out that playing is futile and since it can't win it just learns how to play randomly.

I can imagine that a good player will play moves that is more likely to get the opponent confused, make the game much more complicated and create a situation that one small mistake will eventually lose the game.  Unfortunately all these are something difficult to quantify.

Any advice?",reinforcementlearning,veggie1101,False,/r/reinforcementlearning/comments/cxpuuz/ideas_on_training_how_to_play_a_futile_game/
"""Categorizing Wireheading in Partially Embedded Agents"", Majha et al 2019",1567213747,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cxpqgc/categorizing_wireheading_in_partially_embedded/
Questions about UCT (UCB applied to Trees),1567177951,"In Auer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs, there is no C_p in the proof, however, in Kocsis, L., Szepesvári, C., &amp; Willemson, J. (2006). Improved monte-carlo search, it is not clear to me why under Assumption 1, c_{t, s} can be derived. 

Thank you in advance!",reinforcementlearning,yyt224,False,/r/reinforcementlearning/comments/cxi9ft/questions_about_uct_ucb_applied_to_trees/
[D] Policy Distillation in a continuous action space with no knowledge of teacher distribution,1567125403,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/cx9k5h/d_policy_distillation_in_a_continuous_action/
"I've finally made a Q learning code that solves sliding tile puzzles, but it still needs help!",1567097360,"    import numpy as np
    import random
    import copy
    
    basearray = np.array([i for i in range(4)])   # The area of the grid is 4
    #This is where you can decide the dimensions of the puzzle. If you want it 3x3, switch the 4 above with 9 and the 2's in the two following lines with 3
    
    goalgrid = np.array(np.array_split(basearray, 2)) #the solved grid
    basegrid = np.array(np.array_split(basearray, 2)) #the starting grid before it gets scrambled, so basically, the solved grid as well
    
    unsolvedgrid = [[3, 0], [1, 2]]   #PLUG IN YOUR UNSOLVED GRID HERE. Row lengths should match the dimensions you set earlier.
    
    
    def locate(basisgrid, jqo):    #locates a given number jqo in the grid, by returning a list containing the vertical index and horizontal index
        locationindex = np.where(basisgrid == jqo)
        return [i[0] for i in locationindex]
    
    def Neighbors(basisgrid, jqo):  #returns the list of neighbors of zero
        npgrid = np.array(basisgrid)
        ind = np.where(npgrid == jqo)
        coord = [ho[0] for ho in ind]
        raw = npgrid[coord[0],]
        coll = npgrid[:,coord[1]]
        nbrs = []
        dimen = np.array([raw, coll])
        polarity = [1,-1]
        for dim in dimen:
            for pol in polarity:
                addr = (np.where(dim == jqo))
                ad = addr[0][0]
                nbr = [n for n in dim[ad::pol] if n != jqo]
                nbrs.append(nbr)
        return [nbr[0] for nbr in nbrs if len(nbr) != 0]
    
    sequence = []     # the list that will containing each transformed matrix in the randomization.
    
    def exchangement(basisgrid, jqo):
        global sequence
        newgrid = copy.copy(basisgrid)
        zerolocation = locate(basisgrid, 0)
        jqolocation = locate(basisgrid, jqo)
        newgrid[zerolocation[0]][zerolocation[1]] = jqo
        newgrid[jqolocation[0]][jqolocation[1]] = 0
        return newgrid
    
    def randomizer(basisgrid, Ncounter, Ntimes):
        def randomize(basisgrid, Ncounter, Ntimes):
            zeroloc = locate(basisgrid, 0)
            choices = Neighbors(basisgrid, 0)
            choice = np.random.choice(choices, 1)[0]
            nextrandgrid = exchangement(basisgrid, choice)
            sequence.append(nextrandgrid)
            Ncounter += 1
            randomizer(nextrandgrid, Ncounter, Ntimes)
        if Ncounter &lt; Ntimes:
            randomize(basisgrid, Ncounter, Ntimes)
        elif Ncounter == Ntimes:
            return basisgrid
    sequence.append(basegrid)
    randomizer(basegrid, 0, 100)            #iteration is 100. If your grid is 3+x3+, you better use a bigger number.
    #sequence = np.unique(sequence, axis=0)
    
    sequsquare = np.array(sequence[-1])  #the unsolved grid is the last of the randomization sequence.
    
    def ExchNbrs(nparrayA, nparrayB):    #determines whether 2 grids are one move apart
        if [list(a) for a in nparrayA] != [list(b) for b in nparrayB]:
            zerolocationA = locate(nparrayA, 0) # where 0 is in A!
            jqo = nparrayB[zerolocationA[0]][zerolocationA[1]]   #What value is in 0's location in B
            comparisonGrid = exchangement(nparrayA, jqo)
            if [list(cg) for cg in comparisonGrid] == [list(npB) for npB in nparrayB]:
                return True
            else:
                return False
        else:
            return False
    
    gamma = 0.5
    
    RewardMatrix = [[-1 for _ in range(len(sequence))] for _ in range(len(sequence))]
    for sA, sequsquareA in enumerate(sequence):
        for sB, sequsquareB in enumerate(sequence.copy()):
            if ExchNbrs(sequsquareA, sequsquareB) == True:
                if [list(sb) for sb in sequsquareB] == [list(gg) for gg in goalgrid]:
                    RewardMatrix[sA][sB] = 100
                elif [list(sb) for sb in sequsquareB] != [list(gg) for gg in goalgrid]:
                    RewardMatrix[sA][sB] = 0
            elif ExchNbrs(sequsquareA, sequsquareB) == False:
                RewardMatrix[sA][sB] = -1
    sequence = np.array(sequence)
    Qmatrix = [[0 for _ in range(len(sequence))] for _ in range(len(sequence))]
    
    sequencedict = [[sq, sequsquare] for sq, sequsquare in enumerate(sequence)]
    
    initstate = random.choice(sequencedict)
    
    def Find_Domain(initstate):
        current_vert_row = RewardMatrix[initstate[0]]
        domain = np.where(np.array(current_vert_row) &gt;= 0)
        return domain[0]
    
    def randnext(state_Domain):
        next_horiz_index = np.random.randint(0, len(state_Domain)) #One action
        return next_horiz_index
    
    def update(sequsquare, nextswap, gamma):
        max_indice = [j for j, i in enumerate(Qmatrix[nextswap]) if i == max(Qmatrix[nextswap])]
        if len(max_indice) &gt; 1:
            max_index = random.choice(max_indice)
        else:
            max_index = max_indice[0]
        rewardmax = Qmatrix[nextswap][max_index]
        gammaval = gamma * rewardmax
        Qmatrix[sequsquare[0]][nextswap] = gammaval + RewardMatrix[sequsquare[0]][nextswap]  
    #update(initstate, nextswap, gamma)
    
    for i in range(100000):
        update(initstate, randnext(Find_Domain(random.choice(sequencedict))), gamma)
    
    
    print(""########################################################"")
    print(""Now, the given grid"")
    print(""########################################################"")
    print(""The unsolved puzzle"")
    print(np.array(unsolvedgrid))
    
    sequenceunwrapped = [[list(row) for row in list(matrix)] for matrix in list(sequence)]
    
    givengrid_index = sequenceunwrapped.index(unsolvedgrid)
    
    procedure = [givengrid_index]
    solutionseq = [np.array(unsolvedgrid)] #solution sequence (the list of matrix between the unsolved grid and the solved grid)
    
    def recurser(puzzlegridindex):
        global sequence
        puzzlegrid = sequence[puzzlegridindex]
        if [list(pg) for pg in puzzlegrid] != [list(gg) for gg in goalgrid]:
            nextgrid_indice = [j for j, i in enumerate(Qmatrix[puzzlegridindex]) if i == max(Qmatrix[puzzlegridindex])]
            if len(nextgrid_indice) &gt; 1:
                nextgrid_index = random.choice(nextgrid_indice)
            else:
                nextgrid_index = nextgrid_indice[0]
            nextgrid = sequence[nextgrid_index]
            #nextgrid = sequence[puzzlegridindex][nextgrid_index]
            procedure.append(nextgrid_index)
            solutionseq.append(nextgrid)
            puzzlegridindex = nextgrid_index
            recurser(puzzlegridindex)
        else:
            print(""Done: the puzzle is solved"")
            print(puzzlegrid)
            print(""^^^^^^^"")
            return solutionseq
    
    recurser(givengrid_index)
    
    print(""The solution sequence's length: {}"".format(len(procedure)))
    for solseq in solutionseq:
        print(solseq)
        print(""~~~"")

I've posted here a few times about my project before, and I finally hit some progress. So I've made a code with q-learning that solves sliding tile puzzles (In this case they are 2x2). We're treating the 0 as the blank space that the neighboring numbers (representing the tiles), switches places with. When I first posted this problem, I was stuck because the number of states for the puzzle was too big to work with, and will get worse if it had been a 4x4 or a 5x5 puzzle. So I've learned to do random samples of the puzzle with the randomizer, and use the sequence as my sample state. Other than that, it's a basic Q-learning code.

But it still is faulty. If I were to run it,  the code will still solve the puzzle. But...if you read through the solution sequence, you'll get some reasonable lines of grids, ...but other times, you'll get a grid that does not chronologically proceed from the previous grid, and I don't know if these grids are legitimate, out-of-place errors, or if the code for some reason skipped stuff.

I was also wondering how we can make this code return the shortest paths. I think it's because of the randomization sequence, which gets a lot of redundant grids. I altered the problem by making the sequence a set, but its currently dormant as a comment:

    #sequence = np.unique(sequence, axis=0)",reinforcementlearning,MidThought_Pause,False,/r/reinforcementlearning/comments/cx3hvz/ive_finally_made_a_q_learning_code_that_solves/
What is the difference between Value Iteration and TD Learning?,1567053717,"I was asked this question in an interview, although during the interview (due to nervousness and bad sleep), I couldn't clearly find any difference, but I think both are similar except that value iteration's objective is to learn state values, while in TD learning we are dealing with state-action values. Also, value iteration is strictly model based, while TD learning could be model based or not.

Am I right in my approach? What is the actual difference between the two?",reinforcementlearning,mr_denoza,False,/r/reinforcementlearning/comments/cww4cl/what_is_the_difference_between_value_iteration/
Doubt in curriculum Learning,1567050527,"In curriculum learning you make the agent perform well on simpler tasks and then move on to more complex ones. This might involve either changing environments or reward functions (reward shaping). Now with respect to the actor critic approach, let’s say our agent learns the first simple task, now when we change to a more difficult task we change the reward function. Since the reward function has changed do we throw the critic and initialise a random one cause the critic is biased? Or it’s still better than a random one and after sufficient updates it will work just fine?",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/cwvlzf/doubt_in_curriculum_learning/
"""Evolving Space-Time Neural Architectures for Videos"", Piergiovanni et al 2018 {GB}",1567036481,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cwszwu/evolving_spacetime_neural_architectures_for/
"""Top-K Off-Policy Correction for a REINFORCE Recommender System"", Chen et al 2018 {G} [scaling to millions of items for YouTube video recommendations]",1567030537,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cwrsde/topk_offpolicy_correction_for_a_reinforce/
Question,1567027208,"For a project I need to balance a ball on a plate which I have some control over.

Is there an algorithm in deep reinforcement learning that is proven to converge?

Would there be one that can handle a continuous  action space of say 6? Or should I just have to try out SAC and try to clarify why its possible it CAN work?",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/cwr1zd/question/
[R] Universal Policies to Learn Them All,1566996886,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/cwkf2h/r_universal_policies_to_learn_them_all/
"What are the must-read, ""milestone"" papers of the past 5-10 years?",1566959247,"As title, were there any new theory or methodologies developed in the past 5-10 years that altered the state of research in the field?",reinforcementlearning,kevinccccccc,False,/r/reinforcementlearning/comments/cwelh8/what_are_the_mustread_milestone_papers_of_the/
OpenSpiel: new DeepMind multi-game/environment RL library (&gt;25 games/&gt;20 agents; Python Tensorflow/C++/Swift; Apache license) {DM},1566923240,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cw6t45/openspiel_new_deepmind_multigameenvironment_rl/
"Q-learning: ""Greedy in the Limit with Infinite Exploration"" convergence guarantee",1566910305,"I have a question about Q-learning convergence.

My understanding is that Q-learning is guaranteed to converge if the training is ""GLIE"": Greedy in the Limit with Infinite Exploration

Eg [from here](http://webee.technion.ac.il/people/shimkin/LCS11/ch7_exploration.pdf):

&gt; A learning policy is called GLIE (Greedy in the Limit with Infinite Exploration)
&gt; if it satisfies the following two properties:
&gt; 
&gt; 1. If a state is visited infinitely often, then each action in that state is chosen infinitely
&gt; often (with probability 1).
&gt; 
&gt; 2. In the limit (as t → ∞), the learning policy is greedy with respect to the learned
&gt; Q-function (with probability 1).

This makes a lot of sense to me: you start training with an epsilon of 1, making sure any state could be reached, then you decrease it until it reaches 0, at which point your policy becomes fully greedy.

What surprises me is that in the DeepRL Bootcamp at Berkeley, they express these conditions in terms of **learning rate**, and not of **epsilon**.

[See here](https://drive.google.com/file/d/0BxXI_RttTZAhREJKRGhDT25OOTA/view):

&gt; Convergence conditions:
&gt;
&gt; - You	have	to	explore	enough
&gt; - You	have	to	eventually	make	the	learning	rate
&gt; small	enough
&gt; - …but	not	decrease	it	too	quickly

They mention this paper, which also expresses the GLIE condition in term of learning rate: [Convergence of Stochastic Iterative Dynamic Programming Algorithms](https://papers.nips.cc/paper/764-convergence-of-stochastic-iterative-dynamic-programming-algorithms.pdf)

Are both correct? does it mean that annealing **either** the learning rate **or** the epsilon factor down to 0 guarantees that you will reach an optimal policy? Or do I need both?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cw40ph/qlearning_greedy_in_the_limit_with_infinite/
"a few questions from RL newbie (save/load model , performance improvements)",1566901080," 

Hello I have few questions if you guys dont mind  ,  I am trying to train an agent to play Kung Fu Master no frame skip v4(opengym enviroment) , My algorithm is DDQN with Random sampling replay memory . I use deepmind wrapper (inside atari wrapper) which basically turns frame to gray scale with averaging method , then cuts score and bottom parts , downsamples and gives you 84x84 image (for pytorch)

&amp;#x200B;

I got implementations and theoric stuff from Udemy (pytorch RL course) and David Silver's youtube course. I think i got everything correctly and i am basically using a working algorithm from a paid course. 

    ###### PARAMS ######
    learning_rate = 0.0002
    num_episodes = 7500
    gamma = 0.99
    
    hidden_layer = 512
    
    replay_mem_size = 100000
    batch_size = 32
    
    update_target_frequency = 5000
    
    double_dqn = True
    
    egreedy = 0.9
    egreedy_final = 0.01
    egreedy_decay = 15000
    
    report_interval = 10
    score_to_solve = 35
    
    clip_error = True
    normalize_image = True
    
    file2save = 'enduro_save.pth'
    save_model_frequency = 10000
    resume_previous_training = True

Question 1 

 In this game end of level there is a boss karate master. I am struggling something because i cant get agent to beat boss ever.  Simply hitting boss wont give points just beating him gives points so agent barely reaches that point and rather waits and dont attack to boss or just simply dies (because even if he hits boss he just dies he doesnt use tactics like leg attack which i used how to beat him in actual emulator)  .  


I train on google collab machine which is estimated to be little slower than gtx 1060 gpu(6gb) version which i think its pretty great and better than my own gpu (nvidia 2gb gtx 840m)   
So what should l i do  ? is any of my parameters are viciously wrong? or just simply give more training time (or game is borked ? from design perspective)

\-----

&amp;#x200B;

Question 2

I have another problem whenever I save model and try to load it , it seems like only saving prediction nn parameters , target isnt(i know we load target as copy of prediction with some parameter step)  but it feels like when training again rewards are starting from 1 to get 13-14 takes too much time , whats point of saving model then i quite couldnt understand.( I basically load same model unless there is huge difference i made on system)  . I mean when i train another time  It starts like this

 

                if (i_episode % report_interval == 0 and i_episode &gt; 0):
                    
                    plot_results()
                    
                    print(""\n*** Episode %i *** \
                          \nAv.reward: [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f \
                          \nepsilon: %.2f, frames_total: %i"" 
                      % 
                      ( i_episode,
                        report_interval,
                        sum(rewards_total[-report_interval:])/report_interval,
                        mean_reward_100,
                        sum(rewards_total)/len(rewards_total),
                        epsilon,
                        frames_total
                              ) 
                      )

"" \*\*\* Episode 10 \*\*\*                        Av.reward: \[last 10\]: 0.80, \[last 100\]: 0.09, \[all\]: 0.82                        epsilon: 0.75, frames\_total: 2709 Elapsed time:  00:01:10 ""

   
 \*\*\* Episode 20 \*\*\*                        Av.reward: \[last 10\]: 1.00, \[last 100\]: 0.19, \[all\]: 0.90                        epsilon: 0.65, frames\_total: 4894 Elapsed time:  00:02:07   
 .. etc Isnt this reward  too much low ?  
Or is it normal i dont get it?

&amp;#x200B;

Question 3

\---

Also another thing (i am using your 4 frame stacked implementation from Breakout game)  , i quite dont understand rewards on pong (which was on course) we set beat to 30. But in breakout example I saw course guy  set to 300(as score to beat) i assume this changes for every game . But how do we scale reward and how do we know rewards for each game , i  couldnt find any reward information on OpenGYM website for that game.  (spesific rewards for each game ) I tried to print reward info while emulator loads but didnt get any big values or couldnt do it properly.",reinforcementlearning,paypaytr,False,/r/reinforcementlearning/comments/cw2fos/a_few_questions_from_rl_newbie_saveload_model/
RL library for discrete-continuous action space.,1566871867,"In my task, an agent must choose through a range of actions (lets say actions A, B and C) and perform them under a certain magnitude (lets say , float values between 0 and 100). EX: (action:A) (magnitude:35.74).

That is, a discrete-continuous action space. I was intending to use OpenAI baselines or the stable baselines fork but from what I gather, both cannot solve this problem. I got a bit stuck so I thought I might find some help here, like another library that allows me to set the problem just as I said or any other kind of help would be appreciated. I hope I am not being too dumb, I am new on the subject.",reinforcementlearning,AndReMSotoRiva,False,/r/reinforcementlearning/comments/cvxwyf/rl_library_for_discretecontinuous_action_space/
Reinforcement Learning Research Topics,1566855226,"Hi,
I am looking forward to exploring reinforcement learning. Please suggest some hot research topics in RL.",reinforcementlearning,Triple_Hash,False,/r/reinforcementlearning/comments/cvui6i/reinforcement_learning_research_topics/
Question,1566839613,Is it the case that a state-action value function is slightly better at predict its value than a state value function considering it ignores the current action to be taken? Or is it basically irrelevant?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/cvqzk1/question/
Go environment for training an agent using self play?,1566837006,"I'm looking for a go environment to train an AI to play 9x9 go using self-play in python 3. I've looked around for anything, but there isn't much to go off. Worst case I could always write one myself, but I'd feel better knowing the go rules and scoring were correctly implemented.",reinforcementlearning,Carcaso,False,/r/reinforcementlearning/comments/cvqe4w/go_environment_for_training_an_agent_using_self/
basic question about future expected return,1566830443,"I was just randomly reading about q learning so I don't know much about it, but I was just wondering in the equation where you check the future expected return, is that like a multiple recursion/tabulation thing or would you just check the table at the next potential point and not go any further to check the points after that ( because the score at the next point only should be enough after a large number of iterations )?",reinforcementlearning,disastorm,False,/r/reinforcementlearning/comments/cvoylt/basic_question_about_future_expected_return/
Multi-Agent Fighting Environments?,1566828475,"I was wondering if you know of any opensource envs for 2 agents in a fighting game fighting against each other like OpenAI’s “competitive self play” environments, Preferably in 2D. 

Thanks in Advance.",reinforcementlearning,denizkavi,False,/r/reinforcementlearning/comments/cvokhc/multiagent_fighting_environments/
Anyone succeeded in three poles of dm control suite?,1566754692,"Hello, I'm currently working on three poles of dm control suite(https://github.com/deepmind/dm_control).
I've tried SAC(around 8 million steps), but it doesn't seem to converge to optimal policy. If there's also any better algorithm than SAC, I'd really appreciate it. Thanks.",reinforcementlearning,Cerphilly,False,/r/reinforcementlearning/comments/cvbiti/anyone_succeeded_in_three_poles_of_dm_control/
[D] Good introductory review or book on RL?,1566731694,"Hi /r/reinforcementlearning,

I am rather novice in RL, and trying to learn the basics on my own because I would really like to learn to play around with some RL models. I have Sutton's book, and while it is a great resource to look up some specific methods/tools, it is a bit dense to read back to back to get a big picture of the main ideas in RL.

Would you have any recommendations of readings in the 50 to 150 pages range, of some well written reviews on the field? Ideally something that covers all the core concepts with some explanations, e.g. bandits, different types of DQN, continuous control, model-based/free RL, etc. but without diving into dozens of pages of technical details for each of them?

I found [https://arxiv.org/pdf/1811.12560.pdf](https://arxiv.org/pdf/1811.12560.pdf) which seems pretty good, and there is a tutorial by openAI that is quite well written as well [https://spinningup.openai.com/](https://spinningup.openai.com/), but would love to hear of other suggestions.

Thanks!",reinforcementlearning,erbb896,False,/r/reinforcementlearning/comments/cv7a0c/d_good_introductory_review_or_book_on_rl/
"""A critique of pure learning and what artificial neural networks can learn from animal brains"", Zador 2019",1566684607,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cuzuyk/a_critique_of_pure_learning_and_what_artificial/
"""The Digital Ludeme Project"" (Ludii): using MCTS to automatically evaluate playability of reconstructions of lost ancient board games",1566661619,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cuuzl6/the_digital_ludeme_project_ludii_using_mcts_to/
International Deep Reinforcement Group / Whatsapp,1566650663,"For people who are interested in sharing projects or discussing topics, papers or problems on Deep Reinforcement Learning. Feel free to join: [https://chat.whatsapp.com/JqnHaMnERtb8zRqWlhyQ6l](https://chat.whatsapp.com/JqnHaMnERtb8zRqWlhyQ6l)",reinforcementlearning,sedidrl,False,/r/reinforcementlearning/comments/cusy3t/international_deep_reinforcement_group_whatsapp/
I need clearing up stuff as I'm following through with this project for a Rubiks Cube.,1566617769,"I'm currently trying to do a project on solving Rubiks cubes with reinforcement learning. I've read 3 sources so far, from Medium, a Stanford paper, and the original paper itself. I have some confusion as I'm following through the papers, because there's a few gaps in the explanation, that may be easy for an experienced to fill in the blanks with, but is a challenge to someone who's still learning. I need help clearing stuff up.

So this is what I've followed with so far.

You take a cube in its normal form, or its solved form. You randomly shuffle the cube. You take the sequence of each transformation/configuration that the cube had undergone, between its initially unsolved form and its current scrambled form.

Let's call each individual configuration/state in that sequence, the sequence state.

You take each sequence state: for each sequence state, get the range of all of the possible immediate moves on that cube. I'll refer to a single one of these states as the ""sequence sub-state"". I'm guessing 2 of them will be redundant because it will be the exact same as the previous or next sequence state.

Now here's where I get lost:

According to the original paper, we get a state. (I don't know if its talking about a sequence state, or one of it's smaller sequence sub-states) We get that state and put it through the neural network, which creates an output pair of outputs: the value of the state, and the policy of the state (which is a list of probablities of each sub-state). I'm guessing the cube we have to input through the equation is a given sequence state, not one of it's substate, because in order its policy says a list of the probabilities of substates, so it can't be a given substate since it has nothing beneath it.

But anyways, I want to talk about the NN's output: the value and policy: It say's the value is calculated as the result (among all substates) of A) the substate's value + B) a 1 or -1. The B is decided if the given substate is or isn't the solved state. I'm assuming we're using a tanh function to decide that. I saved my biggest question for last, which is about getting A, the substate's value. What is the substate's value? How do we get that? The substate is a matrix, so I have nothing numerical to work with. None of the tutorials talk about how we get this part of the formula (the substate/s)? 

Also, when do weights and biases come into play, and what given information do we multiply them with?

For now, my mind is preoccupied up to this point of the project, so while I don't (currently) need help understanding how to do the policy output, (which requires softmax cross entropy), but it would be great if you could help me too on that.

If you can help explain this to me, I'll be really thankful. Please tone down explaining it with the mathematical Notations like upside down A's, because I won't know how to read it. Actually, if you can roughly explain it by writing like python, that would be more awesome. Thanks!",reinforcementlearning,MidThought_Pause,False,/r/reinforcementlearning/comments/cuofdm/i_need_clearing_up_stuff_as_im_following_through/
"""Learning to prove theorems via interacting with proof assistants"", Yang &amp; Deng 2019",1566576210,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cufvvr/learning_to_prove_theorems_via_interacting_with/
[D] When Taking Risks Is the Best Strategy (research on fishing fleets),1566573332,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cuf8zu/d_when_taking_risks_is_the_best_strategy_research/
[D] Deep Reinforcement Learning (research) engineer as MSc?,1566564184,"I am from Germany and I looked for jobs both here in Germany and USA for Deep Reinforcement Learning positions. Every single position I've found required a Ph.D. I understand why, the field is new and mostly academic / research work. Still I wonder if anyone has any information about getting a job maybe not as a researcher scientist (where Ph.D. would be required) but maybe as a research engineer when having a [M.Sc](https://M.Sc) degree? As a research engineer you implement papers to solve current problems. The question is, is there any hope for the field of Deep Reinforcement Learning currently? I know for ""classic"" Deep Learning (supervised etc) such positions exist, but I am very interested in deep RL. 

I am nearning the end of my [M.Sc](https://M.Sc). in robotics with the master thesis being on Deep Learning. I am teaching myself deep RL on my free time and would like to pursue a career in that field. I find RL and agents interacting with the environment fascinating. 

Would like to hear your opinion.",reinforcementlearning,Roboserg,False,/r/reinforcementlearning/comments/cudcso/d_deep_reinforcement_learning_research_engineer/
"Sounds good, doesn't work",1566561198,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cuctko/sounds_good_doesnt_work/
[D] Confusion about Value Iteration vs Policy Iteration,1566554729,"I am a bit confused by this ""value vs policy iteration” terminology.

For me, methods such as Q-learning were “Generalized Policy Iteration Methods”: you do a policy evaluation step by performing an action, and use the optimal Bellman equation to update the Q-function. Then you do a policy iteration step where you update your policy pi_n to pi_n+1 which uses this new Q-function in an eps-greedy way.

This is consistent with the statement from [4.6 in S&amp;B](http://www.incompleteideas.net/book/ebook/node46.html):
&gt; Almost all reinforcement learning methods are well described as GPI. That is, all have identifiable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy.

If you look at resources such as the [Berkeley AI course](https://youtu.be/2M7mv4-BPCg?t=1394), the Value Iteration method seems to instead rely on a known transition function T(s, a, s’), which is in general not doable in the context of a reinforcement learning problem.

This StackOverflow answer also seems to agree with these definitions: https://stackoverflow.com/a/42493295/318557

So all this makes sense.

Now what confuses me is the distinction done in this slide from the [Berkeley Deep RL bootcamp](https://youtu.be/S_gwYj1Q-44?t=78): 

I have seen this diagram before, and I don’t understand why Q-learning appears as a Value Iteration method, and why the critic in Actor-Critic Methods is connected to both Policy Iteration and Value Iteration.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cubsvg/d_confusion_about_value_iteration_vs_policy/
Question,1566548565,"Hello I'm fairly new to RL so forgive me if I'm asking a dumb question.

Am I wrong when I say that all algorithms as can be found in the stable baselines library use temporal difference learning? Or only the algorithms that use either a value or Q function?",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/cuay6b/question/
Game Emulators,1566539212,"I'm only a beginner in RL and I've used openai gym for basic games. I'm wondering how do I setup a 'game emulator' for any game? 

I came across this video. The game is Ori and the Blind Forest but I don't know where to begin to replicate something like this.

![video](wd82y549z4i31)",reinforcementlearning,ronaldo_gao,False,/r/reinforcementlearning/comments/cu9md7/game_emulators/
Planning vs Model based RL,1566507030,"Hello, I am having a doubt about the term planning.

If Im not mistaken in Sutton &amp; Barto 2nd edition the term planning and model based RL are synonimous. They use examples like **DynaQ** that incorporates data into the model to calculate Q-values. DynaQ is similar for example to **world models** where they can train inside a virtual world.

Instead in **PlaNet** they say the use planning but they do not calculate any state action value function or policy. They consider certain actions up until a certain horizon H and select the one that gives maximum return.

Both methods are called planning? Is there a word to differentiate them?

Initially I thought PlaNet was planning and DynaQ was an hybrid between model based and model free.",reinforcementlearning,LazyButAmbitious,False,/r/reinforcementlearning/comments/cu3g4u/planning_vs_model_based_rl/
Using larger epsilon with Adam for RL?,1566486964,"I just read, in [this article about using Radam](https://medium.com/autonomous-learning-library/radam-a-new-state-of-the-art-optimizer-for-rl-442c1e830564) for DRL:

&gt; Adam and other adaptive step-size methods accept a stability parameter, in Pytorch called eps, that increases the numerical stability of the methods by ensuring the estimate of the variance is always above a certain level. By default, this value is set to 1e-8. However, in deep RL eps if often set to a much, much larger value. For example, in the original DQN paper it was set to 0.01, six orders of magnitude greater than the default. RAdam accepts this parameter also, with the same default.

I had never paid attention to Adam's `eps` factor. Is this something important in your experience? any other insight on this topic?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/ctytuq/using_larger_epsilon_with_adam_for_rl/
AlphaStar and managing resources,1566457231,"StarCraft is a game that requires management of resources to utilize attacking units and to build more resource collectors. How does AlphaStar know that building resource collectors enables it to do other actions, when it's raw output is a distribution across all actions and illegal actions are manually masked out?",reinforcementlearning,rybkaa,False,/r/reinforcementlearning/comments/cttqvz/alphastar_and_managing_resources/
[D] Explaining Q-learning without using Bellman equations,1566381989,"I’m looking for the most intuitive way to explain Q-learning from scratch. 

Generally, explanations go something like this (in the deterministic setting):

* The Bellman optimality equation says that Q\*(s, a) = r + max_a Q\*(s’, a)
* We want a policy that is that consistent with this equation, so we use this equation as an update rule.
* We start with a policy pi_0 which relies on a randomly initialised Q-table Q_0
* Then we reduce the TD loss at each timestep by updating the Q function such that Q_t+1(s, a) = r + max_a Q_t(s’, a)
* We then use an updated policy pi_t+1 which uses this updated Q-function Q_t+1. We repeat this process until convergence.

This is nice, but I have two problems with this:

* It relies on Bellman equation instead of on the intuition of what happens
* Proving the Bellman equations themselves is not trivial

Instead I am wondering if the following explanation would be clearer:

* The reward hypothesis states that “All goals can be described by the maximization of the expected future cumulative reward”. This is intuitive.
* To maximize its future expected return, the agent should chose the action with the highest Q-value at each timestep (since the Q-value is an approximation of the expected return).
* The idea is to start from a randomly initialised Q-table, act greedily with regards to this Q-table, and use each experience to improve this Q-table.
* How do we improve the Q-table using each experience? (without resorting to the Bellman equation argument?) The idea is to decompose Q(s, a): we know that Q(s_t, a_t) = r_t+1 + Q(s_t+1, a_t+1)
* By performing timestep t, we observe r_t+1. We don’t know a_t+1 at that point, but we know that the agent would perform the action which would maximize Q(s_t+1, a_t+1), so we know that a_t+1 = argmax_a Q(s_t+1, a)
* This means that we can use r_t+1 + max a Q(s_t+1, a) to approximate Q(s_t, a_t)
* This new approximation of Q(s_t, a_t)  is more accurate than the value we had for it before, as we have replaced a part of it from an actually observed value (r_t+1)
* Therefore we can iteratively improve our Q-function this way. The rest is the same (GPI).

What do you think?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cteqbq/d_explaining_qlearning_without_using_bellman/
[Article + Code] Reinforcement Learning (DDPG and TD3) for News Recommendation,1566381953," TL;DR: Reinforcement Learning is the ideal framework for a recommendation system because it has Markov Property. The state is movies rated by a user. Action is the movie chosen to watch next and the reward is its rating. I made a DDPG/TD3 implementation of the idea. The main section of the article covers implementation details, discusses parameter choice for RL, introduces novel concepts of action evaluation, addresses the optimizer choice (Radam for life), and analyzes the results.

I also had released an ml20m dataset version specifically adopted for Markov decision process and to use with RL.

 Reinforcement learning as-is is a pretty hard topic. When I started to dig deeper, I realized the need for a good explanation. **This article, coupled with the code** ***is my school project.*** I am currently in a sophomore year of high school, and I understand the hard mathematical concepts in a more ‘social studies’ kind of way. I hope this article proves to be helpful for newcomers like me. 

&amp;#x200B;

[Loss](https://i.redd.it/d3oea0vuzrh31.png)

[Generated Actions](https://i.redd.it/0whc0bjwzrh31.png)

[Real Actions](https://i.redd.it/s8380mxyzrh31.png)

[AutoEncoder Reconstruction Error](https://i.redd.it/bkkvn7irzrh31.png)

 [https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011](https://towardsdatascience.com/reinforcement-learning-ddpg-and-td3-for-news-recommendation-d3cddec26011)",reinforcementlearning,dotapetro,False,/r/reinforcementlearning/comments/cteq4g/article_code_reinforcement_learning_ddpg_and_td3/
Parallelising DDPG,1566352791,"I want to make my implementation of DDPG exploit parallel computing to run faster, which doesn’t seem that obvious to do (unlike ppo). And when I search for parallel implementations I found a bunch of papers on it and it looks like each of them maybe different. Before I dive into them I’d like to ask which implementations did you guys use and how well did it work out, and was it more CPU heavy or GPU heavy. Any advice would be really helpful!!",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/cta47n/parallelising_ddpg/
[P] Ball &amp; beam gym - control theory lab simulations as Open AI gym environments,1566298954,"Posted this in /r/MachineLearning, but might as well post it here too.

While trying out reinforcement learning I built some custom ball &amp; beam environments since I was already familiar with it from control theory labs. I built it as a first order system where the angle of the beam is under full control (did not want to spend time simulating a motor). So it can be baselined by using a simple PID controller.

[https://github.com/simon-larsson/ballbeam-gym](https://github.com/simon-larsson/ballbeam-gym)

There are currently environments for three objectives:

* Balancing - just keeping the ball on beam
* Setpoint - keeping the ball as close as possible to a setpoint
* Throw - throwing the ball as far as possible to the right

The environments have two different state spaces. The agent can either use key-variables (position, velocity, angle) or the images from the visualization as state space.

Hope someone else wants to try it! :)",reinforcementlearning,lilsmacky,False,/r/reinforcementlearning/comments/cswr6u/p_ball_beam_gym_control_theory_lab_simulations_as/
What are the NLP papers that use recent techniques from Deep RL?,1566253273,"When I made a quick search all I could find was variants of DQN, policy gradient, vanilla reinforce etc but couldn’t find anything that used TRPO/PPO, soft actor critic, curiosity driven exploration etc. is it because they don’t work on NLP task?",reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/csof37/what_are_the_nlp_papers_that_use_recent/
"RL Weekly 29: The Behaviors and Superstitions of RL, and How Deep RL Compares with the Best Humans in Atari",1566222183,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/csh9b3/rl_weekly_29_the_behaviors_and_superstitions_of/
RAdam: A New State-of-the-Art Optimizer for RL?,1566213004,,reinforcementlearning,ChrisNota,False,/r/reinforcementlearning/comments/csfm4b/radam_a_new_stateoftheart_optimizer_for_rl/
Integrating new game into Retro Gym on Colab (Google),1566206535,"Knowledgeable ones of RL, I call upon thy wisdom.

How does one integrate a game into Retro (which is not yet part of Retro) within a Google Colab notebook?  
On my own computer I simply drop the needed files into a folder which I then place into something like ./site-packages/retro/data/stable, and this works perfectly.

Your help will be much appreciated.",reinforcementlearning,AnotherForce,False,/r/reinforcementlearning/comments/csenjb/integrating_new_game_into_retro_gym_on_colab/
[D] What to do about NaNs?,1566162935,"Long story short, I'm training a PPO model in PyTorch and get NaNs in the training in a rather inconsistent pattern. I've tried methods such as zeroing out NaN parameters and gradients. Changing the denominator epsilon for PPO does not appear to change this behavior.

Other ideas I have are that perhaps the Adam optimizer denominator epsilon is too low, and maybe the values passed into the network's softmax functions are growing uncontrollably. I'm wondering if anyone here has encountered problems like this in PPO, and more generally knows of a good way to solve them in RL.

Let me know if further network and training details would help. Thank you.",reinforcementlearning,LookAliveStayAlive,False,/r/reinforcementlearning/comments/cs77u4/d_what_to_do_about_nans/
What to do about NaNs?,1566147876,"[Discussion] Long story short, I'm training a PPO model in PyTorch and get NaNs in the training in a rather inconsistent pattern. I've tried methods such as zeroing out NaN parameters and gradients. Changing the denominator epsilon for PPO does not appear to change this behavior.

Other ideas I have are that perhaps the Adam optimizer denominator epsilon is too low, and maybe the values passed into the network's softmax functions are growing uncontrollably. I'm wondering if anyone here has encountered problems like this in PPO, and more generally knows of a good way to solve them in RL.

Let me know if further network and training details would help. Thank you.",reinforcementlearning,galacticacidtrip,False,/r/reinforcementlearning/comments/cs3uh9/what_to_do_about_nans/
[N] Google files patent “Deep Reinforcement Learning for Robotic Manipulation”,1566080075,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/crssjz/n_google_files_patent_deep_reinforcement_learning/
[R] ma-gym: multi agent environments based on open ai gym,1566076834,,reinforcementlearning,HeavyStatus4,False,/r/reinforcementlearning/comments/crs4s3/r_magym_multi_agent_environments_based_on_open_ai/
Causes of saturation?,1566076440,"Hi everyone, 

In my ddpg RL algorithm I have my state completly normalized in the 0,1 range so is my reward scheme. Still it sometimes tend to saturate. Just regular relu use And a lr of 0.0001, so a low lr But still it saturates, what could be causing this eventhough Everything is normalized?

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/crs1uf/causes_of_saturation/
"""Adversarial Reprogramming of Text Classification Neural Networks"", Neekhara et al 2018",1566067121,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/crq2sz/adversarial_reprogramming_of_text_classification/
Is Solving Sokoban-like puzzles using MC control possible?,1566047512,"By sokoban-like, I refer to problems that require large number of consecutive steps. I have attempted to run MC control onto the Taxi Problem and it seems to not learn at all, while sarsa, expected sarsa and qlearning work just fine.",reinforcementlearning,HalfArmBandit,False,/r/reinforcementlearning/comments/crlyjh/is_solving_sokobanlike_puzzles_using_mc_control/
I used a dqn to beat the hardest flappybird level,1566009220,,reinforcementlearning,WalterEhren,False,/r/reinforcementlearning/comments/crgc8h/i_used_a_dqn_to_beat_the_hardest_flappybird_level/
Tensorflow vs Pytorch for RL,1566001908,"Hi, 

I've done an intro RL course and I want to make AI bots that beat games. Should I use Tensorflow or Pytorch?

Thanks in advance!",reinforcementlearning,ronaldo_gao,False,/r/reinforcementlearning/comments/crf0i1/tensorflow_vs_pytorch_for_rl/
Lightning vs Ignite,1565986953," Hi guys,

&amp;#x200B;

do any of you use lightning ([https://github.com/williamFalcon/pytorch-lightning](https://github.com/williamFalcon/pytorch-lightning)) or ignite ([https://github.com/pytorch/ignite](https://github.com/pytorch/ignite))? I'm tempted to start using one of them, but they seem very similar. Can someone elaborate on the differences maybe? Is one of them strictly better / better maintained?",reinforcementlearning,wingmanscrape,False,/r/reinforcementlearning/comments/crbxbf/lightning_vs_ignite/
DDPG - Batch Normalization Implementation,1565958662,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/cr5pam/ddpg_batch_normalization_implementation/
[R] Online Continual Learning with Maximally Interfered Retrieval,1565957681,"This is an interesting paper, because it's not RL, but it still introduces concepts useful for RL.

**Paper:** https://arxiv.org/abs/1908.04742

**ShortScience summary:** https://www.shortscience.org/paper?bibtexKey=journals/corr/1908.04742

**Author tweet &amp; discussion:** https://twitter.com/MassCaccia/status/1162014601908764672

**Poster:** https://pbs.twimg.com/media/D9C_Bv5U8AAKqxv?format=jpg&amp;name=4096x4096 

**Abstract:**

&gt; Continual learning, the setting where a learning agent is faced with a never ending stream of data, continues to be a great challenge for modern machine learning systems. In particular the online or ""single-pass through the data"" setting has gained attention recently as a natural setting that is difficult to tackle. Methods based on replay, either generative or from a stored memory, have been shown to be effective approaches for continual learning, matching or exceeding the state of the art in a number of standard benchmarks. These approaches typically rely on randomly selecting samples from the replay memory or from a generative model, which is suboptimal.

So basically they are considering a **supervised learning setting** where they get a stream of data and try to learn from it. Just like in RL, they have problems of catastrophic forgetting, and solve it by using a replay buffer.

Now the interesting part: this paper is about a new way to prioritize the samples that should be reused.

The familiar way in RL would be Prioritized Experience Replay. In PER, we calculate or estimate the TD-loss of each sample, and prioritize the samples with higher values (while being careful to not introduce too much biais). There are multiple improvements upon the initial PER scheme, eg The Reactor approximates the TD-loss of samples by leveraging the fact that they are temporally correlated ([more here](https://www.reddit.com/r/reinforcementlearning/comments/cee18x/d_any_progress_regarding_prioritized_experience/)).

In general, PER is concerned with exploiting the experiences as efficiently as possible. If an experience shows potential for efficient learning (high loss) then it will be reused more often.

**So! let’s go back to their idea: ""Maximally Interfered Retrieval”.** Instead of considering how much can be learned from a sample, they are trying to find which of the past samples will be the most affected (""interfered"") by the upcoming batch, and “augment” the batch with them.

In their example they are dealing with images. As they wrote on shortscience.org:

&gt; Learning about dogs and horses might cause more interference on lions and zebras than on cars and oranges. Thus, replaying lions and zebras would be a more efficient strategy.

This is highly relevant for RL, since we also have this problem of an agent reaching a new part of an environment, and which starts to destroy what it had learned in the earlier parts of its training. It’s the good old stability/plasticity dilemma.

**They introduce two ways to implement this method:** either an exhaustive search in the replay buffer, calculating for each sample what would be the loss delta. Or an approach using a generative model, where they look in latent space for the sample that would be the most interfered. Then in both cases they “augment” their training batch with these maximally interfered samples, which limits forgetting.

So I really like this idea in general. Unfortunately I see two challenges to its application in RL:
* Doing an exhaustive search for the max delta loss would be super painful since in RL replay buffers usually contains in the order of 1e6 samples. In their experiment they use max 100 buffer size per class, if my understanding is correct.
* Using a generative model amounts to doing model-based learning, which isn’t working so well yet in RL.

I can see two approaches to mitigate this:
* Find an efficient way to approximate the TD-loss, the same way as The Reactor approximates the TD-loss
* Just brute-force the problem. Calculating the loss delta is embarrassingly parallelizable so just throw compute at it.

At least the brute-force approach could be used to assess if this method would be useful in the RL setting. 

In general, I’m super surprised to see research which is very close from RL, but which makes 0 reference to it. This is not a criticism, I am new to research so I may be missing something. Digging through their references, I am now reading “Gradient based sample selection for online continual learning” (https://arxiv.org/pdf/1903.08671.pdf) which also looks very relevant for us RL people. And this is from Mila, which I thought was very involved in RL?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cr5j28/r_online_continual_learning_with_maximally/
[Question] Exercise 4.8 Sutton &amp; Barto's book Reinforcement Learning: An Introduction (2nd Edition),1565954305,"going through the book, Example 4.3: Gambler’s Problem, I had few questions, which co-incidentally were same as the questions in Ex 4.8.  [Page 84 in this pdf](http://incompleteideas.net/book/bookdraft2018mar21.pdf).  
Will appreciate any help. So here it goes. 

&amp;#x200B;

A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of $100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite

MDP. The state is the gambler’s capital, s 2 {1, 2,. .., 99} and the actions are stakes, a 2 {0, 1,. .., min(s, 100 - s)} The reward is zero on all transitions except those on which the gambler reaches his goal, when it is +1. The state-value function then gives the probability of winning from each state. A policy is a mapping from levels of capital to stakes. The optimal policy maximizes the probability of reaching the goal. Let ph denote the probability of the coin coming up heads. If ph is known, then the entire problem is known and it can be solved, for instance, by value iteration. Figure 4.3 shows the change in the value function over successive sweeps of value iteration, and the final policy found, for the case of ph =0.4. This policy is optimal, but not unique. In fact, there is a whole family of optimal policies, all corresponding to ties for the argmax action selection with respect to the optimal value function. Can you guess what the entire family looks like?  


*Exercise 4.8 Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?*

&amp;#x200B;

&amp;#x200B;

[Figure 4.3](https://i.redd.it/qxln8c3cnsg31.png)",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/cr4ym2/question_exercise_48_sutton_bartos_book/
Reinforcement Learning Discord/Slack,1565916404,"Is there a chatroom where people from this sub or a similar sub hang out?

If not I would like to make one. Is there an interest for something like that?",reinforcementlearning,i_do_floss,False,/r/reinforcementlearning/comments/cqyuy2/reinforcement_learning_discordslack/
"""AutoML: A Survey of the State-of-the-Art"", He et al 2019",1565888956,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cqsqyq/automl_a_survey_of_the_stateoftheart_he_et_al_2019/
"""Superstition in the Network: Deep Reinforcement Learning Plays Deceptive Games"", Bontrager et al 2019",1565887977,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cqsil2/superstition_in_the_network_deep_reinforcement/
[D] Environments for multiple agents?,1565865713,"Hello everyone,

I'm organizing a RL workshop, here's what I intend to do:

- show a shared environment projected on the screen
- introduce various RL methods to participants, and let them train their agents in notebooks. Compute would be provided, and they would use existing mostly implemented algos, filling the blanks based on my explanations, and play with hyperparameter tuning
- then they can push their agent to the shared environment and see it compete with others, with live leaderboard

Note that this wouldn't really be a ""multi-agent"" challenge, as each participant would only submit a single agent, so the other agents could be considered as part of the environment.

**I am looking for suitable environments.**

So far I am considering:

- [Pommerman](https://www.pommerman.com), it seems to be targeted at multi-agent problems but I guess it could work for one agent per person

- https://www.terrarium.ai/, but it looks a bit young

- Rolling my own (would love to reuse an [old game engine](http://lumakey.net/labs/battleground/demo1/) I made in college)",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cqo4sr/d_environments_for_multiple_agents/
Best way to formulate this puzzle-solving problem in terms of reinforcement learning?,1565864269,"Hello to everyone! I'm currently finding some difficulties in trying to formulate a problem in terms of reinforcement learning State-Action-Reward terms. I hope to find some help and inspirations here, so feel free to comment with your ideas!

&amp;#x200B;

**The Goal**

The goal is to train an agent to reconstruct some sort of puzzle.

**The Problem**

* The puzzle is composed of 5 pieces. 
* The pieces are available all at once.
* The pieces may or not be overlapping.
* Each puzzle is different from the previous one (I wish to infer the optimal way to recreate a generic puzzle).

**My Idea**

My idea is to create a game with many rounds, with a puzzle for each round. For each puzzle the first piece is always placed, the agent can then place and move the next pieces and receive a rewards for correctly place an overlapping piece. At the end of the round the agent receives an additional reward for having correctly reconstructed the puzzle or a penalty for having misplaced some piece.

So my scheme for this problem is:

* State -&gt; number of pieces placed and ""points"" in composing the puzzle
* Action -&gt; move next piece and place it
* Reward -&gt; adding ""points"" or ""penalties"" for placing a new piece

**Doubts**

I'm not 100% sure about my approach. What *puzzles* me is:

* There is a better way to formulate the problem?
* I am not exploiting the fact that all the pieces are available at once

&amp;#x200B;

Please feel to ask anything and comment below with opinions and ideas, thank you!",reinforcementlearning,KingAster,False,/r/reinforcementlearning/comments/cqnxl4/best_way_to_formulate_this_puzzlesolving_problem/
Best way to encourage an agent to 'speedrun',1565848387,"This may be a fairly basic question, but what is the best way to encourage an agent to complete a goal as quickly as possible?

As a concrete example, suppose you have an agent that needs to traverse a garden maze to find the exit. At any time, the observation space of the agent is only the states it can immediately walk to, and a GPS vector from its current location to the exit.

What kind of Reward function would you devise to encourage quick completion of the maze? This is an algorithm-agnostic question but let's assume we're using something like DQN.

For example,

R(nonterminal) = 0, R(terminal) = 1/t ?

R(nonterminal) = 0, R(terminal) = T - t ?

R(nonterminal) = 0, R(terminal) = γ^t ?

or something else? Is standard discounting enough to instill urgency in the agent, or is there a reason to pick one of the others?

As an extension to this question, suppose further that the maze is ""almost"" convex in the sense that the GPS distance from agent to exit is a ""decent"" approximation of how close the agent is to success.

Could this be used with reward shaping to help speed up the maze-solving process?

For example, F(s,a,s') = γ * GPSDistance(s') - GPSDistance(s)?

If it turned out the maze was highly non-convex, would this reward shaping *slow down*, or invalidate the learning?",reinforcementlearning,BasicBlazar,False,/r/reinforcementlearning/comments/cqlvue/best_way_to_encourage_an_agent_to_speedrun/
RETRACE Estimator recursive formulation?,1565844873,"ACER  https://arxiv.org/pdf/1611.01224.pdf   gives a recursive formulation of the RETRACE estimator (https://arxiv.org/pdf/1606.02647.pdf). In fact, the same formulation occurs in this paper: https://arxiv.org/pdf/1901.07510.pdf. 

However, I’m confused as to where they are arriving at this formulation. One thing that I find especially puzzling is that Value Function V does not receive an importance sampling weight (usually a rho or c, in the literature).",reinforcementlearning,ml-questions,False,/r/reinforcementlearning/comments/cqldp2/retrace_estimator_recursive_formulation/
Submissions now open for NeurIPS 2019 MineRL Competition on Sample Efficient RL!,1565837463,,reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/cqk48v/submissions_now_open_for_neurips_2019_minerl/
"""Designing agent incentives to avoid reward tampering"" {DM} [on Everitt &amp; Hutter 2019]",1565820065,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cqgmxz/designing_agent_incentives_to_avoid_reward/
How to use personal algorithm to train in Unity ml-agents,1565807519,"Hey everyone. So I had been learning unity ml-agents recently. I understand that the Brain controls the agents which are being trained. And while training we need to pass a configuration file which contains all our hyperparameters to be used for training the agent. This file also contains the trainer parameter, in which we can pass in 'ppo' or any other training algorithm that we would like to use. 

But I wish to use my own ppo implementation which I coded from scratch to train the agents. That is I don't want to use the pre-written algorithms by using trainer parameter in the config file, instead use the one I coded from scratch to train the agent. Any help regarding how can I do it?",reinforcementlearning,BrandNewThanos,False,/r/reinforcementlearning/comments/cqdrs9/how_to_use_personal_algorithm_to_train_in_unity/
"SABER/Rainbow-IQN: ""Is Deep Reinforcement Learning Really Superhuman on Atari?"", Toromanoff et al 2019 [DRL methodology]",1565793760,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cqam0h/saberrainbowiqn_is_deep_reinforcement_learning/
"""Characterizing Attacks on Deep Reinforcement Learning"", Xiao et al 2019",1565725397,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cpy9dr/characterizing_attacks_on_deep_reinforcement/
[D] Learning for Dynamics and Control workshop videos,1565724931,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/cpy5h2/d_learning_for_dynamics_and_control_workshop/
"""bsuite: Behaviour Suite for Reinforcement Learning"", Osband et al 2019 {DM} [Gym-compatible Python environments for DRL benchmarking]",1565722875,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cpxo5v/bsuite_behaviour_suite_for_reinforcement_learning/
What type of reinforcement learning technique is most appropriate for this type of problem?,1565710098,"I've posted here a few days ago. I wanted to find out how to use reinforcement learning in python to make a solving algorithm for tile sliding puzzles like these: [https://en.wikipedia.org/wiki/Sliding\_puzzle#/media/File:Batgirl.gif](https://en.wikipedia.org/wiki/Sliding_puzzle#/media/File:Batgirl.gif). I was originally intent on Q-learning. I've realized that the action space or environment changed every time a move was made. But as I went deeper into the project, I realized it wasn't going to be an appropriate method for my situation...or maybe it can be used for the problem, but was incomplete of another technique. If anyone here can tell about where to go or what type of reinforcement learning technique, that would be fantastic, Thank you!",reinforcementlearning,MidThought_Pause,False,/r/reinforcementlearning/comments/cpup5z/what_type_of_reinforcement_learning_technique_is/
Cyclic Noise Schedule for RL,1565707481,"[Cyclic learning rates](https://arxiv.org/abs/1506.01186) are common in supervised learning.

I have seen cyclic noise schedule used in some RL competitions. How mainstream is it? Is there any publication on this topic? I can't find any. 

In my experience, this approach works quite well.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cpu3qu/cyclic_noise_schedule_for_rl/
Oscillating reward for DQN,1565677833,"Has anyone seen oscillating behavior for episode rewards with DQN? It's a sparse reward(0/1) problem and I can get positive rewards for about hundred steps. But then it begins to oscillating between 0 and 1 for several episodes and can't seem to get a stable positive rewards. 

Just curious if anyone sees this similar issue? I can get better consistent improving rewards by reward function tuning. But I don't really understand why such oscillating? Would it be initialization issue or reward function issue?",reinforcementlearning,Nicolas_Wang,False,/r/reinforcementlearning/comments/cpp7i3/oscillating_reward_for_dqn/
CuLE: GPU accelerated Atari Learning Environment from Nvidia,1565660940,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/cpm7pw/cule_gpu_accelerated_atari_learning_environment/
"Reward Shaping in OpenAI Gym - Box2D (Lunar Lander, Bipedal Walker)",1565635622,"I was looking through the code for the LunarLanderContinuous-v2 environment in particular, but found the same thing in BipedalWalker-v2. My questions is about the following lines of code:

1. [Lunar lander, line 304](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L304)
2. [Bipedal walker, line 433](https://github.com/openai/gym/blob/master/gym/envs/box2d/bipedal_walker.py#L433)

In both cases, the environment does this: `reward = shaping - self.prev_shaping` (if `self.prev_shaping is not None` of course). What is the rationale behind doing this? What effect does this have?

My guess is it stabilises the reward signal, and it does away with any potential scaling issues. Is this a common practice in reinforcement learning?",reinforcementlearning,imperialn00b,False,/r/reinforcementlearning/comments/cpgo9w/reward_shaping_in_openai_gym_box2d_lunar_lander/
Question about rewards in deep Q learning,1565621160," Hello I want to create a deep Q learning agent for a 2 player board  game. My rewards are 250 for winning the game, 100, 80 and 50 for ""good""  moves. My tutor said to me that I should normalize the rewards because  there are only limited amounts of different rewards and there are  possibly infinitely manyQ values. How should I normalize the rewards?  Should I normalize the rewards to \[0,1\] range so that 0,8 represents a  game winning move, 0,3 a good move for example?",reinforcementlearning,Kralex68,False,/r/reinforcementlearning/comments/cpdbum/question_about_rewards_in_deep_q_learning/
RL Weekly 28: Free-Lunch Saliency and Hierarchical RL with Behavior Cloning,1565608556,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/cpaz2f/rl_weekly_28_freelunch_saliency_and_hierarchical/
Can openAI gym cartpole be solved with q-learning ?,1565590517,,reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/cp8f4b/can_openai_gym_cartpole_be_solved_with_qlearning/
leela-zero NN architecture,1565535160,"I am trying to understand the NN architecture given at https://github.com/leela-zero/leela-zero/blob/next/training/caffe/zero.prototxt    


So, I downloaded the NN weights from http://zero.sjeng.org/ . However, I am not sure how to interpret the network weight file.     


Any advices ?

&amp;#x200B;

https://i.redd.it/aeewy4a82uf31.png",reinforcementlearning,promach,False,/r/reinforcementlearning/comments/coxn3f/leelazero_nn_architecture/
How to depict a tile sliding puzzle?,1565488231,"I'm trying to make a tile sliding puzzle solver. 

Array = [[3, 5, 4],
               [2,1,0]]

The problem is simplified as 2x3. 

We have to slide the 0 with any of its neighbors until we get 

[[0,1,2],
 [3,4,5]]

With its respective adjacent arrays being these grids (meaning the arrays that lead you or from that target grid).

[[1,0,2], 
 [3,4,5]]

and

[[3,1,2],
 [0,4,5]]

I was thinking that maybe I have to get every possible combination of this tile array, and put that all in an adjacency matrix (so basically, I treating an entire array as one thing), and then put up the , and I really hope that's not the case because that's a big number.",reinforcementlearning,MidThought_Pause,False,/r/reinforcementlearning/comments/coqubo/how_to_depict_a_tile_sliding_puzzle/
"""Self-Supervised Exploration via Disagreement"", Pathak et al 2019",1565475178,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/coogi5/selfsupervised_exploration_via_disagreement/
"""Deep Reinforcement Learning in System Optimization"", Haj-Ali e al 2019 [review]",1565472510,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/conx90/deep_reinforcement_learning_in_system/
"""EfficientNet-EdgeTPU: Creating Accelerator-Optimized Neural Networks with AutoML""",1565463544,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/com2qq/efficientnetedgetpu_creating_acceleratoroptimized/
"""TuneNet: One-Shot Residual Tuning for System Identification and Sim-to-Real Robot Task Transfer"", Allevato et al 2019",1565463343,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/com181/tunenet_oneshot_residual_tuning_for_system/
"""Sequential replay of non-spatial task states in the human hippocampus"", Schuck &amp; Niv 2018",1565454314,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cok4kn/sequential_replay_of_nonspatial_task_states_in/
Using part of observation vector to represent history of actions for large action set? Like state[a] = (time of execution),1565397906,"Hi guys, I was wondering if you guys ever considered some kind action history inside your observation vector? If so, how were your results? I hope it helps the agent ""remember"" some things happened in the past. 

For example, if the size of the action set is 40 and at the first step of the game, we use action index 2, then second step, we use action index 4, then third step, we use action index 2 again so now our state for step 4 at the beginning is .... 

    State[2] = 3.0  # replace from 1.0 to 3.0 since that's the latest time the action happened 
    State[4] = 2.0

Do you guys think it may help? I don't want to mess with LSTM's yet... 

Also state 40+ would have the other necessary state info. 

Thanks!",reinforcementlearning,final-getsuga,False,/r/reinforcementlearning/comments/cob69b/using_part_of_observation_vector_to_represent/
Research Topics,1565358747,"Hello Guys,

I am a Ph.d candidate in C.S trying to migrate my research to RL.  Would you guys tell some up-to-date interesting research problems in RL?",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/co2nzq/research_topics/
Benchmarking Bonus-Based Exploration Methods on the ALE,1565334533,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cnyteb/benchmarking_bonusbased_exploration_methods_on/
Monte Carlo Policy Gradient,1565287599,"Hello community.

I am new to RL and my question is about using Monte Carlo Policy Gradient (also called REINFORCE, if I am not mistaken), along with a deterministic policy. 

How is this going to impact how I compute the policy gradient? Should I keep the gradient of the log policy or should I change it to an ordinary gradient?",reinforcementlearning,white_noise212,False,/r/reinforcementlearning/comments/cnpi88/monte_carlo_policy_gradient/
Dueling Posterior Sampling for Preference-Based Reinforcement Learning,1565201770,,reinforcementlearning,mellow54,False,/r/reinforcementlearning/comments/cn9ffh/dueling_posterior_sampling_for_preferencebased/
https://arxiv.org/abs/1908.01289,1565200386,,reinforcementlearning,mellow54,False,/r/reinforcementlearning/comments/cn947z/httpsarxivorgabs190801289/
Best video tutorial series for learning Reinforcement Learning for practical usage,1565169386,"I want to learn Reinforcement Learning with some video series, with **more on the practical usage** side instead of more on the theoretical side. These are some of the series, people have recommended to me:

[**Georgia Tech’s “*****Reinforcement Learning*****” course on Udacity**](https://www.udacity.com/course/reinforcement-learning--ud600)

[**Microsoft's ""*****Reinforcement Learning Explained*****"" on edX**](https://www.edx.org/course/reinforcement-learning-explained-10)

[**University of Alabama's ""*****Fundamentals of Reinforcement Learning*****"" on Coursera**](https://www.coursera.org/lecture/fundamentals-of-reinforcement-learning/course-introduction-gYOOM) (released a week ago)

Though I am new to reinforcement learning, I would prefer the course to be a bit **advanced** (slow courses take away my interest). Which one should I do?",reinforcementlearning,subtleseeker9,False,/r/reinforcementlearning/comments/cn3gws/best_video_tutorial_series_for_learning/
Does data scaling have to coherent to all scaled features?,1565115027,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/cmu4ez/does_data_scaling_have_to_coherent_to_all_scaled/
[R] Using multiple heads in RL,1565079649,"I am reading everything about using an ensemble of heads for reinforcement learning.

For now I have found:

- the Bootstrapped DQN approach in [Deep Exploration via Bootstrapped DQN](https://arxiv.org/abs/1602.04621)

- the REM method in [Striving for Simplicity in Off-policy Deep Reinforcement Learning](https://arxiv.org/abs/1907.04543)

- [Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning](https://arxiv.org/abs/1611.01929)

You can also consider the distributional approaches as multi-heads methods:

- C51 from [A Distributional Perspective on Reinforcement Learning](https://arxiv.org/abs/1707.06887)

- QR-DQN from [Distributional Reinforcement Learning with Quantile Regression](https://arxiv.org/pdf/1710.10044.pdf)

What else should I read?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cmnvux/r_using_multiple_heads_in_rl/
Off-Policy DRL with multiple instances of environments.,1565066400,"I am running 50 instances of the synchronous environments, gathering experience, and the update rate was for every 100 steps (2 steps per environment). But that seems to destabilize the training process. I would preferably like to have the number of environments high, but also have a frequent update rate (based on the total steps taken across each environment instance).

There seems to be a trade-off that one needs to do between the number of simultaneous environments, and policy update. I didn't see a lot of work, on explaining that. Any suggestions or guides on what are the ballpark figures on it?",reinforcementlearning,FatherCannotYell,False,/r/reinforcementlearning/comments/cmm1z9/offpolicy_drl_with_multiple_instances_of/
RL Weekly 27: Diverse Trajectory-conditioned Self Imitation Learning and Environment Probing Interaction Policies,1565057844,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/cmklvg/rl_weekly_27_diverse_trajectoryconditioned_self/
[N] Flatland Challenge - Multi-Agent Reinforcement Learning for Transportation Systems,1565033448,"Hi all


We launched the [Flatland Challenge](https://www.aicrowd.com/challenges/flatland-challenge), which is an official challenge of the [Applied Machine Learning Days](https://www.appliedmldays.org/challenges.html). 



# Flatland: Multi-Agent Reinforcement Learning Challenge

*The Flatland Challenge is a competition to foster progress in multi-agent reinforcement learning for real world applications. The [re-scheduling problem (RSP)](https://en.wikipedia.org/wiki/Vehicle_rescheduling_problem), which has traditionally been approached by operations research, serves as an excellent challenge to investigate the possibilies of deep learning for planning in stochastic environments. Different rounds with increasing difficulty and the presence of stochasticity in the environment encourage participants to look beyond classical planning algorithms and come up with solutions for the transport management systems of the future.*

## The Challenge

The challenge requires your creativity and savviness. In 2 submission rounds with increasing difficulty, you can prove that you have what it takes. We invite you to enter the race with your unique solution and to win great prizes - at the same time solving one of the key challenges in the world of transportation!

In contrast to most reinforcement learning challenges the focus of this challenge is not solely on the submission of great algorithms as controllers. We encourage the participants to come up with novel **observation spaces** for this challenge and share them with the community (community prize awarded) to improve performance on this task.

## Real world applications

The Swiss Federal Railways (SBB) operate the densest mixed railway traffic in the world. SBB maintain and operate the biggest railway infrastructure in Switzerland. Today, there are more than 10,000 trains running each day, being routed over 13,000 switches and controlled by more than 32,000 signals. Each day 1.2 million passengers and almost half of Switzerland’s volume of transported goods are transported on this railway network. Due to the growing demand for mobility, SBB needs to increase the transportation capacity of the network by approximately 30% in the future.

The increase in transport capacity can be achieved through different measures, such as [denser train schedules, investments in new infrastructure, and/or investments in new rolling stock](https://smartrail40.ch/index.asp?inc=&amp;lang=en). However, SBB currently lack suitable technologies and tools to quantitatively assess these different measures.

The SBB are therefore looking for novel approaches that can help revolutionize the transportation system of the future.

## Prizes

Your problem solutions mean something to us - hence prizes with a total value of 30k CHF (approx. 30k USD) are reserved for those with the best submissions. You can excel in two categories: The best solution category and the community prize category. Within both those categories your submission is individually ranked taking into account your performance in Round 1 and Round 2. Make sure to check the participation rules before you start. Only submissions conforming to our rules have a chance of winning the prizes.

**Best Solution Prize**: Won by the participants with the best performing submission on our test set. Both of your rankings from the Round 1 and Round 2 are taken into account. Check the leader board on this site regularly for the latest information on your ranking.      

The top three submissions in this category will be awarded the following cash prizes (in Swiss Francs):

**CHF 7’500.- (~USD 7’500) for first prize**

**CHF 5’000.- (~USD 5’000) for second prize**

**CHF 2’500.- (~USD 2’500) for third prize**


**Community Contributions Prize**: Awarded to the person/group who makes the biggest contribution to the community - done through generating new observations and sharing them with the community.


The top submission in this category will be awarded the following cash prize (in Swiss Francs): **CHF 5’000.- (~USD 5’000)**


In addition, we will hand-pick and award up to five (5) travel grants to the Applied Machine Learning Days 2019 in Lausanne, Switzerland. Participants with promising solutions may be invited to present their solutions at SBB in Bern, Switzerland.


&gt; **Note:** It is possible for a participant to win in both categories  

## Participate

Are you up for the challenge? More information about the [Flatland Challenge](https://www.aicrowd.com/challenges/flatland-challenge/) can be found [here](https://www.aicrowd.com/challenges/flatland-challenge/).

## Contribute

Want to help improve and build upon **Flatland**?

Head over to our [gitlab repo](https://gitlab.aicrowd.com/flatland/flatland) to see how you can contribute shaping this environment.

## Contact

For Challenge-related questions (technical and/or content questions): 

* Gitter Channel : [https://gitter.im/AIcrowd-HQ/flatland-rl](https://gitter.im/AIcrowd-HQ/flatland-rl)
* Technical Issues : Please use the [issue tracker]( https://gitlab.aicrowd.com/flatland/flatland/issues) in the public repository
* Discussion Forum : &lt;https://discourse.aicrowd.com/&gt;

We strongly encourage you to use the public channels mentioned above for communications between the participants and the organizers.
But in case look for a direct communication channel, feel free to reach out to us at : 

- **mohanty** [at] **aicrowd.com**
- **erik.nygren** [at] **sbb.ch**

For press inquiries
Please contact SBB Media Relations at &lt;press@sbb.ch&gt;",reinforcementlearning,ML_Erik,False,/r/reinforcementlearning/comments/cmfokr/n_flatland_challenge_multiagent_reinforcement/
"I just released Terrarium.ai, an online persistent environment for AI. It's in it's very early stages and is completely free right now!",1565030505,"Hey everyone!  

I've always been interested in RL and have had a lot of fun messing around with OpenAI's Gym, but I knew it could be done better. My vision was a collection of online environments that models can connect to and learn, not only how to survive against the environment, **but against other agents too**. A true simulation of simple life.  

This is why I built [Terrarium.ai](https://Terrarium.ai). It is exactly what I described above, and I've just released the first version. This is the first step towards an entire universe of persistent online worlds for AI to live in.  

I can't tell you how excited I am to share this, but I've received a bit of capital for this project. This means I can offer it to you all for free! Absolutely no strings attached. The only thing I want out of it is feedback, and to see what you all can do with it.   

Here's some insight into what I'm working on at the moment. Right now agents can only move and eat. They have energy and health which need to be kept up in order to survive. I am working as hard as I can to add more actions such as attacking, communicating, and a more detailed vision system. I really want to add this stuff, but I would like to get some feedback from you guys on how you would like to see them implemented.  

If you are interested in using Terrarium, check out the website and feel free to email me or message me on Reddit and Twitter (info about this is on the website). I would love any feedback I can get in order to make Terrarium a better experience. Tell me what features you want prioritized, how you would like them to be implemented, or what problems you see with the platform that could be improved on!",reinforcementlearning,zollandd,False,/r/reinforcementlearning/comments/cmf09u/i_just_released_terrariumai_an_online_persistent/
Keras: How to dynamically update loss with importance sampling? [Prioritized Experience Replay],1565024694,"\[Prioritized Experience Replay\]

TensorFlow

tf.reduce\_mean(tf.multiply(tf.square(self.error), self.importance\_in))

&amp;#x200B;

PyTorch

(torch.FloatTensor(is\_weights) \* F.mse\_loss(pred, target)).mean()

&amp;#x200B;

Keras (My Attempt)

def custom\_loss(y\_true, y\_pred):

return self.is\_weight \* mean\_squared\_error(y\_true, y\_pred) # self.is\_weight is calculated then stored in an instance variable.

&amp;#x200B;

model.add\_loss(custom\_loss(y\_true, y\_pred))",reinforcementlearning,Zenparalysis,False,/r/reinforcementlearning/comments/cmdou3/keras_how_to_dynamically_update_loss_with/
How to deal with RL algos getting stuck in local optima?,1564990795,"I am using ppo to try to learn breakout, but the agent is stuck in a local optima where the agent waits in a corner cause most of the time the ball after spawning moves towards the corner... that’s it and the agent doesn’t move after that.. the same ppo implementation I used to solve Pendulum-v0 , so the algo is accurate but is stuck in an local minima? How do you deal with this? Not just for breakout but in RL how do you deal with it",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/cm7yu4/how_to_deal_with_rl_algos_getting_stuck_in_local/
DQN - Training details (single target update?),1564949120,"Hi,

I've started my adventure with RL with DQNs as I guess everybody.

There is however one detail about DQN that I was hoping somebody can help me on.

During the training phase some new target values are computed. new\_q(s,a) = R + y.max (Q(s',a'))

However obviously the output of the Q-network is a vector with one value for every action(a) but you only have a new target value for one of them.

Now in some of the simpler examples I've seen online this is ignored.

So in this case:

new\_q = dqn\_network.predict(s)

new\_q\[a\] = R + y . max(Q(s',a'))

But than the training is done across the whole output.

However if I look at for example the code inside keras-rl (or other example code online) it seems certain measures are taken that the training is done only against the action chosen. 

In keras-rl a network with a ""mask"" input is used, but other ways of constructing this is also possible.

&amp;#x200B;

My question is: How important is this? How bad is it to not ""mask"" out the other actions during training?

My assumption here was that the moment you are batch training from replay memory this is probably somewhat important? But I would rather get an answer from somebody that is more experienced than me.

thank you",reinforcementlearning,MarcaunonXtreme,False,/r/reinforcementlearning/comments/cm0w7r/dqn_training_details_single_target_update/
After weeks digging through the Minecraft codebase I finally got environment seeding to work in Minecraft (MineRL),1564942588,,reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/clzkad/after_weeks_digging_through_the_minecraft/
[QUESTION]Extracting States from Environments,1564935679,"What are the most common and/or best ways for state recognition in RL environments(especially for games)?

Thank you.",reinforcementlearning,denizkavi,False,/r/reinforcementlearning/comments/cly5pz/questionextracting_states_from_environments/
The same old question on reproducibility with Keras,1564911809,"I am running Keras with a TensorFlow background. 

Experiments I run and the values I obtain are different with each run eventhough I set all the random seeds of random, numpy and tensorflow. I use OpenAI's environment, so I also seed the environment. 

I set PYTHONHASHSEED as well to 0. I am running on CPU only and basically, I have done everything as mentioned in the Keras FAQ here: [https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development](https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development)

Also checked this thread: [https://github.com/keras-team/keras/issues/2280](https://github.com/keras-team/keras/issues/2280)

Still the values I get are different with each run, what else do I need to do? Any suggestions would be really helpful.

However, when I use the same code with everything else unchanged but set the backend to PlaidML, the values seem to be same across runs.",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/clum4e/the_same_old_question_on_reproducibility_with/
Human In the Loop Reinforcement Learning,1564907086,"Hello Everybody,

I've been reading about reinforcement learning models that can improve in real time based on human(teacher?) input. I want to try this for a flash game.

From looking this up online I found:

[RL Teacher](https://github.com/nottombrown/rl-teacher) \- Which is a Mujoco based model with the human saying which leg etc. is better in a specific situation, this is pretty cool but I don't know much about web development and I don't really know how I could apply my own environment to it. There's also this [article](https://deepmind.com/blog/learning-through-human-feedback/) from deep mind on the same topic and [RL teacher for atari](https://github.com/machine-intelligence/rl-teacher-atari).

[FlashRL](https://github.com/cair/FlashRL) \- Just flash games to RL environments

[Tensorforce](https://github.com/tensorforce/tensorforce) \- İs a environment agnostic RL library for python built on top of tensorflow.

Anyway, do you know any pre-existing applications similar to what I described or have any idea on how I could combine these? Say a flash game being played by the agent and the human saying the actions it takes are a positive or negative reward.

Edit: There's also Nervana System's(Intel?) rl coach that multiple pre-defined environments.

[This](https://www.ijcai.org/proceedings/2018/0817.pdf) paper",reinforcementlearning,denizkavi,False,/r/reinforcementlearning/comments/clu3dc/human_in_the_loop_reinforcement_learning/
How to say algoX trains the agent faster than algoY,1564837270,"I am working with OpenAI's LunarLander environment, and am using DQN and another slightly modified algorithm. Probably there's not much improvement. As in most cases of OpenAI envs, the training is supposed to end when you reach a certain metric (in this case, an average of 200 over the last 100 episodes).

Also, as someone had rightly pointed out to me here, LunarLander has a huge state space and hence the random seeds matter a lot. This is also discussed in OpenAI spinning up found here [https://spinningup.openai.com/en/latest/spinningup/spinningup.html#doing-rigorous-research-in-rl](https://spinningup.openai.com/en/latest/spinningup/spinningup.html#doing-rigorous-research-in-rl)

Hence, I took 16 different sets of seed values, and for each seed set, trained an agent using AlgoX and AlgoY. If I average the #episodes it took to train for each Algo, I can probably say one of them is better than the other. 

Also there's one other issue, I can't let the agent train forever since in some cases, the training does not end in a reasonable timeframe. The average episodes it takes to train is around 600, and this is similar to what you see on the leaderboard as well. Hence, I decided to stop the training at 1000 episodes, and hence I have a few cases where the value is 1000, and this does skew the average favorably, right? Because, had I left it to continue, it might have taken any number of episodes over 1000. How do I deal with this?

Is this an acceptable idea? As in, can I claim AlgoX is better than AlgoY based on this alone? Or do I need to do something else to say one is better than the other? Any suggestions would be helpful.",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/clihwd/how_to_say_algox_trains_the_agent_faster_than/
High level overview of ACKTR?,1564768463,"I'm not sure if I'm getting how ACKTR on a high level.

So is it basically using K-fac to find the trust region to be used to optimize the actor and the critic, which leads to better sample complexity with minimal increase in computational complexity?

Also, the ACKTR paper mentions ""trust region natural policy gradient"". Is that term redundant?

Thanks.",reinforcementlearning,ml4564,False,/r/reinforcementlearning/comments/cl7hwb/high_level_overview_of_acktr/
SeaQuest with DDQN using prioritized replay experience not working as expected?,1564763703," Has anyone used DDQN with Prioritized Experience Replay on SeaQuest? Using ApeX I can solve Pong and others on a single GPU in under 40 minutes, but SeaQuest is consistently shittier compared to a simple DDQN without PER. I'm talking 10k reward vs 3k reward after many hours of training",reinforcementlearning,BoldlyDapper,False,/r/reinforcementlearning/comments/cl6ggv/seaquest_with_ddqn_using_prioritized_replay/
[Question] OpenAI MuJoCo Observation in Humanoid/Ant-v2,1564760381,"Hi,

&amp;#x200B;

Recently I've been working on some experiments using MuJoCo/OpenAI Gym.

And when I was checking the returns from ""env.step()"" on Humanoid-v2 and Ant-v2.

It returns the vector containing most items are zeros so that I have investigated a bit more on the source code like

&amp;#x200B;

\- Humanoid get\_obs: [https://github.com/openai/gym/blob/master/gym/envs/mujoco/humanoid.py#L22](https://github.com/openai/gym/blob/master/gym/envs/mujoco/humanoid.py#L22)

\- Ant get\_obs: [https://github.com/openai/gym/blob/master/gym/envs/mujoco/ant.py#L35](https://github.com/openai/gym/blob/master/gym/envs/mujoco/ant.py#L35)

\- cfrc\_ext impl: [https://github.com/openai/mujoco-py/blob/master/mujoco\_py/pxd/mjdata.pxd#L285](https://github.com/openai/mujoco-py/blob/master/mujoco_py/pxd/mjdata.pxd#L285)

&amp;#x200B;

And found out this issue: [https://github.com/openai/gym/issues/585](https://github.com/openai/gym/issues/585), but even on the issue of this repo, it seems that no one is asking about the actual values in observation from humanoid/ant.

&amp;#x200B;

In summary, I wonder if anyone gets the obs as me??

&amp;#x200B;

=== Info of my env ===

MuJoCo: Version 2.0.0

Python: 3.6.5",reinforcementlearning,Rowing0914,False,/r/reinforcementlearning/comments/cl5qmn/question_openai_mujoco_observation_in/
Can someone explain to me how the gradient update to and weights retrieval from the global shared parameters work in A3C?,1564756931,"I understand that the multiple workers do gradient update to the global network is done asynchronously in A3C ( [https://arxiv.org/abs/1602.01783](https://arxiv.org/abs/1602.01783) ). 

 But how do the workers ensure that they won't retrieve the same parameters from the global network they just updated?

Thank you.",reinforcementlearning,ml4564,False,/r/reinforcementlearning/comments/cl509d/can_someone_explain_to_me_how_the_gradient_update/
Special case of continuous action space RL,1564743470,"Hello guys,

I'm new to RL and are currently working on solving a continuous state-action space problem using policy gradient. 

The environment action space is defined as ratios that have to sum up to 1 at each timestep. Hence, using gaussian policy doesn't seem to be suitable in this case. 

What I was thinking of instead is tweaking the softmax policy (to have the policy network output sum up to 1), but I had hard time determining the loss function to use and eventually its gradient in order to update the network parameters.

Any help or guidance are welcome. Thanks.",reinforcementlearning,white_noise212,False,/r/reinforcementlearning/comments/cl2kqn/special_case_of_continuous_action_space_rl/
A general vectorized env wrapper with buffer,1564713626,,reinforcementlearning,VectorChange,False,/r/reinforcementlearning/comments/cky7l2/a_general_vectorized_env_wrapper_with_buffer/
ACL 2019 | Best Papers Announced,1564672149,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/ckpltq/acl_2019_best_papers_announced/
Artificial Intelligence to speed up trip planning,1564646475,,reinforcementlearning,Ripple2709,False,/r/reinforcementlearning/comments/ckldm0/artificial_intelligence_to_speed_up_trip_planning/
Feeding previous frames or something reccurent?,1564597684,"Hi everyone,

&amp;#x200B;

if you would want to have your agent base its prediction on past data/states would feeding previous frames/states be better than using something reccurent such as lstm? I thought of this on pong from pixels where they fed previous frames instead of using something reccurent?

&amp;#x200B;

could anyone help out?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/ckcdqy/feeding_previous_frames_or_something_reccurent/
"PG Implementation Question - Return calculated once for each rollout, or at each step?",1564588596,,reinforcementlearning,RSchaeffer,False,/r/reinforcementlearning/comments/ckaadb/pg_implementation_question_return_calculated_once/
"""Has dynamic programming improved decision making?"", Rust 2018",1564587419,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cka21i/has_dynamic_programming_improved_decision_making/
Vanila Policy Gradient sometimes just dosent work,1564583441,"So I just finished learning Policy Gradients from OpenAI spinning up. Sometimes if I run it it just dosent learn anything at all. If you see the image these are multiple runs with the exact same parameters.   
Here is my code: [GitHub Link.](https://github.com/DollarAkshay/Artificial-Intelligence/blob/master/OpenAI/CartPole-v0/Cartpole_v0_PG.py) Please give me some kind of feedback.

&amp;#x200B;

Q1. Is this just bad luck that I accept and move on or something wrong with my implementation ?  


Q2. I tried running this same code for `Acrobot-v1` its the opposite scenario. Most of the time The rewards are flat and stuck at -500 and once in a while, it is able to solve it. Usually, if it is able to solve it with the first 25 episodes then it ends up with a good score. Why is that?  


Q3. Another question I have is, why is there no Exploration strategy in Policy Gradient? Seems like we always use the policy to pick an action.  


Q4. I am training after every episode all the samples from that episode. This is fine right?  


&amp;#x200B;

https://i.redd.it/lvfbi6wodnd31.png",reinforcementlearning,DollarAkshay,False,/r/reinforcementlearning/comments/ck98vx/vanila_policy_gradient_sometimes_just_dosent_work/
Using Sentiment Analysis to Build Customer Loyalty,1564578603,,reinforcementlearning,Verma_RJ,False,/r/reinforcementlearning/comments/ck8aug/using_sentiment_analysis_to_build_customer_loyalty/
Road traffic &amp; air traffic simulator recommendations,1564557224,"I want to build 2 separate RL models: one for controlling traffic lights on a highway and another air traffic control for incoming &amp; outgoing planes in an airport.

## Road traffic simulator

I know of SUMO. Any there other simulator recommendations

## Airport air traffic control simulator

I couldn't find anything for this. Any recommendation would help.",reinforcementlearning,AgnosticIsaac,False,/r/reinforcementlearning/comments/ck4zo5/road_traffic_air_traffic_simulator_recommendations/
Summary of how Go-Explore cracked Montezuma's Revenge,1564519072,,reinforcementlearning,antonosika,False,/r/reinforcementlearning/comments/cjxpft/summary_of_how_goexplore_cracked_montezumas/
I Placed 4th in my First AI Competition. Read my write up of my agent on Unity's ObstacleTower AI Challenge.,1564511223,,reinforcementlearning,soho-joe,False,/r/reinforcementlearning/comments/cjvvqw/i_placed_4th_in_my_first_ai_competition_read_my/
How to Create A Concurrent Stochastic Reinforcement Learning Environment For Crypto Trading,1564506804,"Reinforcement learning for trading has a problem with overfitting. A way around this is to give an agent many experiences that are close to the real thing without look-ahead bias, then training on various kinds of real data. Training 100s to 1000s of episodes in an event driven manner can become very slow. [I wrote a blog post about how I sped it up for my agents tests](https://medium.com/@kevinhill_96608/how-to-create-a-concurrent-and-parallel-stochastic-reinforcement-learning-environment-for-crypto-3756d78b7a8e?source=friends_link&amp;sk=f1b3321cbee3f42004eee87285eae27a).

https://i.redd.it/9c691y0s3hd31.png",reinforcementlearning,kivo360,False,/r/reinforcementlearning/comments/cjutsr/how_to_create_a_concurrent_stochastic/
"""Metalearned Neural Memory"", Munkhdalai et al 2019",1564495584,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cjs8qf/metalearned_neural_memory_munkhdalai_et_al_2019/
Gaussian policies for continuous control,1564451432,"I have implemented A2C and PPO and my policy uses Gaussian noise for exploration. The algorithms hardly converge and while there might be a chance my algorithms aren’t correct, it would help me out in debugging for knowing how well do such policies perform. Are there better exploration techniques for actor critic (for continuous control ) and what values of variance are generally suitable for sufficient exploration. An example value of variance would be helpful for me to try out - my actions need to be in between -2 and 2. Also the output of the neural network in most implementations have no activation, (mean of the Gaussian policy) is this the correct approach?",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/cjkxmr/gaussian_policies_for_continuous_control/
Question: model accuracy in model-based RL,1564433866,"Learning a dynamics model using supervised learning from environment interactions in continuous state / action space domains seems fairly straightforward. However, how do you ensure that predictions remain accurate over an N-step prediction horizon (for large N during offline policy rollouts)? Are there any special tricks to make this work sufficiently well? And what makes a solution sufficiently good (e.g. to ensure proper learning and convergence of the policy to a local / global optimum)?",reinforcementlearning,autonomousbeaver,False,/r/reinforcementlearning/comments/cjhbmk/question_model_accuracy_in_modelbased_rl/
Oscillating Behavior with Agent,1564361113,"I've implemented a DQN in an environment similar to Sentdex's Blob environment. When I have one agent and one food, the agent learns to go after the food and get it. When I have one agent and three food spawn, the agent will get stuck in an oscillating behavior. It will get close to the food then just go back and forth near it. Does anyone have any suggestions?

Actions: Up, Down, Left, Right

Rewards: 10 per food, -0.01 penalty per step, -30 penalty for going out of bounds",reinforcementlearning,Takachsin,False,/r/reinforcementlearning/comments/cj3z6v/oscillating_behavior_with_agent/
Question about score decay with DQN,1564349766,"I'm a beginner to RL and DL in general. I was following sentdex's tutorial about coding a DQN which took in color data for a 400x600 frame of the pole cart environment from OpenAI. As I was training it, I noticed that the score would peak to about 50 or so after a few hundred episodes and then would quickly begin to decrease back to 10. Is there anything I'm doing wrong? 

2x100 convolution net , learning rate of .001 and a discount of .99 

Probably should mention that he uses 2x256 , but I used 2x100 because my gpu kept giving me a resource exhausted error. Does the vastly different number of ""neurons"" account for such a big problem?",reinforcementlearning,TheAirplaneDoctor,False,/r/reinforcementlearning/comments/cj1sjx/question_about_score_decay_with_dqn/
Are there any DDPG's variants?,1564328089,"Hi there, After DDPG, are there any its variants based on DDPG?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/cix85m/are_there_any_ddpgs_variants/
My Solutions of Programming Assignments of Stanford CS234: Reinforcement Learning Winter 2019,1564294272,[https://github.com/Huixxi/CS234-Reinforcement-Learning-Winter-2019](https://github.com/Huixxi/CS234-Reinforcement-Learning-Winter-2019),reinforcementlearning,H_uuu,False,/r/reinforcementlearning/comments/cisgaw/my_solutions_of_programming_assignments_of/
Slight doubt in ppo algorithm,1564293195,"The loss can be many things in ppo, but considering the clipped version we have the ratio = pi_new/pi_old times the advantage and the ratio is limited between 1+eps and 1-eps. What I don’t know is during the first run, after we gathered enough amount of experience we have only one policy, and now what will be the ratio? 1? Or maybe the loss will be pi * adv?",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/cisb2d/slight_doubt_in_ppo_algorithm/
Can MountainCar be solved without changing the rewards?,1564249059,"I'm trying to solve OpenAI Gym's MountainCar with a DQN. The reward given is -1 for every frame that it has not gotten to the flag. This means every game seems to end with the same score (-200).

I don't understand how this can ever learn, since it's very unlikely it'll reach the flag from completely random actions, so it will never learn that there is any reward other than -200.

I've seen many people make their own rewards (based on how far up the hill it gets, or its momentum), but I've also seen people say that's just simplifying the game and not the intended way to solve it.

If it's intended to be solved without changing the reward, how?

Thanks!",reinforcementlearning,DanTup,False,/r/reinforcementlearning/comments/cikeq9/can_mountaincar_be_solved_without_changing_the/
Question,1564222420,Are discrete versions of say PPO and SAC better at discrete problems than the discrete only algorithms like rainbow?,reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/cifun7/question/
Best Python tutorials implementing RL?,1564196142,I'm looking for suggestions on online courses that do a good job of walking through RL algorithms in a clear manner. A lot of the ones I've seen just throw the code at you without linking it back to the theory or math. Is there one that you think does both a good job of explaining concepts and actually implementing them in python?,reinforcementlearning,itanorchi,False,/r/reinforcementlearning/comments/cicasw/best_python_tutorials_implementing_rl/
Early AlphaStar Battle.net gameplay results,1564177989,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ci92o5/early_alphastar_battlenet_gameplay_results/
Early AlphaStar Battlen.net gameplay results,1564177962,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ci92h4/early_alphastar_battlennet_gameplay_results/
Is Inverse Reinforcement Learning basically imitation learning + Reinforcement Learning ?,1564155554,"I found this paper [Hierarchical Reinforcement Learning](https://arxiv.org/abs/1803.00590) when i was searching for more info about IRL (inverse reinforcement learning) and i think the idea is basically the same just solved from different directions.  
Am i right and is IRL or the idea of (imitation + reinforcement learning) is the future of RL ? ( i want to here your thoughts)",reinforcementlearning,MohamedRashad,False,/r/reinforcementlearning/comments/ci4fdn/is_inverse_reinforcement_learning_basically/
Reinforcement Learning and Optimal Control by D. Bertsekas,1564148964,"I came across the book and a series of lectures delivered by Prof. Bertsekas at Arizona State University in 2019. Link - http://web.mit.edu/dimitrib/www/RLbook.html
He mentions that the draft of his book is available on his website. But on his website all I see is PDFs of selected sections of chapters. Can anyone help me with some link to the actual complete draft?
Thanks for your time.",reinforcementlearning,not_a_gan,False,/r/reinforcementlearning/comments/ci33gv/reinforcement_learning_and_optimal_control_by_d/
Opinions on free resources to learn Deep Reinforcement Learning,1564138115,,reinforcementlearning,rpicatoste_,False,/r/reinforcementlearning/comments/ci1bvy/opinions_on_free_resources_to_learn_deep/
"""DeepMind and Waymo: how evolutionary selection can train more capable self-driving cars"" {DM} [PBT for 24% reduction in pedestrian-detection CNN error rate]",1564065663,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/chok5u/deepmind_and_waymo_how_evolutionary_selection_can/
"""The Tools Challenge: Rapid Trial-and-Error Learning in Physical Problem Solving"", Allen et al 2019",1564021526,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/chhpki/the_tools_challenge_rapid_trialanderror_learning/
creating environments to simulate behavioral tasks,1564019086," 

I'm a neuroscientist and have recently started using reinforcement learning models (at the moment only Q learning) to analyze data obtained from behavioral tasks to better understand how subjects are completing the task.

Would it be possible to create an environment in something like openai gym that replicates the tasks that I use (think multi armed bandit) with the same actions, rewards, probability of payoff, number of trials etc so I can apply different RL models to see how they complete the task? My goal is to use a variety of RL models to simulate how subjects complete the task and relate this to brain function.",reinforcementlearning,bigfuds,False,/r/reinforcementlearning/comments/chh9vr/creating_environments_to_simulate_behavioral_tasks/
Robots Made Out of Branches Use Deep Learning to Walk,1564017450,,reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/chgzju/robots_made_out_of_branches_use_deep_learning_to/
Winning Solution from the Unity Reinforcement Learning Challenge,1564010417,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/chfpd3/winning_solution_from_the_unity_reinforcement/
New Coursera specialization on RL,1563993844,"There is a new [Coursera specialization](https://www.coursera.org/specializations/reinforcement-learning) on the fundamentals of reinforcement learning.

The specialization is taught out of University of Alberta by Dr. Adam White and Dr. Martha White, with guest lectures from many well known researchers and practitioners in the field. The specialization follows the Sutton Barto textbook from chapter 2 to 13 (give or take a few sections).

Right now, the first course is available. It goes from Bandits to Dynamic Programming and sets a foundation for more advanced topics in the field.

---

Anyways, go sign up and tell your friends :)",reinforcementlearning,andnp,False,/r/reinforcementlearning/comments/chc6h5/new_coursera_specialization_on_rl/
On Value Functions and the Agent-Environment Boundary,1563974685,,reinforcementlearning,mellow54,False,/r/reinforcementlearning/comments/ch85f7/on_value_functions_and_the_agentenvironment/
Inconsistent result of DQN between original paper and dueling DQN,1563967878,"No-op start. The results of random play are consistent. But the results of human and dqn are significantly different. Why?

\# Result in original dqn paper

&amp;#x200B;

https://i.redd.it/9heix0udl8c31.png

\# Result in dueling dqn paper

&amp;#x200B;

https://i.redd.it/lgqmfkcgl8c31.png",reinforcementlearning,VectorChange,False,/r/reinforcementlearning/comments/ch6zd6/inconsistent_result_of_dqn_between_original_paper/
RLDM 2019 Notes - David Abel [PDF],1563942169,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/ch3c5a/rldm_2019_notes_david_abel_pdf/
How would one make an agent do tasks at variable rates?,1563941008,"I’m curious as to how would one make an agent to perform tasks at desired velocities/rates. Like for example how would you train an agent for driving/ running, etc. at different desired velocities. Do we add the target velocity (desired velocity ) as a feature to the NN, and penalise the reward function by how much it is deviating from the desired velocity? Or are there better methods to accomplish this? Any papers/implementations that deal with this kind of problem ?",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/ch35a9/how_would_one_make_an_agent_do_tasks_at_variable/
[D] Theoretical consequences of zeroing out invalid actions,1563920227,"There's a lot of discussion around the implementation of RL algorithms in the situation where a subset of the action space is invalid for a given state space. 

e.g. [https://www.reddit.com/r/reinforcementlearning/comments/c95ats/zeroing\_out\_invalid\_actions/estow40/](https://www.reddit.com/r/reinforcementlearning/comments/c95ats/zeroing_out_invalid_actions/estow40/)

However, I was wondering if there was a link to a paper or if there was anyone who has done a little bit of thinking around the theoretical consequences of the different resolutions of dealing with invalid or illegal actions given a state e.g. 1. simply zeroing out the probabilities of taking an invalid action post softmax

2. performing softmax on the subset of valid actions only

3. assigning probabilties to be near zero for invalid actions

4. any others I haven't mentioned",reinforcementlearning,IAmOnYourSide,False,/r/reinforcementlearning/comments/cgzdvy/d_theoretical_consequences_of_zeroing_out_invalid/
"""Credit Assignment as a Proxy for Transfer in Reinforcement Learning"", Ferret et al 2019 [Transformer critics]",1563918613,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cgz1ny/credit_assignment_as_a_proxy_for_transfer_in/
Using Unity's MLAgents and PPO to land a Falcon 9,1563910786,,reinforcementlearning,SwissArmyApple,False,/r/reinforcementlearning/comments/cgxd6q/using_unitys_mlagents_and_ppo_to_land_a_falcon_9/
How to deal with state variables can only be observed in certain time steps,1563889279,"I am dealing with a reinforcement learning problem where an action is taken **before** some state variables can be observed. Those state variables should be a significant contributor to the optimal policy function, but I’m having trouble thinking of how to formulate the state space so that those variables will be considered at the time of taking an action. Could anyone provide some guidance? Thanks!",reinforcementlearning,IAmOnYourSide,False,/r/reinforcementlearning/comments/cgsr7u/how_to_deal_with_state_variables_can_only_be/
Dealing with combinatorially large action spaces,1563875736,"Is there a recommended way of dealing with large combinatorial, but non-continuous action spaces? In a continuous action space like moving around in 3D space, you can still approximately discretize the space into a small number of actions. But in a large combinatorial action space, like selecting the best combination of 10 cards out of 20 cards, there could be thousands or even millions of actions in each step, and there's no obvious way to approximate them. 

Is this a case where a heuristic is necessary for the problem to even be tractable? Also, since hand-crafted heuristics are usually greedy, is the agent also doomed to be greedy?",reinforcementlearning,BasicBlazar,False,/r/reinforcementlearning/comments/cgql57/dealing_with_combinatorially_large_action_spaces/
Need help on discounting aware importance sampling,1563872881,"From Sutton and Barto Reinforcement Learning Textbook, chapter 5.8 (discounting aware importance sampling) . I couldn't understand what is meant by the degree of partial termination nor partly terminating. Could someone enlighten me?",reinforcementlearning,_muhammadzafran,False,/r/reinforcementlearning/comments/cgq75m/need_help_on_discounting_aware_importance_sampling/
Tensorflow ValueError: No gradients provided for any variable,1563868958,"I am trying to implement Vanilla Policy gradient, which is basically  REINFORCE algorithm that uses an Advantage function. For estimating the  Advantage function the Value function V(s) has to be compute. REINFORCE  with just Return works but after trying to replace it with Advantage  function Im getting an error: ValueError: No gradients provided for any  variable Thank you for your help, if it helps, i will send you the entire code  
\`\`\`python  
# make action selection op (outputs int actions, sampled from policy)     actions = tf.squeeze(tf.multinomial(logits=logits,num\_samples=1), axis=1) #computing value function     value\_app = tf.squeeze(funct.critic\_nn(obs\_ph), axis=1) # make loss function whose gradient, for the right data, is policy gradient     weights\_ph = tf.placeholder(shape=(None,), dtype=tf.float32)     adv\_ph = tf.placeholder(shape=(None,), dtype=tf.float32)     v\_ph = tf.placeholder(shape=(None,), dtype=tf.float32)     act\_ph = tf.placeholder(shape=(None,), dtype=tf.int32) #Loss for actor     action\_masks = tf.one\_hot(act\_ph, n\_acts)     log\_probs = tf.reduce\_sum(action\_masks \* tf.nn.log\_softmax(logits), axis=1)     loss = -tf.reduce\_mean(adv\_ph \* log\_probs) #Loss for critic     critic\_loss = tf.reduce\_mean((v\_ph - weights\_ph)\*\*2) #optimizers     train\_actor = tf.train.AdamOptimizer(learning\_rate=lr).minimize(loss)     train\_critic = tf.train.AdamOptimizer(learning\_rate=1e-3).minimize(critic\_loss)

\`\`\`",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/cgppz7/tensorflow_valueerror_no_gradients_provided_for/
Microsoft to invest $1 billion in OpenAI; OA to build on/exclusively use Microsoft Azure cloud,1563805792,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cgdwv6/microsoft_to_invest_1_billion_in_openai_oa_to/
"I want to learn reinforcement learning using google colab, how can i do it ?",1563794317,"I want to learn reinforcement learning, guide me on how to get started.  Love you all.",reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/cgbxcn/i_want_to_learn_reinforcement_learning_using/
RL Weekly 26: Transfer RL with Credit Assignment and Convolutional Reservoir Computing for World Models,1563793866,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/cgbuyr/rl_weekly_26_transfer_rl_with_credit_assignment/
Improving Customer Experience with Computer Vision Applications,1563787666,,reinforcementlearning,Verma_RJ,False,/r/reinforcementlearning/comments/cgazu1/improving_customer_experience_with_computer/
OpenAI gym Classic Control Environment is giving AttributeError with Google colab.,1563771078,"I am trying to run OpenAI gym Classic Control Environment eg MountainCar-v0 but i am getting Attribute error on basic code. Just calling env.reset() gives the error. Where env is

&amp;#x200B;

`env = wrap_env(gym.make(""Pendulum-v0""))`

&amp;#x200B;

wrap\_env is 

&amp;#x200B;

`def wrap_env(env):`

  `env = Monitor(env, './video', force=True)`

  `return env`

&amp;#x200B;

error that it get is 

![img](a7djcsckcsb31)",reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/cg8sxl/openai_gym_classic_control_environment_is_giving/
"""Machine-learning-guided directed evolution for protein engineering"", Yang et al 2019 [review]",1563735828,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cg2oae/machinelearningguided_directed_evolution_for/
Running OpenAI gym on goggle colab!!,1563735505,"I am trying to run classic control environment on google colab like MountainCar, Cartpole and it's working.  But when I try record the video for seeing the environment in action using Monitor. It gives me attributeError.  I have tried star_ai github code which works superfine on Atari environment even when we use Monitor on the environment but gives the same error. Help me guys.",reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/cg2m1h/running_openai_gym_on_goggle_colab/
Calling model.fit() with batch results in much worse results than calling it for each item?,1563732532,"I've been trying to track down poor performance of a DQN playing CartPole. I compared it to another sample online that performed much better and slowly worked through every difference, isolating the problem to my calling `model.fit()` with a batch:

    def replay(self):
        if len(self.recent_memory) &lt; minibatch_size:
            return
        
        minibatch = random.sample(self.recent_memory, minibatch_size)
        
        state_batch, q_values_batch = [], []
        
        for state, action, reward, next_state, done in minibatch:
            # Get predictions for all actions for the current state.
            q_values = self.model.predict(state)
            
            # If we're not done, add on the future predicted reward at the discounted rate.
            if done:
                q_values[0][action] = reward
            else:
                future_reward = np.amax(self.model.predict(next_state)[0])
                q_values[0][action] = reward + self.gamma * future_reward
            
            state_batch.append(state[0])
            q_values_batch.append(q_values[0])
            
        # Re-fit the model to move it closer to this newly calculated reward.
        self.model.fit(np.array(state_batch), np.array(q_values_batch))

If I change this code to instead call `fit` with `state` and `q_values` directly, the results are far better:

    def replay(self):
        if len(self.recent_memory) &lt; minibatch_size:
            return
        
        minibatch = random.sample(self.recent_memory, minibatch_size)
        
        for state, action, reward, next_state, done in minibatch:
            # Get predictions for all actions for the current state.
            q_values = self.model.predict(state)
            
            # If we're not done, add on the future predicted reward at the discounted rate.
            if done:
                q_values[0][action] = reward
            else:
                future_reward = np.amax(self.model.predict(next_state)[0])
                q_values[0][action] = reward + self.gamma * future_reward
            
            self.model.fit(state, q_values, epochs=1, verbose=0)

This is the opposite of what I expect. I expect that calling `fit` repeatedly will keep re-fitting biased towards the later results, wheras fitting on the whole batch would result in calculating the loss for the whole batch.

Left: single `fit()` on a batch of 32
Right: 32 individual `fit()` calls

[![Comparison of fitting batch versus fitting individually][1]][1]


Both notebooks are available here:

- [Batch fit](https://github.com/DanTup/Stuff/blob/master/SO/Simple_Gym_DDQN_CartPole_DEBUGGING_(Fit_batch).ipynb)
- [Individual fit](https://github.com/DanTup/Stuff/blob/master/SO/Simple_Gym_DDQN_CartPole_DEBUGGING_(Fit_each).ipynb)

  [1]: https://i.stack.imgur.com/UqjfO.png",reinforcementlearning,DanTup,False,/r/reinforcementlearning/comments/cg21li/calling_modelfit_with_batch_results_in_much_worse/
AI learns to play Snake Game using Deep Q-Learning,1563723879,,reinforcementlearning,BrandNewThanos,False,/r/reinforcementlearning/comments/cg0das/ai_learns_to_play_snake_game_using_deep_qlearning/
Possible reason why some RL algorithms (like Q-learning) do not converge with some random seeds,1563695582,"Reinforcement learning is still not well-understood, and to practicioners, it is still akin to voodoo magic: make a deep Q-learning model, it doesn't learn, launch it again, it works. Or, look at the [Wikipedia page for RL](https://en.wikipedia.org/wiki/Reinforcement_learning): one paragraph implies one algorithm, the next implies another, and it all tries to pretend it is talking about something coherent. Why would trying one's best (maximizing) fail seemingly randomly? As far as I know this is yet to be explained satisfactorily, but there is definitely an ironic effect there, crying for at least an informal explanation.

The following applies only to algorithms that *maximize* (like Q-learning), not just converge to expectation (like TD-learning). RL really means ""those that improve based on the past"" (supervised learning is ""improve based purely on the present iteration"", unsupervised is more or less ""compress (find patterns)""); 'improves' includes not only 'reinforced'/'increased' but also 'converges'. Dopamine-based personalities and similar always turn out largely the same, they are fine.

---

The defining principle of action optimisers is ""have more good, less bad"". Viewed internally, this is ""improve"" or ""seek a better future"" (intention); viewed externally, this is ""good stays, bad changes"" (visible effect — speed of change).

Intention is the only explanation needed when an optimizer is viewed in isolation, but what about interacting optimizers? Directly or through an environment, possibly even just with itself (as in recurrent NNs). Is lack of change, or its abundance, good or bad?

Just from common sense, lack of change is called ""surrounded by idiots"", and is bad; similarly, lots of change (especially in the context of considering ways to reach some goal) is good. Good leads to bad, bad leads to good, in an infinite non-obvious loop. Really, it is interspersed through society, appearing practically everywhere (even things like severe wealth inequality); since society very likely evolved without artificial constraints, the same general trends should be seen in naturally-invented optimizing approaches too.

Second-order bad is not communicated to first-order good, and the separation is reinforced. The common good may not be reached (without revolutions/interventions of other concepts into improvement).

What it comes down to is, sometimes Q-learning should fail, and so should humans (with personalities based on some reinforcing methods) — exactly the situation seen in reality, so the explanation is likely correct.

(I am not an expert in this voodoo, and do not have much time to become one, sorry; were there any mistakes? If so, how embarrassing; please critique harshly, as any truth candidate should be.)",reinforcementlearning,Antipurity,False,/r/reinforcementlearning/comments/cfwog2/possible_reason_why_some_rl_algorithms_like/
"[N] AlphaStar to play random anon matches against EU human SC2 Battle.net players [AS upgraded to camera-only, all races, all maps, hard-capped APM]",1563654013,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cfqddm/n_alphastar_to_play_random_anon_matches_against/
"DDQN playing CartPole learns to get full scores, then throws it all away and becomes useless",1563611827,"I've been trying to train a DDQN to play OpenAI Gym's CartPole-v1, but found that although it starts off well and starts getting full score (500) repeatedly at around 600 episodes, it then seems to go off the rails and do worse the more it plays.

I've tried tweaking hyper-parameters but nothing seems to change this trend. I'm not sure what would cause it or how best to try and debug. As far as I can tell, I've done the same things that other examples online do (though with some tweaks, for ex. using Keras instead of TF, etc.).

Full notebook (with a graph of scores at the bottom) is here: https://github.com/DanTup/Stuff/blob/master/SO/Simple_Gym_DDQN_CartPole.ipynb

Any pointers would be appreciated!",reinforcementlearning,DanTup,False,/r/reinforcementlearning/comments/cfjm7v/ddqn_playing_cartpole_learns_to_get_full_scores/
Super Mario NEAT problems,1563605168," So I am attempting to recreate Mar/IO from Sethbling in Python. I am using OpenAI retro to do this. i have edited the game so far, so that each sprite is categorized as such

 

    { 'block':3, 'luigi':1 'mario':2, 'background':0, 'enemies':4 }

 

I then scale this down and pass it in through the algorithm, and train based on that. I am using NEAT(neural evolution of augmented topologies) specifically neat-python.

The problem is, when i start the model, the agent takes no action. This has remained constant in 2-3 generations of a population with 300. There is nothing wrong with the algorithm itself, as I know this for sure. I then added random noise to each frame, which resulted in random, but at least some actions from the agent. **What can I do to make the agent train based off of the original frame data**, and image that is (13 x 13) in shape, without adding random noise to activate nodes? **What is happening to the model, and why is it behaving this way?** Is the input data too static and non-changing? I can link my NEAT hyper-parameters if neccessary...

&amp;#x200B;

&amp;#x200B;

## Starting Image

&amp;#x200B;

https://i.redd.it/do07qhcwmeb31.jpg

        [[0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 2 0 0 0 0 0 0 0 0 0 0]
         [3 3 3 3 3 3 3 3 3 3 3 3 3]]
    

 

## Example Image

&amp;#x200B;

https://i.redd.it/635sopaaneb31.png

        [[0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 3 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 3 0 0 3 3 3 3]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 0 0 0 0 0 0 0 0 0 0]
         [0 0 0 1 0 0 0 2 0 4 0 0 0]
         [3 3 3 3 3 3 3 3 3 3 3 3 3]]",reinforcementlearning,nrmxndal,False,/r/reinforcementlearning/comments/cfiuwh/super_mario_neat_problems/
Implementation of DRL based bidding,1563604482,Does anybody know of some implementation of [this](https://dl.acm.org/citation.cfm?id=3219918) paper by Alibaba on **Deep Reinforcement Learning for Sponsored Search Real-time Bidding?**,reinforcementlearning,BrownIndianChief1903,False,/r/reinforcementlearning/comments/cfirwr/implementation_of_drl_based_bidding/
Incorrect Actions from DQN,1563589372,"I have a pretty simple DQN setup (based off Sentdex's tutorial). I am applying it to a unity environment and I've adjusted sentdex's code to use a vector\_observations instead of pixel observations. The environment is still largely a simple blob environment (control one blob and head towards a food blob.

It's action space is up, down, left, right.

The environment space is \[blob\_x, blob\_y, food\_x, food\_y\]

I am having a problem that the blob does not seem to care about the food. I have defined my own rewards and done function separate from what is read in from the unity environment.

I have 3 rewards:

* \+10 for getting the food
* \-10 for leaving the bounds
* \-0.00001 \* sqrt\[(blob\_x - food\_x)\^2 + (blob\_y - food\_y)\^2\] j

The agent seems to learn to rarely leave the bounds but it never seems to care about the food.

What can cause this to happen? Have I set something wrong in the environment.",reinforcementlearning,Takachsin,False,/r/reinforcementlearning/comments/cfgm30/incorrect_actions_from_dqn/
Help with Mountain Car,1563582978,"# 

Hello,

&amp;#x200B;

&amp;#x200B;

Can  someone help me to find what am I doing wrong in my code when trying to  solve the mountain car? I am still learning python and RL, so any  comment will be welcome.

The code is here: [https://pastebin.com/VPJeAvhy](https://pastebin.com/VPJeAvhy)

Part of it I took from this post: [https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL)

My network doesn't seem to be learning, no matter what I use on the parameters.

&amp;#x200B;

&amp;#x200B;

Thanks.

&amp;#x200B;

&amp;#x200B;

EDIT:  Looks like my learning rate was too low. After increasing it to 0.005  the model solved the problem in about 1500 epochs. But anyway I would  appreciate comments to improve it.",reinforcementlearning,arivar,False,/r/reinforcementlearning/comments/cffkwu/help_with_mountain_car/
"[R] ""Learning Compositional Neural Programs with Recursive Tree Search and Planning"", Pierrot et al 2019 [AlphaZero + Neural Programmer-Interpreters]",1563565874,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/cfc9p1/r_learning_compositional_neural_programs_with/
Strange issue getting OpenAI Gym to display gameplay while training breakout,1563512590,"I've was following the basic PyTorch DQN tutorial\*, and got it to work on cart pole (the default for the tutorial) just fine, and it played a video in a little window of the training, a sample frame, and a live plot of the performance after every episode. I ported it to run on breakout (my favorite atari game), and while according to the plot it's worked very well, no video of the gameplay happens and after staring at what I changed for 5 hours I can't figure out why, no changes I made should effect it. All the other plots display (though the sample frame is removed below it does work when included) Here's the tutorial code and my breakout code with a dif: [https://www.diffchecker.com/nc9dYSHl](https://www.diffchecker.com/nc9dYSHl)

&amp;#x200B;

If anyone could tell me what I did wrong I'd be very appreciative.

&amp;#x200B;

This is the tutorial I started from [https://pytorch.org/tutorials/intermediate/reinforcement\_q\_learning.html](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)",reinforcementlearning,MockingBird421,False,/r/reinforcementlearning/comments/cf3bu4/strange_issue_getting_openai_gym_to_display/
Has anyone analysed strategies learnt by RL?,1563511313,"I am very interested in how RL methods can benefit players in games.

With the recent achievements of RL in Go, StarCraft, Dota etc. I thought there would be many papers that look into the strategies that AlphaGo, AlphaStar, OpenAI Five have learnt.

Although there is a bit of discussion on how AlphaGo used different strategies as its algorithm developed and some quotes in videos with an SC player saying they started using a strategy used by AlphaStar, I haven't been able to find any proper research (e.g. A paper in a journal). 

&amp;#x200B;

So the question is has anyone analysed strategies learnt by RL? Or the strategies not really beneficial to us at all?

**In search of a paper!**",reinforcementlearning,Ashes-in-Space,False,/r/reinforcementlearning/comments/cf34ws/has_anyone_analysed_strategies_learnt_by_rl/
Checkpoint frequency in self-play,1563502209,"Most recent self-play papers don't train against the current version of themselves, but a random older version ([section 4.2 here](https://arxiv.org/pdf/1710.03748.pdf)). I'm doing the same thing, but I can't find any research on how often these previous versions should be checkpointed. I'm currently checkpointing every 10 minutes, but I'm worried that training is diverging early (after about 30 minutes) because the agent overfits to the first few checkpoints.

Logically, a checkpoint every update makes sense, but that would fill up my hard drive pretty fast and saving the model takes a non-negligible amount of time.

I think that the hard drive space problem can be solved by maintaining a list of *""temporary""* checkpoints that are used for a short time and then discarded.

Is there any research on this?",reinforcementlearning,Flag_Red,False,/r/reinforcementlearning/comments/cf1o5l/checkpoint_frequency_in_selfplay/
Pytorch reinforcement learning in C++.,1563488392,"Check out Pytorch-RL-CPP: a C++ (Libtorch) implementation of Deep Reinforcement Learning algorithms with C++ Arcade Learning Environment.

One of the motivations behind this project was that existing projects with c++ implementations were using hacks to get the gym to work and therefore incurring a significant overhead which kind of breaks the point of having a fast implementation. 

Some of the ideas I have is to have something like fastai but for reinforcement learning in c++. I know it's really ambitious so if anyone wants to help out, send a PR! 
Thanks!

[Pytorch-RL-CPP](https://github.com/navneet-nmk/Pytorch-RL-CPP)",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/cez5ws/pytorch_reinforcement_learning_in_c/
Running A3C synchronously..?,1563473615,"Hello, as I have understood it, A3C is essentially A2C but interacting with multiple environments at once. However, emphasis seems to be on the fact that the interactions are asynchronous. Is it not possible to simply interact with multiple environments at once, thus running updates simply using batches of observations.

I tried it but it seems not to learn at all.",reinforcementlearning,jakkes12,False,/r/reinforcementlearning/comments/cew2rc/running_a3c_synchronously/
[D] Can I use RL to sequentially select items from a list?,1563466586,"Let's say the items are ingredients in a cake recipe. I want to optimize the final cake's taste. While cooking, I can select an ingredient and add it to the pot, then bake and taste the cake, and say how good the result is. This gives me a rich reward, at each timestep, which can be positive or negative.

Each ingredient can be used any number of time. Ideally, I'd also like to know how much of the ingredient I should use, but that's not mandatory as a larger quantity can just be modelled as using the same ingredient multiple times in a row.

The ingredients are not the same at each episode. For each ingredient I know a few features eg are they salty? are they fresh? are they liquid?

Also, the order in which I use the ingredients influence the final result.

Are there any known approaches to solve this kind of problem?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/ceuk4z/d_can_i_use_rl_to_sequentially_select_items_from/
raisim - Physics engine for robotics and AI research,1563465541,"Dear RL enthusiasts, 

My name is Jemin and I am working as a post-doc at RSL, ETHZ. I just released my physics engine RAISIM that I have been working on in the past two years. It is a free-of-charge closed-source library and the license is valid for 1 year. But I'll come up with a newer version before it expires. The engine is designed for my research and I expect that it is also useful for many roboticists or RL researchers. We recently published a paper to Science Robotics on RL using this physics engine ([https://youtu.be/aTDkYFZFWug](https://youtu.be/aTDkYFZFWug)). This work is a good proof that RAISIM is a useful tool for sim-to-real transfer!

&amp;#x200B;

There are three related repos

raisimLib ([https://github.com/leggedrobotics/raisimLib](https://github.com/leggedrobotics/raisimLib)): exported cmake library of raisim

raisimOgre ([https://github.com/leggedrobotics/raisimOgre](https://github.com/leggedrobotics/raisimOgre)): ogre visualizer for raisim

raisimGym ([https://github.com/leggedrobotics/raisimGym](https://github.com/leggedrobotics/raisimGym)): gym example of raisim (using stable-baselines)

&amp;#x200B;

[policy trained using raisim \(after 112 seconds of training\)](https://i.redd.it/436bmann13b31.gif)

&amp;#x200B;

The example policy above was trained under two minutes (excluding video recording time). You can run it yourself and give me feedback if you have any. I'll be continuously using and improving this engine!",reinforcementlearning,ultrafrog2012,False,/r/reinforcementlearning/comments/ceubvl/raisim_physics_engine_for_robotics_and_ai_research/
"How is Q-learning with function approximation ""poorly understood"" ?",1563462272,"In the first paragraph of the intro of [the 2017 PPO paper](https://arxiv.org/pdf/1707.06347.pdf), they say:

&gt;Q-learning (with function approximation) fails on many simple problems *and is poorly understood*

What exactly do they mean? I believe/know that it fails on many simple problems, but how is it poorly understood?

My best guess is that they mean, ""why does the technique of (experience replay + target Q network) work?"" because I know those are the two real ""secret sauce"" tricks that made the Atari Deepmind DQN paper technique work.

But, it still seems like we have a pretty good idea of why those work (decorrelating samples and making the bootstrapping work better. So what do they mean?",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/cetmw8/how_is_qlearning_with_function_approximation/
"""General non-linear Bellman equations"", van Hasselt et al 2019 [R]",1563428519,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/ceofeg/general_nonlinear_bellman_equations_van_hasselt/
What could've caused the dip in duration during my cart pole training?,1563421598,,reinforcementlearning,MockingBird421,False,/r/reinforcementlearning/comments/cenci8/what_couldve_caused_the_dip_in_duration_during_my/
[D] Any progress regarding Prioritized Experience Replay?,1563375862,"I'm reading and re-implementing [the PER paper](https://arxiv.org/abs/1511.05952) from early 2016.

As there been any interesting improvement around this idea in the meantime? Any new biais correction scheme?

One thing I want to try is to have a separate process running that will update the loss of the experiences from the buffer while the agent is training. This way, if an experience that initially got a low loss suddenly becomes more relevant, I won't have to wait too long before it is considered again. Maybe this could also help using larger replay buffers?

From a resource point of view, it's just a matter of running inference batches, so I could simply dedicate a separate GPU to this task.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cee18x/d_any_progress_regarding_prioritized_experience/
Using continues action space in policy gradient algorithms,1563350065,"In the equation below is pi(at|st), which is probability of taking an action at in a state st. However, how can I use this equation with continues action space? With the discrete action space it is easy, lets say I have two possible actions, so the output from the neural network would be the probability for action1 and the probability for action2 would be just 1-P(action1). But with continues action space I dont know what should I put into it. Thank you for your help

https://i.redd.it/s7kn6pw4kta31.png",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/ce9spk/using_continues_action_space_in_policy_gradient/
Multi Agent Control Actions,1563327660,"I am working on a project implementing RL. I have a good understanding of Q-Tables and a basic understanding of DQNs. I'm attempting to modify a DQN implementation but I'm a little stuck. My project has 3 agents all controlled by one ""player"". Each agent can move in the x and y direction. If I have a single agent my action choices would look like 

\[up, down, left, right\]. 

How do I apply this to control all 3 agents?",reinforcementlearning,Takachsin,False,/r/reinforcementlearning/comments/ce6arw/multi_agent_control_actions/
Help implementing actor critic,1563314956,"Hello,

&amp;#x200B;

I am trying to implement the (Advantage) Actor-Critic algorithm (A2C?) using TD-learning on the CartPole environment but am having some issues. Specifically, by inspecting the value of the initial state given by the critic, it seems as if the critic does not learn at all. The value is stuck at around 0 even though the minimum possible reward is 9.

&amp;#x200B;

This is the algorithm I have followed: [https://i.redd.it/vq9k73zp5pr11.png](https://i.redd.it/vq9k73zp5pr11.png)

I am using PyTorch for learning and I have tried several different learning rates. I am starting to think that there is something wrong the way I have written the optimization. I have commented my code as well as I can, if anybody would be so kind to have a look, I have uploaded it here (it's really short): [https://github.com/jakkes/RL\_Projects/blob/master/CartPole/a2c/\_\_main\_\_.py](https://github.com/jakkes/RL_Projects/blob/master/CartPole/a2c/__main__.py)

&amp;#x200B;

Thank you",reinforcementlearning,jakkes12,False,/r/reinforcementlearning/comments/ce3v5x/help_implementing_actor_critic/
Question,1563313549,"Lets say I want to solve an infinite horizon problem in which I have to balance a ball on a rotatable surface.   
Is it better to make the problem a finite horizon problem for the training part and then deploy it with an infinite horizon so that it has more experience with the startup situation or won't it matter much?",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/ce3k97/question/
Introducing Huskarl: The Modular Deep Reinforcement Learning Framework,1563313039,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/ce3gbj/introducing_huskarl_the_modular_deep/
Reward function for continuous maze environment,1563309256,"Hello all,

How would one approach reward function design for continuous maze environment?

I've seen some examples where robot reaching terminal state is being rewarded with large value, while for each time step robot did not reach terminal position, it is being penalized with some small value. Of course, robot for collisions and timeouts is being additionally penalized. This approach to me looks like it might result in a slow learning, if any, because of the sparse high reward.

P.S. Under continuous maze environment I'm thinking on a maze created in VRep, explored by differential drive robot with a couple of proximity sensors on robots body. The robot is being controlled by angular speeds of left and right wheel. Even though in VRep environment is 3D, this is basicly a 2D problem.",reinforcementlearning,page47250,False,/r/reinforcementlearning/comments/ce2mo4/reward_function_for_continuous_maze_environment/
When is Taxi-v2 considered solved?,1563307193,Taxi v1 is considered solved at 9.7 over 100 episodes. What is the equivalent for taxi-v2? Some posts that I've read suggest the optimal reward is 10 and others suggest 8.45.,reinforcementlearning,HalfArmBandit,False,/r/reinforcementlearning/comments/ce26dc/when_is_taxiv2_considered_solved/
Watch rl agent learn to chop trees in minecraft,1563302983,,reinforcementlearning,imushroom1,False,/r/reinforcementlearning/comments/ce190f/watch_rl_agent_learn_to_chop_trees_in_minecraft/
A collection of 25+ Reinforcement Learning Trading Strategies - Google Colab. https://colab.research.google.com/drive/1FzLCI0AO3c7A4bp9Fi01UwXeoc7BN8sW,1563300704,,reinforcementlearning,OppositeMidnight,False,/r/reinforcementlearning/comments/ce0qow/a_collection_of_25_reinforcement_learning_trading/
How do you run openAI gym in google colab ?,1563300097,,reinforcementlearning,The_artist_999,False,/r/reinforcementlearning/comments/ce0llj/how_do_you_run_openai_gym_in_google_colab/
Redefine manufacturing process with chatbot applications,1563295079,,reinforcementlearning,Verma_RJ,False,/r/reinforcementlearning/comments/cdzgz3/redefine_manufacturing_process_with_chatbot/
Used Q-learning to teach an AI to navigate a maze,1563292914,,reinforcementlearning,BrandNewThanos,False,/r/reinforcementlearning/comments/cdyz9i/used_qlearning_to_teach_an_ai_to_navigate_a_maze/
Value function approximator,1563286544,"Hi, I  am trying to implement Vanilla Policy Gradient. I am stuck with  estimating a Value function so I can compute an Advantage function. I  know that I should have the second neural network for estimating the  Value function. Should the Value function starts with random parameter  (arbitrary weights of a neural network for estimating V-function)?  According the picture below, does anyone have a simple code in  Tensorflow how I should fit the value function as it is in the point 8?  Also if anyone has a good theoretical&amp;practical tutorial, it will be  really helpful. Thank you  


[Vanilla Policy Gradient](https://i.redd.it/u2haod1vboa31.png)",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/cdxlp0/value_function_approximator/
"Pluribus: ""Superhuman AI for multiplayer poker"", Brown &amp; Sandholm 2019 [ Monte Carlo CFR ""stronger than top human professionals in six-player no-limit Texas hold’em poker""]",1563283412,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cdwzp3/pluribus_superhuman_ai_for_multiplayer_poker/
How to implement Tensorflow LSTM to RL DDPG?,1563280252,,reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/cdwexc/how_to_implement_tensorflow_lstm_to_rl_ddpg/
Why does A3C assume a spherical covariance?,1563229471,"I was re-reading Asynchronous Methods for Deep Reinforcement Learning ([https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)) and I found the following quote interesting:   


&gt;Unlike the discrete action domain where the action output is a Softmax, here the two outputs of the policy network are two real number vectors which we treat as the mean vector and scalar variance σ^(2) of a multidimensional normal distribution with a spherical covariance.

&amp;#x200B;

Nearly every implementation of A3C/A2C that I've seen assumes a diagonal covariance matrix, but not necessarily spherical. At what point did the algorithm change to quit using a spherical covariance matrix? Furthermore, why is it necessary to assume even a diagonal covariance matrix? Couldn't we allow the policy network to learn all n^(2) parameters of the covariance matrix for an action vector of size n?",reinforcementlearning,green-top,False,/r/reinforcementlearning/comments/cdofov/why_does_a3c_assume_a_spherical_covariance/
"""Bridging the gap between perception and action: the case for neuroimaging, AI and video games"", Bellec &amp; Boyle 2019 [brain imitation learning]",1563220733,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cdml8n/bridging_the_gap_between_perception_and_action/
"""Evolving the Hearthstone [Collectible Card Game] Meta"", de Mesentier Silva et al 2019",1563220355,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cdmi3a/evolving_the_hearthstone_collectible_card_game/
"""α-Rank: Multi-Agent Evaluation by Evolution"", Omidshafiei et al 2019 {DM} [ranking AlphaGo/AlphaZero/MuJoCo Soccer/Poker by persistence during evolution of agent populations]",1563220021,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/cdmfe1/αrank_multiagent_evaluation_by_evolution/
"RL Weekly 25: Replacing Bias with Adaptive Methods, Batch Off-policy Learning, and Learning Shared Model for Multi-task RL",1563199874,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/cdhxqj/rl_weekly_25_replacing_bias_with_adaptive_methods/
DQN Mountain Car,1563119192,"I've searched on the Internet but still have no answer. And the question is, ""What is the best average score achieved by DQN( or Double DQN) in OpenAI gym Mountain Car environment?"" Is it around 110 frames? (seen this on the other forum) Thank you in advance!",reinforcementlearning,shawntraynor,False,/r/reinforcementlearning/comments/cd4gk6/dqn_mountain_car/
Tips on solving short episodes with huge action space and state space?,1563104586,I am trying to learn an agent to solve a problem of 2^n states and n actions. I am using a DQN. Roughly 99.9% of actions return to the same state and only a few of them change it. Do you guys have any suggestions on what I should read to make my life easier with this problem?,reinforcementlearning,HalfArmBandit,False,/r/reinforcementlearning/comments/cd23jk/tips_on_solving_short_episodes_with_huge_action/
Pytorch Cpp Rl with ALE,1563099464,"Check out Pytorch-RL-CPP: a C++ (Libtorch) implementation of Deep Reinforcement Learning algorithms with C++ Arcade Learning Environment.

[Pytorch-RL-CPP](https://github.com/navneet-nmk/Pytorch-RL-CPP)",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/cd1ha6/pytorch_cpp_rl_with_ale/
Policy Gradient and Cross-Entropy,1563051195,"I understand the math behind the policy gradient but now I am trying to find its simple implementation in Tensorflow. OpenAI uses (at least for me) complicated way to express loss function. I have found out one way, which consist of using tf.nn.softmax\_cross\_entropy\_with\_logits(). The equation for cross-entropy is as follows:  


https://i.redd.it/grsplqe5w4a31.png

I just want to verify my understanding. We can use the cross-entropy function, because the neural net gives us a predicted distribution (action probability) and as result we take that action. As a result of taking that action, we get a true distribution, so p = 1. And thats why we can use this cross entropy, right?",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/ccucfj/policy_gradient_and_crossentropy/
leela chess PUCT mechanism,1563033609,"How do we know *w\_i* which is not possible to calculate using the tree search only ?

From the [lc0 slide](https://slides.com/crem/lc0#/9), *w\_i* is equal to summation of subtree of *V* ? How is this equivalent to winning ?

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/in5pokivf3a31.png

https://i.redd.it/5p5oajivf3a31.png

https://i.redd.it/2cayocgvf3a31.png",reinforcementlearning,promach,False,/r/reinforcementlearning/comments/ccqx25/leela_chess_puct_mechanism/
Colleges and Research Labs for Reinforcement learning?,1563033344,"Wondering if any one has looked up / researched colleges that offer some specialization in reinforcement learning ( say masters in computer science with specialization or electives in RL ) or research labs that work on RL.   
I am trying to come up with a list, and it is extremely easy to get inundated unrelated ML/DL based results.   
Nevertheless, will hopefully post whatever I gather in case that would help anyone.   


  


P.S. let me know if this is not the correct place for this post, and if it will be more suitable elsewhere. peace.",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/ccqvb2/colleges_and_research_labs_for_reinforcement/
Diverging loss in PPO's ICM with statefull LSTM,1563018595,"Hi, does anyone have idea why Intrinsic Curiosity Model's loss could be diverging? What I might have done wrong in implementation. The problem mainly occurs in action\_hat output, where values reach insanely high numbers like 1e\^23 even after 12 hours of training.

&amp;#x200B;

I use batch processing in both ICM(reward and loss-same as PPO training) and PPO(old policies and training itself) with hidden states as memory which is reseted only between each environment episode.

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/nkm08a1c62a31.png",reinforcementlearning,tomast95,False,/r/reinforcementlearning/comments/ccoh8z/diverging_loss_in_ppos_icm_with_statefull_lstm/
Diverging loss in PPO's ICM for statefull LSTM,1563016582,"Hi, does anyone have idea why output from Intrinsic Curiosity Model diverges? What I might have done wrong? Diverging problem occurs mainly the action\_hat output. It didn't stabilize after 12h of training (about 1e\^23). 

I use batches for processing with hidden cells as memory (reset between each environment episode).",reinforcementlearning,tomast95,False,/r/reinforcementlearning/comments/cco81y/diverging_loss_in_ppos_icm_for_statefull_lstm/
MineRL 0.2.0 released for NeurIPS 2019 Competition!,1562982899,,reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/ccjrkb/minerl_020_released_for_neurips_2019_competition/
Can we parallelize Soft Actor-Critic?,1562961631,"Hey,

could we parallelize it? If not, why?",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/ccfu4v/can_we_parallelize_soft_actorcritic/
Policy Gradient - computing Loss Function,1562944906,"I am trying to program the simplest version of Policy Gradient algorithm using Tensorflow. I do not understand what exactly should I compute. I understand that weights of a policy neural network are updated using this expression:  


[Optimizing policy using gradient ascent](https://i.redd.it/6tq82k5o2w931.png)

To actually use this algorithm, we need an expression for the policy gradient which we can numerically compute. Therefore we derive the gradient into this:  


[Estimation of policy gradient](https://i.redd.it/eqff02ua3w931.png)

1) Is the update (the first equation) done automatically using tensorflow optimizer such as AdamOptimizer? The only thing we need to express/compute is the policy gradient part? Do I understand it correctly so far?  


2) According the OpenAI Spinningup code they compute the loss function, which they use accordingly:",reinforcementlearning,RLbeginner,False,/r/reinforcementlearning/comments/ccc82n/policy_gradient_computing_loss_function/
How to create multiple agents with a DDPG?,1562939330,I am trying to create multiple agents and run them in parallel to train one Pytorch model.  I have a single environment class that I need to create multiple instances of for this to work.  Anyone know of a tutorial or github project that I can reference on how to use threading/multiprocessing or a different way to achieve this?,reinforcementlearning,thinking_computer,False,/r/reinforcementlearning/comments/ccb2vu/how_to_create_multiple_agents_with_a_ddpg/
Striving for Simplicity in Off-policy Deep Reinforcement Learning,1562929648,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cc9gnh/striving_for_simplicity_in_offpolicy_deep/
How to incorporate neural networks into a MCTS?,1562902540,"If you have a Monte-carlo Tree Search algorithm, what do you have to do to incorporate a neural network into it? As far as I know, MCTS gets its Q-values from back-propagating scores from the terminal states of the environment, but neural networks are trained from looking at many training examples? I'm confused how those paradigms fit together.

Please correct me if I'm wrong, but MCTS does not separate training and inference, it's just always doing search, right? But you generally have to train a neural network... so what is your training data for it?

Also, if you have a board game, how is that generally encoded into the input of a neural network? What is the best kind of network to use? And do there exist frameworks that do Deep MCTS or do you generally just try and build a MCTS around an existing library like Tensorflow or Pytorch?",reinforcementlearning,BasicBlazar,False,/r/reinforcementlearning/comments/cc5mv4/how_to_incorporate_neural_networks_into_a_mcts/
DeepMind alphastar algorithm,1562834764,"It seems that AlphaStar used value based algorithms according to their blog. But just curious, is it possible to use policy based algorithm with PySC2 or it's not possible and/or not helpful?",reinforcementlearning,Nicolas_Wang,False,/r/reinforcementlearning/comments/cbtcog/deepmind_alphastar_algorithm/
Use NN to predict policy performance?,1562834648,"When using policy gradient, you end up creating up to one policy per timestep (assuming some actor-critic method).

Then you generally perform some evaluations  of the current policy every so often to see how the training is progressing, eg after every 10 episodes of training, you perform 10 episodes without training to see what is the current performance. 

My question is: would it make sense to feed the policies that have been evaluated to a NN to try to predict their performance? 

You would take the weights/biais of the NN of the policy as input, and the average performance of that policy as output. Generally in RL the policies don’t use too large NN themselves so the input dimension wouldn’t be too crazy. 

Would that make sense? Is it something that is used already?

This could work not only for PG methods, but for any policy search method such as CEM, etc...",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/cbtc70/use_nn_to_predict_policy_performance/
Some real life uses/application of Reinforcement Learning?,1562822705,"Hey all,
I started learning reinforcement learning and most of its uses and applications I found were on games. 
Can anyone tell me some applications/uses of reinforcement learning other than games?",reinforcementlearning,learnercrazy,False,/r/reinforcementlearning/comments/cbrrbv/some_real_life_usesapplication_of_reinforcement/
Was reinforcement learning used in this video to design 3D printed Aerospace parts ?,1562797442,,reinforcementlearning,DisastrousProgrammer,False,/r/reinforcementlearning/comments/cbnerd/was_reinforcement_learning_used_in_this_video_to/
[R] Ranking-Based Reward Extrapolation without Rankings,1562782156,,reinforcementlearning,strangecosmos,False,/r/reinforcementlearning/comments/cbk6j7/r_rankingbased_reward_extrapolation_without/
Shouldn't stochastic policies be made deterministic in production?,1562746568,"My current understanding is that we use stochastic policies mainly for convenience in optimization via policy gradients and state-space exploration. In case we deploy an agent into production however, I would intuitively want to make the agent deterministic to minimize randomness in actions (which might of course still happen due to stochasticity in real-world environments). Otherwise I would see issues like this one: [https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb) the agent reaches the upper reward limit but due to randomness in its actions (my guess from looking at the reward plot), the agent's performance suddenly drops down almost to where it started.

&amp;#x200B;

So a few concrete questions:

\- Is my reasoning right or are there valid reasons to keep stochastic policies for production level agents (assuming they are not supposed to learn anything anymore)

\- If so, are there sources that show how to best turn stochastic agents into deterministic ones?

\- My intuition is that throughout the optimization process, the variance of the stochastic policy and hence the variance of the reward should ideally drop. If so, are there any convergence results or relevant papers?

&amp;#x200B;

Thanks a lot for any input.",reinforcementlearning,SaremS,False,/r/reinforcementlearning/comments/cbe02a/shouldnt_stochastic_policies_be_made/
Suggestion of implementations of RL algorithms,1562732698,"Basically I want a suggestion of some implementations in which the agents are modularized and can be used as a object instead of a runner, train, fit or anything that abstracts the interactions env-agent inside a method or class.

Usually, the implementations I have seen (baselines, rllab, Horizon, etc..) use a runner or a method of the agent to abstract the training, so the interaction is modularized in two phases:

1. agent.train(nepochs = 1000), with the agent having access to the env, in this part the agent learns.

2. agent.evaluate(): this part uses the predictions from the trained model, but the learning is turned-off.

This is great for episodic envs or applications in which you train and evaluate the training and the model and you can encapsulate all that. But my agent needs to keep rolling, full online learning, and it is not an episodic task, so I want a little more of control, something like:

action = self.agent.act(state)

reward, state, info, done = self.env.step(action)

self.agent.update(action, reward, state, done)

Or in case of minibatchs a list and then: agent.update(batch)

I looked inside of some implementations and to adapt them to my needs would make me rewrite 70% of their code.  I'm considering doing this if I don't find anything more usable. 

I'm currently searching all of the implementations I can find as to see if some is suited to my needs, but if anyone can give me a pointer it would be awesome :D",reinforcementlearning,Enryu77,False,/r/reinforcementlearning/comments/cbby89/suggestion_of_implementations_of_rl_algorithms/
SAC Implementation,1562709698,"Does anybody have an implementation of the soft actor critic algorithm written in tensorflow 2? 

I'm considering to try and implement it in a robotics project.

Thanks!",reinforcementlearning,Kartelkraker,False,/r/reinforcementlearning/comments/cb7ldt/sac_implementation/
RL Weekly 24: Benchmarks for Model-based RL and Bonus-based Exploration Methods,1562668427,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/caz55v/rl_weekly_24_benchmarks_for_modelbased_rl_and/
"Researcher tricks m3.euagendas.org, the Twitter analysis website, with adversarial inputs",1562631793,,reinforcementlearning,atomlib_com,False,/r/reinforcementlearning/comments/catboc/researcher_tricks_m3euagendasorg_the_twitter/
Is the Policy Gradient a Gradient?,1562585642,,reinforcementlearning,ChrisNota,False,/r/reinforcementlearning/comments/cajwet/is_the_policy_gradient_a_gradient/
[Easy] Tensorflow.js and GridWorld - From Value Fuction to A3C,1562574585,"Hello.

I posted some articles on the basics of reinforcement learning with the interactive demo.

It is recommended for those who need a foundation or RL for easy content.

\[Caution\] The portion using tensorflow.js can only be guaranteed to run on Google Chrome.

&amp;#x200B;

[https://greentec.github.io/reinforcement-learning-first-en/](https://greentec.github.io/reinforcement-learning-first-en/) : Value Function

[https://greentec.github.io/reinforcement-learning-second-en/](https://greentec.github.io/reinforcement-learning-second-en/) : DQN

[https://greentec.github.io/reinforcement-learning-third-en/](https://greentec.github.io/reinforcement-learning-third-en/) : DQN improvement and Deep SARSA

[https://greentec.github.io/reinforcement-learning-fourth-en/](https://greentec.github.io/reinforcement-learning-fourth-en/) : Actor-Critic, A2C, A3C

[https://greentec.github.io/reinforcement-learning-fifth-en/](https://greentec.github.io/reinforcement-learning-fifth-en/) : Solving problems with a door and a key

&amp;#x200B;

Any feedback is welcome.

Thank you!",reinforcementlearning,greentecq,False,/r/reinforcementlearning/comments/caiewi/easy_tensorflowjs_and_gridworld_from_value/
Help for Implementing REINFORCE for continuous state and action space,1562571041,"As the title suggests I’m trying to implement the classical REINFORCE Algo for an environment with continuous states and actions. As I understand it, the neural network should output the mean and variance of a Gaussian distribution for each action, and for the experience stage I sample the actions from distribution. Ok and those will be my true labels. But what will be my predicted labels? Predict the same parameters and again sample the distribution? Also if there’s an implementation that you know of, could you please point me in the right direction.",reinforcementlearning,pickleorc,False,/r/reinforcementlearning/comments/cahzjj/help_for_implementing_reinforce_for_continuous/
Sudden explosion of value loss during training A2C-LSTM,1562504866,"Hello all, 

&amp;#x200B;

I'm trying to train A2C-LSTM on my own custom environment (kinda navigation task on 13x13 map, very similar to Taxi-V2 from openai). For sanity check, i firstly trained my model on Pong and it works fine. The original hyper-parameter setting was from openai baseline, and after tested with pong i fixed some of them(increased time-step of LSTM from 5 to 20, Downsized CNN, ...)

&amp;#x200B;

During Training on my own env, the value loss suddenly exploded after experiencing some successful episodes from range 1e-3 to 10\^2 or 3. The gradient of critic feed-forward network is also exploded. I'm using small learning rate (initiated with 7e-4, and decay with time) and gradient clipping. 

&amp;#x200B;

Please, any kind of advice or questions will be welcomed. Right now I'm suspicious of reward scale of my environment(but it seems like reasonable to me, -0.1 time penalty and +2 for success episode), absent of batch or layer normalization in my LSTM.",reinforcementlearning,pky3436,False,/r/reinforcementlearning/comments/ca6n3c/sudden_explosion_of_value_loss_during_training/
Huge explosion of Value loss in A2C-LSTM,1562504098,,reinforcementlearning,pky3436,False,/r/reinforcementlearning/comments/ca6ixf/huge_explosion_of_value_loss_in_a2clstm/
Study material for reinforcement learning,1562476060,"I am just starting out in reinforcement learning. I tried jumping directly to the codes but it all got confusing. Can somebody provide me a step by step guide of how to begin learning “reinforcement learning”. 
Thank you",reinforcementlearning,learnercrazy,False,/r/reinforcementlearning/comments/ca39lr/study_material_for_reinforcement_learning/
Looking for environments with invalid actions to test a new feature for stable-baselines,1562437766,"I'm working on adding the utilization of a binary vector to tell an agent which actions are valid. I would like to test it before on as many environments as possible before submitting the pr. 

So, if you have such an environment, please comment a link to it. Requirements: at the beginning of each episode, every action has to be valid, and for subsequent steps, add the binary vector to the info dict with the key 'valid_actions"". 

Thanks in advance!",reinforcementlearning,Turings_Ego,False,/r/reinforcementlearning/comments/c9wvnx/looking_for_environments_with_invalid_actions_to/
"""Tsallis Reinforcement Learning: A Unified Framework for Maximum Entropy Reinforcement Learning"", Lee et al 2019",1562365317,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c9m2g2/tsallis_reinforcement_learning_a_unified/
"""Learning World Graphs to Accelerate Hierarchical Reinforcement Learning"", Shang et al 2019",1562365229,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c9m1wa/learning_world_graphs_to_accelerate_hierarchical/
"[D] How come RL / DRL is significantly less popular than DL / DS, despite DRL grabbing the main spotlights?",1562357367,"What grabbed my attention to pursue DRL was way back when AlphaGo won against Lee Se-dol, and I know a lot of my classmates, friends, and others were influenced greatly by achievements of DRL (from both OpenAI and DeepMind). However, when it comes to career path; they all ended up either pursuing data science or ML (DL) engineering. Most of them know almost nothing about DRL. MMost material online (MOOC, blog posts, articles, etc) that I've found has been about DL. Am I just biased to believe that there seems to be a hidden barrier which many do not wish to cross to the RL / DRL side?",reinforcementlearning,Naveos,False,/r/reinforcementlearning/comments/c9kkit/d_how_come_rl_drl_is_significantly_less_popular/
Marketing strategy with Machine Learning decision making,1562332563,,reinforcementlearning,atomlib_com,False,/r/reinforcementlearning/comments/c9fu9d/marketing_strategy_with_machine_learning_decision/
"""Benchmarking Model-Based Reinforcement Learning"", Wang et al 2019 [ME-TRPO, SLBO, MB-MPO, PILCO, iLQG, GPS, SVG, RS, MB-MF, PETS-RS/PETS-CEM, TRPO, PPO, TD3, SAC]",1562289417,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c9a2x8/benchmarking_modelbased_reinforcement_learning/
Zeroing out invalid actions,1562261595,"Hi,

I've been trying to train an agent for a custom environment, where in each state there's 25 discrete moves that can be taken. But in any given state only approx. 10 of these are valid, meaningful actions. So far I've been assigning a large negative reward for choosing an invalid move, but the learning is very slow. As part of debugging I thought eliminating invalid moves entirely would dramatically speed up learning.

I'm using PPO, and my first thought is to just zero out the action-value outputs for invalid moves at the very end of the tensorflow graph - basically just take a vector of 0's and 1's for invalid/valid action, and multiply that by the action-value predictions made by the network. Is this sensible? To me, this would mean that those actions will just never be sampled by the network during training. 

I'm also not super sure about then normalising the new action-value outputs, since some of the actions will now have 0 outputs. Should I normalise the remaining non-zero action logits so that the sum of the probabilities would still equal 1?",reinforcementlearning,pap_n_whores,False,/r/reinforcementlearning/comments/c95ats/zeroing_out_invalid_actions/
"""Human Replay Spontaneously Reorganizes Experience"", Liu et al 2019",1562253338,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c93qek/human_replay_spontaneously_reorganizes_experience/
Research topic suggestion for DRL,1562236562,"Hello, I am looking for DRL research topic for my master thesis. Right know, I think curiosity-driven policy based methods for a problem would be interesting for me. Could you suggest any problem that maybe make me tweek standart methods a little bit since it is an important real life problem? Or if you have any other suggestions, I would love to listen.

Thank you",reinforcementlearning,dominozor,False,/r/reinforcementlearning/comments/c9131i/research_topic_suggestion_for_drl/
Multi-Agent RL Labs,1562152624,"Hi,

I am planning a research visit as a part of my doctoral studies. I am looking for labs you think are cutting edge in multi-agent reinforcement learning. I have few places already but I want to leave no stone unturned.",reinforcementlearning,visiting_researcher,False,/r/reinforcementlearning/comments/c8mv30/multiagent_rl_labs/
Contextual Bandit not learning based on context,1562137321,"I'm training a DQN on a contextual bandit problem, but the network always has one of the actions rated highest, no matter the context. When I trained a DQN on the raw pixels, it seemed to work fine, albeit pretty slowly, but after passing in the actual game state information for training to a new network, it doesn't seem to learn from context anymore. Can someone help me with understanding the possible issues?",reinforcementlearning,alexhuhcya,False,/r/reinforcementlearning/comments/c8kxis/contextual_bandit_not_learning_based_on_context/
Has anyone used A3C with multiple machine or with gpus?,1562118380,In the paper it says it was intended to be run on a single machine with threads using shared memory. This seems like it could run into resource constraints. Are there any implementations that run on multiple machines? Are there any that run using GPUs? How about both?,reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/c8i39c/has_anyone_used_a3c_with_multiple_machine_or_with/
"""Evolutionary implementation of Bayesian computations"", Czégel et al 2019",1562113937,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c8hcvh/evolutionary_implementation_of_bayesian/
Soft Actor Critic Stable Baseline Implementation,1562086026,"Has anyone had experience looking into the stable baseline version of the Soft Actor Critic model? I am working on editing it now and I ran into an issue when trying to log the entropy values that the model uses to encourage exploration. In the loss functions that it calculates it uses the log of the policy saying ""logp\_pi is the log probability of actions taken by the policy"", then defining the policy loss as

&gt;policy\_kl\_loss = tf.reduce\_mean(self.ent\_coef \* logp\_pi - qf1\_pi)

where qf1\_pi is the q-value and ent\_coef is the entropy coefficient used to scale the bonus to the reward based on the entropy of the policy at a given state and action. 

&amp;#x200B;

The issue I am running into is that when I print out the value of logp\_pi it is sometimes positive and sometimes negative. Seeing as it is commented as being equal to the log of the probability it should always be negative. If it was always positive I would assume it is just the negative log of the policy but that isn't the case either. 

&amp;#x200B;

Is anyone more familiar with SAC or the stable-baseline implementation specifically who knows why I am getting these values for the log probability? 

&amp;#x200B;

Here is the gihub repo with the [SAC folder](https://github.com/hill-a/stable-baselines/tree/master/stable_baselines/sac)",reinforcementlearning,Beor_The_Old,False,/r/reinforcementlearning/comments/c8bjq9/soft_actor_critic_stable_baseline_implementation/
"""Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog"", Jaques et al 2019",1562085984,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c8bjey/way_offpolicy_batch_deep_reinforcement_learning/
Implementation of Distributed DQN agents,1562082978,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/c8ax9z/implementation_of_distributed_dqn_agents/
"""Stochastic Latent Actor-Critic (SLAC): Deep Reinforcement Learning with a Latent Variable Model"", Lee et al 2019 {BAIR}",1562079230,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c8a6fz/stochastic_latent_actorcritic_slac_deep/
Help needed on a custom RL environment,1562078408," 

Hello,  
I am having issues with my custom RL environment and I would love to hear the opinions of experienced people.  
Task description: The goal of the environment is to fill up the area with items, utilizing as much space as possible by sequentially picking the locations and orientation for each item by using the image as the state representation. There are hundreds of different items that need to be packed with various different characteristics and the pending ones are shown at the bottom as a queue (right to left).

&amp;#x200B;

https://i.redd.it/4gpqclzgiw731.gif

  
**\*\*State space\*\***: 3 channel 128x128 image  
**\*\*Action space\*\***: so far i have tried both continuous and discrete action representations. The discrete representation generally obtains more consistent and better results.  
1. Discrete, total number of actions - 128x96x2=24576, representing every valid grid point where the next item can be placed, doubled for both horizontal or vertical orientations.  
2. Continuous - \[x, y, orientation\]. 3 values ranging (0; 1) that represent the x, y position of the next item (adjusted to the nearest gridpoint) and the orientation (rounded to be either 0/1 - horizontal/vertical).  
**\*\*Rewards, termination\*\*** The episode terminates if the item is placed on top of another item and the reward is proportional to the filled area, as well as extra reward for aligning edges of the items (one of the many reward functions that were tried so far).  
**\*\*Architecture\*\*** For feature extraction a convolutional net with 6-12 layers is used, resulting in a featuremap of size 1024-32768 (depending on the action space). There are two separate heads for the actor and critic and I have experimented with various sizes of the fully connected layers for action/value outputs.  
**\*\*Algorithms\*\*** So far experimented with all the available algorithms of  *\*stable\_baselines\**, tried a lot of hyperparameter configurations, currently mostly using PPO for the fast training.  
So far I have tried many things but nothing seems to bring reasonable results (the fill area goes up to around 70%). The algorithms converge on repeating the same trajectory and there is barely any exploration while the characteristics of upcoming items in the queue seem to be ignored. I am probably expecting too much and over-estimating what can be achieved with RL in this case.  
Mind you, that this environment is only a part of the end problem, where the goal is to build not only layers but stacks out of these items, where each item has a height, weight support and locations, on top of which other items can be placed (the yellow parts in the image).

  
**\*\*My questions\*\***:  
1. Any suggestions or comments about the state and action representations? Is there a better way to represent things other than an image?  
2. Is this even a feasible problem to be tackled with RL? Do you think that the model could generalize to new unseen layer configurations and items?   
3. Since I have a lot of data of how these layers should be built I was trying to pretrain the models with behavioural cloning from *\*stable\_baselines\**. This approach uses the images with appropriate actions as target labels to pretrain the agents in a supervised fashion. The results seem to be very bad as the loss keeps exploding during training and the trained models repeat the same action. Does anyone have experience with using this pretraining method with an image-based environment?",reinforcementlearning,andberne,False,/r/reinforcementlearning/comments/c8a0pi/help_needed_on_a_custom_rl_environment/
Donkey Car vs AWS DeepRacer?,1562020334,I wish to purchase one of these two devices to test various approaches to autonomous robotics (including Reinforcement Learning). Any thoughts on which is better?,reinforcementlearning,chip_0,False,/r/reinforcementlearning/comments/c815st/donkey_car_vs_aws_deepracer/
"""Deep Neuroevolution of Recurrent and Discrete World Models"", Risi &amp; Stanley 2019 {Uber}",1562008851,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c7ynwd/deep_neuroevolution_of_recurrent_and_discrete/
Implementation of Distributed DQN agents,1562006654,"Hi, I'm new to multiprocessing. I created an openAI gym environment and am running DQN(in pytorch) on it. It takes too long to train. My CPU utilization goes up to 726%(8 cores) while GPU utilization is just 1%. I want to use multiprocessing to distribute the compute and run multiple agents at the same time interacting with this gym(or possible even split this into smaller gym envs dealing with specific cases of those agents). Is there any reference code for me to get into it and understand how to parallelize, scale and distribute multiple agents' training and the gyms on different nodes of GPU and CPU ?If multi processing isn't the right idea, then what is the best course of action? How do i remove the overhead of the interaction with the gym from cpu to gpu?  I would be extremely grateful for any hints or clues on how to proceed. I've hit a bottleneck in my progress with this.",reinforcementlearning,Nike_Zoldyck,False,/r/reinforcementlearning/comments/c7xwez/implementation_of_distributed_dqn_agents/
Question about PPO training procedure,1562003572,"When the model (network) is not recurrent, after calculating the returns, it seems that one can treat each of the steps in the collected the samples as independent of each other for the update iterations. 

But when the model is recurrent, it seems that one has to keep the sequences together, and do update iterations on sequences, mostly because only the *first* rnn state is fed in for the update. 

So each ""sample"" in the minibatch is a sequence of steps, while with non-recurrent version each sample is just one step.

However, how bad would it be to disassemble the sequences even when the model is recurrent. Each sample would be one step, and rnn states are fed in for each step as well?

This question is mostly because I'm lazy to refactor a bunch of code for minibatch collating and network code to adopt for updates on sequences, and intuitevley I think the non-sequnce updates should still work, just maybe slower.",reinforcementlearning,amnezzia,False,/r/reinforcementlearning/comments/c7wu17/question_about_ppo_training_procedure/
"""Unsupervised Learning of Object Keypoints for Perception and Control"", Kulkarni et al 2019 {DM}",1561996663,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c7ufy1/unsupervised_learning_of_object_keypoints_for/
40 Resources to Completely Master Markov Decision Processes,1561983109,,reinforcementlearning,rodolfo-mendes,False,/r/reinforcementlearning/comments/c7rt0t/40_resources_to_completely_master_markov_decision/
How low can actor loss go?,1561975060,"Hi,

&amp;#x200B;

For critic loss the baseline is 0, however what is the lowest the actor loss could go, since i am wondering where that is when looking at some actor loss graphs. i am using ddpg

&amp;#x200B;

[Orange is actor loss, blue is critic](https://i.redd.it/702zewh70o731.png)

&amp;#x200B;

&amp;#x200B;

any ideas?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/c7qoq3/how_low_can_actor_loss_go/
struggle in avoiding steplimit?,1561973896," In my DDPG model i have a steplimit of 100, however my model somewhat works, however it ""struggles"" in avoiding the steplimit, since i intergrated an ""exit"" actoin that would atleast give the agent less than the steplimit punsihmentr of -1, so i wasn't able to figure out why its not doing for example 1 step before the steplimit the exit action, i even defined the current step in the state, but still didn't work. my second theory is that the memory buffer may be too low, since the occurance of a steplimit state is quite ""rare"" compared to other states, therefore it barely replays that experience, atleast not enough? anyone has some tips?",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/c7qjni/struggle_in_avoiding_steplimit/
Curiosity based exploration,1561963603,"Assuming problem like sonic level, and regular exploration policy like epsilon greedy. It is impossible to get to the end of the level while being random but when the epsilon decay and reaching further into the level it is hard to learn new moves as the epsilon decayed. Catch 22.

There are curiosity based rewards, why isn't there curiosity based exploration, as if we detect states we haven't seen before we raise epsilon value?",reinforcementlearning,What_Did_It_Cost_E_T,False,/r/reinforcementlearning/comments/c7p9nu/curiosity_based_exploration/
Please have a look at my new Gym Snake environment,1561940135,"This environment is an implementation of the classic Snake game, with multi-agent snake, randomly generated maps, and several options for observation spaces. I wrote a thorough blog post [here](https://jfpettit.github.io/Introducing-gym-snakerl/). Take a look, I appreciate feedback or advice. The blog post contains a link to my GitHub repository, if you just want to look straight at the code.

Thanks.",reinforcementlearning,jcobp,False,/r/reinforcementlearning/comments/c7ln1r/please_have_a_look_at_my_new_gym_snake_environment/
Q-learning agents compete against each other,1561903452,,reinforcementlearning,BrandNewThanos,False,/r/reinforcementlearning/comments/c7edx1/qlearning_agents_compete_against_each_other/
I'm training a RL agent using DQN on OpenAI's LunarLander environment and I find that the training duration seems to fluctuate a lot.,1561897494,"I just wrote a simple DQN to train using Keras. And I use some seed values for tensorflow and the normal random.

But the results are changing vastly when I change just the seeds. Is this normal/acceptable? How do I know anything about my agent's performance when it is varying so much.

Sometimes I can get it trained in 300 episodes, other times it takes more than 700 odd episodes and this seems weird to me. Looks like the seed values are more important than even the hyperparameters.

Can someone please shed some light on this, or confirm if this is normal? Is it just my mistake in some way or is this the way it is? If so, then how do I compare algos? Is training it 100 times and then comparing averages the only way to say Algo X is better than Algo Y?",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/c7crvv/im_training_a_rl_agent_using_dqn_on_openais/
[N] OpenAI 2017 Tax filings just became available,1561818442,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c6zkgd/n_openai_2017_tax_filings_just_became_available/
I am unable to get the same results in two different executions eventhough I have seeded everything.,1561789885,"I am a beginner, and I am working on OpenAI's LunarLander and I'm trying to do a simple DQN to train the agent. I have seeded the environment and the np.random at the very beginning. Also, I haven't simply done a random.seed(), cause I believe that will seed it with current time and that would lead to different results obviously. Rather I seeded with some specific number, say 0.

I ran the same program twice without any change, but the results are a whole lot different. The first one is almost trained in 250 episodes, but the second looks like it'll easily take &gt;400 episodes or such.

What am I missing?",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/c6vjmy/i_am_unable_to_get_the_same_results_in_two/
"""RHPO SAC-X: Regularized Hierarchical Policies for Compositional Transfer in Robotics"", Wulfmeier et al 2019 {DM}",1561758564,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c6q97g/rhpo_sacx_regularized_hierarchical_policies_for/
"""RH-HOP SAC-X: Regularized Hierarchical Policies for Compositional Transfer in Robotics"", Wulfmeier et al 2019 {DM}",1561758487,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c6q8na/rhhop_sacx_regularized_hierarchical_policies_for/
"""Anxiety, Depression, and Decision Making: A Computational Perspective"", Bishop &amp; Gagne 2019",1561753970,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c6pb48/anxiety_depression_and_decision_making_a/
[1906.04358] Weight Agnostic Neural Networks,1561751231,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/c6oq5h/190604358_weight_agnostic_neural_networks/
"orrb: ""OpenAI Remote Rendering Backend"" {OA} [MuJoCo+Unity3d environment for Dactyl training]",1561751200,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c6opxk/orrb_openai_remote_rendering_backend_oa/
What's the deal with gamma? When should you want it to not be ~0.9? How does this change with batching?,1561657636,"For clarity, I know what the discount factor gamma *does*, but not how do decide what value to use for it in given problems.

Let's take the simplest SARSA update: we say `L = (r + gamma*Q(s', a') - Q(s, a)).pow(2)`, and then try to minimize L.

I think I get idea: you want to make Q(s,a)'s value closer to its ""real"" value. You definitely got a reward of `r` this timestep, so you don't discount that, but Q(s',a') is less trustworthy because its value is based on even more distant rewards, and doing the same exact (s',a') pair later may give a different value for Q(s',a'). Is that about right?

Two examples to contrast: 1) an environment with a shaped reward where the closer the agent is to a target in a given time step, the higher the reward returned that time step, and each episode lasts for a set time (100 steps), and 2) a sparse reward environment like Mountain Car, where every time step the agent's penalized a small amount (-0.01), and only gets the big reward (+1.0) if it reaches some goal.

It seems like in the first env, if gamma=0, it would still learn to get to the higher value region, though it might take longer than if gamma &gt; 0. On the other hand, in the second env, it seems like for gamma=0, it could never really learn because it relies on info being ""passed back"" from the final reward through the intermediate Q(s,a) values, so it needs gamma &gt; 0.

For something like Mountain Car, is there any reason not to just use gamma = 0.999... ? [Wikipedia mentions](https://en.wikipedia.org/wiki/Q-learning#Discount_factor) the fact that using a NN for the Q function can lead to instabilities, and that starting with a smaller gamma could help with that.

Two other related questions: should the choice for gamma change if the rewards are stochastic vs deterministic (i.e., r(s,a) isn't constant) ?

and, should this change with respect to updating in batches? That is, if you don't update Q every time step, but instead every N time steps, and then you accumulate the rewards across the batch?",reinforcementlearning,seahawksdog86,False,/r/reinforcementlearning/comments/c68le0/whats_the_deal_with_gamma_when_should_you_want_it/
StarAi: Deep Reinforcement Learning Course,1561644004," Way back in 2017 when Deepmind released their PySC2 interface - we thought it would be a fantastic opportunity to create a competition to help accelerate the current state of the art in ML.

&amp;#x200B;

We thought that such a competition would need a big $ prize pool in order to attract talent to try help solve the ""Starcraft problem"". We tried to copy the model of the original [Xprize](https://en.wikipedia.org/wiki/Ansari_X_Prize) and use insurance bonds to try finance the $ prize purse. [This document,](https://github.com/star-ai/StarAi-LectureSlides/blob/master/StarAi%20Artificial%20Intelligence%20Competition2.pdf) literally bounced around to insurance brokers all around the world- but we got no takers :). Lucky for us - as we all know by now Deepmind more or less solved the Starcraft problem this year.

&amp;#x200B;

One thing we realised, early circa 2018 is that there were no bringing RL down to earth courses out there to help people get involved in the envisioned Starcraft competition. So we went ahead and made it ourselves :)

&amp;#x200B;

I know that other great resources such as OpenAi's spinning up have come out since then, but we would like to present our work and open source it to the community. We hope this content inspires someone out there to do great things!

&amp;#x200B;

[https://www.starai.io/](https://www.starai.io/)

.",reinforcementlearning,sigmoidp,False,/r/reinforcementlearning/comments/c65uye/starai_deep_reinforcement_learning_course/
[D] Habits I Picked Up While Learning Machine Learning,1561637418,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/c64qya/d_habits_i_picked_up_while_learning_machine/
[P] Lab: Organize ML Experiments,1561637389,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/c64qtd/p_lab_organize_ml_experiments/
[R] Learning Belief Representations for Imitation Learning in POMDPs [UAI 2019],1561615672,,reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/c61v9k/r_learning_belief_representations_for_imitation/
[R] Learning Belief Representations for Imitation Learning in POMDPs [UAI 2019] https://arxiv.org/abs/1906.09510,1561615588,,reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/c61uv1/r_learning_belief_representations_for_imitation/
Learning Belief Representations for Imitation Learning in POMDPs,1561614956,[https://arxiv.org/abs/1906.09510](https://arxiv.org/abs/1906.09510),reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/c61roz/learning_belief_representations_for_imitation/
"On ""Meta Reinforcement Learning"", Lilian Weng",1561579576,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c5v13s/on_meta_reinforcement_learning_lilian_weng/
"""Monte Carlo Gradient Estimation in Machine Learning"", Mohamed et al 2019 {DM} [review]",1561573939,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c5tm4l/monte_carlo_gradient_estimation_in_machine/
Should I care about Bayesian RL?,1561565146,"I am just learning about Bayesian Neural Network, something I had no idea existed. Now, since I am mostly interested about RL, I am wondering if I should dive into Bayesian RL, eg [reading this](https://arxiv.org/abs/1609.04436).

So my question is: should I explore this direction? has there been any interesting recent discovery in this area? is it likely to be an important component of future RL systems?

Learning RL by yourself is super hard, because you have no one to guide your learning process :-/",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/c5rndq/should_i_care_about_bayesian_rl/
"""Apple, Google, and Facebook Are Raiding Animal Research Labs""",1561562653,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c5r5d1/apple_google_and_facebook_are_raiding_animal/
Off-the-shelf algorithms,1561552815,"Hi,  


I designed my own custom environment and wanted to apply RL Algorithms to it. My interest here lies on if RL Algs are capable of solving this environment and I haven't been successfull so far. I browsed Github but often find Code for algs that are not that good or differ strong from the main idea of the algorithm. So I wanted to ask what is the go to library or repo for using RL Algs, I know baselines but would like to see some alternatives.",reinforcementlearning,salah3,False,/r/reinforcementlearning/comments/c5pbxe/offtheshelf_algorithms/
Replaybuffer taking whole ram and swap,1561527347,"Hoi, so im working my way through the openaigym. Up until now my ram had always enough space but with the 3x96x96 state from carracing-v0, my 16g of ram and 3g of swap partition reach their limit after one or two episodes. 
Im taking steps at every step of the environment and tried replaybuffer sizes as low as 300k. I have another 11gb on my gpu but I’m not sure how to utilize them or whether they are needed for the models later on.
How can i combat ram shortage during learning with some clever code ?",reinforcementlearning,gimme-rewards,False,/r/reinforcementlearning/comments/c5lux9/replaybuffer_taking_whole_ram_and_swap/
[R] Exploring Model-based Planning with Policy Networks,1561513131,,reinforcementlearning,baylearn,False,/r/reinforcementlearning/comments/c5j3x4/r_exploring_modelbased_planning_with_policy/
[R] Iterative Model-Based Reinforcement Learning Using Simulations in the Differentiable Neural Computer,1561510681,,reinforcementlearning,inarrears,False,/r/reinforcementlearning/comments/c5ilo3/r_iterative_modelbased_reinforcement_learning/
"RE:Wheeler experimental framework for OpenAI ""Requests for Research: HER edition""",1561508097,"Project is focused on [HER](https://openai.com/blog/ingredients-for-robotics-research/) and trying to address [request for research from OpenAI](https://openai.com/blog/ingredients-for-robotics-research/#requestsforresearchheredition) on this topic.

* HER + multi-step returns : Floating-N-steps + HER + GAE
* On-policy HER : via PPO algorithm ( On-Policy and Off-Policy settings )
* Combine HER with recent advances in RL : combining HER with cooperation of learning algorithm in one shot (DDPG + PPO; or other combination of algorithm is possible)

In addition comes up with concepts like :

* Cross algorithm cooperation : PPO + DDPG
* MROCS : multiple rewards per one critic(state), sparse reward per subtask

code : [https://github.com/rezer0dai/rewheeler](https://github.com/rezer0dai/rewheeler)

blog : [https://rezer0dai.github.io/rewheeler/](https://rezer0dai.github.io/rewheeler/)",reinforcementlearning,rezer0dai,False,/r/reinforcementlearning/comments/c5i2ps/rewheeler_experimental_framework_for_openai/
"""Shaping Belief States with Generative Environment Models for RL"", Gregor et al 2019 {DM}",1561495801,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c5f1kt/shaping_belief_states_with_generative_environment/
"""BatchBALD: Human in the Loop: Deep Learning without Wasteful Labelling"", Kirsch et al 2019",1561488385,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c5d61g/batchbald_human_in_the_loop_deep_learning_without/
µniverse: RL environments for HTML5 games,1561480233,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/c5b4eg/µniverse_rl_environments_for_html5_games/
So what could cause the agent not to converge in an increasing reward/average?,1561477527,"Hi everyone,

&amp;#x200B;

I am using a ddpg model where it's a package delivery environment. the reward is based on the distance from the goal and the positive rewards are indicated how well it did, furthermore there is a steplimit of -1.

&amp;#x200B;

So these were the results from ca 12-14K Iterations in this environment and the results aren't that pleasing...

&amp;#x200B;

&amp;#x200B;

[Actor \(orange\) critic\(blue\) loss:](https://i.redd.it/fvm0kbjpwi631.png)

&amp;#x200B;

[the action probabilities](https://i.redd.it/elwzt5ytwi631.png)",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/c5agnd/so_what_could_cause_the_agent_not_to_converge_in/
How can I go beyond just copying and pasting algorithms?,1561473870,"I’m just starting out on reinforcement learning and so far I know how some basic RL algorithms work yet I feel like I have not learned anything.

Whenever I implement an algorithm it just feels like I’m copying/pasting the code and just changing the environment variables and picking different hyperparameters. I guess that’s what makes RL general purpose. 

I’m wondering how people that don’t have a strong math background go about solving RL problems.",reinforcementlearning,FireStory,False,/r/reinforcementlearning/comments/c59lbv/how_can_i_go_beyond_just_copying_and_pasting/
Clipping the PPO entropy bonus,1561417193,"Has anyone played around w/ clipping of the entropy bonus in PPO? b/c even if a sample is clipped(i.e. gradient is zero) the gradient coming from entropy will be non-zero.

&amp;#x200B;

If you have, what sort of clipping schemes did you try?",reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/c4xnjg/clipping_the_ppo_entropy_bonus/
"""Deep Reinforcement Learning for Industrial Insertion Tasks with Visual Inputs and Natural Rewards"", Schoettler et al 2019 [residual RL for cable-insertion with robot arms]",1561412339,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c4was1/deep_reinforcement_learning_for_industrial/
How can full reproducibility of results be possible when we use GPUs,1561407318,"Even when we set all the random seeds of numpy, gym and tensorflow to be the same, how can we expect the result be be reproducible. Are the GPU computations not full of race conditions, making the results slightly different? I get different results of TD3 on MuJoCo tasks by simply running them on a different machine, even though all seeds are the same.",reinforcementlearning,rl_if,False,/r/reinforcementlearning/comments/c4uuq9/how_can_full_reproducibility_of_results_be/
"RL Weekly 22: Unsupervised Learning for Atari, Model-based Policy Optimization, and Adaptive-TD",1561385801,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/c4olzt/rl_weekly_22_unsupervised_learning_for_atari/
Proto-Value Functions in modern RL,1561382782,"Hi everyone,

&amp;#x200B;

I've recently started reading about [proto-value functions](http://jmlr.csail.mit.edu/papers/volume8/mahadevan07a/mahadevan07a.pdf) in my studies of transfer learning. The idea of using an environment's transition function to create state abstractions is a powerful one.

&amp;#x200B;

Does anyone have a good place to go after this paper, for more ideas like it? Or insights into its application in more complex domains?",reinforcementlearning,asdfwaevc,False,/r/reinforcementlearning/comments/c4ntbd/protovalue_functions_in_modern_rl/
"""Wasserstein Reinforcement Learning"", Pacchiano et al 2019",1561251905,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c3x8s5/wasserstein_reinforcement_learning_pacchiano_et/
Discord Reading groups in Theoretical Machine Learning including Reinforcement Learning,1561227619,"I created a discord server to organise open-ended, collaborative reading groups for theoretical machine learning. We are open to all topics and we organise and discuss papers, books etc. I feel like I learn much better when working with other people. If you feel the same way come join us at [https://discord.gg/K2Pwm2E](https://discord.gg/K2Pwm2E)",reinforcementlearning,clarice_lispectacula,False,/r/reinforcementlearning/comments/c3t0bf/discord_reading_groups_in_theoretical_machine/
How to get out of local maxima (PPO),1561208249,"Hello. During my research, I couldn't find a way to get out of local maxima. The environment is 'CarRacing-v0', and the action space is continuous; steering\[-1 to 1\], acceleration\[0 to 1\], and break\[originally 0 to 1 but modified to 0 to 0.2\]. I used the policy-gradient method, PPO, implemented by stable-baselines 2.6.0.

&amp;#x200B;

To my best knowledge, the way to avoid local maxima is ...

\- Lower batch size

\- Higher learning rate

&amp;#x200B;

I've done several parameters tuning based on these concepts, but the agent is still acting like a dumb.

(mostly no movement, or spinning in one way)

 

Are there any insights? 

&amp;#x200B;

Thanks in advance.",reinforcementlearning,jucho2725,False,/r/reinforcementlearning/comments/c3pe9m/how_to_get_out_of_local_maxima_ppo/
Condensing high dimensional or large state space into smaller space?,1561189803,"There’s dimensionality reduction, has this been researched much for condensing high dimensional or large state space into smaller space? What are other existing techniques for doing this? Is there any downside for it?",reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/c3myd0/condensing_high_dimensional_or_large_state_space/
Using reinforcement learning to trade Bitcoin for massive profit,1561162866,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/c3iqz3/using_reinforcement_learning_to_trade_bitcoin_for/
Looking for a resarch-focused internship in Reinforcement Learning,1561156102,"I know this sounds so picky and indeed I couldn't find good opportunity on the Internet after a while of googling..

But if someone happens to know anything related,,, could you please share some on this post??

&amp;#x200B;

*If possible I'd like to focus on some domain as shown below in relation to RL*

\- Robotics: Grasping objects or solving some simple tasks

\- Efficient usage of human demo in RL

etc..",reinforcementlearning,Rowing0914,False,/r/reinforcementlearning/comments/c3hgpn/looking_for_a_resarchfocused_internship_in/
"""Common neural code for reward and information value"", Kobayashi &amp; Hsu 2019",1561135433,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c3d044/common_neural_code_for_reward_and_information/
Discount rate - confusion about how far into the future is being considered...,1561108827,"As will become clear below, I'm completely new to RL and ML in general, but finding it fascinating and actually getting a  bit addicted. I wonder if you guys could shed some light on something which I'm sure is relatively simple.

I've been playing with OpenAI's Cartpole and Mountain Car with the help of many examples including this blog post: [https://keon.io/deep-q-learning/](https://keon.io/deep-q-learning/)

My question relates specifically to his this line of code where we predict the future discounted reward:  

    target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

This suggests to me we're only applying the discount for the very next step? I don't understand how this is helping our model plan 20, 50 or more steps into the future? Should we not do this multiple steps into the future, each time further discounting the reward?

I'm finding Mountain Car much more challenging because it require's this relatively long-term strategy.",reinforcementlearning,Diohead,False,/r/reinforcementlearning/comments/c38ew5/discount_rate_confusion_about_how_far_into_the/
Training Minecraft agent,1561092269,"I'm working on training a Minecraft agent to do some specific tasks like chopping wood, navigating to a particular location... link for more details..minerl.io

I'm wondering how do I train my agent's camera? I have dataset of  human recordings, tried supervised learning with that but the agent just keeps going round and round.

 What RL algorithms should I try? If you have any material, links that will help... please shoot them at me!! 

Thanks :)",reinforcementlearning,HypersportR8,False,/r/reinforcementlearning/comments/c3689a/training_minecraft_agent/
State Transition Probability and Policy - Difference?,1561055446,"Hey guys.  
During my research, I haven't been able to figure out how the state transition probability p(s' | s, a) relates to the policy π(s, a), are there any?

&amp;#x200B;

To my understanding, they both determine how an action in a given state result to a future state, but how so?",reinforcementlearning,Tomorrowood,False,/r/reinforcementlearning/comments/c2zog4/state_transition_probability_and_policy_difference/
Simplest environment that requires exploration?,1561052597,"For a presentation, I'm looking for a very simple environment (ideally an OpenAI Gym) that requires **exploration** to solve.

Ideally something super simple, `Discrete` action and observation states like Frozen Lake or CliffWalk, but unfortunately those can be fully solved without exploring.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/c2z3co/simplest_environment_that_requires_exploration/
Persistent Reinforcement Learning Environment with Real-Time Configuration,1561033482,,reinforcementlearning,wysgui89,False,/r/reinforcementlearning/comments/c2vdi0/persistent_reinforcement_learning_environment/
Gym Env for stock training or financial application,1561022384,"Does anyone know if there is some already written gym env for stock trading?   
To try on some RL agents on?   
Some well written easy to use.   


Thanks, 

In the mean time I'm writing one my self.",reinforcementlearning,CryptoMustache,False,/r/reinforcementlearning/comments/c2ttry/gym_env_for_stock_training_or_financial/
When to Trust Your Model: Model-Based Policy Optimization,1561016795,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/c2t5qn/when_to_trust_your_model_modelbased_policy/
RNNs for solving POMDPs,1560947444,I've recently read about POMDPs. Can a form of RNN generally solve a POMDP or are there cases where this is not possible?,reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/c2gi1o/rnns_for_solving_pomdps/
Variable number of actions,1560937663,"As far as I know, neural networks internally work with fixed input/output dimensions. But in chess, for example, the number of avaliable actions per state depends on said state.

&amp;#x200B;

Still the network used in the A3C algorithm is supposed to directly output the policy distribution. How does it do that, then?",reinforcementlearning,luiscastro193,False,/r/reinforcementlearning/comments/c2f20p/variable_number_of_actions/
OpenAI Five Features Predictor,1560934526,"Hey guys,

I started to look at OpenAI Five model's architecture and some features remain a mystery for me. Here is the model specification [Model Architecture](https://i.stack.imgur.com/1uQGT.jpg). I am particularly interested in the features presented at the end of this blog post: [OpenAI Five Benchmark: Results](https://openai.com/blog/openai-five-benchmark-results/). The agents are able to output a prediction of various in-game features like opponents location, future hits etc. 

The predictor in action: 

[OpenAI Five Planning Ahead v2](http://www.youtube.com/watch?v=pNfV5nQXVCQ """")

[OpenAI Five Predicting the Game](http://www.youtube.com/watch?v=iV4wd2zekJk """")


I don't see the corresponding heads in the diagram above. I am also curious to know how do these outputs were trained. My guess is that during training, a centralized critic has access to the full state of the game and provides a training signal to local predictors attached to each individual actor. This methodology can be inspired from [1]. I also wonder whether these predicted features are used as additional context for the model-free algorithm, or if they are just learnt for monitoring purposes (and maybe making sure that sufficiently rich features are being learnt by the model).

I will appreciate any additional information. Thanks!

Cited Reference:
[1] Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments. Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch. [arxiv.org](https://arxiv.org/abs/1706.02275)",reinforcementlearning,bOmrani,False,/r/reinforcementlearning/comments/c2enuo/openai_five_features_predictor/
What is dynamic programming? Some quotes in the lens of reinforcement learning,1560932593,,reinforcementlearning,gfrison,False,/r/reinforcementlearning/comments/c2efq8/what_is_dynamic_programming_some_quotes_in_the/
Rewards suddenly dip down,1560887225,"I am working on a deep reinforcement learning problem. The policy network has the same architecture as the one Deepmind published in 'Playing Atari with Deep Reinforcement Learning'. In the initial stage the behavior seems to be normal, i.e the agent is learning gradually. However, after a while the rewards suddenly go down by a lot. The TD errors also seem to be going up at the same time. I'm not sure how to interpret this problem. 

My hypotheses are:

1. The policy network is overfitting
2. Some filters fail to activate thereby misrepresenting the state information

I would really appreciate if you guys could give me some tips to narrow down this problem debug it. Cheers.",reinforcementlearning,rohitbokade94,False,/r/reinforcementlearning/comments/c26vgq/rewards_suddenly_dip_down/
Rewards suddenly go down,1560886576,,reinforcementlearning,rohitbokade94,False,/r/reinforcementlearning/comments/c26qmp/rewards_suddenly_go_down/
Does DQN have problem planning long term?,1560876310,"I tried implementing DQN agent for Snake game ([https://www.youtube.com/watch?v=wDbTP0B94AM](https://www.youtube.com/watch?v=wDbTP0B94AM)). The model can learn pretty well during early stage when the snake length is shorter than 20-30 but after that it struggle to stay alive. After 100k steps, I watch the agent plays the game and it seems that it usually just go for immediate reward rather than organize itself beforehand. It avoid hitting itself here and there though but when the reward is nearby, it just go for the reward. I assume that this behavior happened because it doesn't plan ahead enough (not further than 5 time steps).

&amp;#x200B;

The model see input as 5 channel including body, head, tail, food and size. I've tried multiple approach of game representation but overall they aren't significantly different.

&amp;#x200B;

Is there any suggestion? Should I change the algorithm or the game representation?",reinforcementlearning,51616,False,/r/reinforcementlearning/comments/c24kf6/does_dqn_have_problem_planning_long_term/
Why the learned DQN agent of gym CartPole-v0 is not that stable?,1560837828,"Recently, I've implemented the deep reinforcement learning described in the DeepMind's famous Nature Paper 2015. I trained the agent to play the simplest \`CartPole-v0\` built in OpenAI Gym. I found that the trained agent was not always worked well to control its balance. Sometimes it can work perfectly, but sometimes not. Why that happened, ? I've adjusted  the hyperparameters several times but just the same. Here is my code in [github](https://github.com/Huixxi/TensorFlow2.0-for-Deep-Reinforcement-Learning/blob/master/tf2_dqn_simple.py), and here is the online version in google [colab](https://colab.research.google.com/drive/1htYLGy0577RHw4gMY8R48y-GVlk_wLLl). I wrote with tensorflow 2.0 but other version can also works well. Can anyone help me with that?",reinforcementlearning,H_uuu,False,/r/reinforcementlearning/comments/c1yjwq/why_the_learned_dqn_agent_of_gym_cartpolev0_is/
"""AlphaStar: Mastering the Game of StarCraft II"", David Silver ICML talk {DM}",1560823747,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c1wfcz/alphastar_mastering_the_game_of_starcraft_ii/
"""Direct Policy Gradients: Direct Optimization of Policies in Discrete Action Spaces"", Lorberbom et al 2019 {DM/Technion/GB} [policy gradient over tree/sequence search]",1560819359,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c1vp11/direct_policy_gradients_direct_optimization_of/
RL Weekly 21: The interplay between Experience Replay and Model-based RL,1560813671,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/c1upv7/rl_weekly_21_the_interplay_between_experience/
[N] Stable Baselines v2.6.0 released: Hindsight Experience Replay (HER) with SAC/DDPG/DQN support + Evolution Strategy bridge,1560808039,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c1tmh9/n_stable_baselines_v260_released_hindsight/
Steplimit/terminal issue,1560795513,"Hi everyone,

&amp;#x200B;

I got an agent working with a ddpg model in an environment with a steplimit of 100 steps, however after some time it seems to converge in a policy where all it does is reach the steplimit, so i tried to avoid this by punishing the steplimit with -1 however that didn't resolve anything, seeing on the EMA, it's not that drastic because epsilon usually comes in for the clutch some time before it, but if you look at the outputs of the network, then its definetly a policy for only going for the steplimit, so what could be a solution for this? I thought of maybe indicating which step he is on, however i never see anybody else use that, so are there other ideas?

&amp;#x200B;

Thanks

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/c1r1by/steplimitterminal_issue/
[N] Hindsight Experience Replay (HER) with SAC/DDPG/DQN support + Evolution Strategy bridge | Stable Baselines v2.6.0,1560785645,,reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/c1oxyz/n_hindsight_experience_replay_her_with_sacddpgdqn/
Does COMA work better than a simple Policy Gradient with a centralised critic? Why?,1560775702,"The Counterfactual Multi-Agent (COMA) uses the following policy gradient:

[Policy Gradient](https://i.redd.it/b4x7ifqtvw431.png)

[COMA-advantage function](https://i.redd.it/93q5cwnovw431.png)

Why would this be better than using the following advantage function:

&amp;#x200B;

[Advantage function with all-seing critic](https://i.redd.it/urwvoagpww431.png)

Where  R(\\tau\^a) is the return of the trajectory followed by agent a. And V(s, u\^{-a}) is the value function where s is the all-seing state and u\^{-a} contains the actions of all agents but agent a.

&amp;#x200B;

Thank you very much for all your help!

&amp;#x200B;

Link to the paper: [https://arxiv.org/abs/1705.08926](https://arxiv.org/abs/1705.08926)",reinforcementlearning,forgaibdi,False,/r/reinforcementlearning/comments/c1n44l/does_coma_work_better_than_a_simple_policy/
DDPG not converging for VRep maze environment,1560750402,"Hello all.  


I'm sorry for the lengthy post, just wanted to provide with implementation details in advance.

Also, sorry for the code, I know it's a mess.

&amp;#x200B;

I've been using a combination of PyTorch and VRep to setup environment for the differential drive robot to learn maneuvering the maze. The robot only has 6 IC proximity sensors along its rim that measure distances from 10-80cm. As a first simple version, I've created VRep environment containing just a narrow corridor (width of the corridor is \~2\*robot\_width). Starting position for the robot is somewhere in the middle of the corridor, with the goal point being somewhere at the end of the corridor, with goal velocities being \~0 m/s. Idea is that my learned agent handles both navigation and low level robot control at the same time.

&amp;#x200B;

Now, I've been using preexisting DDPG implementation which works on Gym environments, but it does not seem to converge for my case. I thought that using only 6 proximity sensors is the part of the problem, so I've introduced to the state vector position and velocity readings, together with errors for the desired state (as some papers suggest).

&amp;#x200B;

I would appreciate any help.

&amp;#x200B;

Details:  


ddpg\_agent

    import numpy as np
    import random
    import copy
    from collections import namedtuple, deque
    import pickle
    
    from model import Actor, Critic
    
    import torch
    import torch.nn.functional as F
    import torch.optim as optim
    
    
    GAMMA = 0.99            # discount factor
    TAU = 1e-3              # for soft update of target parameters
    LR_ACTOR = 1e-3         # learning rate of the actor 
    LR_CRITIC = 1e-3        # learning rate of the critic
    WEIGHT_DECAY = 0.01     # L2 weight decay
    BATCH_SIZE = 128         # minibatch size
    MIN_BUFFER_SIZE = 500   # minimal size of replay buffer
    BUFFER_SIZE = int(1e6)  # replay buffer size
    
    #device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
    device = torch.device(""cpu"")
    
    class Agent():
        """"""Interacts with and learns from the environment.""""""
        
        def __init__(self, state_size, action_size, random_seed):
            """"""Initialize an Agent object.
            
            Params
            ======
                state_size (int): dimension of each state
                action_size (int): dimension of each action
                random_seed (int): random seed
            """"""
            self.state_size = state_size
            self.action_size = action_size
            self.seed = random.seed(random_seed)
            
            # Actor Network (w/ Target Network)
            self.actor_local = Actor(state_size, action_size, random_seed).to(device)
            self.actor_target = Actor(state_size, action_size, random_seed).to(device)
            self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)
    
            # Critic Network (w/ Target Network)
            self.critic_local = Critic(state_size, action_size, random_seed).to(device)
            self.critic_target = Critic(state_size, action_size, random_seed).to(device)
            self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)
    
            # Noise process
            self.noise = OUNoise(action_size, random_seed)
            
            # Replay memory
            self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)
        
        def step(self, state, action, reward, next_state, done):
            """"""Save experience in replay memory, and use random sample from buffer to learn.""""""
            # Save experience / reward
            self.memory.add(state, action, reward, next_state, done)
    
        def act(self, state, add_noise=True):
            """"""Returns actions for given state as per current policy.""""""
            state = torch.from_numpy(state).float().to(device)
            self.actor_local.eval()
            with torch.no_grad():
                action = self.actor_local(state).cpu().data.numpy()
            self.actor_local.train()
            if add_noise:
                action += self.noise.sample()
            return np.clip(action, -2, 2)
    
        def reset(self):
            self.noise.reset()
    
        def start_learn(self):
            if len(self.memory) &gt; MIN_BUFFER_SIZE:
                experiences = self.memory.sample()
                self.learn(experiences, GAMMA)
            
        def learn(self, experiences, gamma):
            """"""Update policy and value parameters using given batch of experience tuples.
            Q_targets = r + γ * critic_target(next_state, actor_target(next_state))
            where:
                actor_target(state) -&gt; action
                critic_target(state, action) -&gt; Q-value
    
            Params
            ======
                experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples 
                gamma (float): discount factor
            """"""
            states, actions, rewards, next_states, dones = experiences
    
            # ---------------------------- update critic ---------------------------- #
            # Get predicted next-state actions and Q values from target models
            actions_next = self.actor_target(next_states)
            Q_targets_next = self.critic_target(next_states, actions_next)
            # Compute Q targets for current states (y_i)
            Q_targets = rewards + (gamma * Q_targets_next * (1 - dones)).detach()
            # Compute critic loss
            Q_expected = self.critic_local(states, actions)
            critic_loss = F.mse_loss(Q_expected, Q_targets)
            # Minimize the loss
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            # torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)
            self.critic_optimizer.step()
    
            # ---------------------------- update actor ---------------------------- #
            # Compute actor loss
            actions_pred = self.actor_local(states)
            actor_loss = -self.critic_local(states, actions_pred).mean()
            # Minimize the loss
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
    
            # ----------------------- update target networks ----------------------- #
            self.soft_update(self.critic_local, self.critic_target, TAU)
            self.soft_update(self.actor_local, self.actor_target, TAU)                     
    
        def soft_update(self, local_model, target_model, tau):
            """"""Soft update model parameters.
            θ_target = τ*θ_local + (1 - τ)*θ_target
    
            Params
            ======
                local_model: PyTorch model (weights will be copied from)
                target_model: PyTorch model (weights will be copied to)
                tau (float): interpolation parameter 
            """"""
            for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):
                target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)
    
        def save_samples(self):
            fileSamples = open('samples.obj', 'w')
            pickle.dump(self.memory, fileSamples)
    
        def load_samples(self):
            fileSamples = open('samples.obj', 'r')
            return pickle.load(fileSamples)
    
    
    class OUNoise:
        """"""Ornstein-Uhlenbeck process.""""""
    
        def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):
            """"""Initialize parameters and noise process.""""""
            self.mu = mu * np.ones(size)
            self.theta = theta
            self.sigma = sigma
            self.seed = random.seed(seed)
            self.reset()
    
        def reset(self):
            """"""Reset the internal state (= noise) to mean (mu).""""""
            self.state = copy.copy(self.mu)
    
        def sample(self):
            """"""Update internal state and return it as a noise sample.""""""
            x = self.state
            dx = self.theta * (self.mu - x) + self.sigma * np.array([np.random.randn() for i in range(len(x))])
            self.state = x + dx
            return self.state
    
    class ReplayBuffer:
        """"""Fixed-size buffer to store experience tuples.""""""
    
        def __init__(self, action_size, buffer_size, batch_size, seed):
            """"""Initialize a ReplayBuffer object.
            Params
            ======
                buffer_size (int): maximum size of buffer
                batch_size (int): size of each training batch
            """"""
            self.action_size = action_size
            self.memory = deque(maxlen=buffer_size)  # internal memory (deque)
            self.batch_size = batch_size
            self.experience = namedtuple(""Experience"", field_names=[""state"", ""action"", ""reward"", ""next_state"", ""done""])
            self.seed = random.seed(seed)
        
        def add(self, state, action, reward, next_state, done):
            """"""Add a new experience to memory.""""""
            e = self.experience(state, action, reward, next_state, done)
            self.memory.append(e)
        
        def sample(self):
            """"""Randomly sample a batch of experiences from memory.""""""
            experiences = random.sample(self.memory, k=self.batch_size)
    
            states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)
            actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)
            rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)
            next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)
            dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)
    
            return (states, actions, rewards, next_states, dones)
    
        def __len__(self):
            """"""Return the current size of internal memory.""""""
            return len(self.memory)

&amp;#x200B;

Model:

    import numpy as np
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    def hidden_init(layer):
        fan_in = layer.weight.data.size()[0]
        lim = 1. / np.sqrt(fan_in)
        return (-lim, lim)
    
    class Actor(nn.Module):
        """"""Actor (Policy) Model.""""""
    
        def __init__(self, state_size, action_size, seed, fc1_units=600, fc2_units=400, fc3_units=300):
            """"""Initialize parameters and build model.
            Params
            ======
                state_size (int): Dimension of each state
                action_size (int): Dimension of each action
                seed (int): Random seed
                fc1_units (int): Number of nodes in first hidden layer
                fc2_units (int): Number of nodes in second hidden layer
            """"""
            super(Actor, self).__init__()
            self.seed = torch.manual_seed(seed)
            self.fc1 = nn.Linear(state_size, fc1_units)
            
            self.bn1 = nn.BatchNorm1d(fc1_units)
            
            self.fc2 = nn.Linear(fc1_units, fc2_units)
            self.fc3 = nn.Linear(fc2_units, fc3_units)
            self.fc4 = nn.Linear(fc3_units, action_size)
            self.reset_parameters()
    
        def reset_parameters(self):
            self.fc1.weight.data.uniform_(*hidden_init(self.fc1))
            self.fc2.weight.data.uniform_(*hidden_init(self.fc2))
            self.fc3.weight.data.uniform_(*hidden_init(self.fc3))
            self.fc4.weight.data.uniform_(-3e-3, 3e-3)
    
        def forward(self, state):
            """"""Build an actor (policy) network that maps states -&gt; actions.""""""
            # x = F.relu(self.bn1(self.fc1(state.unsqueeze(0))))
            x = F.relu(self.fc1(state))
            x = F.relu(self.fc2(x))
            x = F.relu(self.fc3(x))
            return torch.tanh(self.fc4(x))
    
    
    class Critic(nn.Module):
        """"""Critic (Value) Model.""""""
    
        def __init__(self, state_size, action_size, seed, fcs1_units=600, fc2_units=400, fc3_units=300):
            """"""Initialize parameters and build model.
            Params
            ======
                state_size (int): Dimension of each state
                action_size (int): Dimension of each action
                seed (int): Random seed
                fcs1_units (int): Number of nodes in the first hidden layer
                fc2_units (int): Number of nodes in the second hidden layer
            """"""
            super(Critic, self).__init__()
            self.seed = torch.manual_seed(seed)
            
            self.fcs1 = nn.Linear(state_size, fcs1_units)
            
            self.bn1 = nn.BatchNorm1d(fcs1_units)
            
            self.fc2 = nn.Linear(fcs1_units+action_size, fc2_units)
            self.fc3 = nn.Linear(fc2_units, fc3_units)
            self.fc4 = nn.Linear(fc3_units, 1)
            self.reset_parameters()
    
        def reset_parameters(self):
            self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))
            self.fc2.weight.data.uniform_(*hidden_init(self.fc2))
            self.fc3.weight.data.uniform_(*hidden_init(self.fc3))
            self.fc4.weight.data.uniform_(-3e-3, 3e-3)
    
        def forward(self, state, action):
            """"""Build a critic (value) network that maps (state, action) pairs -&gt; Q-values.""""""
            # xs = F.relu(self.bn1(self.fcs1(state.unsqueeze(0))))
            xs = F.relu(self.fcs1(state))
            
            x = torch.cat((xs, action), dim=1)
            x = F.relu(self.fc2(x))
            x = F.relu(self.fc3(x))
            return self.fc4(x)

&amp;#x200B;

Main:

    import torch
    import time
    import numpy as np
    import matplotlib.pyplot as plt
    
    from ddpg_agent import Agent
    from labEnv import LabEnv, MobRob
    import gc
    import pickle
    
    
    def main():
        vrepHeadlessMode = True
    
        state_dim = 18  #  x, y, yaw, vx, vy, v_yaw, e_x, e_y, e_yaw, e_vx, e_vy, e_v_yaw, prox 0 ... prox5
        action_dim = 2
        action_space = np.array([[-2, 2], [-2, 2]])
        action_lim = [-2.0, 2.0]  # 2 o/sec is the max angular speed of each motor, max. linear velocity is 0.5 m/s
    
        learn_every = 1  # number of steps after which the network update occurs [20]
        num_learn = 1  # number of network updates done in a row [10]
    
        episodes = 10000
        steps = 500
    
        desiredState = [-1.4, 0.3, -np.pi, 0.0, 0.0, 0.0]  # x, y, yawAngle, vx, vy, yawVelocity
    
        mobRob = MobRob(['MobRob'],
                        ['leftMotor', 'rightMotor'],
                        ['proximitySensor0', 'proximitySensor1', 'proximitySensor2', 'proximitySensor3', 'proximitySensor4',
                         'proximitySensor5'])
        env = LabEnv(mobRob, vrepHeadlessMode)
    
        random_seed = 7
        mobRob = Agent(state_dim, action_dim, random_seed)
    
        if mobRob is not None:
            print('mobRob agent initialized')
        else:
            print('mobRob agent failed to initialize')
    
        total_num_of_steps = 0
        actions = np.zeros((episodes, steps+1, action_dim), dtype=np.float)
        total_rewards = []
        save_rewards = []
        durations = []
        for episode in range(episodes):
            cur_state = env.restart(desiredState)
            mobRob.reset()
            start_time = time.time()
            reason = ''
            episode_rewards = []
            for step in range(steps+1):
                total_num_of_steps += 1
                action = mobRob.act(cur_state)
                actions[episode][step] = action
                # print(action)
                new_state, reward, done = env.step(action, desiredState)
                mobRob.step(cur_state, action, reward, new_state, done)
    
                cur_state = new_state
                episode_rewards.append(reward)
    
                if step % learn_every == 0:
                    for _ in range(num_learn):
                        mobRob.start_learn()
    
                if step &lt; steps and done and ~env.collision:
                    reason = 'COMPLETED'
                    break
    
                if step == steps: # time budget for episode was overstepped
                    reason = 'TIMEOUT  '
                    break
    
                if env.collision:
                    reason = 'COLLISION'
                    break
    
            mean_score = np.mean(episode_rewards)
            min_score = np.min(episode_rewards)
            max_score = np.max(episode_rewards)
            total_rewards.append(mean_score)
            duration = time.time() - start_time
            durations.append(duration)
            save_rewards.append([total_rewards[episode], episode])
            eta = np.mean(durations)*(episodes-episode) / 60 / 60
            if eta &lt; 1.0:
                etaString = str(np.round(eta * 60, 2)) + "" min""
            else:
                etaString = str(np.round(eta, 2)) + "" h""
    
            print(
                '\rEpisode {}\t{}\tMean episode reward: {:.2f}\tMin: {:.2f}\tMax: {:.2f}\tDuration: {:.2f}\tETA: {}'
                    .format(episode, reason, mean_score, min_score, max_score, duration, etaString))
    
            gc.collect()
    
        torch.save(mobRob.actor_local.state_dict(), './actor.pth')
        torch.save(mobRob.critic_local.state_dict(), './critic.pth')
        np.save('mean_episode_rewards', save_rewards)
    
    
    if __name__ == ""__main__"":
        main()

Typical output:  


Episode 3179	TIMEOUT  	Mean episode reward: 0.36	Min: 0.00	Max: 0.69	Duration: 36.86	ETA: 134.16 h

Episode 3180	TIMEOUT  	Mean episode reward: 0.22	Min: 0.00	Max: 0.72	Duration: 37.39	ETA: 134.12 h

Episode 3181	COLLISION	Mean episode reward: 0.26	Min: -49.50	Max: 0.54	Duration: 29.11	ETA: 134.08 h

Episode 3182	COLLISION	Mean episode reward: -0.39	Min: -50.00	Max: 0.21	Duration: 9.50	ETA: 134.02 h

Episode 3183	COLLISION	Mean episode reward: -0.06	Min: -50.00	Max: 0.32	Duration: 27.10	ETA: 133.98 h

Episode 3184	COLLISION	Mean episode reward: 0.38	Min: -49.32	Max: 0.69	Duration: 37.90	ETA: 133.94 h

Episode 3185	COLLISION	Mean episode reward: -0.52	Min: -50.00	Max: 0.21	Duration: 7.28	ETA: 133.88 h

Position reached!

Speed reached!

Episode 3186	COMPLETED	Mean episode reward: 0.39	Min: 0.00	Max: 80.72	Duration: 37.73	ETA: 133.84 h

Episode 3187	COLLISION	Mean episode reward: 0.36	Min: -49.36	Max: 0.68	Duration: 34.68	ETA: 133.8 h

Episode 3188	COLLISION	Mean episode reward: 0.32	Min: -49.43	Max: 0.62	Duration: 35.35	ETA: 133.76 h

Episode 3189	COLLISION	Mean episode reward: -0.23	Min: -50.00	Max: 0.21	Duration: 16.59	ETA: 133.71 h

Episode 3190	TIMEOUT  	Mean episode reward: 0.39	Min: 0.00	Max: 0.65	Duration: 38.15	ETA: 133.67 h

Episode 3191	TIMEOUT  	Mean episode reward: 0.35	Min: 0.00	Max: 0.67	Duration: 37.76	ETA: 133.63 h

Position reached!

Episode 3192	COMPLETED	Mean episode reward: 0.41	Min: 0.00	Max: 30.71	Duration: 36.36	ETA: 133.59 h

Episode 3193	TIMEOUT  	Mean episode reward: 0.35	Min: 0.00	Max: 0.72	Duration: 36.98	ETA: 133.55 h",reinforcementlearning,page47250,False,/r/reinforcementlearning/comments/c1jwq6/ddpg_not_converging_for_vrep_maze_environment/
Gym Tutorial: The Frozen Lake – Reinforcement Learning for Fun,1560738609,,reinforcementlearning,rodolfo-mendes,False,/r/reinforcementlearning/comments/c1i7dm/gym_tutorial_the_frozen_lake_reinforcement/
why best epsilon around 0.1?,1560706382,I recently noticed in [this](https://itnext.io/reinforcement-learning-with-multi-arm-bandit-decf442e02d2) post that best epsilon(exploration vs exploitation)  is around 0.1. why not 0.2 or 0.5? Is there any theorem proves this?,reinforcementlearning,gecicihesap17,False,/r/reinforcementlearning/comments/c1ci6a/why_best_epsilon_around_01/
"""ICML 2019 Notes"", David Abel",1560700469,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c1bech/icml_2019_notes_david_abel/
"""Efficient Exploration in Reinforcement Learning through Time-Based Representations"", Machado 2019",1560697953,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c1ay4m/efficient_exploration_in_reinforcement_learning/
Personal website for tensorflow 2.0 + deep RL! Not a huge amount of content just yet (up to basic DQN with cartpole).,1560650658,,reinforcementlearning,ajavdk,False,/r/reinforcementlearning/comments/c14yin/personal_website_for_tensorflow_20_deep_rl_not_a/
"CrystalBall: SAT solving, Data Gathering, and Machine Learning [training random forests to replace SAT solver heuristics]",1560618810,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c0zmhj/crystalball_sat_solving_data_gathering_and/
"""SAT Solvers as Smart Search Engines"" [training random forests for more efficient SAT solving]",1560618658,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c0zlfx/sat_solvers_as_smart_search_engines_training/
How to Handle Slow Environment?,1560595460,"I have an environment that's slow. How do I speed the environment up? I tried some threading, yet that only sped it up slightly.",reinforcementlearning,kivo360,False,/r/reinforcementlearning/comments/c0w0rs/how_to_handle_slow_environment/
"""AI Habitat""/""Replica"" dataset: room simulator w/18 photorealistic 3D environments {FB}",1560543159,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c0opp5/ai_habitatreplica_dataset_room_simulator_w18/
Creating cudtom environment for furniture placing,1560460013,"Hey guys,
So I had this idea of implementing a custom environment simulating placement of furniture into a room to see if I will be able to somewhat place furniture ""good"" using Reinforcement Learning. But doing so was not that successful yet...
I started of with implementing the environment in the style of OpenAi Gyms environments (having step, reset, render...) to use standard implementations of RL algorithms without changing much. I had to make some choices which I think are responsible for not having that much success, so I wanted to hear some opinions on that.

To make things not too complicated I had a very simple reward function giving a reward the nearer the furniture is to the wall, just to see if  the agent is able to learn this but it wasn't able to. I think the responsible factor is my Design of space and action space because the observation space is just one big matrix where each row represents a furniture with its category (shelf, bed, table...) being One Hot Encoded + the coordinates of the edges of the bounding box. So the Matrix is n x (m + k) big where n is the number of items in the room, m the Number of categories and k being coordinates of the 4 edges of the bounding Box. My Action space is one long vector with the length of n*j where j is the x and y component of which I want to move Move my furniture. Both spaces are continuous and I tried many algorithms such as A3C, A2C, PPO, ACKTR and many more but while being able to increase the reward it didn't really maximize it or come close to the maximun, it gave me the impression that my spaces are too big. I move all items every step by x and y and hope that it learns of how to move them to achieve a good reward.

So if anyone has a clue whether or not my problem is badly desgined or if the way in which I do steps in the environment is not good or even other approaches to solve this kind of problems if RL is not suited for it I would be happy about comments.",reinforcementlearning,salah3,False,/r/reinforcementlearning/comments/c0beu8/creating_cudtom_environment_for_furniture_placing/
"""VISR: Fast Task Inference with Variational Intrinsic Successor Features"", Hansen et al 2019 {DM}",1560447243,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c08rzd/visr_fast_task_inference_with_variational/
Anyone that has 3 action probabilities chart image for me?,1560440833,"Hi everyone,

&amp;#x200B;

I am looking for an image of 3 action probabilities/outputs from either softmax or sigmoid from a RL environment. It doesn't really matter which environment neither the rewards, i am trying to analyze the behaviour and how it would look with 3 actions, the probabilities.

&amp;#x200B;

 I personally struggeling in having the RL agent work with 3 actions at the same time, i don't know as of now how it comes, it uses 2 out of 3 actions and the other one is like around 0, all the time. the action that he decides not to use is seemingly random and is atleast always a diffrent action, but i wasn't able to figure out why this is coming from (softmax).

&amp;#x200B;

I am interested to see how they would look. an image of a probability chart would be nice of the 3 actions. Thanks in advance for helping me out in my analyses.

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/c07fps/anyone_that_has_3_action_probabilities_chart/
How do I Plot Complex Rewards,1560431752,"I'm looking at a blog about reward function design. I want to be able to plot all of the rewards I create just like what I'm seeing in the blog post. The example they put together had two pieces of code that looked like the following:

&amp;#x200B;

&amp;#x200B;

[They graphed this like a champ](https://i.redd.it/0vw6g234j4431.png)

&amp;#x200B;

How did they do this on matplotlib? I wanna copy this example and have a graph visualization for it.",reinforcementlearning,kivo360,False,/r/reinforcementlearning/comments/c05oxb/how_do_i_plot_complex_rewards/
Has anyone experimented with end to end normalization techniques?,1560399452,"I'm training an RL algorithm on time series data that is tricky to normalize. It's hard to determine the min and max of the data. The data of course is also non stationary. So I'm trying to decide between two different approaches:

1) I can probably get rough estimates for the min and max and just use Min/Max scaling. It won't be perfect, and it would be kind of difficult to ensure all my features are roughly the same order of magnitude.

2) I've started doing some research into end to end normalization techniques. This one seems promising https://arxiv.org/pdf/1902.07892.pdf. 

Does anyone have any thoughts? Does that technique seem worthwhile?",reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/c0187x/has_anyone_experimented_with_end_to_end/
"""Structured agents for physical construction"", Bapst et al 2019 {DM}",1560391790,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/c00089/structured_agents_for_physical_construction_bapst/
"""Search on the Replay Buffer: Bridging Planning and Reinforcement Learning"", Eysenbach et al 2019",1560391303,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bzzxd2/search_on_the_replay_buffer_bridging_planning_and/
"""When to use parametric models in reinforcement learning?"", van Hasselt et al 2019 {DM} [sample-efficient Rainbow DQN]",1560390798,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bzzu7x/when_to_use_parametric_models_in_reinforcement/
Questions about insertion policy of Prioritized Experience Replay,1560358712,"While reading the PER implementation from OpenAI, I have some questions about its insertion policy.

&amp;#x200B;

First one is about the eviction. When add new experience to full memory, their implementation evicts the oldest experience just as normal experience replay. But evicting the experience with lowest priority seems more natural for me. Especially when the speed of experience generation is similar or faster than training(e.g. Ape-X or R2D2), the sampled experiences would be dominated by just new experiences. Why don't they just replace the low-prioritized experience?

&amp;#x200B;

Second question is about the priority of new, unseen experiences. I used the term 'unseen' indicating not yet sampled new experiences. The original paper says every newly generated experiences should be sampled once, and OpenAI implemented that as assigning priority of new arrived experiences as the maximum priority of already seen experiences. I think it's not harmful to performance in most cases, but why do they use such approache instead of keeping the unseen data in seperated storage and then sample experiences from there first? Maybe due to the ambuguity of importance sampling ratio assignment?

&amp;#x200B;

As these questions are not bug-report of implementation, I posted it here rather than OpenAI github issue page. Please let me know if here is not the proper place to ask.",reinforcementlearning,wwiiiii,False,/r/reinforcementlearning/comments/bztoz9/questions_about_insertion_policy_of_prioritized/
"OpenAI Five @RedisConf19 | ""Reinforcement Learning on Hundreds of Thousands of Cores""",1560310675,"Speaker: Henrique Ponde de Oliveira Pinto

(Finally) a bit more than the blog post on how OpenAI Five training was orchestrated (using Redis).

Cool stuff!",reinforcementlearning,YoshML,False,/r/reinforcementlearning/comments/bzm82n/openai_five_redisconf19_reinforcement_learning_on/
Subcortical substrates of explore-exploit decisions in primates revealed using POMDP,1560291239,"https://doi.org/10.1016/j.neuron.2019.05.017

The explore-exploit dilemma refers to the challenge of deciding when to forego immediate rewards and explore new opportunities that could lead to greater rewards in the future. While motivational neural circuits facilitate learning based on past choices and outcomes, it is unclear whether they also support computations relevant for deciding when to explore. We recorded neural activity in the amygdala and ventral striatum of rhesus macaques as they solved a task that required them to balance novelty-driven exploration with exploitation of what they had already learned. Using a partially observable Markov decision process (POMDP) model to quantify explore-exploit trade**-**offs, we identified that the ventral striatum and amygdala differ in how they represent the immediate value of exploitative choices and the future value of exploratory choices. These findings show that subcortical motivational circuits are important in guiding explore-exploit decisions.",reinforcementlearning,fullcolorbrain,False,/r/reinforcementlearning/comments/bziyj4/subcortical_substrates_of_exploreexploit/
Learning to Learn with Probabilistic Task Embeddings (Berkeley),1560278916,,reinforcementlearning,Beor_The_Old,False,/r/reinforcementlearning/comments/bzgibg/learning_to_learn_with_probabilistic_task/
How to *more intelligently* debug RL roadblocks?,1560277018,"A while ago I [made this post](https://www.reddit.com/r/reinforcementlearning/comments/9sh77q/what_are_your_best_tips_for_debugging_rl_problems/) asking for tips on debugging when you run into a problem with RL.

However, I think the majority of the advice can be summed up with:

1) Test bits individually to make sure they're doing what they should
2) Don't go down a rabbit hole of fiddling with hyperparameters
3) Log/record/display everything, and ""look for things that are acting funny""

and I just want to be clear that I'm not disparaging that advice, it's actually really good, I'm thankful, and I know I'm asking a tricky, general question!

But I want to get to the ""next level"". I think I know the theory well enough, and I've successfully done a few toy problems, but I'm still here banging my head against the wall.

I'll take a practical example I'm struggling with now: gym's `Pendulum-v0`, which has a continuous action space of [-2, 2], and three state variables (`(cos(theta), sin(theta), theta_dot)`). I'm trying to solve it with a fairly simple AC setup and PyTorch. I'm using the RMSprop optimizer, and 2 (or 3) fully connected NN layers, with 50 (or 100) units in each layer, to approximate pi (the policy) and V (the value function/baseline).

To select the actions, like in [the A3C paper](https://arxiv.org/pdf/1602.01783.pdf), I have the pi NN have two outputs, mu and sd2 (the standard deviation squared). Every time step, I select an action `a` from a normal distribution with that mu and sd**2. Then, I calculate that `pi(a)` (just from the equation of a normal dist. with that mu/sd**2), and iterate the agent to get the reward from that time step.

Also like the A3C paper (for the Pendulum problem), I'm doing all the updates at once, at the end of each episode (so it's basically MC with V as the baseline). For each time step (after the episode) I accumulate the rewards from t to t_max as `r_accum` (with gamma = 0.99), then say `V_loss = (r_accum - V_list).pow(2).sum()`. For the policy gradient, I do `policy_loss = -(torch.log(pi_list)*(r_accum - V_list)).sum()`, and then zero grads, backwards the losses, step the optimizer, etc.

And I'm just not seeing any learning, going up to about 20k episodes. I'm plotting to TensorBoard (losses, rewards, weights, biases, gradients), but nothing is striking me as an obvious culprit. It gets varying rewards, the V_loss seems to decrease to 0, and the policy_loss usually kind of wanders but eventually goes to 0 (I think because it's also proportional to (r_accum - V_list) which is also going to 0).

But I think this is a perfect learning example. This is doable (...right?), it seems mostly correctly set up, and it's probably a fairly simple fix if I knew how to diagnose it. For the more experienced RL'ers out there, where would you start? What would you look at? What would you verify is working correctly?

Here are some of my guesses/notes:
* I haven't actually seen any straightforward implementations of a vanilla PG algo solving Pendulum-v0. In the A3C paper, they add an LSTM to it. There are a bunch of DDPG papers online, but that's a pretty different story. I found one A3C that doesn't seem to have an LSTM, so I'll check that out.
* Do I need experience replay? Maybe the variance is just too high using essentially REINFORCE with this problem, so I need to be getting much better data efficiency (or running it for a ton longer) ?
* I was worried that maybe it was never actually getting to positions where it could get a high enough reward (to ""motivate"" it to reach those positions), but I plotted some trajectories and it's definitely getting up to the top (by swinging wildly anyway), where R = 0, so it's experiencing them, just not repeatably.

Things I've tried (but maybe not systematically enough):
* Different initial LRs
* Different optimizers
* Different number of hidden layers/units
* Shared pi/V NN body (with diff output layers) vs not
* Changing amount of entropy
* Adding correlated noise
* Using TD residual instead of MC version
* Clipping the gradient
* Different gamma values

Anyway, I'd love it if anyone has any more general advice for how to think about and go about solving RL problems. I of course want to solve this one, but I want a more general way of thinking.",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/bzg3l2/how_to_more_intelligently_debug_rl_roadblocks/
Are all Stochastic Environments Partially Observable?,1560270771,"I'm reading [this article](https://www.freecodecamp.org/news/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f/) from [this RL course](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/), which generally explains things nicely.

I'm a bit surprised at the following explanations:

&gt; Deterministic policies are used in deterministic environments. These are environments where the actions taken determine the outcome. There is no uncertainty. For instance, when you play chess and you move your pawn from A2 to A3, you’re sure that your pawn will move to A3.
&gt; 
&gt; On the other hand, a stochastic policy outputs a probability distribution over actions.
&gt; 
&gt; It means that instead of being sure of taking action a (for instance left), there is a probability we’ll take a different one (in this case 30% that we take south).
&gt; 
&gt; The stochastic policy is used when the environment is uncertain. We call this process a Partially Observable Markov Decision Process (POMDP).

**First,** I'm a bit surprised at the statements ""Deterministic policies are used in deterministic environments"" / ""The stochastic policy is used when the environment is uncertain"". 

I'm not sure I see the point of stochastic policies for stochastic environments. Ok, as he explains afterward, it may help with exploration, and it helps for aliased states. 

But you can handle exploration in other ways with deterministic policies (like DDPG does) and aliased states sound to me like a corner case. In general, I don't see why a stochastic policy would do better in a stochastic env than a deterministic policy.

**Second,** I'm surprise at the link between ""environment is uncertain"" and ""Partially Observable Markov Decision Process (POMDP)"".

My understanding was that you these were orthogonal concerns:

- An environment can be stochastic, eg FrozenLake, where there's some randomness that you have to deal with but that you can't ""learn"" (you can learn to deal with it but you can't predict the noise).

- Or you can have a Partially Observable environment, where not all the information about the state are part of the observations. But you are able to learn how to handle this lack of information by keeping track of what happened in the past. Eg playing VisDoom, you can keep track of where your opponent moved, even if he's now out of view, using [something like DRQN](https://arxiv.org/pdf/1507.06527.pdf).

What are your thoughts?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bzeogk/are_all_stochastic_environments_partially/
"""Data Shapley: Equitable Valuation of Data for Machine Learning"", Ghorbani &amp; Zhou 2019",1560266927,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bzdweg/data_shapley_equitable_valuation_of_data_for/
Can next state and action be same in Deep Deterministic Policy Gradient?,1560254326,"I am trying to apply deep deterministic policy gradient (DDPG) on a robotic application. My states consist of the joint angle positions of the robot and my actions are also its joint angle positions. Since, DDPG produces a continuous policy output where states are directly mapped onto the actions, can I say that my next state and action will be same? Simplistically, the input of the policy network will be the current state and the output will be the next state?",reinforcementlearning,mr_denoza,False,/r/reinforcementlearning/comments/bzbjze/can_next_state_and_action_be_same_in_deep/
'Batches' in DQN,1560213389,"In DQN, we use an experience replay buffer D that stores transitions.

When we do an update with a batch size of N, do we update the agent N times? Or do we update our network using a batch of size N minimizing the mean loss?",reinforcementlearning,ajavdk,False,/r/reinforcementlearning/comments/bz5p14/batches_in_dqn/
"""Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP"", Yu et al 2019 {FB}",1560204030,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bz3ygp/playing_the_lottery_with_rewards_and_multiple/
REINFORCE vs Actor Critic vs A2C?,1560184678,"I'm trying to implement an AC algo for a simple task. I've read about many of the different PG algos, but actually got myself kind of confused.

I think [this blog post](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html#policy-gradient) by Lilian Weng is pretty accurate, for reference of the things I'm comparing

Here's what I'm partly confused about. In REINFORCE, it's Monte Carlo, so we do a whole episode without any updates, and then update at the end of the episode (*for* each step of the episode) by accumulating the rewards and updating the policy. So it's unbiased because it only depends on R's. And apparently it was a thing back when REINFORCE was proposed that you could use a baseline function to reduce variance?

Then, she presents AC methods, where instead of just using returned R's, we also have a critic NN (she uses Q as the critic, but you can use V instead). So now you can update weights at each episode step, because the critic can provide the approximate advantage to the policy update with `adv = r_t - V(s_t+1) - V(S_t)`. So it *is* biased now, because it's getting updated with approximated values.

Then, in A2C or A3C, it seems like they go *back* to a MC method, using V as a baseline.

So what's the deal? Are there actually good times to use bootstrapping methods (like the vanilla AC method she shows) ? I think I get what's going on, but I don't understand when to use which, or why they chose a MC method for A3C.",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/bz017h/reinforce_vs_actor_critic_vs_a2c/
Reward function for Stochastic Frozen Lake Problem,1560169064,"Hi guys, I'm quite new to Reinforcement Learning, and started to code some problems like Frozen Lake problems myself, so that I can get a better understanding of the concept.

 I could successfully(?) code for Deterministic Frozen Lake problem [here](https://github.com/AbishekSeshan/Reinforcement-Learning-Projects/blob/master/det_frozen_lake.py). 

Now, I tried to twist the problem to make it stochastic. There are 5 actions that the agent can take: N,E,W,S and no action, and there is an nxn grid where some cells can contain puddles/holes. Whenever the agent chooses any one of the directional actions(NEWS), it has 60% probability to go to the adjacent cell, and 40% probability to slip and reach further to the next cell. For eg: (assume indexing is columnwise) if my agent is in cell 0 and it takes north action, it has a 60% chance of going north to cell 1 and 40% chance of slipping further to cell 2. I'm currently learning policy and value iteration algorithms(Bellman's equation), and hence have tried to implement the codes using the same. But for the stochastic case, my code is giving unexpected results because of wrong logic in the reward function. 

For the deterministic case, I have taken reward function R(s,a,s'), i.e the agent will get the reward based on current state, the action it chooses and destination state. But in stochastic case, where s' can be anything, I'm not able to define a proper reward function.  [You can find my current progress here.](https://github.com/AbishekSeshan/Reinforcement-Learning-Projects/blob/master/stochastic_frozen_lake.py) 

1. Can you guys help me with coming up with an efficient reward function? And it would also mean a lot  if you could review my deterministic code (It'd help me figure out if I'm going in the right direction) (Criticism is strongly welcome)

2. Can you guys cite some good resources to learn OpenAI Gym? The official documentation is very very brief. 

PS - I have not used gym for my codes. I made a simple environment myself by making a class named ""state"" and its attributes like index, puddle etc, and defined each cell of the grid as an instance of the class. 

Thanks in advance!",reinforcementlearning,The_One_Nerd,False,/r/reinforcementlearning/comments/byx5c8/reward_function_for_stochastic_frozen_lake_problem/
When doing RL using time series data is it beneficial to do any work to make sure the time series is stationary?,1560118562,In normal time series forecasting you usually do some preprocessing to ensure the time series is stationary. Detrending and making sure your input has constant variance. Now you could do this with RL but couldn’t the model lose info that it could use to form a better strategy?,reinforcementlearning,iamiamwhoami,False,/r/reinforcementlearning/comments/bypvze/when_doing_rl_using_time_series_data_is_it/
Deep Reinforcement Learning with TensorFlow 2.0 | Roman Ring,1560096742,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/bylts9/deep_reinforcement_learning_with_tensorflow_20/
Could dreams be a form of MC/n-step learning?,1560064074,"Looking into higher order conditioning and how we can model the way our brain responds to stimuli via a TD model similar to RL, I was thinking in an online environment if the brain may in fact break up deeper forms of learning into variable length ""episodes"". When it comes time to update its internal weights/learning at the end of an episode, could this be what dreaming is? Of course dreaming couldn't involve a strict offline update for a fixed episode, that could easily lead to the death of the organism if it can't immediately wake up, so rather I was thinking is it possible that dreaming is a very complicated form of semi-offline learning, capable of being interrupted if needed whilst partially updating ""episodes"" of experience? Since we don't have to dream ""all in one go"" (a partial dream still gives us some rest) perhaps the way the update takes place is more complicated than simply updating G recursively in one go. Perhaps it might update its weights in a continuous fashion that is capable of being interrupted with minor penalty. What are your thoughts on this?",reinforcementlearning,clanleader,False,/r/reinforcementlearning/comments/byhp7p/could_dreams_be_a_form_of_mcnstep_learning/
Grid based RL environment suggestions,1560051195,"I'm looking for environments similar to OpenAI's Taxi and FrozenLake. More like Taxi, the better.

Thanks!",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/byg55o/grid_based_rl_environment_suggestions/
MineRL Competition on Reinforcement Learning in Minecraft Launched!,1560024780,,reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/byc1kq/minerl_competition_on_reinforcement_learning_in/
Introducing Google Research Football: A Novel Reinforcement Learning Environment,1560018969,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/byb1vt/introducing_google_research_football_a_novel/
Punish or do nothing at terminals' reward?,1560016734,"Hi everyone,

&amp;#x200B;

So in an environment where the terminal action/reward is a timeout, should you punish or do just nothing? Meaning you stop the environment because of a timeout. Since R=R in the terminal step, is there an use in punishing the agent for reaching this terminal reward, but also should you leave it untouched, it could end up with terminal being the easiest way out?

&amp;#x200B;

Any ideas?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/byanun/punish_or_do_nothing_at_terminals_reward/
Workshop on Learning for Dynamics and Control (video recordings),1559982485,,reinforcementlearning,jkoendev,False,/r/reinforcementlearning/comments/by5v7x/workshop_on_learning_for_dynamics_and_control/
[Meta] /r/RL subreddit traffic 2018-2019: continued increase in RL interest,1559947877,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/by0ziq/meta_rrl_subreddit_traffic_20182019_continued/
"""DeepMDP: Learning Continuous Latent Space Models for Representation Learning"", Gelada et al 2019 {GB}",1559941625,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bxzuul/deepmdp_learning_continuous_latent_space_models/
"""1000x Faster Data Augmentation"": ""Population Based Augmentation (PBA): Efficient Learning of Augmentation Policy Schedules"", Ho et al 2019",1559940722,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bxzosl/1000x_faster_data_augmentation_population_based/
"My article on semantic text segmentation through Deep-Q Learning. Do go easy on me though, it’s my first article ever. :3",1559926447,,reinforcementlearning,TheHawkGriffith,False,/r/reinforcementlearning/comments/bxwxfn/my_article_on_semantic_text_segmentation_through/
"""Finding Friend and Foe in Multi-Agent Games"", Serrino et al 2019 {MIT} [deep CFR for near-human level _Avalon_ team play]",1559924796,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bxwlyb/finding_friend_and_foe_in_multiagent_games/
"""StyleNAS: An Empirical Study of Neural Architecture Search to Uncover Surprisingly Fast End-to-End Universal Style Transfer Networks"", An et al 2019",1559924734,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bxwlig/stylenas_an_empirical_study_of_neural/
Remembering states in RL,1559877965,"Let's just say I want to store all states visited by my RL agent in V (size N, let's say). Later the usecase would be to search V for the state closest to any given state X.

My questions:
1. What data structure do I go about using here. I would obviously want to search in less than O(N) because the #states generally go huge in most problems. Will something like LSH work? Given X I can find which bucket it belongs to and then compare only with states in that bucket.

2. How can I take a good sample of states in V instead of all visited states?

Any suggestions/links to paper would be really helpful.",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/bxpv3u/remembering_states_in_rl/
Issues with my SAC implementation,1559853669,"Hello guys,

I wanted to test my sac implementation on the RoboschoolHumanoidFlagrun-v1 environment. However the algorithm didn't succeed in learning a good policy that met my expectations nor solve the task. 

&amp;#x200B;

Here are my scalars:

[The weird jumping artifacts come from pausing and continuing training.](https://i.redd.it/jde841hors231.png)

&amp;#x200B;

And here is my implementation:

[https://github.com/Fable67/Soft-Actor-Critic-Pytorch](https://github.com/Fable67/Soft-Actor-Critic-Pytorch)

&amp;#x200B;

It would be so cool if someone got time to look over the implementation. However if you see any pattern in the graphs it'll would also be very helpful.

&amp;#x200B;

Thanks in advance!!!

&amp;#x200B;

Jonas Stepanik",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/bxlpfc/issues_with_my_sac_implementation/
[Amateur project] Looking for resources to understand how to build an optimized line follower bot.,1559839313,"I am trying to build an optimized line follower bot for a college project and I was hoping that I would be able to use reinforcement learning for it.

While ideally I would like to go through traditional literature for reinforcement learning, I won't be able to do that for this project.

So, I was hoping to that someone can direct me towards the relevant literature for this.

Things I already know/am decently good at:

College level general math 
Classical Statistical learning
Deep Learning
Markov decision processes (not in extreme detail)
Tools for deep learning: Pytorch, tensorflow, AWS etc.
Reinforcement learning (a very superficial overview)

What I am looking for:

Literature that might be relevant to a lone follower bot and allow a deep dive into reinforcement learning.

Ideas on how to build such a system

General advice




Thank you!",reinforcementlearning,harsh2803,False,/r/reinforcementlearning/comments/bxivyb/amateur_project_looking_for_resources_to/
Opinions on Layer Normalization,1559838957,"What are your thoughts on Layer Normalization as an alternative to Batch Normalization?

paper: [https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)
code: [https://github.com/CyberZHG/keras-layer-normalization](https://github.com/CyberZHG/keras-layer-normalization)",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bxitcl/opinions_on_layer_normalization/
Solutions for Sutton &amp; Barto's RL book,1559822996,"Could somebody share the solutions for Sutton &amp; Barto's RL book? I emailed my solutions to the authors and requested them to send me the solutions but I haven't heard back from them.

&amp;#x200B;

Thanks in advance! :D",reinforcementlearning,mind_juice,False,/r/reinforcementlearning/comments/bxfziq/solutions_for_sutton_bartos_rl_book/
How does centralized learning with decentralized execution help in multi-agent RL settings?,1559805596,"Of late, the paradigm taken by many multi-agent RL papers is to use a centralised critic while learning and use decentralised policies with only local observations  while execution. 

Basically, the central critic gets to see the whole set of observations, and make decisions during training, and the agents are on their own during execution. 

How does this help? 

In situations where cooperation is needed, 
Eg. Two robots cooperating to lift a table. During execution, won't the second agent need to know the state of the first agent so that it can position itself accordingly? 

If it's a task which can be executed without needing the states of other agents to make decisions, then why do you need a central critic to learn in the first place? Independent agents can learn their roles through exploration right? 

Can someone explain why this paradigm works with a simple example?",reinforcementlearning,shura04,False,/r/reinforcementlearning/comments/bxdt8h/how_does_centralized_learning_with_decentralized/
Simple Q Learning based simulations,1559799096,"The popular Grid world simulation.

 [https://prajwalsouza.github.io/Experiments/Q%20learning%20Grid%20World.html](https://prajwalsouza.github.io/Experiments/Q%20learning%20Grid%20World.html)

Obstacle avoidance with Q learning

 [https://prajwalsouza.github.io/Experiments/Q%20Ball.html](https://prajwalsouza.github.io/Experiments/Q%20Ball.html) 

&amp;#x200B;

&amp;#x200B;

[Grid World :  https:\/\/prajwalsouza.github.io\/Experiments\/Q&amp;#37;20learning&amp;#37;20Grid&amp;#37;20World.html](https://i.redd.it/zzvrr5at9o231.jpg)

&amp;#x200B;

&amp;#x200B;

[Obstacle Avoidance  https:\/\/prajwalsouza.github.io\/Experiments\/Q&amp;#37;20Ball.html ](https://i.redd.it/3fuho6zv9o231.jpg)",reinforcementlearning,prajwalsouza,False,/r/reinforcementlearning/comments/bxcysn/simple_q_learning_based_simulations/
Papers on Hierarchical Reinforcement Learning with macro actions,1559759486,"Does anyone know of any papers that look at doing hierarchical reinforcement learning with macro actions? 

&amp;#x200B;

The 2 I have found so far are:

* Strategic Attentive Writer for Learning Macro-Actions - [https://arxiv.org/abs/1606.04695](https://arxiv.org/abs/1606.04695)
* Learning to Repeat - [https://arxiv.org/pdf/1702.06054.pdf](https://arxiv.org/pdf/1702.06054.pdf)

&amp;#x200B;

Does anyone know of any others?",reinforcementlearning,__data_science__,False,/r/reinforcementlearning/comments/bx61vv/papers_on_hierarchical_reinforcement_learning/
Papers + Codebase with Linear FA in RL,1559757394,I am looking for recent papers/codebases which use linear function approximation for control either to estimate Critic/Value (V) in Policy Gradients or Q values preferably in an online setting.,reinforcementlearning,agent_rl,False,/r/reinforcementlearning/comments/bx5mg1/papers_codebase_with_linear_fa_in_rl/
Google AI ‘EfficientNets’ Improve CNN Scaling,1559757033,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/bx5jrf/google_ai_efficientnets_improve_cnn_scaling/
Looking for teammates for MineRL competition,1559753418,Hey guys...Is anyone interested in forming a team for MineRL competition on Aicrowd. Ease reach out to me if anyone is interested...,reinforcementlearning,prem_kumar27,False,/r/reinforcementlearning/comments/bx4tw3/looking_for_teammates_for_minerl_competition/
Looming for teammates for MineRL competition,1559752794,Hey guys...Is anyone interested in forming a team for mineRL competition. Please DM me.,reinforcementlearning,prem_kumar27,False,/r/reinforcementlearning/comments/bx4pfk/looming_for_teammates_for_minerl_competition/
What is the license of the Atari 2600 games?,1559739875,"I am writing an article about DQN, and I've been asked to clarify the license of all the external resources that I use: images, videos, etc.

I have included some images and videos of Pong and Space Invaders. However, it's not clear to me what license these games are under. 

Are they old enough to be public domain? I mean, I install them using `pip install gym[atari]` and bam, everything works, so it doesn't feel like I'm using illegal ROMs.

Also, I see the ROMs in this public OpenAI folder, but again no license information: https://github.com/openai/atari-py/tree/master/atari_py/atari_roms",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bx2bnv/what_is_the_license_of_the_atari_2600_games/
Can Reinforcement Learning be used for Diablo 2 or Final Fantasy X?,1559720447,"First of all, I want to say that I am not a programmer.  

But I would like to know if with the help of Open AI's Gym a program could be written, which my computer can use to learn to play Diablo 2 or Final Fantasy X? 

I myself can not program that, why I would look for Freelancer then. 

But before that, I would first have to determine the feasibility of this idea.",reinforcementlearning,ubslucky,False,/r/reinforcementlearning/comments/bwzy2h/can_reinforcement_learning_be_used_for_diablo_2/
Does anyone know of a Keras implementation of IQN?,1559705656,,reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bwy26o/does_anyone_know_of_a_keras_implementation_of_iqn/
Your Tesla is learning to drive by itself,1559674090,,reinforcementlearning,chip_0,False,/r/reinforcementlearning/comments/bwse12/your_tesla_is_learning_to_drive_by_itself/
Off-policy learning on random data or non-optimal policy data,1559667061,"My understanding is that off-policy RL learns from data which is not obtained from the policy that we are using.  The data from the policy is non-optimal and we're using it to find the optimal policy.

I was wondering have there been any studies (or intuition) on what type of data is best for off-policy learning.  Would 100% exploratory data (i.e., random) be better because it covers the entire state-space equally?  Or would data obtained from a non-optimal policy (i.e., exploited with a wrong policy) be better as the policy used is ""closer to optimal"" than a random policy?

I was thinking that there would be a trade-off.  For example, if the optimal policy samples from a region which we do not have the data for, due to collecting data with a wrong policy, then it would be hard to train for the optimal policy.  However, random will likely have some (and possibly more) data in the optimal policy region because it is not concentrated in a non-optimal region.",reinforcementlearning,milkteaoppa,False,/r/reinforcementlearning/comments/bwqzh1/offpolicy_learning_on_random_data_or_nonoptimal/
Looking for competitive environments,1559658355,"I would like to know about competitive environments that can be used for reinforcement learning, which are available online or easily implemented. I already know about OpenAI's competitive MuJoCo Environments which would actually fit my needs rather perfectly, but i want to train my agents on a grid, where i can not afford to get MuJoCo for.   
Instead of online available i would also be interested in ideas, what could be implemented easily.   


Best case would be somewhat simple to learn, but in an continuous action space, but both is not strictly necessary.",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/bwpb0z/looking_for_competitive_environments/
Looking for intuitive paper on inverse RL,1559656125,Has anyone read a paper on inverse RL that would be understandable to those outside the computer science community? I'm trying to put together a small list of key papers in inverse RL for someone in the cognitive sciences and need a resource that gives a friendly introduction to the topic. Any help is appreciated.,reinforcementlearning,AgentRL,False,/r/reinforcementlearning/comments/bwowna/looking_for_intuitive_paper_on_inverse_rl/
Discord Server for Intermediate RL,1559645599,"[https://discord.gg/9B8fRkK](https://discord.gg/9B8fRkK) Here! I've made a discord server  
Anyone is free to join. This is a followup to  [https://www.reddit.com/r/reinforcementlearning/comments/bvwjyz/looking\_for\_a\_mentor\_or\_studymate/](https://www.reddit.com/r/reinforcementlearning/comments/bvwjyz/looking_for_a_mentor_or_studymate/)    
An overwhelming response... Thats why",reinforcementlearning,Syzygianinfern0,False,/r/reinforcementlearning/comments/bwnbvu/discord_server_for_intermediate_rl/
Inverse Reinforcement Learning in Contextual MDPs,1559636947,https://arxiv.org/pdf/1905.09710.pdf,reinforcementlearning,TomZahavy,False,/r/reinforcementlearning/comments/bwmc47/inverse_reinforcement_learning_in_contextual_mdps/
Using reinforcement learning to trade Bitcoin for massive profit,1559635962,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/bwm8fy/using_reinforcement_learning_to_trade_bitcoin_for/
Why isn’t the terminal Boolean used in Q-learning?,1559633049,"I see a lot of Q-learning implementations which disregard the `terminal` Boolean, eg:

`new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)`


Why is that? Shouldn’t we ignore the Q-value of the terminal state, as we do in DQN?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bwlx1f/why_isnt_the_terminal_boolean_used_in_qlearning/
Provably efficient reinforcement learning with rich observations,1559627964,,reinforcementlearning,violentdeli8,False,/r/reinforcementlearning/comments/bwlafj/provably_efficient_reinforcement_learning_with/
[D] What does this mean? in Chapter 11.2 of An Intro to Reinforcement Learning,1559622870,"The book 'Reinforcement Learning : An introduction' second edition by Sutton and Barto, mentions the following about off policy divergence problem.  
(you can see that http://incompleteideas.net/book/bookdraft2017nov5.pdf)
"" Key to this example is that the one transition occurs repeatedly without $w$ being updated on other transitions. This is possible under off-policy training because the behavior policy might select actions on those other transitions which the target policy never would. For these transitions, $\lo_t$ would be zero and no update would be made. Under on-policy training, however, $\lo_t$ is always one. Each time there is a transition from the $w$ state to the $2w$ state, increasing $w$, there would also have to be a transition out of the $2w$ state. That transition would reduce $w$, unless it were to a state whose value was higher (because $\gamma \le 1$) than $2w$, and then that state would have to be followed by a state of even higher value, or else again $w$ would be reduced. ""
I'm not absolutely convinced by this part. So, I have few questions about this part. 
(1) Is the sentence ""the one transition occurs repeatedly without $w$ being updated on other transitions."" means that there is only one transition and the only one transition update $w$?

(2) In the sentence ""This is possible under off-policy training because the behavior policy might select actions on those other transitions which the target policy never would."", this means the behavior policy can make transition from $w$ state to $2w$ state even if the the target policy can't make the transition (w -&gt; 2w).

(3) In ""Each time there is a transition from the $w$ state to the $2w$ state, increasing $w$, there would also have to be a transition out of the $2w$ state."", what is the meaning of ""there would also have to be a transition out of the $2w$ state.""?

Could you please explain this ambiguous part to me with simple examples?",reinforcementlearning,170928,False,/r/reinforcementlearning/comments/bwkm29/d_what_does_this_mean_in_chapter_112_of_an_intro/
UC Berkeley’s RL-Powered SOLAR Accelerates Robotic Learning,1559615101,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/bwjesa/uc_berkeleys_rlpowered_solar_accelerates_robotic/
[R] [1905.12204] Scalable and transferable learning of algorithms via graph embedding for multi-robot reward collection,1559610932, [https://arxiv.org/abs/1905.12204](https://arxiv.org/abs/1905.12204),reinforcementlearning,hywkkang,False,/r/reinforcementlearning/comments/bwipk3/r_190512204_scalable_and_transferable_learning_of/
Minimal PyTorch DQN Implementation for Research,1559608919,[https://github.com/econti/minimal\_dqn](https://github.com/econti/minimal_dqn),reinforcementlearning,25825085802358,False,/r/reinforcementlearning/comments/bwida8/minimal_pytorch_dqn_implementation_for_research/
How AI Learns to Play Games – Reinforcement Learning for Fun,1559581170,,reinforcementlearning,rodolfo-mendes,False,/r/reinforcementlearning/comments/bwd0wk/how_ai_learns_to_play_games_reinforcement/
Hierarchical Actor Critic Implementation in PyTorch for Mountain Car env,1559579231,,reinforcementlearning,IIstarmanII,False,/r/reinforcementlearning/comments/bwcn2d/hierarchical_actor_critic_implementation_in/
Using Adam Optimizer in PPO and similar off-policy optimization procedures,1559565734,"I have a question regarding the usage of the Adam optimizer in off-policy optimization of policy gradients like off-policy Vanilla PO, PPO, A2C, etc, primarily i'm interested in PPO, but i suppose this should hold for all of them:

&amp;#x200B;

Pseudocode is roughly:   


    for iterations: 
        generate episodes 
        (if actor critic enrich episodes with value estimates)
        for optimization_epochs: 
            for minibatch sampled from episodes: 
                optimize policy
                (if actor critic: optimize critic)

admittedly using several optimization epochs on the same sampled data seems to be only semi-justified, but is one of the main motivations for PPO:   


The issue with the Adam is the following: Contrary to Prediction/Regression problems, the target is not stationary anymore (at least in the case of actor-critics). Therefor i am not sure, whether keeping an Adam from iteration to iteration is actually viable (because the basic assumptions for it assume a stationary target if i understand it correctly).  So i see two options here:   
(1)

    INITIALIZE ADAM
    for iterations: 
        generate episodes 
        (if actor critic enrich episodes with value estimates)
        for optimization_epochs: 
            for minibatch sampled from episodes: 
                optimize policy (with ADAM)
                (if actor critic: optimize critic)

(2)

    for iterations: 
        (RE-) INITIALIZE ADAM
        generate episodes 
        (if actor critic enrich episodes with value estimates)
        for optimization_epochs: 
            for minibatch sampled from episodes: 
                optimize policy (with ADAM)
                (if actor critic: optimize critic)

So the question is: Should for each iteration a new Adam be used, or do you reuse the old Adam even though you have moving targets? Should (1) or (2) be prefered?",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/bwa840/using_adam_optimizer_in_ppo_and_similar_offpolicy/
How AI Learns to Play Games,1559559026,"A brief introduction to Reinforcement Learning:  


[https://reinforcementlearning4.fun/2019/06/03/how-ai-learns-play-games/](https://reinforcementlearning4.fun/2019/06/03/how-ai-learns-play-games/)",reinforcementlearning,rodolfo-mendes,False,/r/reinforcementlearning/comments/bw9a2t/how_ai_learns_to_play_games/
So techniques like MAML or Reptile meta-learning learns a good initialization?,1559518864,"Recently got into few shot learning, and meta-learning

&amp;#x200B;

MAML:

[https://arxiv.org/pdf/1703.03400.pdf](https://arxiv.org/pdf/1703.03400.pdf)

Reptile:

[https://d4mucfpksywv.cloudfront.net/research-covers/reptile/reptile\_update.pdf](https://d4mucfpksywv.cloudfront.net/research-covers/reptile/reptile_update.pdf)

&amp;#x200B;

First  of all, the whole point of those meta-learning is to, at the end of the  day, find a good initialization of the NN's parameters such that when  you use those parameters to train on a specific task, it could learn  fast?

&amp;#x200B;

Secondly,  what specifically do they mean by ""X shot, Y way"" ? For image  classification, there would be total of Y classes, and X examples for  each Y class, thus a total of XY pictures in a single task. But under  reinforcement learning setting, what does it mean to sample a task?  Could a set of tasks be defined as, for example, {space invaders, Go,  chess...} ? (in MAML, once you sample a task, you are supposed to run K  trajectories). The problem could be, what if each task has different  input size? For image classification, you could only select fixed sized  pictures, but for RL, the input size could vary.

&amp;#x200B;

Thirdly,  how does Reptile work? It only uses first order optimization, and thus  I'd assume it'd be faster to meta-learn. But I'm still not understanding  the intuitive reason why Reptile could perform just as well as MAML.",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/bw3wx1/so_techniques_like_maml_or_reptile_metalearning/
CS 294-112. Deep Reinforcement Learning by Sergey Levine. UC Berkeley. Fall 2018,1559512567,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/bw2tr7/cs_294112_deep_reinforcement_learning_by_sergey/
Help regarding StarCraft II and REINFORCE,1559494439,"Hi everyone,

I'm very much a total beginner in the field of reinforcement learning so this might seem like a silly issue.

I'm trying to implement a reinforcement learning agent for StarCraft II. And I know that the state-of-the-art approach is A2C (or relational learning, according to latest Deepmind paper) but I wanted to try and use a regular REINFORCE algorithm.  I didn't expect it to match the level of Deepmind's A3C, or any other implementation I found but I hoped it would at least manage to play the MoveToBeacon map. 

&amp;#x200B;

I've had it running for over 18 hours and it doesn't show any sign of improvement.

&amp;#x200B;

So the question I want to ask is this. Is there reason to believe that a REINFORCE based agent will simply not be able to converge on an optimal strategy for the game?",reinforcementlearning,mrDzejkop,False,/r/reinforcementlearning/comments/bvzg58/help_regarding_starcraft_ii_and_reinforce/
Looking for a Mentor or StudyMate,1559476057,"Hello world! I'm an amateur at RL. I'm currently exploring the fields related to Q Learning like the tables, DQN, DDQN, Double DQN, PER, and I will be moving into stuff like Policy Gradients and Actor Critic. I use Tensor Flow (with the high level keras APIs).   
If you are a person on the same level as me or wish to just be a mentor/friend please tag along!!",reinforcementlearning,Syzygianinfern0,False,/r/reinforcementlearning/comments/bvwjyz/looking_for_a_mentor_or_studymate/
Deep RL Papers,1559467429,"Where can I get a curated list of all the papers discussed in cs294-112, deep rl course at UCB?",reinforcementlearning,namuchan95,False,/r/reinforcementlearning/comments/bvvn72/deep_rl_papers/
[arXiv] Distributional Policy Optimization: An Alternative Approach for Continuous Control,1559463692,[https://arxiv.org/abs/1905.09855](https://arxiv.org/abs/1905.09855) (code linked in the PDF).,reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/bvva26/arxiv_distributional_policy_optimization_an/
Adversarial Policies: Attacking Deep Reinforcement Learning {BAIR},1559440905,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/bvshil/adversarial_policies_attacking_deep_reinforcement/
Is anyone interested in forming up a team for NIPS mineRL competition?,1559426271,,reinforcementlearning,kirarpit,False,/r/reinforcementlearning/comments/bvq4gq/is_anyone_interested_in_forming_up_a_team_for/
What tool(s) should I use for saving/loading experience tuples when they become too large to fit in memory?,1559404770,"Hi I'm learning RL and trying to apply  algorithms with replay memories to problems where the observations are raw images.

a replay memory of the size/image resolution I want won't fit in memory, and I'm looking for an efficient way to pipeline it to disk, but hide the cost of reads, writes and copies to gpu while keeping the GPU busy.

Any frameworks/examples you could recommend for doing this?",reinforcementlearning,_llucid_,False,/r/reinforcementlearning/comments/bvmaj6/what_tools_should_i_use_for_savingloading/
Difference between DPPO and Parallelized Sampling in PPO?,1559331757,"[This](https://arxiv.org/pdf/1707.02286.pdf) DeepMind paper describes an algorithm called ""Distributed Proximal Policy Optimization"" that is essentially PPO in a distributed setting. Using multiple workers and a parameter server, a shared model is updated every time a certain number of workers are ready to send their updates. In the parallelized implementations of PPO I've seen, the parallelism is in the form of sampling from multiple environments at once to explore more of the state-action space in the same time.

&amp;#x200B;

Is ""Distributed Proximal Policy Optimization"" as described in the DeepMind paper really any different from sampling from multiple environments at once?",reinforcementlearning,GirlImJustABird,False,/r/reinforcementlearning/comments/bvbyzg/difference_between_dppo_and_parallelized_sampling/
Has anyone applied few shot learning for RL?,1559314399,"Few shot learning has seen a tremendous success in image classification. If there had to be in the order of 1000 pictures to be able to ""generalize"" pretty well, with few shot learning, it could do so in the order of 10 pictures. 

&amp;#x200B;

Specifically, the meta-learning techniques like MAML or even better improved, Reptile, has shown to be successful in other machine learning tasks, it'd be naturally to combine Reptile with, say, DQN. 

&amp;#x200B;

In fact, the authors of MAML directly suggest it should be applied to RL, and yet i haven't really seen any papers that shows MAML or Reptile is a great technique for DQN or DDPG...etc 

&amp;#x200B;

Has anyone tried it for RL? It is a common problem in RL, especially for model free RL, to require a ton of sample of data (a ton of sample trajectories), and so I'd assume Reptile could help, and could even make it more stable",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/bv8hqv/has_anyone_applied_few_shot_learning_for_rl/
"""Human-level performance in 3D multiplayer games with population-based reinforcement learning"", Jaderberg et al 2019 {DM] [update of Jaderberg et al 2018]",1559241201,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/buwqwy/humanlevel_performance_in_3d_multiplayer_games/
A question for any players of the game Civilizations... or any strategy game,1559238507,,reinforcementlearning,shakakaZululu,False,/r/reinforcementlearning/comments/buw7fv/a_question_for_any_players_of_the_game/
Promote long-term action variability whilst maintaining short-term similarity,1559217225,"Consider an MDP with discrete actions between 0 and 1. Say one wants to promote subsequent period similar actions, for instance, for every \[a\_t, a\_t+1, a\_t+2\] should be fairly similar, but also promoting long-term episodic variability, that is, the model should be fairly stable in short term periods, but within the episodes display a certain degree of volatility. How can one achieve this? And not less important, how to incorporate it in an MDP? We can't simply store actions in the MDP, it would violate memorylessness and create an impossibly large MDP.",reinforcementlearning,Unless13,False,/r/reinforcementlearning/comments/bus7yx/promote_longterm_action_variability_whilst/
Top 5 Free Courses in Reinforcement Learning,1559182368,,reinforcementlearning,rodolfo-mendes,False,/r/reinforcementlearning/comments/bunm7m/top_5_free_courses_in_reinforcement_learning/
"""EfficientNet: Improving Accuracy and Efficiency through AutoML and Model Scaling"", Tan &amp; Le 2019 {GB}",1559165444,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bukidf/efficientnet_improving_accuracy_and_efficiency/
What is Q-learning Automated testing?,1559157852,How to apply q-learning in automated testing of android applications?,reinforcementlearning,codexblaze,False,/r/reinforcementlearning/comments/buiyfj/what_is_qlearning_automated_testing/
"""Graph Representations for Higher-Order Logic and Theorem Proving"", Paliwal et al 2019 {G}[Representing Theorems as Graphs Improves State-of-the-Art by 50% in Deep Automated Theorem Proving for HOList]",1559157802,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/buiy4r/graph_representations_for_higherorder_logic_and/
"VICE-RAQ: ""End-to-End Deep Reinforcement Learning without Reward Engineering"" {BAIR} [GAIL-like robot arm training]",1559141731,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bufnyw/viceraq_endtoend_deep_reinforcement_learning/
"COBRA: Data-Efficient Model-Based RL through Unsupervised Object Discovery and Curiosity-Driven Exploration, Watters et al 2019 {DM}",1559140820,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/bufhlm/cobra_dataefficient_modelbased_rl_through/
Reward Based Epsilon Decay,1559129671,"[The Post .](https://aakash94.github.io/Reward-Based-Epsilon-Decay/)


So I recently tried out openai's cart pole problem with some tweaks that my friends found interesting. 


I also wanted to try out GitHub Pages so thought that I should make a post on it. 



Nothing too fancy here. 
Suggestions, ideas, criticisms are welcome.",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/budite/reward_based_epsilon_decay/
Reward Based Epsilon Decay https://aakash94.github.io/Reward-Based-Epsilon-Decay/,1559128968,"So I recently tried out openai's cart pole problem with some tweaks that my friends found interesting.  
I also wanted to try out GitHub Pages so thought that I should make it a  [post](https://aakash94.github.io/Reward-Based-Epsilon-Decay/).  
Nothing too fancy here.   
Suggestions, ideas, criticisms are welcome.",reinforcementlearning,jhakash,False,/r/reinforcementlearning/comments/buderb/reward_based_epsilon_decay/
Vmin Vmax in C51-dqn (A Distributional Perspective on Reinforcement Learning),1559128008,"How to determine Vmin Vmax values when using c51 in other domains then atari?

I thought it should have something to do with the minimum or maximum total reward that can be achieved per game, but in the article they used -10,10 and in sonic retro winning rainbow used -200,200 (and total reward was 4000+)\\

&amp;#x200B;

Any thoughts about other than trying values out?",reinforcementlearning,What_Did_It_Cost_E_T,False,/r/reinforcementlearning/comments/bud9jx/vmin_vmax_in_c51dqn_a_distributional_perspective/
How does the Dyna Q algorithm works?,1559119278,"I'm having a hard time trying to understand how the dyna Q algorithm works. I put the picture which helps me to understand. My questions are:

* What planning really means? (it's the (f) in this picture)
* What the n represents?
* Why a term Model(S,A) is used?

Thanks a lot for your help",reinforcementlearning,Nolwww,False,/r/reinforcementlearning/comments/buc5bq/how_does_the_dyna_q_algorithm_works/
What is the difference between real experience and simulated experience in reinforcement learning?,1559118308,"Hello, 

&amp;#x200B;

In different RL books, a question that often occurs is  if our function updates must be based on real experience or simulated experience? (and if both, how much of each?). This brings me to the following question:

What is the difference between real experience and simulated experience?

&amp;#x200B;

Thanks a lot for your help !!",reinforcementlearning,Nolwww,False,/r/reinforcementlearning/comments/buc1kk/what_is_the_difference_between_real_experience/
Paper suggestions for safety in RL,1559113055,"By safety, I am talking about systems/models where due to some disturbance an agent can land up outside of safe space, and then the agent can somehow go back inside the safe space. Any papers in this direction would be helpful.

Also anything involving exploration envelope similar to Mann and Choe's AAAI paper would be helpful: https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/view/3722

Thanks",reinforcementlearning,Nas1729,False,/r/reinforcementlearning/comments/bubfek/paper_suggestions_for_safety_in_rl/
GitHub - utilForever/RosettaStone: Hearthstone simulator using C++ with some reinforcement learning,1559112631,,reinforcementlearning,utilForever,False,/r/reinforcementlearning/comments/bubdiv/github_utilforeverrosettastone_hearthstone/
"""DeepHOL-Zero: Learning to Reason in Large Theories without Imitation"", Bansal et al 2019",1559096141,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bu8yjq/deepholzero_learning_to_reason_in_large_theories/
[P] Playing SuperMario Bros. without knowing any scores using flow-based curiosity method,1559051472," Hi Reinforcementlearning

&amp;#x200B;

I'll introduce a new method to play SuperMario Bros. using RL agent and without knowing the scores from the environment (pure exploration). We employ optical flow for evaluating the novelty of states to guide the RL agent.

I hope you find it useful.

&amp;#x200B;

Here are some links:

\- Demo video: [https://www.youtube.com/watch?v=w-a6akKpWT0](https://www.youtube.com/watch?v=w-a6akKpWT0)

\- Github: [https://github.com/hellochick/MarioO\_O-flow-curioisty](https://github.com/hellochick/MarioO_O-flow-curioisty)

\- Arxiv: [https://arxiv.org/abs/1905.10071](https://arxiv.org/abs/1905.10071)",reinforcementlearning,Kanahei,False,/r/reinforcementlearning/comments/bu09sc/p_playing_supermario_bros_without_knowing_any/
Help with Dueling DQN,1559050312,"I have been trying to implement DDQN with Atari environment. I am using tf.keras for the network. How do I split the sequential model into advantage and value streams?

This is my model -  [https://del.dog/zayubegiwa.py](https://del.dog/zayubegiwa.py) 

This is the summary -  [https://del.dog/alehotegic.md](https://del.dog/alehotegic.md)",reinforcementlearning,Syzygianinfern0,False,/r/reinforcementlearning/comments/bu02ej/help_with_dueling_dqn/
Loss function has huge spikes (FrozenLake from OpenAI + Deep-Q-learning),1559031885,"[Loss function](https://imgur.com/a/p7vr0Ox)

I am trying to solve a [4x4 FrozenLake](https://gym.openai.com/envs/FrozenLake-v0/) with no slippage (so completely deterministic - just navigating a 4x4 grid). I am trying to use [Deep-Q-learning with experience replay](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf). I know this is a huge overkill, but I just want to understand the basics.

However, when I train my network (which is just two small ReLu hidden layers + linear output layer) I get large spikes in my loss function. The game gives +1 reward if the finish is reached and 0 otherwise, and I worked it out that the spikes happen when the +1 reward is achieved.

Clearly, when the agent gets +1 reward it tells a lot of information to the neural network and I kind of see why the loss function spikes.

My question: is there a non-ad-hoc way to fix this, or is deterministic navigation on a grid just too simple of a problem to use DQN here?",reinforcementlearning,Yajirobe404,False,/r/reinforcementlearning/comments/btxgq8/loss_function_has_huge_spikes_frozenlake_from/
Any thoughts on Recurrent Value Functions?,1559013124,"Just can't across this one (https://arxiv.org/abs/1905.09562) on Twitter. This one proposed alternative to value functions if I understand correctly. Seemed interesting from the first read. I want to try structured exploration using this technique. Anyone else interested in this one? Also, domain experts, what are your thoughts on this one?",reinforcementlearning,agent_rl,False,/r/reinforcementlearning/comments/btuzd7/any_thoughts_on_recurrent_value_functions/
"RL Weekly 19: Curious Object-Based Search Agent, Multiplicative Compositional Policies, and AutoRL",1558948879,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/btjxbr/rl_weekly_19_curious_objectbased_search_agent/
Any information on building and deploying RL models/agents in languages such as C++?,1558889850,"I'm currently taking an RL course over the summer, and I'm enjoying it, especially after taking ML in the Spring. As one might expect, we usually use Python/OpenAI/TensorFlow and etc...

&amp;#x200B;

While these are great for an educational setting and even some production settings, I don't see how these APIs can be implemented for something like a hardware robot. Especially one that has its own multi-threaded processes as well as running on some Unix/QNX like environment. Sure you can still get Python to talk to the C++ controls software and use the Python API to inject feedback into the controllers, but are there other options? 

&amp;#x200B;

If you have any resources or articles, I'd greatly appreciate them! 

&amp;#x200B;

Thanks in advance!",reinforcementlearning,atomoclast,False,/r/reinforcementlearning/comments/bta8k1/any_information_on_building_and_deploying_rl/
"[P] ""minimalRL"": short PyTorch teaching implementations of REINFORCE/TD Actor-Critic/DQN/PPO/DDPG/A3C for CartPole",1558889782,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bta81q/p_minimalrl_short_pytorch_teaching/
[P] Implementations of basic RL algorithms with minimal codes!,1558882986,"Hi,

I recently implemented basic RL algorithms such as

REINFORCE, vanilla actor-critic, DDPG, A3C, DQN and PPO with PyTorch.

&amp;#x200B;

Characteristics are as follows :

* Each algorithm is complete within a single file.
* Length of each algorithm is up to 100\~150 lines of codes.
* Every algorithm can be trained within 30 seconds, even without GPU.
* Envs are fixed to ""CartPole-v1"". You can just focus on the implementations.

&amp;#x200B;

As you can see in the name of the repository,

I tried to make the code as brief and intuitive as possible.

&amp;#x200B;

Hope you enjoy :)

Thank you.

&amp;#x200B;

[https://github.com/seungeunrho/minimalRL](https://github.com/seungeunrho/minimalRL)",reinforcementlearning,seungeun07,False,/r/reinforcementlearning/comments/bt8wl8/p_implementations_of_basic_rl_algorithms_with/
Help regarding Implementation of PPO - Value Loss seemingly not converging,1558843032,"I'm working on an implementation of PPO, which i plan to use in my (Bachelors) Thesis. To test whether my implementation works, i want to use the LunarLanderContinuous-v2 Environment. Now my implementation seems to work just fine, but plateues much too early - At an average reward of \~ -1.8 reward per timestep, where the goal should be somewhere around \~ +2,5 reward per timestep. As the implementation generally learns i am somewhat confused, as to why it then pleateus so early.   
Some details regarding my implementation, also here is the [github repo](https://github.com/LJKS/ppo/blob/master/ppo.py):

 \- I use parallelized environments via openai's subproc\_vecenv  
 \- I use the Actor Critic Version of PPO  
 \- I use Generalized Advantage Estimation as my Advantage term  
 \- I only use finished runs (every run used in training has reached a terminal state)  
 \- Even though Critic loss in the graphic below looks small it is actually rather large, as the rewards are normalized and therefor the value targets are actually rather small

 \- The Critic seemingly predicts a value independent of the state it is fed  - that is it predicts for every state just the average over all the values. That seems like harsh underfitting, which is weird as the network is already rather large for the problem in my opinion. But this seems to be the most likely cause for the problem in my opinion.   


[Progress over time of the algorithm](https://i.redd.it/z12t2k5nah031.png)

Hope a question to my specific implementation is ok - i've seen others before, but don't want to annoy people with my problems too much. Can't offer much as a thank you for anyone who helps, but you surely would get into the acknowledgement of my thesis &lt;3 If there are any questions open please ask away, i will try to answer as soon and thoroughly as possible.",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/bt3m0d/help_regarding_implementation_of_ppo_value_loss/
Offline training RL Model with Pre-Existing Data,1558827843,"It seems like most RL implementations do ""online"" training.  That is, the data is collected from the environment while the policy is being optimized.  (At time step t, RL model takes action a\_t which results in state s\_t+1 and reward r\_t and the policy is updated accordingly.  Then next time step t+1 occurs.) 

As a result, the sequential data will be highly dependent on the policy (and how it's being trained).

My understanding is that this naturally results in having to re-collect data every time you train the RL model.  E.g., the hyper parameters change and the policy now acts differently.  Therefore, sequential data would become different.

I'd like to ask whether there are any common techniques for ""offline"" training with pre-existing sequential data, which already exist and was not collected independent of the RL model.  Being able to train with pre-existing data would be much more sample efficient than recollecting data every time the hyper parameters change.

Please correct me if I have any misconceptions.  Thanks!",reinforcementlearning,milkteaoppa,False,/r/reinforcementlearning/comments/bt1933/offline_training_rl_model_with_preexisting_data/
Is Gym default code using an algorithm to learn how to perform a task?,1558827553,The default starter code provided in the openAI homepage was able to solve the MountainCar problem relatively quickly. I was wondering if there is some RL algorithm behind the starter code or if it was just taking random actions.,reinforcementlearning,FireStory,False,/r/reinforcementlearning/comments/bt17gs/is_gym_default_code_using_an_algorithm_to_learn/
Model Based Vs Model Free doubt,1558813175,"Hi guys! I'm new to this subreddit and just started learning RL (Emma Brunskill Stanford lectures). In the first lecture, she explained Model free vs Model Based RL, which I couldn't understand AT ALL tbh. I'm not able to imagine how can an agent work without a model. Can anyone help me out by giving some easy/layman examples to illustrate the difference? 

Also, can I post more doubts here, if it's not against the rules? If it is not, how often should I bother you guys? Or alternatively, can we have a weekly thread for discussion on RL? 
Sorry if I broke any rules.",reinforcementlearning,The_One_Nerd,False,/r/reinforcementlearning/comments/bsyloo/model_based_vs_model_free_doubt/
Reward Function Design,1558730650,"I'm working on a project on autonomous navigation and I'm not sure about validity of my reward function. 

It should reach target as fast as possible, so I consider giving a negative reward for each time step. The episode ends if it collides, so I think I should give high negative reward for collision. Otherwise it may prefer colliding rather than trying to reach target. So a high reward for reaching the target can also be added.

Reward function that I considered:

If reached target: + Big X (Episode ends)

If collides: - Big Y (Episode ends)

Else: - distance to target

&amp;#x200B;

Do you have any advice? Similiar RL problems or related papers might be helpful.",reinforcementlearning,cptrs,False,/r/reinforcementlearning/comments/bsm0fv/reward_function_design/
Example of RL agent,1558730026,My name is Adnan Makda. I am from a non-programming background. I am currently doing my bachelors in architecture design. I am doing a thesis wherein I want to use reinforcement learning algorithms. I having trouble in making and RL agent. can someone suggest some good examples of RL which I can modify a bit and use.,reinforcementlearning,theadnanmakda,False,/r/reinforcementlearning/comments/bslvvg/example_of_rl_agent/
My name is Adnan Makda. I am from a non-programming background. I am currently doing my bachelors in architecture design. I am doing a thesis wherein I want to use reinforcement learning algorithms. I having trouble in making and RL agent. can someone suggest some good examples of RL which I can mod,1558729414,,reinforcementlearning,theadnanmakda,False,/r/reinforcementlearning/comments/bslrfw/my_name_is_adnan_makda_i_am_from_a_nonprogramming/
Paper Submissions Break NeurIPS 2019 Paper Submission System,1558720891,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/bsk3a8/paper_submissions_break_neurips_2019_paper/
"""Exploring a Pixel-Maze with Evolution Strategies (CMA-ES)""",1558708219,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bshm0x/exploring_a_pixelmaze_with_evolution_strategies/
How do you decide NN model parameters and/or optimize them?,1558695003,"Hi,

&amp;#x200B;

So I'm trying to get started with keras-rl, and I'm using OpenAI to test run the different algorithms there. But when it comes to the examples provided, there's always a ""Next, we build a simple model"" and they build an NN model. 

&amp;#x200B;

My question is how do we determine the number of hidden layers and hidden layer nodes here? I've used a little Keras for machine learning, but there I could do something like Bayesian optimization using skopt with the train and test data. Over here, I'm just confused.

&amp;#x200B;

If there's another approach to understand all this better, please do let me know.

&amp;#x200B;

keras-rl github for reference: [https://github.com/keras-rl/keras-rl](https://github.com/keras-rl/keras-rl)

&amp;#x200B;

Thanks.",reinforcementlearning,iWishForMoreTea,False,/r/reinforcementlearning/comments/bsff5z/how_do_you_decide_nn_model_parameters_andor/
Time series analysis using RL,1558691421,"I've been reading this [paper](https://www.intechopen.com/online-first/training-deep-neural-networks-with-reinforcement-learning-for-time-series-forecasting). This paper deals with forecasting the next point in time series using reinforcement learning, it uses Deep Belief networks trained with stochastic gradient ascent. I've able to grasp most of the paper. but I am not able to understand the  PI function(eq 16) and step 2 of the SGA algorithm. it says **Predict a future data y\_t=x\_t+1 according to a probability y\_t∼π(x\_t,w) with ANN models which are constructed by parameters W.** 

and the step 4 of the same algorithm where they compute the characteristic eligibility e\_i(t) which is the partial derivative of the log of PI function parameterized by x\_t and w with respect to w\_i.

&amp;#x200B;

can anyone please explain what the author is trying to say here? or point to some implementation of code similar to this approach.",reinforcementlearning,dangling_pntr,False,/r/reinforcementlearning/comments/bsex9s/time_series_analysis_using_rl/
RL ppo alrorithm: understanding value loss and entropy plots,1558685280,"Hey I'm working on a visual tracking program based on RL. And I think there maybe something wrong with my training process. I asked on [datascience.stackexchange.com](https://datascience.stackexchange.com) but lately found r/reinforcementlearning here maybe a better place to ask.

&amp;#x200B;

My question on [datascience.stackexchange.com](https://datascience.stackexchange.com): [my question](https://datascience.stackexchange.com/questions/51656/rl-ppo-alrorithm-understanding-value-loss-and-entropy-plot).

&amp;#x200B;

So in short, I wonder why in PPO algorithm, as said in [this post](https://medium.com/aureliantactics/understanding-ppo-plots-in-tensorboard-cbc3199b9ba2),  the value loss should increase first and then decrease. 

Also I think the entropy should increase from the view of the expression of the total loss, while should decrease considering the reward converges, is it a balance that entropy turns out decreasing  in the plots of the above post?

Finally, my entropy decreases very slowly, to which side should I adjust beta?

&amp;#x200B;

If someone could ask the above questions with loss funcions explained, I would probably know whether my training process is right. 

&amp;#x200B;

My newest training plots :

https://i.redd.it/ebqnh5ys94031.png

https://i.redd.it/1ikzy3ys94031.png

https://i.redd.it/15zmu6xs94031.png",reinforcementlearning,lincer_sa,False,/r/reinforcementlearning/comments/bse7l5/rl_ppo_alrorithm_understanding_value_loss_and/
Samsung AI Makes the Mona Lisa ‘Speak’,1558650354,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/bs96cq/samsung_ai_makes_the_mona_lisa_speak/
"""Meta-learners' learning dynamics are unlike learners'"", Rabinowitz 2019 {DM}",1558633066,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bs5vii/metalearners_learning_dynamics_are_unlike/
How do I get started with Reinforcement Learning?,1558628732,"Hi. I am a Computer Science student and am currently powering through Andrew Ng's ML course. I am interested in learning about Reinforcement Learning.

I am not just interested in learning about the current libraries and modules available for implementing RL, I am interested in learning about the underlying algorithms and mathematics as well, from a further research point of view.

I would also need some resources for the prerequisite topics in ML, Maths, Data Analytics and Programming.

My current background:

1 Sem of Programming in C and Python

1 Sem of Data Structures and Algorithms

1 Sem of Single-Variable Calculus

1 Sem of Linear Algebra

Upcoming: 1 sem of Probability and Statistics, 1 sem of Discrete Maths

How should I go about learning and practising RL?

Thanks.",reinforcementlearning,pakodanomics,False,/r/reinforcementlearning/comments/bs50hm/how_do_i_get_started_with_reinforcement_learning/
"ACM Announces Best Doctoral Paper, Learning to Learn with Gradients",1558626824,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/bs4mjn/acm_announces_best_doctoral_paper_learning_to/
"[R] [1902.02542] Predict Globally, Correct Locally: Parallel-in-Time Optimal Control of Neural Networks",1558619039,,reinforcementlearning,PeterPrinciplePro,False,/r/reinforcementlearning/comments/bs346t/r_190202542_predict_globally_correct_locally/
Multi Dimensional Continuous Action Space,1558612029,"Hey guys,  


I've have an environment taking multiple continuous variables for an action to move an object to it's new coordinates X, Y and with it's new rotation R. What I am currently struggling with is in how to represent a probability distribution for taking this action for my neural network. My first approach was learning a vector of 3 means and 3 stds but with this I am not sure in how to calculate log probabilities for the loss, can I just add them together? Sorry for the question, I am rather new to the RL world.",reinforcementlearning,salah3,False,/r/reinforcementlearning/comments/bs1xw8/multi_dimensional_continuous_action_space/
Need help with exercise 12 (Racetrack) from chapter 5 (Monte Carlo Methods) from the 2nd Edition of the Reinforcement Learning book by Sutton &amp; Barto,1558601632,"Hello, people! :)

A month ago, I started learning Reinforcement Learning by reading ""Reinforcement Learning: An Introduction"" (2nd Edition) by Sutton &amp; Barto and by watching David Silver's lectures. At the same time, I am trying to solve the exercises from the book (everything went well so far), but I am stuck at exercise 12 (Racetrack) from chapter 5 (Monte Carlo Methods). My code implementation is (I assume) correct, but I think that I am missing something about the values of the parameters $\alpha$, \$epsilon$, $\gamma$ and total number of episodes. At the moment, it takes me about 100,000 steps to finish a single episode, which makes the whole process impractical... If you have already solved this exercise, can you please share with me the values of these parameters, or anything else that I should take care of or be careful about? 

Thanks in advance,
Jugoslav
P.S. If you need any implementation details or the code, I am willing to provide them.",reinforcementlearning,jStojcheski_,False,/r/reinforcementlearning/comments/bs0jt3/need_help_with_exercise_12_racetrack_from_chapter/
"""Curiosity Killed the Mario"": implementing intrinsic curiosity for NES Mario",1558578131,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/brxn92/curiosity_killed_the_mario_implementing_intrinsic/
[N] REPLAB: A Reproducible Low-Cost Arm Benchmark for Robotic Learning,1558567747,,reinforcementlearning,counterfeit25,False,/r/reinforcementlearning/comments/brvv7z/n_replab_a_reproducible_lowcost_arm_benchmark_for/
"[Project] Massively parallel, vectorised implementation of Snake and RL solution",1558562838,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bruym7/project_massively_parallel_vectorised/
NeurIPS 2019 Will Host Minecraft Reinforcement Learning Competition,1558556146,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/brtmq5/neurips_2019_will_host_minecraft_reinforcement/
"""Software-defined far memory in warehouse scale computers"", Lagar-Cavilla et al 2019 [Gaussian processes for deciding what RAM to compress to free up space]",1558551735,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/brspaa/softwaredefined_far_memory_in_warehouse_scale/
Code review?,1558545710,"Hello!

Are people here open to reviewing some code samples?

I have a tabular q-learning agent that works well on frozenlake using numpy arrays. I tried to replicate this exact agent using tensorflow, but for some reason I'm incapable of reproducing the results.",reinforcementlearning,ajavdk,False,/r/reinforcementlearning/comments/brrihf/code_review/
A selection of Datasets for Machine learning,1558542273,,reinforcementlearning,atomlib_com,False,/r/reinforcementlearning/comments/brqtj8/a_selection_of_datasets_for_machine_learning/
[R] [1902.01119] The Natural Language of Actions,1558489699,,reinforcementlearning,PeterPrinciplePro,False,/r/reinforcementlearning/comments/briv7y/r_190201119_the_natural_language_of_actions/
[R] [1902.05542] Unsupervised Visuomotor Control through Distributional Planning Networks,1558486858,,reinforcementlearning,PeterPrinciplePro,False,/r/reinforcementlearning/comments/briea5/r_190205542_unsupervised_visuomotor_control/
Question about using RL for learning Piano,1558469316,I'm considering attempting to build an agent that can teach itself to play piano. The agent would ultimately need to master the song for it to be successful. I was considering feeding it midi and having each midi note being a reward so that it can start mashing the right midi notes to make it to the end and therefore complete the song correctly. I don't intend on using robot hands but I would have a switch for each key so that when the agent selects a key or a chord it would play it and count.  Is this a viable way for the agent to learn to play a song?,reinforcementlearning,kalavala93,False,/r/reinforcementlearning/comments/brf75z/question_about_using_rl_for_learning_piano/
How to deal with a lot of state values?,1558466228,"Hi everyone,

&amp;#x200B;

So i am experimenting with feeding  my RL DDPG agent historical data of previous states, so i have a state of 8 values and that works OK, however when i want to increase the state with previous frames values, then i will be getting 44 values. The whole state is scaled down to -1 and 1 and the ony thing what i am doing is really only ""duplicating"" (previous) frames, because they mean the same and also are scaled the same. I tried 2 and 3 hidden layers all (leaky) relu and softmax and batch size 64, lr 0.001 and GAMMA =0.99. So besides the model be going insane/saturated, so i am somewhat confused by that, because i practically just leveraged all parameters, like 64/128/200 neurons every hidden layer and all. Also noticeable is that it barely (if not training, but filling memort) its also never convergeing between softmax actions with argmax. 

&amp;#x200B;

Anyone got any ideas on how this works with a bigger state?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/brel4l/how_to_deal_with_a_lot_of_state_values/
"Build tools in machine learning projects, an overview",1558448417,,reinforcementlearning,atomlib_com,False,/r/reinforcementlearning/comments/brazhg/build_tools_in_machine_learning_projects_an/
Can I use a Replay Buffer in A2C/A3C? Why not?,1558439749,"It seems that the consensus is that it is not possible to use a replay buffer with A2C.

I don't understand why. The value network update is [the same as in DQN](https://cdn-images-1.medium.com/max/1320/1*KlX2-kNXRYLAYpdnI8VPiA.png) (without a replay buffer and without a target network).

The update doesn't rely on trajectories: you have a list of (s, a, r, s') and you reduce the TD-error of the current policy from that. So, the TD-backup is completely off-policy, no?

Therefore, I don't see why learning from old episodes would be a problem. Sure, the value network needs to see the newer episodes, otherwise it would lag behind the PG policy. But I don't see why we couldn't use the old episodes as well to stabilize the value network as happens with DQN.

(Following [this post](https://www.reddit.com/r/learnmachinelearning/comments/bl4kel/a2c_am_i_missing_the_point/) and [this old post](https://www.reddit.com/r/reinforcementlearning/comments/7uq96o/can_a2cacktr_be_trained_with_a_saved_dataset_of/).)",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/br9hc3/can_i_use_a_replay_buffer_in_a2ca3c_why_not/
Evolving Rewards to Automate Reinforcement Learning,1558431301,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/br8arb/evolving_rewards_to_automate_reinforcement/
A reinforcement learning environment for self-driving cars in the browser,1558430498,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/br8718/a_reinforcement_learning_environment_for/
"Doubt about implementation of ""Neural Network Dynamics for Model-Based Deep Reinforcement Learning with Model-Free Fine-Tuning""",1558428022,"
Hello, does someone have experience with this paper https://arxiv.org/pdf/1708.02596.pdf ?

I am trying to implement the dynamics model part and in fig 5 they seem to have a very high accuracy for it in a multistep prediction of 100 timesteps into the future.

My own model is able to predict quite accurate the next step f(s,a) -&gt; s' but trying to predict into 100 timesteps into the future seems to give very bad results. Only the first 5 or so are somewhat correct.

The neural network I use is the following (that I think its like the paper).

class FFNN(nn.Module):
    def __init__(self, n_input, n_out, n_hidden=500, activation='relu'):
        super(FFNN, self).__init__()

        self.fc1 = nn.Linear(n_input, n_hidden)
        self.fc2 = nn.Linear(n_hidden, n_hidden)
        self.fc3 = nn.Linear(n_hidden, n_out)

        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'tanh':
            self.activation = nn.Tanh()

    def forward(self, x):
        out = self.activation(self.fc1(x))
        out = self.activation(self.fc2(out))
        out = self.fc3(out)

        return out

I collect random trajectories s,a,r,s', standarize(mean,std) and train it in two ways:

a) to predict delta_s = s'-s as in the paper 
b) to directly predict s

I had a little better results predicting directly s.

The only thing I can thing that the paper uses that I do not is the Gaussian noise they put into the observations but I do not think that could make the results much much better.

Any ideas?

Thanks in advance for any help you can give!",reinforcementlearning,LazyButAmbitious,False,/r/reinforcementlearning/comments/br7w59/doubt_about_implementation_of_neural_network/
State of the art Algorithm,1558418247,"Hello guys, could you do me a quick update. What is currently the state of the art algorithm in deep RL. Thank you so much anh wish you a good day 🙆🏻‍♀️",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/br6phd/state_of_the_art_algorithm/
"""Model-Based Reinforcement Learning from Pixels with Structured Latent Variable Models"" {BAIR}",1558391237,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/br2ayh/modelbased_reinforcement_learning_from_pixels/
Confusion about GAE implementation in ppo2-baselines,1558349120,"Hi, I'm trying to make sense out of the GAE implementation presented in the OpenAI baselines code:

After sampling, they do the following for the mini-batch:

[https://github.com/openai/baselines/blob/master/baselines/ppo2/runner.py#L53-L66](https://github.com/openai/baselines/blob/master/baselines/ppo2/runner.py#L53-L66)

    
    
    # discount/bootstrap off value fn
    mb_returns = np.zeros_like(mb_rewards)
    mb_advs = np.zeros_like(mb_rewards)
    lastgaelam = 0
    for t in reversed(range(self.nsteps)):
        if t == self.nsteps - 1:
            nextnonterminal = 1.0 - self.dones
            nextvalues = last_values
        else:
            nextnonterminal = 1.0 - mb_dones[t+1]
            nextvalues = mb_values[t+1]
        delta = mb_rewards[t] + self.gamma * nextvalues * nextnonterminal - mb_values[t]
        mb_advs[t] = lastgaelam = delta + self.gamma * self.lam * nextnonterminal * lastgaelam
    mb_returns = mb_advs + mb_values

And before optimization, they form the advantage, such that:

[https://github.com/openai/baselines/blob/master/baselines/ppo2/model.py#L136-L139](https://github.com/openai/baselines/blob/master/baselines/ppo2/model.py#L136-L139)

    advs = mb_returns - mb_values
    
    # Normalize the advantages
    advs = (advs - advs.mean()) / (advs.std() + 1e-8)

First, I'm not understanding the reasoning behind adding together the advantages and values in the first code snippet, and in the second, the values are subtracted. This led me to believe that the mb\_values in the second code snippet was values predicted with PI\_new, but from what I can see this is not the case?

&amp;#x200B;

In any case, what is the reason for for NOT using PI\_NEW values in the equation?",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/bqtxp5/confusion_about_gae_implementation_in/
feeding previous frames not working correct?,1558288685,"Hi everyone,

&amp;#x200B;

So in DDPG i am try to feed previous frames in my state, from 8 to 44 values, however with every setting or thing i do this it doesn't work, it usually resolves in straight up saturation where as i don't with the 8 value state. I tried diffrent neuron combinations such as 80, 160, 320 and 290 without luck, when i switch back to 8 values it stays sane. i feed previous frames by just saving the frame and feeding it on top of the current frame as one combined state.

&amp;#x200B;

Anyone got an idea?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/bqjvd9/feeding_previous_frames_not_working_correct/
Deep Reinforcement Learning Tutorial Series - DQN (Deep Q Networks) ; https://www.youtube.com/watch?v=4Xj3la71dB0,1558283802,"Hi guys,

&amp;#x200B;

I started a tutorial series a while age which I'm doing alongside my studies. I thought it could help other students get started with DRL without having to go through large books and unnecessary amount of reading.

I recently uploaded the second part of this series where I talk about DQN including my implementation of it! [https://www.youtube.com/watch?v=4Xj3la71dB0](https://www.youtube.com/watch?v=4Xj3la71dB0)

I'll soon be uploading a follow up video where I'll go through the code and apply it on a different environment.

&amp;#x200B;

I would love to hear some constructive criticism so that I can improve my self. Additionally, if I missed something that'd be beneficial for everyone's and my improvement too.

Additionally, if you'd like to help out with the code / suggestions on videos please do get in touch :)

Thanks in advance.",reinforcementlearning,S_T47,False,/r/reinforcementlearning/comments/bqiy9m/deep_reinforcement_learning_tutorial_series_dqn/
"Having trouble with PPO, rewards crashing",1558272858,"I'm trying to get good performance for a 3D ball balancing environment using PPO.  I've tried playing around with the learning rate, number of hidden layers and layer size.  Usually training goes well but eventually the rewards go off a cliff.  I assume it would eventually just plateau if I implemented PPO correctly and had the right hyperparameters.  Where should I look if I'm consistently getting this result?  Does this indicate a bug in my implementation of PPO?  Thanks!

[rewards plot](https://imgur.com/a/fCRJjh9)",reinforcementlearning,tho121,False,/r/reinforcementlearning/comments/bqh01v/having_trouble_with_ppo_rewards_crashing/
generate simulated data,1558264590,"Hi, assume I'd like to distribute a dataset for testing (off-policy) RL algorithms. Now for various reasons I can't distribute the original dataset but would like to generate a simulated one that's as close as possible.

Is there any research on generating simulated MDPs? Maybe GANs can be used for this?

Thanks, and apologies for any imprecise terminology, I'm new to the field.",reinforcementlearning,medcode,False,/r/reinforcementlearning/comments/bqfsdh/generate_simulated_data/
"[P] PyTorch-backed, research-friendly RL baselines",1558253606,,reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/bqeexz/p_pytorchbacked_researchfriendly_rl_baselines/
When to use done in a sparse reward?,1558177757,"Hi,

&amp;#x200B;

So i asked this question before, but i still don't quite get it...

&amp;#x200B;

If you have an environment that is very sparse and only the terminal reward is available, such as in the taxi driver environment. If you'd give it a terminal reward, but also with done, since the environment ends after that terminal action, then nothing is done with gamma and the reward = R? Whiles in my head atleast.... thats why gamma is there in the first place? To tell which actions lead to this specific reward = R + Gamma \* Q\_, but in a sparse reward you don't, taking in consideration that after terminal reward the env ends....

&amp;#x200B;

How is this done?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/bq2v98/when_to_use_done_in_a_sparse_reward/
RL from pixels?,1558123140,"Hi,

  
When you want RL from pixels, how do you do this? like whats the approach? Also very confused about how Pong from Pixels with 6400 inputs was solved with only 200 units, like how do these ratios work with pixels?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/bpvy5b/rl_from_pixels/
"Regarding Performance of Critics in PPO, A2C and similar approaches.",1558098237,"I have a question regarding the performance of Critics and modern Actor Critic methods like the ones mentioned above: 

  
When i train them, even on (i guess) simple environment like LunarLanderContinuous, the Critic's performance is lackluster: The value it outputs seems merely minimize loss by outputting the overall mean over the values encountered, i.e. it's output seems not to take into regard the actual state, but rather only is able to predict how good the actor performs in general (that is it just predicts the mean over all values). I can see this, as the MSE in training the Critic stays high, the predictions mean is more or less equal to the mean of the targets, but the variance of the predictions is close to zero, while the variance of value targets is comparatively high. Is this problem typical, maybe even expected, or might there be something wrong with my implementation?   


Thanks for your intuitions and best regards!  


PS.: I'm aware PPO is not necessarily using an Actor-Critic approach, but from what i have seen, it seems to be the most common usage.",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/bpr0e1/regarding_performance_of_critics_in_ppo_a2c_and/
Great Explanation of Policy Gradient,1558088009,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bppenc/great_explanation_of_policy_gradient/
[Beginner Questions] Continuous control for autonomous driving simulation CARLA,1558070912,"Hi,

I'm part of a student team where we're gonna train a reinforcement learning agent with the goal to eventually complete some (as of now undisclosed) simple tasks in [CARLA](http://carla.org/).

We don't really have experience with RL but are familiar with deep learning.

Possible algorithms from initial literature review: PPO, TD3, SAC.

Implementation: PyTorch (it's just easier to debug, we can't use TF 2.0)

Project setup: First run experiments on [CarRacing](https://gym.openai.com/envs/CarRacing-v0/), then extend implementation to CARLA

My first question regards on-policy vs. off-policy: Is there a way to make an informed decision about this beforehand without trial and error?

Second question: Does anyone have experience with the mentioned algorithms and how they compare against each other? I'm particularly interested in performance, implementation complexity and sensitivity to parameter settings (I've searched this subreddit already and read for instance [this post](https://www.reddit.com/r/reinforcementlearning/comments/9sw3m0/what_is_the_state_of_the_art_for_mujoco/))

Third question: Has anyone worked with CARLA before, maybe even with one of the mentioned algorithms?

So far we're leaning towards TD3 as it seems to give strong performance while at the same time the author provides a very clear implementation to build on.

Thanks in advance to everyone helping out!",reinforcementlearning,timo_kk,False,/r/reinforcementlearning/comments/bpn9c1/beginner_questions_continuous_control_for/
Resources to learn about actor critics &amp; vanilla policy gradient,1558040895,"I'm trying to use reinforcement learning to train a bot for a custom game. I'm using openai's spinning up and have gotten alright results however I'm struggling editing the models/algorithms. Such changing the action space to multiple categorical outputs. There are also terms I don't understand like V_loss and KL

Is there a good resource to learn about actor critics and reinforcement learning algos such as vanilla policy gradient?

Thanks for your time.",reinforcementlearning,acover,False,/r/reinforcementlearning/comments/bpi6i6/resources_to_learn_about_actor_critics_vanilla/
Negative Actor loss in Actor-Critic and how long can it go?,1558036006,"Hi,

&amp;#x200B;

So i've seen Actor loss being negative, but still decending quite nice, however how deep can it go? It usually just keeps on decending, but unlike 0 with the critic loss, whats the maximum the actor loss could go?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/bph5ga/negative_actor_loss_in_actorcritic_and_how_long/
CS234: Reinforcement Learning. Stanford University. Winter 2019,1558029119,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/bpfpfy/cs234_reinforcement_learning_stanford_university/
PPO using Multivariate Normal Distribution,1558027058,"Hi Folks,

&amp;#x200B;

I am trying to implement PPO in a continuous action space, where there are two possible actions to take. For this I want to model the actions as Gaussian distribution and because they have a correlation I am using Multivariate Normal Distribution, where there is a  vector of means of size actions x 1 and covariance matrix of size action x action that my NNs need to select. The main issue comes with the implementation, I am using Pytorch distribution package, in particular the [https://pytorch.org/docs/stable/distributions.html#multivariatenormal](https://pytorch.org/docs/stable/distributions.html#multivariatenormal) multivariate normal function.  The issue I am facing is regarding the batch training, I am not able to go through the batch. For getting just a sample the following line works,

&amp;#x200B;

MultivariateNormal(self.l3(x).squeeze(), scale\_tril = torch.diag((F.elu(self.l4(x))+1).squeeze())) 

&amp;#x200B;

But iterating over a batch seems unfeasible.   Has anyone seen an implementation using this function or any other implementation using PPO with continuous actions in pytorch? I've seen implementation using categorical distribution, in which you can directly plug the logits in it making everything a lot more easier. 

&amp;#x200B;

Many thanks!",reinforcementlearning,kashemirus,False,/r/reinforcementlearning/comments/bpfa7r/ppo_using_multivariate_normal_distribution/
Looking for a practical Deep Reinforcement Learning Book,1558014429,"Hello all,

I recently was reading [Hands-on Machine Learning with Scikit-learn and Tensorflow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/) and was amazed by how immediately useful it was. It is filled with elegant discussion of best practices, (Which initialization method to use when you are using certain activations, Whether to standardize or normalize data etc...) without sacrificing the theoretical aspect.

&amp;#x200B;

Is there a practitioners book that you could recommend for Deep Reinforcement Learning? Yes, I am familiar Sutton-Barto but I am looking for a bit close to applications.

&amp;#x200B;

Thank you very much!",reinforcementlearning,LupusPrudens,False,/r/reinforcementlearning/comments/bpcr6k/looking_for_a_practical_deep_reinforcement/
[1905.06002] Simitate: A Hybrid Imitation Learning Benchmark,1557996163,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/bpa359/190506002_simitate_a_hybrid_imitation_learning/
[D] Greedy Hyperparameter Tuning,1557969310,"I've always thought that tuning hyperparameters in RL was a hard problem, but then I read the paper  
   *Fast Efficient Hyperparameter Tuning for Policy Gradients* ([https://arxiv.org/abs/1902.06583](https://arxiv.org/abs/1902.06583))  
which basically just pushes the optimization of hyperparameters inside the improvement policy loop of PG algorithms. More precisely, we replace

    for every choice psi of hyperparameters:
      choose initial policy pi
      while not converged:
        pi &lt;- improve(pi, psi)
      J[psi] &lt;- get_performance(pi)
    psi = arg_max(J)

with

    choose initial policy pi
    while not converged:
      for every choice psi of hyperparameters:
        new_pi[psi] &lt;- improve(pi, psi)
        J[psi] = estimate_performance(new_pi[psi])     # uses weighted IS
      pi &lt;- choose_best(new_pi, J)

In the first case we find the optimal hyperparameters, while in the second we should find a full schedule, but I'm not sure that's something we can reuse. I'd say we just get a final policy pi.

The performance of each candidate new\_pi\[psi\] is estimated by (weighted) importance sampling (IS) on the most recent trajectories, i.e. the ones sampled from pi. If new\_pi\[psi\] and pi are KL-close enough, then IS should give a good enough estimate. Most(?) SOTA algorithms employ KL constraints so that shouldn't be a problem, but if that isn't the case, we can simply ignore new\_pi\[psi\] that are too KL-far.

My question is this: *How can a greedy algorithm find good hyperparameters, in general?*

We all know about the *exploration-exploitation dilemma* in RL and it's clear that some hyperparameters, one way or another, control the balance between exploration and exploitation. Wouldn't a greedy algorithm such as the one described here almost always prefer exploitation over exploration since exploitation is almost always advantageous in the short run? The effects of some choices only manifest in the long run, so why should such an algorithm work, in general?",reinforcementlearning,Kiuhnm,False,/r/reinforcementlearning/comments/bp6coe/d_greedy_hyperparameter_tuning/
Bruteforcing NES _Arkanoid_: depth-first search of an approximate MDP simulator implemented in C++,1557957594,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bp48i4/bruteforcing_nes_arkanoid_depthfirst_search_of_an/
Weights and Gradients in backprop of DRL algorithms,1557955607,"I've seen that there is some algorithms such as SWA (Stochastic Weight Averaging) and algorithms that perform averaging of logits from several mini batches before doing backprop.

&amp;#x200B;

I've tried to implement weight averaging where I train multiple policies for several epochs before taking the weights, average them and apply to an inference policy. The results were not impressive, to say the least, where the agent seemingly performed worse than random. The first question is: Is this done in practice at all? Are there any papers on mixing together weights in RL? (In any way)

&amp;#x200B;

The next is gradients. For instance, if I have 1 inference policy and several ""trainers"". How would one perform updates? Would all policies train on the same batch? Would they train on different batches (mini-batches for instance) or is this a bad thing to do in general? How would I proceed to use the training progress of multiple policies to learn a ""superior policy"" via the gradients?

&amp;#x200B;

In general, I'm looking for papers and knowledge regarding this applied to RL, and if there is any code I'll consume that as well :)",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/bp3tx9/weights_and_gradients_in_backprop_of_drl/
"""Reinforcement Learning"" course. University of Waterloo. Spring 2018",1557949338,,reinforcementlearning,asuagar,False,/r/reinforcementlearning/comments/bp2h7g/reinforcement_learning_course_university_of/
Implementing SAC-X algorithm (from paper 'Learning by Playing' Riedmiller et al.),1557936396,"I'm trying to implement algorithm from [SAC-X paper](https://arxiv.org/pdf/1802.10567.pdf) from DeepMind. They use robotic arm environment there, but I've decided to solve some simpler environment using their architecture first. It should have continuous action space, so I chose [LunarLanderContinuous](https://gym.openai.com/envs/LunarLanderContinuous-v2/) from gym.   
In the paper they have actor-critic with off-policy update ([retrace paper](https://arxiv.org/pdf/1606.02647.pdf)). Actor returns mean and variance in Gaussian distribution, which is then used to sample actions (i.e. for each action we have mean-variance pair). In paper both actor's and critic's neural network have separate head for each task, but in Lunar Lander we have only one task, so only one head in neural network.  


Now what's unclear:  
1) Here's how they define loss for critic:  


[Critic loss](https://i.redd.it/ints24de0ey21.png)

But in retrace paper Qret is a little bit different (i.e. )

&amp;#x200B;

[Qret from retrace paper](https://i.redd.it/wll1pdg51ey21.png)

It uses expectation of Q(s(j+1)) instead of Q(s(i)). Which one is correct?  


2) In previous equations to compute Qret we need to get expectation of Q(s(i)). For discrete case it's clear how to calculate it (having probabilities from policy), I'm not sure how to do it in case of continuous action space?  


3) Also in previous equations to compute Ck we need to divide probability densities of current policy and behavioral policy. But in LunarLander we have two actions, so should I average these values or something else?  


4) Here's actor loss from paper:  


[Actor loss](https://i.redd.it/nejgi4yscey21.png)

To compute gradients they use reparametrization trick, which is described in Appendix B.1. in the paper. I'm not entirely sure that I've implemented it correctly

&amp;#x200B;

My current code doesn't converge on LunarLander environment and I could have done many obvious mistakes, so I would be very grateful if you could take time to look through it (comments with '???' symbols correspond to my questions here)  
Here's code (it uses PyTorch): [https://www.codepile.net/pile/6lMXJoO8](https://www.codepile.net/pile/6lMXJoO8)",reinforcementlearning,inconst,False,/r/reinforcementlearning/comments/bozsr5/implementing_sacx_algorithm_from_paper_learning/
[P] Cool ML slides from Berkeley,1557933365,,reinforcementlearning,Jaxon_K,False,/r/reinforcementlearning/comments/boz6a3/p_cool_ml_slides_from_berkeley/
Looking for good implementations of Rainbow,1557899028,"Like the title says, I've been looking for some good implementations of Rainbow. I saw that Google's Dopamine has a implementation of Rainbow but it doesn't seem to be a full implementation, not sure if that is a big deal or not. Does anyone know of a well tested Rainbow implementation that I could use?",reinforcementlearning,sturdyplum,False,/r/reinforcementlearning/comments/botyls/looking_for_good_implementations_of_rainbow/
What’s the status of Evolution Strategies/Deep Neuroevolution?,1557853963,"In late 2017 Uber released five papers making great claims about neuroevolution. I was just reminded of it today looking at the [PPO-CMA paper](https://www.reddit.com/r/reinforcementlearning/comments/boi4m0/ppocma_proximal_policy_optimization_with/).

Has there been more progress in this direction? Any major publication or practical use?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bolom6/whats_the_status_of_evolution_strategiesdeep/
PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,1557835982,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/boi4m0/ppocma_proximal_policy_optimization_with/
PPO convergence time,1557821534,"Anyone have any sources on how long training time PPOs need in order to converge to an optimal policy?

Thanks!",reinforcementlearning,kborgk,False,/r/reinforcementlearning/comments/bog0sy/ppo_convergence_time/
Off-policy correction in n-step Q-learning.,1557804130,"Hi, I have some trouble on off-policy correction in n-step Q-learning. 

&amp;#x200B;

1. Let's define the target value of n-step Q-learning as (R\_t + rR\_{t+1} + r\^2R\_{t+2} + ... + r\^n max Q\_{t+n}(S\_{t+n}, A\_{t+n})), and assume that our behavior policy is e-greedy.

Then don't we need off-policy correction when we update the Q function?

And if it's the case, and if we use importance sampling, is it right that we cannot use the samples with action that was not the argmax of Q function since target policy would never choose them? i.g. cut off the trajectory from non-greedily selected samples when training?

&amp;#x200B;

2. Even though the definition of target value is different in A3C paper-the length of trajectory is not fixed-it seems anyway we need correction but they don't have one. Is there any theoretical background for this or they just ignored since there was no significant drop in performance?",reinforcementlearning,wwiiiii,False,/r/reinforcementlearning/comments/bodhdv/offpolicy_correction_in_nstep_qlearning/
Does Mujoco license renewable after 1-year free license?,1557803933,"Hi, does anyone know if Mujoco license is renewable after 1-year use?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/bodg7u/does_mujoco_license_renewable_after_1year_free/
environments with interpolation difficulty?,1557733131,"I'm looking for environments that easily allow for interpolating ""difficulty"". Does anyone know of good examples? I'm most interested in continuous control environments but would also find discrete control examples useful. Currently for discrete control I've been considering a modified sokoban to increase/decrease the gridsize and number of goals. For continuous control I don't think something like MuJoCo is easily modified to interpolate difficulty.",reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/bo0gp1/environments_with_interpolation_difficulty/
[P] Landing a rocket - my first Unity ML Agents project.,1557720894,"Video - [https://youtu.be/ynMYVdb7mO8](https://youtu.be/ynMYVdb7mO8)

It was always my little dream to do exactly this project. I know people have done it before, but being a beginner in both Unity and Reinforcement Learning I thought I share the results of my little dream :P

&amp;#x200B;

The agent taught itself how to land, no human taught it or shown how to do it. Trained for 500k steps (ca. 1h), could have ended sooner - [image](https://puu.sh/DrBnF/110c39f4a7.png)

&amp;#x200B;

Maximum reward/score is 20, its almost impossible to reach. It would mean 0 landing speed + 0 horizontal offset from the target. Needless to say, humans have no chance to land like that :P

Reward function - [click me](https://puu.sh/DrBt2/8cce720f40.png)

&amp;#x200B;

Will retrain with different parameters. Right now  batch\_size: 10 and  buffer\_size: 100, rest default.

&amp;#x200B;

Todo:

\- Add fuel consumption

\- Lessen the thrust of sideways engines

\- Simulate something more similar to a SpaceX landing, where a rocket falls for a period of time before landing

\- Add human controlled rocket for competitive play

\- Host the game on the web

\- ???

&amp;#x200B;

ps. Elin Musk, hire me! :D",reinforcementlearning,Roboserg,False,/r/reinforcementlearning/comments/bnypuv/p_landing_a_rocket_my_first_unity_ml_agents/
Using Value target and value estimation in Generalized Advantage Estimator,1557693452,"Context: Implementing PPO. Trying to use GAE: 

[GAE as described in Proximal Policy Optimization Algorithms; Schulman et al.](https://i.redd.it/ut2ajqrrbux21.png)

My question is concerned with the third line of math (12): To compute delta\_t (i think TD Residuals is the technical term) one needs the value of the state. Is the output of the critic network here used for both V(s\_t) and V(s\_(t+1))? I sadly have no intuition for which part here the actual value (computed from the rewards) and for which part the value estimation is used. Thank you for you help, and maybe even an intuition for why to use which one?",reinforcementlearning,LJKS,False,/r/reinforcementlearning/comments/bntumw/using_value_target_and_value_estimation_in/
[AI application] Let your machine teach itself to play flappy bird!,1557559398,,reinforcementlearning,1991viet,False,/r/reinforcementlearning/comments/bn9fgh/ai_application_let_your_machine_teach_itself_to/
NeurIPS 2019: The MineRL Competition for Sample-Efficient Reinforcement Learning,1557521862,,reinforcementlearning,imushroom1,False,/r/reinforcementlearning/comments/bn3ptd/neurips_2019_the_minerl_competition_for/
Questions about Unity ML Agents with multiple cloned environments,1557502203,"So a couple of questions about instantiating multiple environments, when having only 1 brain but many cloned environments with several agents, such as here - [https://puu.sh/DqqsK/6d73097c9c.png](https://puu.sh/DqqsK/6d73097c9c.png)

**1.** In `tensorboard` the `Cumulative Reward`, is this the reward of ONE agent, or of ALL the agents in ALL the cloned areas? I mean we have only 1 brain, but many agents, so I am not sure if its a sum of all rewards, or only of one training area.

**2.** So if I had ONE agent I would train, say, for 20.000 steps. If I have 10 agents, 1 brain, would I train for 2.000 steps to achieve the same amount of experience?

**3.** In the command console of `mlagents-learn`, it shows the number of steps while training. Isn't it a convention of reinforcement learning to show the number of completed episodes, and not the number of steps? 2000 steps could contain 20 episodes or only one long episode.

**4.** How many training areas can I instantiate before it becomes a bottleneck? I mean now we also could open several instances of unity as of Unity Agents 0.8 with `--num-envs`. So how do I choose the number of training areas AND the number of environments? Say I have a modern i7 6700k CPU with 8 threads and 4 cores.

**5.** I am confused about the word ""step."" We set the number of `max steps` to train in config.yaml. Say we do 5.000. Then we have `max steps` in the academy, so the max. number of steps per episode. Say we set it to 1000. Does that mean I would get only 5x full episodes in all my training of 5.000 steps?

**6.** Is it possible to speed up the simulation by calculating the whole physics and environments every 5 ticks/steps or so? I know we can do `decision interval` but the simulation and physics itself still runs every frame/tick/step of the unity engine.

**7.** While training a file named UnitySDK.loggets created and is now over 3 GB in size. This file is NOT deleted even after closing Unity. How do I turn this log file off or get it deleted automatically? This log file is created only during training and seems to be a ""feature"" of unity agents, not unity itself.",reinforcementlearning,Roboserg,False,/r/reinforcementlearning/comments/bmzomw/questions_about_unity_ml_agents_with_multiple/
[R] ICLR 2019 Notes,1557496847,,reinforcementlearning,sorrge,False,/r/reinforcementlearning/comments/bmyndj/r_iclr_2019_notes/
Does Policy Gradient use Generalized Policy Iteration?,1557476854,"Here's the definition of GPI from Sutton:

&gt; Policy iteration consists of two simultaneous, interacting processes, one making the value function consistent with the current policy (policy evaluation), and the other making the policy greedy with respect to the current value function (policy improvement).
-- http://incompleteideas.net/book/first/ebook/node46.html

But actually, Policy Gradient can also be described in terms of successive ""policy evaluation"" (perform a rollout) and ""policy improvement"" (backprop according to PG theorem).

So I was wondering if I could qualify Policy Gradient as a ""GPI methods"", even though it doesn't use a value function?

Or, is there a name for a higher-level class of algorithms that describes successive policy evaluation/improvement that don't necessarily rely on a value function?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bmvqq6/does_policy_gradient_use_generalized_policy/
"""Domain Randomization for Sim2Real Transfer"", Lilian Weng",1557433002,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bmonyf/domain_randomization_for_sim2real_transfer_lilian/
"""An End-to-End AutoML Solution for Tabular Data at KaggleDays"" {G} [writeup of AutoML's 2nd place in Kaggle competition]",1557432509,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bmok86/an_endtoend_automl_solution_for_tabular_data_at/
What environments are you using?,1557427540,"I’m curious which environments people are using:

-	Atari?
-	Mujoco?
-	Roboschool?
-	OpenAI gyms beyond the above?
-	DeepMind Control Suite?
-	OpenSim RL?
-	Unity env? (How are they called? Not familiar with those)
-	Malmo?
-	Other...?

And for what purpose: to learn RL? For research? To test your code?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bmnjbx/what_environments_are_you_using/
What neural net architectures to use for Actor &amp; Critic?,1557426931,"Hey guys, 

&amp;#x200B;

I'm working on a hobby project using PPO. I wondering what your nerual net architectures look like for your actor and critic? I didn't see much guidance on what they should look like.

Right now, I have a convolutional feature extractor, then just a few layers of Dense + LeakyReLU  + Dropout.  


Anyone have any suggestions or resources I should look at?

&amp;#x200B;

Thanks!",reinforcementlearning,DickNixon726,False,/r/reinforcementlearning/comments/bmnf0a/what_neural_net_architectures_to_use_for_actor/
"""Meta-learning of Sequential Strategies"", Ortega et al 2019 {DM} [review of Bayesian RL interpretation of meta-RL]",1557423441,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bmmpja/metalearning_of_sequential_strategies_ortega_et/
High memory usage when doing Monte Carlo estimation,1557420918,"I'm asking this question because I was implementing Trust Region Policy Optimization, but I get the feeling that this is a problem that will pop up going forward with other algorithms:

&amp;#x200B;

The memory usage seems completely insane, and I'm wondering if I'm totally doing it wrong, or if this is just a fact of implementing deep RL using Monte Carlo estimation methods.

&amp;#x200B;

For example, in order to replicate the results in the research paper, it would require me to store information about hundreds of sampled trajectories, which themselves are \~400k timesteps long. Not to mention that if you're using PyTorch/Tensorflow/etc. this means that you're storing a computation graph for every entry. 

&amp;#x200B;

I did notice that the authors don't mention using a GPU, and instead write that they used a ""16 core machine."" So this leads me to believe that it is really memory intensive and that for this reason they are not using a GPU because you could never fit this all into memory (barring the case in which you're a monolithic company and you have access to a massively parallel GPU cluster).

&amp;#x200B;

To reiterate my question: Is implementing a RL algorithm involving MC estimation from entire trajectories simply extremely memory intensive, or is there some trick here that I am missing?",reinforcementlearning,vandelet_industries,False,/r/reinforcementlearning/comments/bmm7hm/high_memory_usage_when_doing_monte_carlo/
Soft Actor-Critic with Discrete Actions,1557420092,"Does anyone know if it is possible (or how) to use Soft Actor Critic with discrete actions instead of continuous actions? Or even better has anyone seen an implementation of this on github somewhere?

&amp;#x200B;

Open AI [here](https://spinningup.openai.com/en/latest/algorithms/sac.html) say:

&gt; An alternate version of SAC, which slightly changes the policy update rule, can be implemented to handle discrete action spaces. 

But then they don't explain the required change to the policy update rule",reinforcementlearning,__data_science__,False,/r/reinforcementlearning/comments/bmm1dj/soft_actorcritic_with_discrete_actions/
[Beginner Question] How to work with continuous states coding-wise?,1557417872,"I'm new to RL and have been struggling a bit with translating theory into application. Based on some advice here, I'm writing (adapting) my own code from scratch.

I'm following this [code](http://www.viralml.com/reinforcement-learning-simple-example) (in addition to Sutton and Barto) as reference, but am mainly struggling with the following:

What I'm trying to do is to find the best green-time for traffic signals given number of waiting cars at every leg (queue length). For the sake of simplicity, let's assume it's a fake intersection with only 1 approach (the signal is there to protect pedestrians or whatever).

The actions, as I see them, should be: *extend green time in the next phase*, *hold*, *reduce green time in the next phase*.

The reward will be: *- Delta(total delay)*

The struggle is here, I think the state should be: *&lt;queue length on approach (q), green time on approach (g)&gt;*.

Conceptually, it's not very confusing, but in the code I linked, every state had a reward or queue matrix with rows for states and and columns for potential actions. My matrices should have 3 columns, **but how do I define the rows?**

I there a way to treat q and g continuously? Or do I need to discretize? Even if I discretize, if theoretically, q goes from 0 to inf, is there anything I should be careful about or should I just make sure that there are enough rows to ensure that the realistic maximum of q is covered.

I apologize if these questions are trivial, but I'm trying! Thank you!",reinforcementlearning,MarshmallowsOnAGrill,False,/r/reinforcementlearning/comments/bmllgi/beginner_question_how_to_work_with_continuous/
"Fast-Pytorch with Google Colab: Pytorch Tutorial, Pytorch Implementations/Sample Codes",1557390459," This        repo aims to cover Pytorch details, Pytorch example    implementations,     Pytorch sample codes, running Pytorch codes with    Google Colab (with   K80   GPU/CPU) in a nutshell. It will be updated in   time.

**Tutorial Link:** [**https://github.com/omerbsezer/Fast-Pytorch**](https://github.com/omerbsezer/Fast-Pytorch)

## Table of Contents:

* 🔥[Fast Pytorch Tutorial](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchtutorial)  

   * [Pytorch Playground](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchplayground)  

      * 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Pytorch_Playground.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Pytorch_Playground.ipynb)
   * [Model (Neural Network Layers)](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#model)
   * [Optimizer](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#optimizer)
   * [Loss Functions](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#lossfunctions)
   * [Pooling Layers](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#poolinglayers)
   * [Non-linear activation functions](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#nonlinearactivation)
   * [Basic 2 Layer NN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#example)
* 🔥[Fast Torchvision Tutorial](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#torchvisiontutorial)  

   * [ImageFolder](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#imagefolder)
   * [Transforms](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#transforms)
   * [Datasets](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#datasets)
   * [Models](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#torchvisionmodels)
   * [Utils](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#utils)
* 🔥[Pytorch with Google Colab](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchcolab)
* 🔥[Pytorch Example Implementations](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchexamples)  

   * [MLP](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#mlp)  

      * MLP 1 Class with Binary Cross Entropy (BCE) Loss: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_1class_BinaryCrossEntropyLoss.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_1class_BinaryCrossEntropyLoss.ipynb)
      * MLP 2 Classes with Cross Entropy Loss: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_2class_CrossEntropyLoss.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_2class_CrossEntropyLoss.ipynb)
      * MLP 3-Layer with MNIST Example: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_3layer_MNIST.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/MLP_3layer_MNIST.ipynb)
   * [CNN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#cnn)  

      * CNN with MNIST Example: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Mnist.ipynb)
      * Improved CNN with MNIST Example: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Improved_CNN_Mnist.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/Improved_CNN_Mnist.ipynb)
   * [CNN Visualization](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#cnnvisualization)  

      * CNN Visualization: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Visualization.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/CNN_Visualization.ipynb)
   * [RNN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#rnn)  

      * RNN Text Generation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/RNN_word_embeddings.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/RNN_word_embeddings.ipynb)
   * [Transfer Learning](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#transferlearning)  

      * Transfer Learning Implementation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/TransferLearning.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/TransferLearning.ipynb)
   * [DCGAN](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#dcgan)  

      * DCGAN Implementation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/DCGAN.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/DCGAN.ipynb)
   * [ChatBot](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#chatbot)  

      * Chatbot Implementation: 📗[\[Colab\]](https://colab.research.google.com/github/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/ChatBot.ipynb), 📓[\[Notebook\]](https://github.com/omerbsezer/Fast-Pytorch/blob/master/Learning_Pytorch/ChatBot.ipynb)
* 🔥[Pytorch Sample Codes](https://github.com/omerbsezer/Fast-Pytorch/blob/master/README.md#pytorchsamplecodes)

**Extra:**  Reinforcement Learning Tutorial:

[**https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo**](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)

**Extra:**         Image  Generation With AI:  Generative Models Tutorial with        Python+Tensorflow  Codes (GANs, VAE,  Bayesian Classifier Sampling,        Auto-Regressive Models,  Generative  Models in RL)

[**https://github.com/omerbsezer/Generative\_Models\_Tutorial\_with\_Demo**](https://github.com/omerbsezer/Generative_Models_Tutorial_with_Demo)

**Extra:**    LSTM-RNN Tutorial with LSTM and RNN Tutorial with Demo with Demo    Projects such as Stock/Bitcoin Time Series Prediction, Sentiment    Analysis, Music Generation using Keras-Tensorflow

[**https://github.com/omerbsezer/LSTM\_RNN\_Tutorials\_with\_Demo**](https://github.com/omerbsezer/LSTM_RNN_Tutorials_with_Demo)",reinforcementlearning,obsezer,False,/r/reinforcementlearning/comments/bmh5gz/fastpytorch_with_google_colab_pytorch_tutorial/
Research Minecraft server that records data for massive imitation learning dataset!,1557356318,,reinforcementlearning,MadcowD,False,/r/reinforcementlearning/comments/bmc5n5/research_minecraft_server_that_records_data_for/
Do I need a CNN?,1557351936,"I've been looking to all the DQN reinforcement learning strategies, and each ""game"" they train on is some old Atari emulator. However, what if you have the coordinates of the objects and where they need to go to achieve an award. Are there models or examples where a CNN was not needed?",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bmba3m/do_i_need_a_cnn/
Goal transfer in reinforcement learning,1557323938,"Hi everyone!

I'm doing some work on a method where we take a trained predictive model, and adapt it to a new scenario by modifying the goals we are trying to achieve. In other words, the underlying predictive model is the same, but by changing goals, we are able to adapt it to new scenarios.

I was wondering if anyone here knows about other techniques for transferring predictive models to new tasks where goals are different. In short, other transfer learning techniques we should be comparing our technique to. More info in it is available here:

[https://twitter.com/Jallafsen/status/1114070073973649408](https://twitter.com/Jallafsen/status/1114070073973649408)

and here:

[https://arxiv.org/abs/1904.02435](https://arxiv.org/abs/1904.02435)",reinforcementlearning,Jallafsen,False,/r/reinforcementlearning/comments/bm5lk3/goal_transfer_in_reinforcement_learning/
Best way to construct features for Q-learning with LVFA,1557322290,"I'm about to start a project where I use depth-image (Kinect) and optical flow information in my state representation. Because these can be rather large, I am going to use Auto-Encoders to extract features of a manageable size and then use them together with Linear Value Function Approximation (LVFA) for Q-learning. The reward is simply the speed of the robot (I want the robot to go as fast as possible while avoiding obstacles).

Note that I am not trying to do Deep RL. The features (Auto-Encoder) and Q value function will not be learned jointly.

I would like to know if anyone has tried a similar approach, and if features extracted in such a way (not trained jointly) give good-ish results emperically. Is there anything else that I should be aware of before proceeding with this project?

TLDR - Do features (from NN) not jointly trained with the value function (with Linear approximation) work just as good as Deep RL (emperically)? If not, what's the best way (save for handcrafting)?",reinforcementlearning,uakbar,False,/r/reinforcementlearning/comments/bm5b86/best_way_to_construct_features_for_qlearning_with/
What to do with terminal/done on sparse reward design?,1557298429,"Hi,

&amp;#x200B;

i have a similair environment as taxi drive, so will take that environment in mind. So when you do terminal=True then you do R = R and don't use gamma or any, so this is useful on for example steplimit, however if you have a sparse reward design this is really not useful? For example if you have a super sparse reward design and you have the environment like taxi in mind. if the taxi driver drops of a passenger it would get 20 reward, but if that reward isn't getting calculated with gamma and all it's quite pointless? but you'd still need to do terminal = True, because the episode did end?

&amp;#x200B;

I don't get this, Could someone inform me on how this works?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/bm21g6/what_to_do_with_terminaldone_on_sparse_reward/
Searching: collaborator for research in multi agent reinforcement learning,1557295300,"Hello, i am a PhD candidate currently researching RL.

I am looking for a collaborator to research about multi agent reinforcement learning. The goal would be to discuss and implement new ideas on the topic. Depending on the results, writing a paper on the subject.

The starting point would be the paper: ""Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments"" by Lowe et al (2018). I have an idea of a simple follow up in their architecture but also want to discuss important aspects for more complex environments such as exploration, reward system and shared state/action in the multi agent framework.

If you are interested in the collaboration or in a RL study group to implement/discuss algorithms please comment or PM me.

Best Regards.",reinforcementlearning,EpiphanyBot,False,/r/reinforcementlearning/comments/bm1nbt/searching_collaborator_for_research_in_multi/
"Code for ""Learning Self-Imitating Diverse Policies"" [ICLR 2019]",1557290529,,reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/bm108b/code_for_learning_selfimitating_diverse_policies/
"Noob Question: I want to use Q-Learning for traffic signal operation (i.e. get the best green times), what package to use and where to start?",1557257024,"To preface: I know coding at an intermediate level and know how reinforcement learning works mathematically to a decent extent. However, I'm struggling to find out which package would best suit the class exercise I'm working on. Specifically, given a traffic signal (a typical 4-leg signal), I need to use Q-learning to adaptively select the best green time for each approach that would result in least delays.

Through my search, I keep running into Gym, but the environments seem pre-defined and, at least for what I've been reading over the past few hours, it's still not very clear to me how I can define my own problem .

Any pointers to which guides/packages for Python to look at? Mainly, I already have the signal operations coded, but now need to feed the states, policies and rewards to some RL package that can do the number crunching.

Thank you very much and sorry if this question is too trivial! It's my first foray into coding with RL.",reinforcementlearning,MarshmallowsOnAGrill,False,/r/reinforcementlearning/comments/bluy2l/noob_question_i_want_to_use_qlearning_for_traffic/
"RL Weekly 17: Information Asymmetry in KL-regularized Objective, Real-world Challenges to RL, and Fast and Slow RL",1557236078,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/blqrsn/rl_weekly_17_information_asymmetry_in/
Visualizing Policy Optimization,1557217868,"Often, I hear top scientists say that they often use simple environment because they are easy to visualize, for example, Pendulum, which can be illustrated in 2-D.

I can definitely see the benefits of visualize the training, but I've not seen any examples where training is visualize in terms of where the algorithm current is on the state-space surface. I guess for ""hard problems"" this is possible (why else would we need to optimize), but still, for simple problems, lets say 4 dimensions, (CartPole), would this be possible. If yes, is there any examples, and how would one approach to visualize such optimization tasks?

&amp;#x200B;

A nice visualization in the supervised setting is here: [https://playground.tensorflow.org](https://playground.tensorflow.org/), and i assume this could also be done for RL?",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/blo6eb/visualizing_policy_optimization/
Summary: Conservative Policy Iteration,1557214109,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/blnqz9/summary_conservative_policy_iteration/
OpenAI gym multi-wrapper,1557174729,"Hello guys, I using an openAI gym enviroment. I want to modify both it observation and reward. How should I do that. Use wrapper to wrap another wrapper or use a more general wrapper ?
Thank you in advance !",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/blh9rf/openai_gym_multiwrapper/
"""Robots that Learn to Adapt"": MAML {BAIR} [on Nagabandi et al 2019, ""Learning to Adapt in Dynamic, Real-World Environments Through Meta-Reinforcement Learning""]",1557170398,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/blge64/robots_that_learn_to_adapt_maml_bair_on_nagabandi/
"""The MineRL Competition on Sample Efficient Reinforcement Learning using Human Priors"", Guss et al 2019",1557168692,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/blg1rs/the_minerl_competition_on_sample_efficient/
Deep Reinforcement Learning tutorial series,1557157688,"Hi all,

I’m starting my new deep reinforcement learning tutorial series! I figured if I can help someone understand it.... then I know it too.

Would highly appreciate some constructive criticism and tips to improve my knowledge!
Especially if I did something wrong 😬

The first video is up and running 
https://www.youtube.com/watch?v=gTNNXi9ApVU",reinforcementlearning,S_T47,False,/r/reinforcementlearning/comments/bldst2/deep_reinforcement_learning_tutorial_series/
Are there any standard environment for developping multi-agent reinforcement learning algorithm?,1557135467,both for cooperative and competitive tasks,reinforcementlearning,aineqml,False,/r/reinforcementlearning/comments/bla06x/are_there_any_standard_environment_for/
multi-armed bandit Upper Confidence Bound proof,1556937024,"Hi guys, 

I am currently studying UCB from the Sutton and Barto book, and I am struggling to formulate a proof for the upper confidence bound they use. All that I know, is hoeffding’s inequality is usually the theory used to estimate confidence bounds when we don’t have the sampling distribution, or we don’t know the variance.  I think the “variance” part for Sutton en Barto UCB formula comes from this theory but I can’t derivate it from Hoeffding. 

Do you guys know any reference for me to read more about it? 

Thanks",reinforcementlearning,raphaOttoni,False,/r/reinforcementlearning/comments/bkgk5w/multiarmed_bandit_upper_confidence_bound_proof/
Advantage Normalization,1556915184,"Does it make sense to normalize advantages? Some implementations use it (eg: openaAI spinningup). I understand that it can be used to keep the advantage in a certain range, but you lose information about the relative importance of certain actions.",reinforcementlearning,alexmvdk,False,/r/reinforcementlearning/comments/bkd2c2/advantage_normalization/
Critic Function Chasing its Tail?,1556914206,"Consider calculating the discounted return for a stream of rewards. Let's imagine that our trajectory gets cut off early (say, because the current rollout has ended). Then we often bootstrap rewards as such:

&amp;#x200B;

G\_t = r\_t + gamma\*r\_{t+1} + gamma\^2 \* r\_{t+2} ... + gamma\^k V(s\_t)\_{t+k}

&amp;#x200B;

Basically, we treat the reward from the last timestep as being the predicted value of that timestep. This is present in the spinningup implementation of PPO and baselines implementation of PPO, for example.

&amp;#x200B;

What I don't understand is how this could possible ever converge, since our critic tries to predict G\_t, but G\_t (for rollouts that don't end at the end-of-episode) depends on our critic.",reinforcementlearning,alexmvdk,False,/r/reinforcementlearning/comments/bkcvwn/critic_function_chasing_its_tail/
High-quality baselines implemented by PyTorch,1556852938,[https://github.com/Officium/RL-Experiments](https://github.com/Officium/RL-Experiments),reinforcementlearning,VectorChange,False,/r/reinforcementlearning/comments/bk3qe9/highquality_baselines_implemented_by_pytorch/
Controlling Gradient Magnitudes in Critic Updates,1556849860,"Hello all, first time reddit poster here. This seems like a good community! Hopefully you can help me with something.

&amp;#x200B;

I'm implementing Random Network Distillation (RND) to train an agent on Montezuma's Revenge. RND trains a PPO agent using intrinsic reward. I'm having troubles getting it to work because it seems like my critic has some problems.

&amp;#x200B;

**VARIANCE FROM TRAJECTORY DYNAMICS**

Because you may visit the same state on different trajectories in Montezuma's Revenge, the total intrinsic return from that state can change drastically between episodes. Because of this, **my critic usually has a prediction error (even with Huber loss) in the hundreds.** I am worried that my critic is unable to learn anything because of the magnitude of the updates.

&amp;#x200B;

**VARIANCE FROM BOOTSTRAPPING**

Also, RND uses a rollout of length 128 over which it collects data. (This may seem small but as far as I'm aware this is because they run many parallel agents to smooth out the gradient updates). When calculating advantage, the 'last reward' that the agent sees is estimated as being

&amp;#x200B;

r\_t + gamma\*V(s\_t)

&amp;#x200B;

because the rollout might end before the episode is over and so the agent has to try to guess what the reward WOULD have been by the end of the episode. (The agent doesn't care that we assign a lot of reward to one timestep, since we only use the rewards to calculate the returns, and the agent cant tell the difference once we make that calculation). 

&amp;#x200B;

Since we bootstrap the reward for the last timestep like this, when it comes time to calculate returns, they are actually based partially on the estimation of the critic. Then, we update the critic to try to match the returns. If the critic has high prediction error, then these updates can be kind of meaningless. **When I look a the critic loss on tensorboard it doesn't seem to converge so I definitely thing this is a problem**.

&amp;#x200B;

**NEXT STEPS**

* How should I go about debugging if my critic is having troubles learning? Save some kind of sample data and see how much predictions change between parameter updates?
* Is gradient clipping common/supported? It looks like the PPO2 implementation in OpenAI baselines uses it.

&amp;#x200B;

**THINGS I KNOW**

I know that the model I am using for the critic is capable of learning. It also learns extrinsic returns which are essentially zero. Since most rewards from the environment are zero, the extrinsic critic quickly learns a constant-zero function and the extrinsic critic loss drops to zero.

&amp;#x200B;

**IMAGES**

&amp;#x200B;

&amp;#x200B;

[Extrinsic critic loss quickly converges to 0.](https://i.redd.it/k7ev89bwnwv21.png)

&amp;#x200B;

[Intrinsic critic loss doesn't converge. X axis is epochs.](https://i.redd.it/7gst70bonwv21.png)

[Intrinsic return. Note that the intrinsic return predictions \('value'\) match closely. I'm not sure if this is because the critic is a good predictor or because bootstrapping overrides everything.](https://i.redd.it/026d067znwv21.png)

[Intrinsic return predictions. ](https://i.redd.it/s1d244t4owv21.png)

[Difference between critic prediction and calculated returns. Since I used huber loss this is similar to the critic loss in general.](https://i.redd.it/60s6lk3aowv21.png)",reinforcementlearning,alexmvdk,False,/r/reinforcementlearning/comments/bk39im/controlling_gradient_magnitudes_in_critic_updates/
"""Reinforcement Learning, Fast and Slow"", Botvinick et al 2019 {DM} [review of memory &amp; meta-learning, neuroscience parallels]",1556813043,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bjwog0/reinforcement_learning_fast_and_slow_botvinick_et/
RL is not responding how I want it to,1556723584,"I have mentioned this before, but I am trying to build an air hockey robot. Right now, I am in charge of making sure this thing ""thinks."" So I built a simulator and having it play different opponents or human.

I have let it train for a few days, and my results are...interesting. In this current iteration, I reward it with a value of 1 if it scores and a value of -1 if it gets scored on. I tried other rewards before and I got some bizarre behavior. (If we reward it on closing the distance between itself and the puck or on hitting the puck, some models would camp out in the corner and hit it against the wall.)

The current strategy is that it will essentially just move forwards and backwards. This is a valid strategy and works decently well. It does goes elsewhere to meet the puck if needed to, but when the game is fast-paced, going up and down will intercept it and it will be able to defend or score that way.

And, this is great. It is proof that my stuff is working and it is learning. However, this does not reflect air hockey. It does, but my team wants it to ""feel"" like it is another human. So, it should be able to close on the puck like a human. But, the ai learned to most optimal solution while humans engaged in suboptimal ones all the time. What should I do to help it feel more human? It trains with a lot of noise with various opponents. 

I guess this is a great problem to have. Unfortunately, this explanation will not fly for the kids at a science center wanting to play it.",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bji8jo/rl_is_not_responding_how_i_want_it_to/
Do you guys have some RL Researcher you follow as something like a mentor?,1556707005,"If so can you tell me who it is? I would like to have some clarity during my course of study, making sure I am not leaving important foundational knowledge out and that sorta thing.",reinforcementlearning,TheHawkGriffith,False,/r/reinforcementlearning/comments/bjflk1/do_you_guys_have_some_rl_researcher_you_follow_as/
Avoiding cycles,1556701600,"I'm working on a problem where I need to find the shortest path to the goal state given a starting state.  The reward is extremely sparse: one positive reward for the single goal state and negative rewards for reaching any other state. There are e20 states in total.

 Now I can solve this problem using Monte Carlo Tree Search, but extracting a path with this method can take around 30 minutes. I'd rather have an agent making a few mistakes than taking a long time calculating a path. So I've trained a network that takes the current state and outputs and outputs the q-values for each action. When I deploy this agent it works rather well and blindingly fast compared to MCTS. It does, however, occasionally get stuck in cycles of certain states. This can be avoided in MCTS by performing a breadth-first search on the path.  Is there some similar solution for RL-agents that use a network without any associated tree structure? 

Here's what I've tried so far: whenever the agent encounters a previously visited state it momentarily puts the exploration rate from 0 to 1 for the next move. The value of the state that it got stuck in probably has a very high value, so it is likely the agent will gravitate towards the same state the next move. I might also try training during a run. Whenever an agent gets stuck in a state, train it so that the value of the state goes down as it has many negative rewards associated with it.

I'd like to hear what ideas you guys have. I've been stuck in this state for quite a while",reinforcementlearning,matigekunst,False,/r/reinforcementlearning/comments/bjez97/avoiding_cycles/
Reinforcement Learning Explained,1556692984,,reinforcementlearning,gnarvind,False,/r/reinforcementlearning/comments/bje3pr/reinforcement_learning_explained/
Animal-AI Olympics has officially released the competition environment,1556659081,"\- Official Website: [http://www.animalaiolympics.com/](http://www.animalaiolympics.com/)

\- GitHub: [https://github.com/beyretb/AnimalAI-Olympics](https://github.com/beyretb/AnimalAI-Olympics)",reinforcementlearning,Rowing0914,False,/r/reinforcementlearning/comments/bj8sj5/animalai_olympics_has_officially_released_the/
Why overfitting is bad in DQN?,1556638892,"It is mentioned by [Fu 2019](https://arxiv.org/abs/1902.10250) that overfitting might have a negative effect on training DQN. They showed that with either *early stopping* or *experience replay* this effect could be reduced. The first is reducing overfitting, the latter is increasing data.

It doesn't only have negative effects on the returns though, my test shows that it has a negative effect on *value errors* as well (diff. between predict V and ground truth V). I observed frequently with limited data that the training diverged almost 100% of the time (on small nets). Since increasing the amount of data could reduce the chance of divergence, I think this is an effect from overfitting.

Overfitting should mean low training loss, however, my observation is that there is a strong correlation between *TD loss* and *value error*. That is if I see a jump in TD loss, I could expect to see a jump in value error around that moment.

Or it is not overfitting because it is not really *fit* (i.e. high loss) but over-optimization that is for sure.

Now the question is why?

There are two points:

* If it is overfitting, overfitting should have a positive effect because remembering values for all training states correctly is hardly a bad thing. (In fact, my training data is a *superset* of my testing data, so remembering should be fine.)
* If it doesn't fit, this begs a question what over-optimization really does. It doesn't seem to fit, but it does have a negative effect. How could that be?",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/bj4s4m/why_overfitting_is_bad_in_dqn/
"[Project] I trained a real robot to learn the ""puckworld"" game using RL",1556631753,,reinforcementlearning,diddilydiddilyhey,False,/r/reinforcementlearning/comments/bj3gn9/project_i_trained_a_real_robot_to_learn_the/
"RL Weekly 16: Why Performance Plateaus May Occur, and Compressing DQNs",1556615192,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/bj15fe/rl_weekly_16_why_performance_plateaus_may_occur/
My DQN is unable to solve LunarLander-v2 after 10k Episodes,1556587461,"Hello Reddit,

Over the past 2 days I have tried to solve the OpenAI gym environment LunarLander-v2. But I cannot seem to be able to solve it. I have not used any custom reward function, just the rewards that Gym provides

### **Hyper Parameters :**

|Metric|Value|
|------|-----|
|epsilon|1.0|
|epsilon_decay|0.9995|
|epsilon_min|0.01|
|gamma|0.99|
|learning_rate|0.0001|
|batch_size|64|
|replay_buffer|100k|

I have used a simple Dense Neural Network with 2 hidden layers and relu activation function
`state, 24, 36, action`

### **Metrics :**
Epsilon and Number of Frames : https://i.imgur.com/5B8CDkr.png
Loss and Reward : https://i.imgur.com/SDu6pjc.png  

I have tried varying the replay_buffer, batch_size, learning_rate, epsilon_decay and neural net but I haven't had much success. At this point I don't know if I should just continue training and hope it will eventually learn. Also I am quite the noob when it comes to reinforcement learning, but I understand Q-Learning and DQN very well. Any help or advice would be much appreciated.

Full Source Code : https://github.com/DollarAkshay/Artificial-Intelligence/blob/e1d4c82ee49dbccea74efb9710d732125bb7aa8e/OpenAI/LunarLander-v2/LunarLander_v2_DQN.py",reinforcementlearning,DollarAkshay,False,/r/reinforcementlearning/comments/bixg57/my_dqn_is_unable_to_solve_lunarlanderv2_after_10k/
"""ProductNet: a Collection of High-Quality Datasets for Product Representation Learning"", Wang et al 2019 {Amazon}",1556577230,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bivqd6/productnet_a_collection_of_highquality_datasets/
When is considered Mountain Car solved?,1556561511,"I am writing an essay and I am experimenting with the Mountain Car environment of OpenAI Gym.In their github site there are contradictory information about when is the challenge solved. 

On the leaderboard page ([https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0](https://github.com/openai/gym/wiki/Leaderboard#mountaincar-v0)), it states:

""*MountainCar-v0 defines ""solving"" as getting average reward of -110.0 over 100 consecutive trials.""*

&amp;#x200B;

An in the page of the problem ([https://github.com/openai/gym/wiki/MountainCar-v0](https://github.com/openai/gym/wiki/MountainCar-v0)) it says:Solved Requirements

Solved Requirements: None yet specified

&amp;#x200B;

As I need to have some citations, does anybody know some source to point to the right answer?

I already checked the Moore90 thesis and I couldn't find any answer there.",reinforcementlearning,RulerD,False,/r/reinforcementlearning/comments/bispc6/when_is_considered_mountain_car_solved/
Board Game Game Records Request,1556554737,,reinforcementlearning,cavedave,False,/r/reinforcementlearning/comments/birdht/board_game_game_records_request/
Why IMPALA has no bias correction term? (IMPALA vs ACER),1556526895,"&amp;#x200B;

I am looking for an algorithm with following conditions: 

1. off-policy
2. actor-critic
3. Works in discrete action space

It seems both IMPALA(Espholt et al., 2018) and ACER(Wang et al., 2017) satisfy the conditions, hence I've read both papers.

After reading the paper, I could not understand why IMPALA  has no bias correction term. 

Although both algorithm introduced truncated importance sampling technique for reducing variance, 

but only ACER added bias correction term to compensate the error incurred from importance weight clipping.  

[\(ACER\) The right term is bias correction term ](https://i.redd.it/tm8kw76ey5v21.png)

Therefore, I assume gradient estimate from IMPALA is biased, while gradient estimate from ACER is unbiased.

Furthermore, I guess that is why performance of the algorithm from IMPALA decreases as ""policy-lag"" increases.

I attached a figure E.1. from IMPALA paper.

[As the policy-lag \(the number of update steps the actor policy is behind learner policy\) increases, performance  of V-trace decreases.](https://i.redd.it/8m5avq6fx5v21.png)

&amp;#x200B;

Do I misunderstand any concept? 

Please help. Thank you.",reinforcementlearning,seungeun07,False,/r/reinforcementlearning/comments/bin4za/why_impala_has_no_bias_correction_term_impala_vs/
[R] Ray Interference: a Source of Plateaus in Deep Reinforcement Learning,1556508696,,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/bijxry/r_ray_interference_a_source_of_plateaus_in_deep/
[Research] Learning Finite State Representations of Recurrent Policy Networks | Deep Reinforcement Learning | Playing Pong with 3 states,1556498870,,reinforcementlearning,HeavyStatus4,False,/r/reinforcementlearning/comments/bii9xj/research_learning_finite_state_representations_of/
"[D] Does the gradient calculation for an LSTM have to be done in a loop, or can it be ""vectorized"" ?",1556492241,"I'm trying to use the AC method reported in [""Asynchronous Methods for Deep Reinforcement Learning""](https://arxiv.org/pdf/1602.01783.pdf) for a project. The relevant algorithm is shown in pseudocode at the bottom, Algorithm S3.

I'm using the same LSTM for the policy and value networks, but then two different Linear layers for the policy and value outputs (as they do). In pytorch:

```
output, hidden = self.rnn(input.view(1, 1, -1), hidden)
output_V = self.out_V(output.squeeze(1))
output_pi = torch.softmax(self.out_pi(output.squeeze(1)), dim=1)
```

At the beginning of an episode, I set the hidden state to all zeros. Then, I iterate through the episode, collecting an array of `V`, `pi`, and `R` (one entry for each step of the episode).

Here's the part I'm confused about. In the pseudocode for the paper, they have: [image of the relevant section](https://i.stack.imgur.com/RNhUl.png)

Does this gradient collecting/calculating have to be done in a loop, though? That is, could it just be done by creating a summed/discounted `R` array (e.g., if there were only 3 time steps, it would be `[R_0 + gamma*R_1 + gamma**2*R_2]`) and then calculating the gradients all at once by multiplying/etc the vectors?

I'm a little sketchy on backprop with LSTMs, but I've seen this done in a few implementations now so I think I must be missing something. thanks for any advice or ideas.",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/bih7bf/d_does_the_gradient_calculation_for_an_lstm_have/
Critic not converging in DDPG,1556456190,"I'm trying to control a multi rotor using DDPG, for whatever reason the actor is improving while the critic is not. I'm not really sure what could cause this.  
observations are: Velocity (X,Y,Z), Angle(X,Y,Z), Position Error (X,Y,Z), previous position error(X,Y,Z)

I've put a link to the github

 [https://github.com/Big-Beef/DDPG\_Drone/tree/Roll\_Pitch](https://github.com/Big-Beef/DDPG_Drone/tree/Roll_Pitch) 

The line to load a previous agent is because my computer died in training and I didn't want to lose where I'd got to",reinforcementlearning,hiro_ono,False,/r/reinforcementlearning/comments/biayin/critic_not_converging_in_ddpg/
Code examples of controlling multiple units with RL,1556446977,"Anyone knows a resources (papers, articles and especially repositories) regarding controlling multiple units with RL.

The controlled units should not be fixed, for example in Real Time Strategy  the agent builds various units (workers, soldiers ...) and later controls  them.  During the game various units could die and new ones are built.

I think good contemporary example is AlphaStar,  while OpenAI Five  controls just a single agent. This might be incorrect since I've never played those games.",reinforcementlearning,bobiblazheski,False,/r/reinforcementlearning/comments/bi9vft/code_examples_of_controlling_multiple_units_with/
How Recurrent Neural Networks Work: An Application and Algorithm-based Approach,1556403266,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/bi4afb/how_recurrent_neural_networks_work_an_application/
Creating Bitcoin trading bots that don’t lose money using deep RL,1556399942,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/bi3qmd/creating_bitcoin_trading_bots_that_dont_lose/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1556332792,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/bhunz8/interested_in_artificial_intelligence_machine/
[R] Real Human Scale Biped Robot Walking via Reinforcement Learning,1556307858,,reinforcementlearning,p-morais,False,/r/reinforcementlearning/comments/bhqk6t/r_real_human_scale_biped_robot_walking_via/
Hi we published an article about Modeling for Reinforcement Learning,1556203145,,reinforcementlearning,jkoendev,False,/r/reinforcementlearning/comments/bh9e7h/hi_we_published_an_article_about_modeling_for/
Modeling for Reinforcement Learning: Double pendulum on a cart,1556201751,"Hi I published an article about modeling for reinforcement learning

[https://medium.com/@JonasCoen/modeling-for-reinforcement-learning-and-optimal-control-double-pendulum-on-a-cart-394f46b7ec7e?source=friends\_link&amp;sk=b1d33aada3cf45844142563a3044b8c2](https://medium.com/@JonasCoen/modeling-for-reinforcement-learning-and-optimal-control-double-pendulum-on-a-cart-394f46b7ec7e?source=friends_link&amp;sk=b1d33aada3cf45844142563a3044b8c2)",reinforcementlearning,jkoendev,False,/r/reinforcementlearning/comments/bh956z/modeling_for_reinforcement_learning_double/
How do we test to make sure the RL models are working as they are supposed to?,1556201437,"This may seem like a stupid question, but I feel this is very important.

I have been able to implement these algorithms and have gotten them to work, but how do we know if they are working as they ought to. Since the actions it is trained on are stochastic, this seems like an impossible task.",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bh934y/how_do_we_test_to_make_sure_the_rl_models_are/
Summary: SimPLe,1556170334,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/bh4yz9/summary_simple/
c51 vs Dueling DDQN,1556144653,"I am in charge of creating an AI powered air hockey robot for the St. Louis science center. In order to learn how to play the game, I have looked into many q-learning algorithms. It seems that c51 and dueling are the best two. Although I understand the allure of dueling, it makes sense that q-values should be drawn from a distribution. For q-learning to learn the ""best"" policy, it would have to explore all the possible states. We all know that is impossible. This is why I think c51 would be better. I am curious what the community's opinion on it is.",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bh0w18/c51_vs_dueling_ddqn/
Candy Crush with RL?,1556143932,"Can people create an agent with reinforcement learning to play Candy Crush? I have already searched Google, but I have not yet achieved something concrete and as I am new in the area if someone can help me in what algorithms and subjects I need to study, I will be grateful!

&amp;#x200B;

I'm also interested in creating a real project that takes the Candy Crush from the browser and learns it by itself. But really I do not know if the Candy Crush problem can be solved with A.I",reinforcementlearning,rodrigoleite89,False,/r/reinforcementlearning/comments/bh0r8o/candy_crush_with_rl/
[D] Have we hit the limits of Deep Reinforcement Learning?,1556141195,,reinforcementlearning,RupaliBhati,False,/r/reinforcementlearning/comments/bh08j1/d_have_we_hit_the_limits_of_deep_reinforcement/
Theory behind exploration strategy for continuous action spaces,1556131774,"Hello guys,

getting more involved into continuous action spaces lately, one technique of exploration was used very often. They let the policy model output a mean and a standard deviation so you can sample from that Gaussian. It's clear to me that this technique adds exploration by involving randomeness in action sampling. 

&amp;#x200B;

However since the model is able to adjust both parameters why doesn't it just learn to output a standard deviation of 0 so it has more control over output? To make it clearer: What prevents it from decreasing exploration and increasing exploitation?

&amp;#x200B;

Sorry if this question is very basic, I'm currently new to continuous action spaces in DRL.

Thanks for any help though.",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/bgyec3/theory_behind_exploration_strategy_for_continuous/
Parallelized cross entropy method on CartPole and Pendulum,1556117860,,reinforcementlearning,ADGEfficiency,False,/r/reinforcementlearning/comments/bgvm35/parallelized_cross_entropy_method_on_cartpole_and/
"Constant, linearly decreasing, or exponentially decreasing epsilon greedy strategies?",1556114314,,reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bguxrj/constant_linearly_decreasing_or_exponentially/
Ways to minimize maximization bias in DQN networks,1556066127,"It is well known there exists this ""maximization bias"" with DQN networks. This can either cause models to converge slower to the optimal policy or it can cause models to converge to a non-maximal policy (essentially, it achieves a locally maximal policy instead of the global maximal policy). From what I can tell, there is no real way to mitigate this effect except with memory replay, and even then this bias is still there.

&amp;#x200B;

DQN networks use NNs to approximate the Q-value function, and NNs are not the best at function approximations. There is this error that comes with using the NNs. So, you will have two bias being summed up together, the maximization bias and the biases that are inherent to NNs.

&amp;#x200B;

In all literature I have found, it seems that no one has really considered this as a major problem. It is known that Batch Normalization and Dropout  help eliminate errors associated to NNs. Would it be wise to use either techniques in RL? We might not be able to do much about the maximization bias, but there are strategies to mitigate these other errors. I have tried to scour the internet and [archiv.org](https://archiv.org) for an answer, but there seems to be nothing on this issue.",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bgnzro/ways_to_minimize_maximization_bias_in_dqn_networks/
"[D] ""Observations from OpenAI's Five (Dota 2)"", jshek",1556056754,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bgmbqr/d_observations_from_openais_five_dota_2_jshek/
"""Exploring the Limitations of Behavior Cloning for Autonomous Driving"", Codevilla et al 2019",1556056659,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bgmb46/exploring_the_limitations_of_behavior_cloning_for/
Can you use Thompson Sampling instead of epsilon greedy for exploration/exploitation in Deep Q-Learning?,1556055477,"For my project, I have implemented DDQN, Dueling-DDQN, and c51 DDQN algorithms. Some use an epsilon greedy approach and others use a decreasing epsilon greedy approach for choosing actions. I was curious if you could replace this with Thompson Sampling. I mean, you can always replace stuff, but I have read that epsilon greedy approaches are theoretically some of the best because they ""converge"" to the correct policy. I did not know if Thompson Sampling would offer similar theoretical results.",reinforcementlearning,thetonus1150,False,/r/reinforcementlearning/comments/bgm3fv/can_you_use_thompson_sampling_instead_of_epsilon/
What sucked about the Deep RL Poster Sessions at NeurIPS 2018,1556025555,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/bggaco/what_sucked_about_the_deep_rl_poster_sessions_at/
How reinforcement learning can help in solving real-world problems?,1555984554,,reinforcementlearning,ai-lover,False,/r/reinforcementlearning/comments/bgajby/how_reinforcement_learning_can_help_in_solving/
Karpathy discusses use of Tesla car fleet for active learning of object classification &amp; trajectory prediction CNNs,1555976627,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bg96m1/karpathy_discusses_use_of_tesla_car_fleet_for/
Setting Standard Deviation of TRPO/PPO to zero during prediction,1555966558,"If I have a trained TRPO or PPO RL agent, what happens if std is set to 0 during prediction and we only estimate based on mean? In practice in many cases, it works better if we do that, but it not seem theoretically sound to me.

What are the theoretical flaws of deterministic prediction only based on mean? Where are the cases in practice that it could break things?",reinforcementlearning,matineh_sh,False,/r/reinforcementlearning/comments/bg7a4s/setting_standard_deviation_of_trpoppo_to_zero/
"Why is there no importance sampling in original DQN, even though it's off-policy algorithm?",1555951323,"Let's assume that we are using off-policy algorithm.

With that algorithm, we would calculate value function of target policy by sampling from behavior policy. But if we use the sample of behavior policy without any correction, the result would be the value function of behavior policy, not the target policy.

So we use importance sampling technique to correct it. By giving weights(in terms of target policy distribution and behavior policy distribution), we can make it unbiased(or biased, but bias converges to zero as continuing sampling) estimator.

But in case of DQN, we don't have such correction process. What am I missing now?",reinforcementlearning,wwiiiii,False,/r/reinforcementlearning/comments/bg49n7/why_is_there_no_importance_sampling_in_original/
Learning to Paint with Model-based Deep Reinforcement Learning,1555942918,,reinforcementlearning,hzwer,False,/r/reinforcementlearning/comments/bg2o57/learning_to_paint_with_modelbased_deep/
Can you use the mean instead of weighted average for TD lambda?,1555935872,"&amp;#x200B;

https://i.redd.it/5vukmt226tt21.png

&amp;#x200B;

It makes sense to find the weighted average of the returns for various N in TD(N) with weights that exponentially decay like shown in the picture if there were to be infinite number of returns 

&amp;#x200B;

But if there were to be a finite number of returns, i.e. the episode has a terminal state, then couldn't you just use the mean of those returns instead?",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/bg1hm3/can_you_use_the_mean_instead_of_weighted_average/
RL Weekly 15: Learning without Rewards: from Active Queries or Suboptimal Demonstrations,1555931663,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/bg0uex/rl_weekly_15_learning_without_rewards_from_active/
PPO continuous action space problems,1555842713,"Hey i'm currently using an pytorch ppo implementations from higgsfield/RL-Adventure-2 to train a robot driving in an complex enviroment. Unfortunately i have some problem with the continuous actions space. I trained the robot before via A3C model and a discret action space which was working fine. My problem is that at the start my mu for the normal distribution is 0.3 or something like that. After the first gradient descent update it's getting heigher very quickly like 10 up to 100 (my std is arround 1 most of the time). This is bad because my max allowed Robots speed is 1 (m/s) . I could introduce a Tanh() activation funktion in the last layer of the actor-critic, but then it outputs either 1 or -1 which is not very helpfull for learning a behaviors. My first thougth was that my learning rate is to heigh but it doesen't seem to have a big impact. Also i am giving extra negativ rewards for actions bigger than 1 but the algorithmen doesen't seem to care. Does someone knows what kind of problems such behavior can cause? Since it's for my university i can't upload the code.",reinforcementlearning,Piyt1,False,/r/reinforcementlearning/comments/bfnibx/ppo_continuous_action_space_problems/
Introducing Deep Reinforcement Learning,1555813268,,reinforcementlearning,AurelianTactics,False,/r/reinforcementlearning/comments/bfk3kl/introducing_deep_reinforcement_learning/
"""End-to-End Robotic Reinforcement Learning without Reward Engineering"", Singh et al 2019",1555720581,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bf6gxk/endtoend_robotic_reinforcement_learning_without/
"[N] OA5 AmA: ""Hello - we're the dev team behind OpenAI Five!"" [mostly about Arena]",1555712418,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bf54k6/n_oa5_ama_hello_were_the_dev_team_behind_openai/
Policy gradients: loss tends to zero but reward does not increase,1555704221,"Finishing up a project that varies the amount of fully connected layers in a network to observe its effect on average rewards in the OpenAI Gym environment. One and two hidden layers work fine for the problem. 

Four fully connected layers (200-&gt;100-&gt;50-&gt;25 neurons) does not, as I expected. It’s performance is barely better than an agent taking random actions, and I’m seeking to explain why as the final part of my analysis.

In a vanilla policy gradient approach, if the loss goes down but the reward does not increase, does this just mean that the network can accurately predict the outcome of the policy? As in it just settled on a crap policy? Or is the network too deep and this is something like a vanishing gradient problem?

Activation functions between layers are ReLU and output is softmax.",reinforcementlearning,colonel_farts,False,/r/reinforcementlearning/comments/bf3mlo/policy_gradients_loss_tends_to_zero_but_reward/
[D] Large-scale imitation learning/apprenticeship learning for self-driving cars?,1555686594,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bf079w/d_largescale_imitation_learningapprenticeship/
OpenAI Five Arena has begun: leaderboard of global human results against OA5 [current results: 444-0; best players' match length: 27m],1555639196,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bett3e/openai_five_arena_has_begun_leaderboard_of_global/
Convolutional layer output size,1555623905,"Hi people,

I'm stuck on a problem and was wondering if anyone would be able to provide me with some answers, as I have not found any when googling. In the following code snippet from ChainerRL they list the output of the last convolutional layer as 3136 when put into the fully-connected (linear) layer:

&amp;#x200B;

1. **class** NatureDQNHead(chainer.ChainList):
2.  """"""DQN's head (Nature version)""""""
3.  **def** \_\_init\_\_(self, n\_input\_channels=4, n\_output\_channels=512,
4.                  activation=F.relu, bias=0.1):
5.  self.n\_input\_channels = n\_input\_channels
6.  self.activation = activation
7.  self.n\_output\_channels = n\_output\_channels
8.         layers = \[
9.             L.Convolution2D(n\_input\_channels, 32, 8, stride=4,
10.                             initial\_bias=bias),
11.             L.Convolution2D(32, 64, 4, stride=2, initial\_bias=bias),
12.             L.Convolution2D(64, 64, 3, stride=1, initial\_bias=bias),
13.             L.Linear(**3136**, n\_output\_channels, initial\_bias=bias),                &lt;--------------HERE
14.  \]
15.  super(NatureDQNHead, self).\_\_init\_\_(\*layers)
16.  **def** \_\_call\_\_(self, state):
17.         h = state
18.  **for** layer **in** self:
19.             h = self.activation(layer(h))
20.  **return** h

&amp;#x200B;

How do you calculate that output? I need to know as normally None works fine in ChainerRL, as it automatically sets the variable to whatever the output is, but when trying to create an A3C model this is no longer ok. I need the output of the last convolutional layer for the following NN:

&amp;#x200B;

1. **class** QFunction(chainer.ChainList):
2.  **def** \_\_init\_\_(self, obs\_size):
3.  self.conv0 = L.ConvolutionND(2, None, 32, ksize=(8,8), stride=(4,4), pad=(3,3))
4.  self.conv1 = L.ConvolutionND(2, 32, 64, ksize=(4,4), stride=(2,2), pad=(3,3))  
5.  self.conv2 = L.ConvolutionND(2, 64, 128, ksize=(3,3), stride=(1,1), pad=(3,3))
6.  self.l0 = L.Linear(**None**, 512)                          &lt;------What should the input be at None?
7.         layers = \[
8.  self.conv0,
9.  self.conv1,
10.  self.conv2,
11.  self.l0
12.  \]
13.  super(QFunction, self).\_\_init\_\_(\*layers)
14.  **def** \_\_call\_\_(self, state):
15.         h = state
16.  **for** layer **in** self:
17.             h = F.relu(layer(h))
18.  **return** h

&amp;#x200B;

Notice that it also has padding, as it's needed to satisfy an assertion in ChainerRL of some arbitrary calculation of the parameters in the layer. If someone could answer this, I would greatly appreciate it.",reinforcementlearning,RealOden,False,/r/reinforcementlearning/comments/ber60a/convolutional_layer_output_size/
Visualizing stock trading agents using Matplotlib and OpenAI Gym,1555610119,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/beolhz/visualizing_stock_trading_agents_using_matplotlib/
[N] Lc0 chess engine [AlphaZero clone] Wins 'CCC 7: Blitz Bonanza' chess competition,1555609353,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/beog6z/n_lc0_chess_engine_alphazero_clone_wins_ccc_7/
What to do when the agent does nothing?,1555604068,"If an agent were to learn to play a game that even for humans, would receive positive rewards almost equally likely as negative rewards that are almost equally likely in magnitude, then the agent simply chooses to do nothing all the time. 

&amp;#x200B;

Aside from making the agent explore more, could scaling the positive rewards such that their magnitude is bigger than that of negative rewards be helpful? I'm not talking about normalizing the rewards in any sense, but intuitively, multiplying the rewards by a fixed constant b &gt; 1 if the reward is positive, would amplify it and make the agent more likely to take the action that leads to such reward.",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/benfcj/what_to_do_when_the_agent_does_nothing/
How to understand the math.,1555552551,"So over the past few weeks I have been dedicating myself to learning RL. I've been working with policy gradient methods and have implemented both a2c and a3c and got them to work. From what I saw online it seemed like the next logical progression was to move onto TRPO. However while the concepts in vanilla policy gradient made sense to me, I've been struggling quite allot in the past few days to make sense of the TRPO concepts. I understand the general idea, taking steps that don't change the policy too much. But the math behind optimizing this get so complicated so fast. I don't have any sort of numerical optimization background. What do I need to do to understand this math? What did you guys do? I could sit down with a numerical optimizations text book and learn it the hard way. But that almost seems like overkill. Am I supposed to understand the underlying math, or should I treat it like a black box? Currently I have no immediate plans for going into the RL field, but It's definitely something that I would like to do at some point. Will not having a solid understanding of this math hurt me later?",reinforcementlearning,sturdyplum,False,/r/reinforcementlearning/comments/beg11b/how_to_understand_the_math/
Definition of value function with entropy-extended rewards,1555529415,"Can't figure out how to rewrite the value function as an expectation over advantage function and value function, keep getting that the entropy is zero?

&amp;#x200B;

[http://mathb.in/32926](http://mathb.in/32926)",reinforcementlearning,arnekvist,False,/r/reinforcementlearning/comments/bebyxj/definition_of_value_function_with_entropyextended/
rom object representation,1555525506,"So i have a rom of a hacked super mario game (it has 2 players: Mario and Luigi). Feeding in the raw pixel data of this results in very poor rewards. I was wondering if there was a way to transform this ROM into tile representation.

Basically what this means, is that each sprite is converted into a single digit. The background is 0, the coins and powerups are 1, enemies 2, and so on. This has already been done for instances of Super Mario, but I was wondering how I could apply this to a new ROM with the same sprites.

How do I create a tile representation for a hacked version of Super Mario?

an example of tile representation can be found here:[video](https://www.youtube.com/watch?v=qv6UVOQ0F44)

So far, I have used OpenAI retro to run Mario",reinforcementlearning,nrmxndal,False,/r/reinforcementlearning/comments/beb7kv/rom_object_representation/
"""T-REX: Extrapolating Beyond Suboptimal Demonstrations via Inverse Reinforcement Learning from Observations"", Brown et al 2019",1555515047,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/be95vd/trex_extrapolating_beyond_suboptimal/
"""NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection"", Ghiasi et al 2019",1555512120,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/be8ll6/nasfpn_learning_scalable_feature_pyramid/
Need help with SARSA with Linear Value Func. Approx.,1555505543,"I've been trying to implement SARSA with LVFA. So far, I've implemented the following code, but it doesn't seem to work (doesn't converge to the correct Q factors for even the simplest of problems).

    def SARSA_LVFA(theta, phi, r, s, gamma = 0.5, a = 0.005):
        """""" SARSA algorithm with LVFA """"""
        limit = 10**5
        
        # choose action u from eps-greedy policy
        if np.random.random() &lt; 0.9:
            u = np.argmax(np.matmul(phi(s).T, theta))
        else:
            u = np.random.choice(U)
        
        for i in range(limit):
            phi_s = phi(s)                            # get features for current state s
            s_ = np.random.choice(S, p = f(s, u))     # perform action u (noisy model)
            phi_s_ = phi(s_)                          # get features for new state s_
            
            # choose action u_ from eps-greedy policy
            if np.random.random() &lt; 0.9:
                u_ = np.argmax(np.matmul(phi_s_.T, theta))
            else:
                u_ = np.random.choice(U)
            
            # caculate temporal difference delta
            td_target = r(s, u) + gamma * np.matmul(theta[:, u_].T, phi_s_)
            delta = td_target - np.matmul(theta[:, u].T, phi_s)
            
            # update feature weights
            theta[:, u] = theta[:, u] + a * delta * phi_s.T
            
            s = s_
            u = u_
            
        return theta

The TD update rule that I've implemented is the following (it is derived from stochastic gradient descent on TDBE from what I understand) :

[TD update with LVFA](https://i.redd.it/gazf09nujts21.png)

Any help with why my code doesn't work will be greatly appreciated! Thanks!

&amp;#x200B;

Also, some notes on the code:

* `U` is the action space and `S` is the state space.
* `theta` is a weight matrix of shape `len( phi ) x len( U )` , where `phi` is the feature (column) vector for a state `s`.
* You get your Q-matrix by simply doing `np.matmul(Phi.T, theta)`, where `Phi` is just a collections of all your feature vectors \[ `phi(s1)` | `phi(s2)` | … | `phi(sN)` \].
* Leave any other questions you might have in the comments!",reinforcementlearning,uakbar,False,/r/reinforcementlearning/comments/be7flm/need_help_with_sarsa_with_linear_value_func_approx/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1555439000,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/bdxb5h/interested_in_artificial_intelligence_machine/
"Google AutoML reaches 2nd place in a Kaggle competition [""Google’s AI Experts Try to Automate Themselves""]",1555433080,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bdw5g7/google_automl_reaches_2nd_place_in_a_kaggle/
What are the techniques to make RL stable?,1555432149,"Currently, I'm working on DQN, but other than prioritized experience replay, or double Q network, target Q network....etc

&amp;#x200B;

What are some technical tricks (not specific to any RL algo) I could apply generally to any RL algo to make it more stable? 

&amp;#x200B;

A few I could think of is to 

1) clip the reward 

2) huber loss or alikes for the Q loss instead of the typical mean squared version (for DQN, that would be minimizing the mean squared bellman error) 

3) NN's gradient clipping.",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/bdvyvi/what_are_the_techniques_to_make_rl_stable/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1555392929,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/bdq87d/interested_in_artificial_intelligence_machine/
Soft Actor-Critic implementation in Tensorflow 2.0,1555353957,"Hello guys,

about 2 month ago I've been implementing SAC in pytorch. I was quite satisfied with it since it was fast and working really well on various tasks. 

&amp;#x200B;

I tried translating my code into Tensorflow 2.0, which at first went really well as I was done with it in about 1 hour.

However it performs really bad. It is not just **really** slow it also doesn't seem to achive good results like my pytorch implementation did. Since I'm not experienced with Tensorflow in general, I don't know if I have a mistake regarding Tensorflow or a typo anywhere. 

&amp;#x200B;

I would really appreciate someone looking over it. 

&amp;#x200B;

    import gym
    import numpy as np
    import os
    import tensorflow as tf
    import tensorflow_probability as tfp
    from tensorflow import keras
    from tensorflow.keras import layers, optimizers
    import random
    
    
    LEARNING_RATE = 3e-4
    GAMMA = 0.99
    REPLAY_SIZE = 1000000
    HIDDEN_SIZE = 300
    BATCH_SIZE = 256
    MEAN_LAMBDA = 1e-3
    LOGSTD_LAMBDA = 1e-3
    Z_LAMBDA = 0
    TAU = 1e-3
    LOGSTD_MIN = -20
    LOGSTD_MAX = 2
    
    
    class NormalizedActions(gym.ActionWrapper):
        def _action(self, action):
            low = self.action_space.low
            high = self.action_space.high
            action = low + (action + 1.0) * 0.5 * (high - low)
            action = np.clip(action, low, high)
            return action
    
        def _reverse_action(self, action):
            low = self.action_space.low
            high = self.action_space.high
            action = action / (high - low) * 2 - low - 1
            action = np.clip(action, low, high)
            return action
    
    
    class ReplayBuffer(object):
        def __init__(self, size):
            self.size = size
            self.buffer = []
            self.ptr = 0
    
        def add(self, state, action, reward, next_state, done):
            if len(self) &lt; self.size:
                self.buffer.append(None)
            self.buffer[self.ptr] = (state, action, reward, next_state, done)
            self.ptr = (self.ptr + 1) % self.size
    
        def sample(self, batch_size):
            batch = random.sample(self.buffer, batch_size)
            s, a, r, ns, d = map(np.array, zip(*batch))
            return s, a, r, ns, d
    
        def __len__(self):
            return len(self.buffer)
    
        def save(self, path):
            buf = np.array(self.buffer)
            info = np.array([self.size, self.ptr])
            np.save(f""{path}/buffer.npy"", buf)
            np.save(f""{path}/info.npy"", info)
    
        def load(self, path):
            if os.path.exists(path):
                self.buffer = np.load(f""{path}/buffer.npy"").toList()
                self.size, self.ptr = np.load(f""{path}/info.npy"")
    
    
    class ValueNetwork(keras.Model):
        def __init__(self, obs_size, hidden_size):
            super(ValueNetwork, self).__init__()
    
            self.hidden_1 = layers.Dense(
                hidden_size, input_dim=obs_size, activation=""relu""
            )
            self.hidden_2 = layers.Dense(hidden_size, activation=""relu"")
            self.output_1 = layers.Dense(1, activation=None)
    
        def call(self, x):
            return self.output_1(self.hidden_2(self.hidden_1(x)))
    
    
    class SoftQNetwork(keras.Model):
        def __init__(self, obs_size, action_size, hidden_size):
            super(SoftQNetwork, self).__init__()
    
            self.hidden_1 = layers.Dense(
                hidden_size,
                input_dim=obs_size + action_size,
                activation=""relu""
            )
            self.hidden_2 = layers.Dense(hidden_size, activation=""relu"")
            self.output_1 = layers.Dense(1, activation=None)
    
        def call(self, state, action):
            o = tf.concat([state, action], axis=1)
            return self.output_1(self.hidden_2(self.hidden_1(o)))
    
    
    class PolicyNetwork(keras.Model):
        def __init__(self, obs_size, action_size, hidden_size, log_std_min, log_std_max):
            super(PolicyNetwork, self).__init__()
    
            self.lsmin = log_std_min
            self.lsmax = log_std_max
    
            self.hidden_1 = layers.Dense(
                hidden_size,
                input_dim=obs_size,
                activation=""relu""
            )
            self.hidden_2 = layers.Dense(hidden_size, activation=""relu"")
            self.mean = layers.Dense(action_size, activation=None)
            self.log_std = layers.Dense(action_size, activation=None)
    
        def call(self, state):
            h = self.hidden_1(state)
            h = self.hidden_2(h)
            mean = self.mean(h)
            log_std = self.log_std(h)
            log_std = tf.clip_by_value(log_std, self.lsmin, self.lsmax)
    
            return mean, log_std
    
        def forward_action(self, state):
            state = tf.expand_dims(tf.constant(state, dtype=tf.float32), axis=0)
            mean, log_std = self.call(state)
            std = tf.exp(log_std)
    
            # reparameterization trick
            z = mean + tf.random.normal(mean.shape, mean=0., stddev=1.) * std
            action = tf.tanh(z)
            action = tf.stop_gradient(action).numpy()
            return action[0]
    
        def evaluate(self, state):
            mean, log_std = self.call(state)
            std = tf.exp(log_std)
    
            normal = tfp.distributions.Normal(loc=mean, scale=std)
    
            z = mean + tf.random.normal(mean.shape, mean=0., stddev=1.) * std
            action = tf.tanh(z)
    
            log_prob = normal.log_prob(
                z) - tf.math.log(1 - tf.math.square(action) + 1e-6)
            log_prob = tf.reduce_sum(log_prob, axis=1, keepdims=True)
    
            return action, log_prob, z, mean, log_std
    
    
    def apply_clean(MODEL_DIR):
        if tf.io.gfile.exists(MODEL_DIR):
            print('Removing existing model dir: {}'.format(MODEL_DIR))
            tf.io.gfile.rmtree(MODEL_DIR)
        tf.io.gfile.mkdir(MODEL_DIR)
    
    
    class SAC(object):
        def __init__(self, env_name, model_dir=None, MAX_ITERATIONS=3000001, MAX_STEPS=1000, EVAL_FREQ=10000):
            self.MAX_ITERATIONS = MAX_ITERATIONS
            self.MAX_STEPS = MAX_STEPS
            self.EVAL_FREQ = EVAL_FREQ
            self.iteration = 0
    
            self.env = NormalizedActions(gym.make(env_name))
    
            self.a_dim = self.env.action_space.shape[0]
            self.o_dim = self.env.observation_space.shape[0]
    
            self.v_net = ValueNetwork(self.o_dim, HIDDEN_SIZE)
            self.v_target_net = ValueNetwork(self.o_dim, HIDDEN_SIZE)
            self.v_target_net.set_weights(self.v_net.get_weights())
    
            for target_v, v in zip(self.v_target_net.trainable_variables, self.v_net.trainable_variables):
                target_v.assign(v)
    
            self.q0_net = SoftQNetwork(
                self.o_dim, self.a_dim, HIDDEN_SIZE)
            self.q1_net = SoftQNetwork(
                self.o_dim, self.a_dim, HIDDEN_SIZE)
    
            self.pi_net = PolicyNetwork(
                self.o_dim, self.a_dim, HIDDEN_SIZE, LOGSTD_MIN, LOGSTD_MAX)
    
            self.log_alpha = tf.Variable(0.)
    
            self.replay_buffer = ReplayBuffer(REPLAY_SIZE)
    
            self.v_loss = keras.losses.MeanSquaredError()
            self.q_loss = keras.losses.MeanSquaredError()
    
            self.v_optim = optimizers.Adam(learning_rate=LEARNING_RATE)
            self.q0_optim = optimizers.Adam(learning_rate=LEARNING_RATE)
            self.q1_optim = optimizers.Adam(learning_rate=LEARNING_RATE)
            self.pi_optim = optimizers.Adam(learning_rate=LEARNING_RATE)
            self.alpha_optim = optimizers.Adam(learning_rate=LEARNING_RATE)
    
            self.entropy_target = - \
                tf.reduce_prod(tf.constant(
                    self.env.action_space.shape, dtype=tf.float32)).numpy()
    
            self.model_dir = model_dir
            self.checkpoint_dir = os.path.join(model_dir, ""checkpoints"")
            
            self.checkpoint_dir_v = os.path.join(
                self.checkpoint_dir, ""ValueNetwork"")
            self.checkpoint_dir_q_0 = os.path.join(
                self.checkpoint_dir, ""SoftQNetwork0"")
            self.checkpoint_dir_q_1 = os.path.join(
                self.checkpoint_dir, ""SoftQNetwork1"")
            self.checkpoint_dir_p = os.path.join(
                self.checkpoint_dir, ""PolicyNetwork"")
            self.checkpoint_dir_r = os.path.join(
                self.checkpoint_dir, ""ReplayBuffer"")
    
            self.checkpoint_prefix_v = os.path.join(self.checkpoint_dir_v, ""ckpt"")
            self.checkpoint_prefix_q_0 = os.path.join(
                self.checkpoint_dir_q_0, ""ckpt"")
            self.checkpoint_prefix_q_1 = os.path.join(
                self.checkpoint_dir_q_1, ""ckpt"")
            self.checkpoint_prefix_p = os.path.join(self.checkpoint_dir_p, ""ckpt"")
    
            self.checkpoint_v = tf.train.Checkpoint(
                model=self.v_net, optimizer=self.v_optim)
            self.checkpoint_q_0 = tf.train.Checkpoint(
                model=self.q0_net, optimizer=self.q0_optim)
            self.checkpoint_q_1 = tf.train.Checkpoint(
                model=self.q1_net, optimizer=self.q1_optim)
            self.checkpoint_p = tf.train.Checkpoint(
                model=self.pi_net, optimizer=self.pi_optim)
    
            self.checkpoint_v.restore(
                tf.train.latest_checkpoint(self.checkpoint_dir_v))
            self.checkpoint_q_0.restore(
                tf.train.latest_checkpoint(self.checkpoint_dir_q_0))
            self.checkpoint_q_1.restore(
                tf.train.latest_checkpoint(self.checkpoint_dir_q_1))
            self.checkpoint_p.restore(
                tf.train.latest_checkpoint(self.checkpoint_dir_p))
            self.replay_buffer.load(self.checkpoint_dir_r)
    
            # apply_clean(model_dir)
    
        def _update(self):
            state, action, reward, next_state, done = self.replay_buffer.sample(
                BATCH_SIZE)
            state = tf.constant(state, dtype=tf.float32)
            action = tf.constant(action, dtype=tf.float32)
            reward = tf.expand_dims(tf.constant(reward, dtype=tf.float32), axis=1)
            next_state = tf.constant(next_state, dtype=tf.float32)
            done = tf.expand_dims(tf.constant(done, dtype=tf.float32), axis=1)
    
            with tf.GradientTape(persistent=True) as tape:
                v = self.v_net(state)
                q0 = self.q0_net(state, action)
                q1 = self.q1_net(state, action)
                action_new, logprob, z, mean, logstd = self.pi_net.evaluate(state)
    
                alpha_loss = - \
                    tf.reduce_mean(
                        (self.log_alpha * tf.stop_gradient((logprob + self.entropy_target))))
    
                with tape.stop_recording():
                    alpha_grads = tape.gradient(alpha_loss, self.log_alpha)
                    self.alpha_optim.apply_gradients(zip([alpha_grads], [self.log_alpha]))
    
                alpha = tf.exp(self.log_alpha)
    
                q_new = tf.minimum(
                    self.q0_net(state, action_new),
                    self.q1_net(state, action_new)
                )
                v_target = q_new - alpha * logprob
                v_loss = tf.constant(0.5) * self.v_loss(v,
                                                        tf.stop_gradient(v_target))
    
                q_target = reward + GAMMA * \
                    (tf.constant(1.) - done) * self.v_target_net(next_state)
                q0_loss = tf.constant(0.5) * self.q_loss(q0,
                                                         tf.stop_gradient(q_target))
                q1_loss = tf.constant(0.5) * self.q_loss(q1,
                                                         tf.stop_gradient(q_target))
    
                pi_loss = tf.reduce_mean(alpha * logprob - q_new)
                pi_mean_loss = MEAN_LAMBDA * tf.reduce_mean(tf.square(mean))
                pi_logstd_loss = LOGSTD_LAMBDA * tf.reduce_mean(tf.square(logstd))
                pi_z_loss = Z_LAMBDA * \
                    tf.reduce_mean(tf.reduce_sum(tf.square(z), axis=1))
                pi_reg_loss = pi_mean_loss + pi_logstd_loss + pi_z_loss
                pi_loss = pi_loss + pi_reg_loss
    
                with tape.stop_recording():
                    q0_grads = tape.gradient(
                        q0_loss, self.q0_net.trainable_variables)
                    self.q0_optim.apply_gradients(
                        zip(q0_grads, self.q0_net.trainable_variables))
    
                    q1_grads = tape.gradient(
                        q1_loss, self.q1_net.trainable_variables)
                    self.q1_optim.apply_gradients(
                        zip(q1_grads, self.q1_net.trainable_variables))
    
                    v_grads = tape.gradient(v_loss, self.v_net.trainable_variables)
                    self.v_optim.apply_gradients(
                        zip(v_grads, self.v_net.trainable_variables))
    
                    pi_grads = tape.gradient(pi_loss, self.pi_net.trainable_variables)
                    self.pi_optim.apply_gradients(
                        zip(pi_grads, self.pi_net.trainable_variables))
    
                    for target_v, v in zip(self.v_target_net.trainable_variables, self.v_net.trainable_variables):
                        target_v.assign(target_v * (1. - TAU) + v * TAU)
    
            del tape
    
            return tf.reduce_mean(q_target).numpy()
    
        def train(self):
            while self.iteration &lt; self.MAX_ITERATIONS:
                s = self.env.reset()
                epoch_reward = 0
                epoch_return = []
    
                for i in range(self.MAX_STEPS):
                    a = self.pi_net.forward_action(s)
                    ns, r, d, info = self.env.step(a)
    
                    self.replay_buffer.add(s, a, r, ns, d)
                    if len(self.replay_buffer) &gt;= BATCH_SIZE:
                        ret = self._update()
                        epoch_return += [ret]
    
                    s = ns
                    epoch_reward += r
                    self.iteration += 1
    
                    if self.iteration % 50000 == 0:
                        self.checkpoint_v.save(self.checkpoint_prefix_v)
                        self.checkpoint_q_0.save(self.checkpoint_dir_q_0)
                        self.checkpoint_q_1.save(self.checkpoint_prefix_q_1)
                        self.checkpoint_p.save(self.checkpoint_prefix_p)
                        self.replay_buffer.save(self.checkpoint_dir_r)
                        print('saved checkpoint.')
    
                    # if self.iteration % self.EVAL_FREQ == 0:
                    #    self.writer.add_scalar(
                    #        ""info/eval_rewards"", self.test(), self.iteration)
    
                    if d:
                        break
    
                mean_epoch_return = np.mean(epoch_return)
    
                # self.writer.add_scalar(""info/epoch_reward"",
                #                       epoch_reward, self.iteration)
                # self.writer.add_scalar(""info/average_epoch_return"",
                #                       mean_epoch_return, self.iteration)
                print(
                    f""Iter {self.iteration} - Reward {epoch_reward} - Return {mean_epoch_return}"")
    
            export_path = os.path.join(self.model_dir, 'export')
            tf.saved_model.save(self.pi_net, export_path)
            print('Saved SavedModel for exporting.')
    
    
    
    if __name__ == ""__main__"":
        sac = SAC(""LunarLanderContinuous-v2"", model_dir=""LunarLanderContinuous-v2"")
        sac.train()

Thanks alot for any help I get.",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/bdjrdg/soft_actorcritic_implementation_in_tensorflow_20/
"OA: ""How to Train Your OpenAI Five"" [""800 petaflop/s-days and experienced about 45,000 years of Dota self-play over 10 realtime months""]",1555351076,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bdj582/oa_how_to_train_your_openai_five_800/
Explanation subtraction average advantage Dueling Architecture [Wang 2015],1555322124,"Why is the average or max advantage subtracted from the advantage when calculating the Q-value in the dueling architecture?

The (original) formula for computing the q-value from value and advantage is as follows: Q(s,a;theta,alpha, beta)=V(s;theta,beta)+A(s,a;theta,alpha). Where theta is the shared parameters of the two streams and alpha and beta the parameters of the advantage and value subnetworks respectively.

[Wang et al. 2015](https://arxiv.org/abs/1511.06581) use: Q(s,a;theta,alpha, beta)=V(s;theta,beta)+(A(s,a;theta,alpha)-average\_advantage). They give the explanation: given Q we cannot recover V and A... lack of identifiability is mirrored by poor practical performance when the original equation is used. Why does non-identifiability cause poor practical performance and why does introducing the subtraction of the mean or max increase optimisation stability?",reinforcementlearning,matigekunst,False,/r/reinforcementlearning/comments/bde5oi/explanation_subtraction_average_advantage_dueling/
RL Weekly 14: OpenAI Five and Berkeley Blue,1555318873,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/bdds8u/rl_weekly_14_openai_five_and_berkeley_blue/
"OA Final announcement: 'OpenAI Five Arena' will offer worldwide access to play against OA5, 18-21 April 2019",1555195430,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bcw8mm/oa_final_announcement_openai_five_arena_will/
"OA Final announcement: 'OpenAI Five Arena' will offer worldwide access to play against OA5, 18-12 April 2019",1555195301,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bcw7ux/oa_final_announcement_openai_five_arena_will/
Clear and Concise DDPG and TD3 Implementations in PyTorch,1555187789,,reinforcementlearning,AurelianTactics,False,/r/reinforcementlearning/comments/bcuy6l/clear_and_concise_ddpg_and_td3_implementations_in/
The Promise of Hierarchical Reinforcement Learning,1555183653,,reinforcementlearning,SaveUser,False,/r/reinforcementlearning/comments/bcu7hx/the_promise_of_hierarchical_reinforcement_learning/
"[N] OpenAI Five DoTA2 Finals match livestream has begun: match against OG, plus additional OA announcement at end",1555181164,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bctqmv/n_openai_five_dota2_finals_match_livestream_has/
Anyone interested in learning?,1555057724,"Hello, recently I have been reading the book by Richard S. Sutton and Andrew G. Barto. I am trying to grasp the concept and try the exercises. It would be amazing if anyone else is interested in discussing the same.",reinforcementlearning,bathon,False,/r/reinforcementlearning/comments/bcb5u0/anyone_interested_in_learning/
stepwise REINFORCE?,1555052892,"In the Sutton book, REINFORCE is explained to do updates after each episode is done. The returns are from my understanding discounted with gamma and then used to update the parameters of the algorithm.

I assume that the word EPISODIC means that the updates happen after each episode.

My question is simply: Would an algorithm with the same features as REINFORCE except that batch training is stepwise (let us consider parameters updates after 16 steps) still be the REINFORCE algorithm? Or does it have another fancy name?

&amp;#x200B;

\*\*Sidenote\*\*

I've worked about with REINFORCE and A2C but figured I had implemented REINFORCE with stepwise updates. For some reason, I find it to be much more stable then A2C (with entropy loss and action\_value as a baseline) and reaching a marginally better average reward, but I am still questioning why A2C would fall behind. The testbench is cartpole (Any other environments with FAST training that is to recommend?)",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/bcanl3/stepwise_reinforce/
Create Custom OpenAI Gym Environments From Scratch — A Stock Market Example,1555010074,,reinforcementlearning,notadamking,False,/r/reinforcementlearning/comments/bc3vvw/create_custom_openai_gym_environments_from/
Using AI to Solve Collaborative Challenges by Playing StarCraft,1554924561,,reinforcementlearning,adammathias,False,/r/reinforcementlearning/comments/bbq8um/using_ai_to_solve_collaborative_challenges_by/
"""Self-Adapting Goals Allow Transfer of Predictive Models to New Tasks"", Ellefsen &amp; Torresen 2019",1554920192,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bbpdbm/selfadapting_goals_allow_transfer_of_predictive/
Improving Robot Control With Residual RL,1554909930,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/bbnb4m/improving_robot_control_with_residual_rl/
Need help with features,1554904581,I'm trying to control a traffic signal's duration of phases in a SUMO simulation. Can anyone suggest me some features that I can extract and use in training traffic signal agent with Q-Learning with function approximation?,reinforcementlearning,kambleakash,False,/r/reinforcementlearning/comments/bbmbfp/need_help_with_features/
[R] Oriol Vinyals - Generating Visual Programs with Agents [Video],1554879927,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/bbj0wy/r_oriol_vinyals_generating_visual_programs_with/
"In a DQN, instead of having epsilon, can we use Q values as weighting?",1554842774,"My understanding of epsilon greedy is that we’ll have some epsilon (starting at 1 and going to some minimum). That percent of the time, we choose a random action instead of the optimal one predicted by the DQN. But instead of doing that, can we use the Q values as a soft max type weighting?

If we have 4 actions (UP, DOWN, LEFT, RIGHT), if the Q values are (1.42, -0.002, 0.45, -2), we can scale that. Since UP has the highest value, it’ll have the highest probability of being randomly sampled. RIGHT would have the lowest. 

It seems this way, the network would eventually learn to SERIOUSLY reward positive actions and still sample the negative ones a bit (for exploration).",reinforcementlearning,shamoons,False,/r/reinforcementlearning/comments/bbd570/in_a_dqn_instead_of_having_epsilon_can_we_use_q/
[P] Using Reinforcement Learning to Design a Better Rocket Engine,1554835512,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bbbnrs/p_using_reinforcement_learning_to_design_a_better/
Unexpected observation space for CartPole-v0,1554827716,"I'm surprised by the observation space I get through introspection for `CartPole-v0`.

According to [the official doc](https://gym.openai.com/docs/#spaces) here's what I should get:

https://i.stack.imgur.com/NOG6z.png

However here's what I get:

    print(env.observation_space.low)
    print(env.observation_space.high)
    #[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]
    #[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]

I'm using [the latest version of `gym`](https://pypi.org/project/gym/0.7.4/#history):

    !pip list|grep gym
    gym                 0.12.1   

Any idea what's going on?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/bba31k/unexpected_observation_space_for_cartpolev0/
"""Policy Gradient Search: Online Planning and Expert Iteration without Search Trees"", Anthony et al 2019 {DM/OA/GB}",1554818937,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/bb8djd/policy_gradient_search_online_planning_and_expert/
Scalable Muscle-actuated Human Simulation and Control(SIGGRAPH 2019),1554808287,,reinforcementlearning,tomatpasser,False,/r/reinforcementlearning/comments/bb6me9/scalable_muscleactuated_human_simulation_and/
RL and Control as Probabilistic Inference,1554806148,"Some papers emerged in this subfield the last years, and  last year Sergey Levine wrote a [Tutorial](https://arxiv.org/abs/1805.00909.pdf) on it. However, I've some troubles grasping the consequences of it. Let me explain. 

The theory starts by formulating the distribution of all possible trajectories p(\tau) and model their probability by making them proportional to the exponentiated return G you get on them. This essentially means, better trajectories are ""exponentially"" more likely then worse. So far so good. 

Then in principle you can start to define a optimal policy. Lets choose the policy \pi to be the one which results in zero KL divergence between \pi(\tau) and p(\tau). You essentially define your policy to be identical to p(\tau).
If you do this you end up with an policy optimization objective where you need to maximize the reward you get on the way as well as the entropy of the policy. This is used to give the method its name: ""Maximum Entropy reinforcement learning"". 

However, as far as I now the choice of the distance measure between the two distributions is absolutely arbitrary. Why not use a symmetric distance like the [Jensen–Shannon divergence](https://en.wikipedia.org/wiki/Jensen%E2%80%93Shannon_divergence)? As long as this distance is zero if p(\tau) = \pi(tau) the math works out. Sure the math does not fall so nicely in place and you do not end up with such a nice ""also optimize entropy"" result, but at its core it can be done in this way too. You could draw completly different conclusions. 

What did I miss? What are other more obvious benefits I did miss?",reinforcementlearning,hubert_schmid,False,/r/reinforcementlearning/comments/bb6ccc/rl_and_control_as_probabilistic_inference/
Why not just accumulate all the future rewards for DQN?,1554804512,"[https://arxiv.org/pdf/1710.02298.pdf](https://arxiv.org/pdf/1710.02298.pdf)

&amp;#x200B;

For Rainbow algorithm, when considering the target value, instead of just taking the current immediate reward, you accumulate N step future rewards which makes sense since multi N step rewards would be more accurate than the single N step. 

&amp;#x200B;

But then, why not just accumulate all the future rewards? Aside from slower training time, would it be better or worse? 

&amp;#x200B;

The main reason for using the double DQN, or distributional DQN....etc is due to overestimation bias and inaccuracies and noise in the target value due to such aformentioned imperfections in the target Q network. But by considering all the future rewards, there would be no uncertainties or inaccuracies in the target value, no?",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/bb64ui/why_not_just_accumulate_all_the_future_rewards/
"[D] Confused about ""env.is_done""",1554800416,"I want to share my confusion when implementing DQN with OpenAI gym. The confusion here is because we have **two different cases for which** `env.is_done = True`

1. when the env reaches a terminal state (e.g., the agent died),
2. when the env reaches the limit maximum number of steps.

&amp;#x200B;

We all know that terminal states are special because at a terminal state s' :  `Q(s, a) = reward`. While at a non-terminal state s':  `Q(s, a) = reward + gamma max_b Q(s', b)`

 

By default  `env=gym.make(""CartPole-v1"")` creates an env with a limit of maximum 500 steps. At the 500-th step, `env.is_done` will be `True`. So checking a terminal state with `env.is_done=True` isn't enough because we can wrongly label the 501-th state as a terminal state.

&amp;#x200B;

There are two ways to fix this problem: 

1. use `env=gym.make(""CartPole-v1"").unwrapped` which returns an env without the limit.
2. use the condition `env.is_done=True and not env._past_limit()`  for checking a terminal state.

&amp;#x200B;

Hope this will clear the same confusion for other people also :-)",reinforcementlearning,xcodevn,False,/r/reinforcementlearning/comments/bb5mzl/d_confused_about_envis_done/
"[D] Tiny question: R_1+R_2 cannot be factorized into Q_1+Q_2, right?",1554793183,"I'm sorry if this is an obvious thing but I don't think my head's working right now... The Q-values based on the summation of two reward functions (e.g. coming from task A and task B or an intrinsic and extrinsic reward function) cannot be expressed by the sum of two separate Q-functions learned on each reward function, but has to be learned by one Q-function on the sum, right?

I think my mind's being confused with the approach in [RND](https://arxiv.org/abs/1810.12894), but they're talking about the value function in an on-policy agent.",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/bb4skm/d_tiny_question_r_1r_2_cannot_be_factorized_into/
"Terminology Clarification in ""Hindsight Policy Gradient""",1554786180,"&amp;#x200B;

[What does this term mean?](https://i.redd.it/q7yfsdrh76r21.png)

Consider an environment such as Gym FetchReach-&gt; what does this term imply?

does it mean, for a given transition (of lets say 50 steps), consider each ""acheived\_goal"" at every step as a possible goal (so we have 50 goals?) and calculate the policy gradient according to the equations given?

What does p(g) mean exactly? and how do we calculate it?",reinforcementlearning,glkc93,False,/r/reinforcementlearning/comments/bb3wn6/terminology_clarification_in_hindsight_policy/
Exploration in RL: Workshop @ ICML 2019 on June 14-15,1554763389,,reinforcementlearning,AlexCoventry,False,/r/reinforcementlearning/comments/bb08vm/exploration_in_rl_workshop_icml_2019_on_june_1415/
"In a DQN, would it make sense to attenuate gamma a bit in each step?",1554752698,"If we have a known number of steps (say, 500), would it make sense to start gamma high (0.99) and then slowly decay it for each additional time step? Since, as you get closer to the end, the future value is less and less of a factor?",reinforcementlearning,shamoons,False,/r/reinforcementlearning/comments/bay6ub/in_a_dqn_would_it_make_sense_to_attenuate_gamma_a/
PPO takes a long time to train?,1554740082,"Hi guys,

&amp;#x200B;

I'm running a custom simple environment - though it does include probabilities computed on each step, which are slow by definition - via Ray's RLLib on EC2. I'm currently using 2 V60 GPU's and 32 cores, and each training cycle is taking me at least 5-6 seconds. Now my question is: these environments typically take a long time to converge - or I simply failed in hyper-parameter tuning, let me know if this is often the case hindering convergence horribly - requiring thousands of millions of runs. How on earth do researchers and companies afford such a thing? Even one million cycles, would represent roughly 58 days under this setup, not to mention the sheer cost. What am I seeing wrong? Is it merely a question of hardware capacity - they use hundreds of GPUs?

&amp;#x200B;

Will accept any kind soul's help on fixing this crazy convergence cost!",reinforcementlearning,Unless13,False,/r/reinforcementlearning/comments/bavpra/ppo_takes_a_long_time_to_train/
How to calculate State Space in Reinforcement Learning?,1554728611,"Let's say I have a state representation of s = (x,y). If x = \[0, 10\] (discretize by 1)  and y = \[0,20\], can we declare that the state space is equal to x\*y = 200?",reinforcementlearning,Jackborogar,False,/r/reinforcementlearning/comments/batjte/how_to_calculate_state_space_in_reinforcement/
Trouble with DDPG for Drone Control,1554715089,"I'm trying to control a drone in continuous action space using DDPG. Because it's a weird kind of drone (VTOL tilt tricopter) I've made it in Simulink and am now using the deep learning toolbox from MATLAB 2019 but not having much luck. I've tried simplifying the model so it's VERY similar to this one:  [https://harikrishnansuresh.github.io/assets/deep-rl-final.pdf](https://harikrishnansuresh.github.io/assets/deep-rl-final.pdf)   


Only difference is i'm ignoring battery constraints have replaced the observations \[x, y, z, x\_goal, y\_goal, z\_goal\] with \[x\_error, y\_error, z\_error\]. I've tried their reward function, and basing it entirely on the distance to the targer, both with no luck. My settings for batch size etc as given in their Table.1 have all been tried exactly the same as theirs, as well as messing around with the settings a bit and all with absolutely no success. 

&amp;#x200B;

I think it has something to do with the critic network, which always predicts the same average reward no matter what I seem to try. The average reward seems to be pretty much random though so who knows. 

Heres a link to my GitHub  [https://github.com/Big-Beef/DDPG\_Drone](https://github.com/Big-Beef/DDPG_Drone). I'm pretty new to RL and reddit so any help would be the great",reinforcementlearning,hiro_ono,False,/r/reinforcementlearning/comments/barmeh/trouble_with_ddpg_for_drone_control/
Approximate Dynamic Programming vs Reinforcement Learning?,1554685600,"Hi, I am doing a research project for my optimization class and since I enjoyed the dynamic programming section of class, my professor suggested researching ""approximate dynamic programming"". After doing a little bit of researching on what it is, a lot of it talks about Reinforcement Learning. They don't distinguish the two however. Does anyone know if there is a difference between these topics or are they the same thing?",reinforcementlearning,Spaceman776,False,/r/reinforcementlearning/comments/banjjb/approximate_dynamic_programming_vs_reinforcement/
Can gamma be greater than 1 in a DQN?,1554665616,"If I have a DQN, and I care A LOT about future rewards (moreso than current rewards), can I set gamma to a number greater than 1? Like 1.1 perhaps?",reinforcementlearning,shamoons,False,/r/reinforcementlearning/comments/bak46z/can_gamma_be_greater_than_1_in_a_dqn/
MountainCar with Actor Critic,1554647131,"Hello, 

&amp;#x200B;

i'm trying to implement an actor critic model in order to solve the MountainCar problem. The environment is not the Openai one, but an other that I have to use. 

All the files are on this link: [https://github.com/nbrosson/Actor-critic-MountainCar/](https://github.com/nbrosson/Actor-critic-MountainCar/) 

And there is only one file that I have to modify: [agent.py](https://agent.py)

&amp;#x200B;

While my [agent.py](https://agent.py) run without errors, my model is not learning. My model is based on this one: [https://github.com/nikhilbarhate99/Actor-Critic](https://github.com/nikhilbarhate99/Actor-Critic)

&amp;#x200B;

Have you got any recommandations??

&amp;#x200B;

Thank you so much !! ",reinforcementlearning,Nolwww,False,/r/reinforcementlearning/comments/bagqn5/mountaincar_with_actor_critic/
"My loss is going to zero, but my rewards aren't increasing that much",1554589392,"I have a pretty plain DQN with a MSE loss function. I'm measuring reward as a percent change between steps. At each step, my agent can take an action and gets a reward. After about 500 episodes (200 time steps each), my loss goes to: 0.00009, but my reward stays between (-0.01 and 0.04). I want my reward to continue to go up, closer to 0.3 or so.

Are there any best practices that I'm missing? Is this common? I want to switch to a huber loss, but before I do that, I want to understand the reasons behind why my loss and reward don't seem to be strongly correlated.",reinforcementlearning,shamoons,False,/r/reinforcementlearning/comments/ba9p8d/my_loss_is_going_to_zero_but_my_rewards_arent/
[Question] DQN algorithm does not work well on CartPole-v0,1554577113,"Hi,, I know it's quite harsh to read my code tho, I have been stuck at this issue for a couple of weeks and still not able to solve it... I just put the same question on StackOverflow so that if possible could anyone have a look at it and give me some piece of advice??

[https://stackoverflow.com/questions/55552366/dqn-algorithm-does-not-converge-on-cartpole-v0](https://stackoverflow.com/questions/55552366/dqn-algorithm-does-not-converge-on-cartpole-v0)",reinforcementlearning,Rowing0914,False,/r/reinforcementlearning/comments/ba7o55/question_dqn_algorithm_does_not_work_well_on/
What are some nice RL class project ideas in robotics?,1554499639,"&amp;#x200B;

https://i.redd.it/xy6pc77bjiq21.png

We have to pick one of the above robots for our RL class project (graduate level). Any ideas?

&amp;#x200B;

Thanks!

&amp;#x200B;

Note: No deep RL (more traditional approaches, like linear val func approx., etc, etc).",reinforcementlearning,uakbar,False,/r/reinforcementlearning/comments/b9wwh6/what_are_some_nice_rl_class_project_ideas_in/
"Could reward 0 be considerd ""Negative?""",1554489304,"Hi everyone,

&amp;#x200B;

So in the case of something like the taxi driver environment you would get a positive reward for dropping off a passenger and a negative reward for not dropping it correct, however taking the example that there are not any other negative rewards in the environment, a reward of 0 could then be considerd ""negative"" for him? I am asking this, because my network seems to be behaving quite difficult with an action that could give a positive or negative reward (it's given in the state though).

&amp;#x200B;

What are your thoughts on this?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/b9uvvj/could_reward_0_be_considerd_negative/
Open Source | DeepMind,1554486612,,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/b9ucdw/open_source_deepmind/
"""Scalable Muscle-actuated Human Simulation and Control"", Lee et al 2019",1554474105,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b9rsiv/scalable_muscleactuated_human_simulation_and/
Should I increase my target value for the terminal step of my DQN agent?,1554461472,"I have a fixed number of timesteps for my agent to do what he does and at each time step, I have something like:

target = reward + gamma * Q(sn, an)

However, at the last time step, I simple have:

target = reward

I then train my DQN with my current state and the target. However, if my gamma is high (close to 1), it seems that my final time step will have a target that’s almost half of what it would be in normal timesteps. 

So if I have 4 actions, and my predicted Q values for each action give me values like [-0.24, 0.34, 0.11. -.03], and in this case, I’d pick the 2nd action. 

So my target would be whatever my current reward is (say 0.01) plus a discounted 0.34. It’s the last time step that concerns me, however. As it would only be what my current reward is. 
",reinforcementlearning,shamoons,False,/r/reinforcementlearning/comments/b9po70/should_i_increase_my_target_value_for_the/
Animal AI Olympics starting in June,1554406191,,reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/b9h7nc/animal_ai_olympics_starting_in_june/
Action space in DQN,1554393104,"Hello guys, according to what is know right now, to the DQN will give us Q value on each action sampled from action space. And this is assumed that action space is discrete. So can we apply DQN to continuous action space ?",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/b9eg9c/action_space_in_dqn/
"When implementing a basic DQN, should I restrict actions to what's possible for each state?",1554391802,"As an example, if I'm playing Pong, I have 3 potential actions for each timestep: MOVEUP, DONOTHING, MOVEDOWN. If my paddle is already at the top, would it make sense when I'm picking an action to NOT allow it to pick MOVEUP?

I'm using epsilon greedy, so in both cases (random and network selected), I could disallow it. If it is randomly chosen, I could simply let it sample again. If my network chooses move up, I can move to the next highest value until a legal action is selected.",reinforcementlearning,shamoons,False,/r/reinforcementlearning/comments/b9e68e/when_implementing_a_basic_dqn_should_i_restrict/
Looking for raw learning curves data - OpenAI Baselines on MuJoCo,1554365911,"Hey,

In order to save some time, I was hoping there is a collection of raw data based on 5+ seeds for each MuJoCo env (v2) using the various OpenAI baselines implementations (PPO, A2C, TRPO, DDPG, ...).

OpenAI provide a link to [http://htmlpreview.github.io/?https://github.com/openai/baselines/blob/master/benchmarks\_mujoco1M.htm](http://htmlpreview.github.io/?https://github.com/openai/baselines/blob/master/benchmarks_mujoco1M.htm) however, these results are only partial and seem to be far from what other works achieve with these algorithms.

&amp;#x200B;

Thanks in advance :)",reinforcementlearning,chentessler,False,/r/reinforcementlearning/comments/b9a2c1/looking_for_raw_learning_curves_data_openai/
"""Deep COACH: Deep Reinforcement Learning from Policy-Dependent Human Feedback"", Arumugam et al 2019",1554309519,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b90b1f/deep_coach_deep_reinforcement_learning_from/
🏃 QWOP RL Agent improve suggestions.,1554276717,"I've in a rabbit hole for the past couple of weeks trying to implement an agent capable of efficiently playing QWOP.

Since I'm pretty new to RL I started with Vanilla Policy Gradients but I'm pretty sure there should be a better approach. 

My next step is to capture a better representation of input and then implement PPO.

here is the source [https://github.com/juanto121/qwop-ai](https://github.com/juanto121/qwop-ai)

If you have some suggestions on how to improve the agent please help me to build a world in which QWOP is a solved problem. ",reinforcementlearning,juanto121,False,/r/reinforcementlearning/comments/b8uwgt/qwop_rl_agent_improve_suggestions/
Crossposting from learnML; dont know which fits best,1554230663,,reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/b8n6km/crossposting_from_learnml_dont_know_which_fits/
Getting started for DRL,1554225978,"Hello Folks,

I  have started learning RL/DRL. I have been watching David Silver's course on Youtube also started CS234. I am finding difficult to digest the math fundamentals. Could anyone please provide me good links or resource to go over for the math to better understand RL.

&amp;#x200B;

Thanks in advance.",reinforcementlearning,chitrang6,False,/r/reinforcementlearning/comments/b8m72j/getting_started_for_drl/
Shared network for Actor+Critic: how to weight the two update contributions during back-prop?,1554213300,"Hi, I am training an actor-critic structure.

I can compute the  `critic_loss` and the `actor_loss`

I first started with two separate networks (one actor and one critic). Hence during the update phase, the parameters of each net are updated independently wrt their own loss (one for actor, one for critic).

One could also try to have **one single network** (actor and critic have **shared some layers at their beginning**), since both are trying to retrieve relevant info from the same state.

My question is: **How do I weight between the two gradient contributions when updating the single net parameters during the back-propagation?** With another hyperparameter? Any good practice about its value (i.e. give more importance to the actor or to the critic, organize some value scheduling over time, ...)?",reinforcementlearning,chauvinSimon,False,/r/reinforcementlearning/comments/b8jiyu/shared_network_for_actorcritic_how_to_weight_the/
How to transfer learning from a smaller state space to a bigger state space?,1554211633,"Dear r/reinforcementlearning,

I have a 10x10 gridworld with rewards and enemies. I have been teaching my agent 3x3 grid around itself with a small neural network. It learns to be reactive quite quickly by getting rewards and avoiding enemies in it's close vicinity. However, I want the perceptual field to progressively get larger, let's say first to 5x5 and than to 7x7. Is there any way for me to transfer what has been learned in the 3x3 partial view to make 5x5 training faster? I have been thinking about keeping the final layers of the network fixed and changing the input layer to a larger size and training only the input layer weights. What do you think?

Thank you for all your feedback!",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/b8j7o1/how_to_transfer_learning_from_a_smaller_state/
Having trouble with math for RL,1554190030,"I'm in my third year of Uni studying Computer science. 
I'm self teaching myself RL. Until now, I have completed
&gt; David Silver's RL course 
&gt; CS294-112 by Sergey Levine
&gt; Sutton and Barto RL book 

Currently I'm going through the deep RL bootcamp.

Throughout these courses, I have had problems understanding the math that they're describing (especially if they're going through a research paper)

Obviously, it's state of the art research, but I really wanna understand the optimization techniques, intuitions behind TRPO, DDPG etc. 

I've studied deep learning and I understand the math behind that. I've read the ISLR. Pretty much covered on the calculus, statistics, probability part. 

What math courses/books would you recommend for me to understand the math behind RL? 



",reinforcementlearning,l0gicbomb,False,/r/reinforcementlearning/comments/b8fz3y/having_trouble_with_math_for_rl/
Scheduled Intrinsic Drive: A Hierarchical Take on Intrinsically Motivated Exploration,1554181522,"Exploration in sparse reward reinforcement learning remains a difficult open challenge. Many state-of-the-art methods use intrinsic motivation to complement the sparse extrinsic reward signal, giving the agent more opportunities to receive feedback during exploration. Most commonly, these signals are added as bonus rewards, which results in the mixture policy faithfully conducting neither exploration nor task fulfillment for an extended amount of time. In this paper, we instead learn separate intrinsic and extrinsic task policies and schedule between these different drives to accelerate exploration and stabilize learning. Moreover, we introduce a new type of intrinsic reward denoted as successor feature control (SFC), which is general and not task-specific. It takes into account statistics over complete trajectories and thus differs from previous methods that only use local information to evaluate intrinsic motivation. We evaluate our proposed scheduled intrinsic drive (SID) agent using three different environments with pure visual inputs: VizDoom, DeepMind Lab and OpenAI Gym classic control from pixels. The results show a greatly improved exploration efficiency with SFC and the hierarchical usage of the intrinsic drives. 

A video of the experimental results can be found at [https://www.youtube.com/watch?v=4ZHcBo7006Y&amp;feature=youtu.be](https://www.youtube.com/watch?v=4ZHcBo7006Y&amp;feature=youtu.be).",reinforcementlearning,kopandy,False,/r/reinforcementlearning/comments/b8eqce/scheduled_intrinsic_drive_a_hierarchical_take_on/
Issues with A3C implementation.,1554167483,"I am making an a3c implementation for a project in one of my classes. And for the life of me I can't get it to work. I have read probably 10 different online implementations to try and find bugs on mine. I have scrutinized my code to a degree I didn't think was possible. At this point I could probably recite it by memory. It trains well enough on simple environments, such as the gym cartpole but more complex environments such as pong fail to make any progress. For some other environments (sc2 mini games) it learns good behaviors and scores highly, only to forget them completely. Is there anyone who could help guide me in what should I do. What are some common a3c pitfalls I may be falling too? Could it all just be hyper parameter tuning? Anyone with more experience that would be willing to answer some more complicated questions that I have.? What are some methods you use for effectively debugging code? At this point I'll take any advice.  I can't post/share my code because that would violate the rules of the project nor would I expect anyone to read through it and do my work for me. ",reinforcementlearning,sturdyplum,False,/r/reinforcementlearning/comments/b8ce5d/issues_with_a3c_implementation/
"RL Weekly 13: Learning to Toss, Learning to Paint, and How to Explain RL",1554163398,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/b8blh1/rl_weekly_13_learning_to_toss_learning_to_paint/
"""Optimal policy for multi-alternative decisions"", Tajima et al 2019",1554158168,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b8ag7e/optimal_policy_for_multialternative_decisions/
Stanford 2019 - Reinforcement Learning Intro Playlist,1554150735,,reinforcementlearning,fxidiot,False,/r/reinforcementlearning/comments/b88hc8/stanford_2019_reinforcement_learning_intro/
Bezos: Build your own reinforcement learning framework (Github),1554098449,,reinforcementlearning,gebninja,False,/r/reinforcementlearning/comments/b7y6z0/bezos_build_your_own_reinforcement_learning/
DQN fails to learn to sustainably manage the resource,1554085088,"Hey, based on DeepMind publication, I've recreated the environment and I am trying to make the DQN find and converge to an optimal policy. The task of an agent is to learn how to sustainably collect apples (objects), with the regrowth of the apples depending on its spatial configuration (the more apples around, the higher the regrowth). So in short: the agent has to find how to collect as many apples as he can (for collecting an apple he gets a reward of +1), while simultaneously allowing them to regrow, which maximizes his reward. (If he depletes the resource too quickly, he looses future reward ending up in a suboptimal solution).

&amp;#x200B;

However, the agent fails to converge regardless of fiddling with hyperparameters and it seems like I've ran out of plenty of options. Any ideas? I would be eternally grateful for any tips. Here you can find some more info:

[https://datascience.stackexchange.com/q/48322/70558](https://datascience.stackexchange.com/q/48322/70558)

&amp;#x200B;

Thanks!",reinforcementlearning,mw_molino,False,/r/reinforcementlearning/comments/b7w230/dqn_fails_to_learn_to_sustainably_manage_the/
[1902.06865] Hyperbolic Discounting and Learning over Multiple Horizons,1553990567,,reinforcementlearning,AurelianTactics,False,/r/reinforcementlearning/comments/b7hp8u/190206865_hyperbolic_discounting_and_learning/
How to do Fast Game Simulation for Reinforcement Learning in Unity (question),1553971607,,reinforcementlearning,Internal_Mark,False,/r/reinforcementlearning/comments/b7ej9z/how_to_do_fast_game_simulation_for_reinforcement/
"""Meta-Learning surrogate models for sequential decision making"", Galashow et al 2019 {DM} [neural processes]",1553828408,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b6s5kx/metalearning_surrogate_models_for_sequential/
[1903.01599v2] Learning Dynamics Model in Reinforcement Learning by Incorporating the Long Term Future,1553822069,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/b6r4fe/190301599v2_learning_dynamics_model_in/
"""AlphaX: eXploring Neural Architectures with Deep Neural Networks and Monte Carlo Tree Search"", Wang et al 2019",1553818708,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b6qjm3/alphax_exploring_neural_architectures_with_deep/
[N] Pre-train your RL agent with Behavior Cloning - Stable-Baselines v2.5.0 Released,1553806462,,reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/b6o6la/n_pretrain_your_rl_agent_with_behavior_cloning/
Raw 2D/3D state as input. CNN or flatten?,1553783600,"I'm working with a 100x100x5 input where the goal is to successfully learn an policy in the arbitrary environment. To learn this policy, I'm using DQN. I've read ton of papers where the authors use raw game state (lets say an image) of 80x80x3, and they apply the usual Mnih 15 architecture with 3 CNN layers with some set kernel size.

&amp;#x200B;

In my case the state is raw, and every ""pixel"" got meaningful (atleast to me :P) information, that I dont want to waste by abstracting through a wierd CNN configuration. Is there any guideline/best practices when it comes to handling such raw input data. And how would you recommend me structuring a network when having 100x100x5 **or** 50x50x5 input (or 25x25x5, my question is if you would do anything different in these cases to structure your net).

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/b6jij0/raw_2d3d_state_as_input_cnn_or_flatten/
Multiple Simultaneous Actions,1553770626,"I'm studying reinforcement learning right now and we are going through the Sutton and Barto book, and I've come across a problem that I can't conceptualize a satisfactory answer to. 

&amp;#x200B;

How can a single agent be designed to select multiple simultaneous actions? 

&amp;#x200B;

E.g. Let's say that you have a sports betting bot, and it should be capable of betting on any number of teams in a single timestep, or even betting for and against the same team in the same timestep, in the event that arbitrage is possible. 

&amp;#x200B;

The only solution that comes to mind is to run multiple agents in parallel, but this doesn't satisfy the requirements of the question.

&amp;#x200B;

Is it possible to structure this such that a single instance of the agent can perform any combination of the possible actions in a given timestep?

&amp;#x200B;

If so, what would such an architecture look like? 

&amp;#x200B;",reinforcementlearning,dorian821,False,/r/reinforcementlearning/comments/b6hcct/multiple_simultaneous_actions/
Meta-Reinforcement Learning,1553711041,,reinforcementlearning,pirate7777777,False,/r/reinforcementlearning/comments/b67qxk/metareinforcement_learning/
Need guidance on how to create a game AI,1553673799,"I want to create an AI to play a game that basically takes in the screen as an input similar to this https://imgur.com/a/pcpJUfE
and then the AI will read in the values of the numbers and click on the highest value,  which will then present another screen to click on the highest number again,  Im not sure if reinforcement learning is a good way to achieve this, or if other methods are more suited to creating a bot to play this game,  any help/guidance would be really appreciated",reinforcementlearning,demonslay677,False,/r/reinforcementlearning/comments/b61pfs/need_guidance_on_how_to_create_a_game_ai/
Why is it popular to perform FrameSkipping in Atari but not in MuJoCo?,1553651609,"In many papers that use Atari experiments they also perform frame skipping. Also in the DeepMind lab experiments shown in PlaNet they also used FrameSkipping. But I have yet to see anyone use this in MuJoCo.

&amp;#x200B;

My guess is that it's because the other environments(Atari/DM Suite) use images as observations",reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/b5yiiz/why_is_it_popular_to_perform_frameskipping_in/
MDP vs. state space model,1553646892,"In control theory, the state space model is usually used as the representation for system dynamics where the Markov decision process is used in the standard reinforcement learning literature. 
 There is a really fundamental difference in the worldviews associated with these models.  State space models are often derived from derived from first principles, whereas MDPs are purely statistical. 

However, it feels like one of these representations can be viewed as a more general version of the other, but I'm not sure which is which.

Do any of you feel that RL tasks, especially continuous control tasks could benefit from considering dynamics to be state space models rather than MDPs?",reinforcementlearning,TheJCBand,False,/r/reinforcementlearning/comments/b5xoqq/mdp_vs_state_space_model/
"""Unifying Physics and Deep Learning with TossingBot"" {G} [use of physics model for planning]",1553639577,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b5wc2a/unifying_physics_and_deep_learning_with/
Rationale behind policy and value 'heads' as a joint network architecture?,1553639542,"Is there any supporting rationale behind having multiple heads to the same TensorFlow graph---I think used by architecture's like DeepMind's AlphaZero---or is it just a performance optimization strategy?

It seems just on intuition that having weights shared between the two distinct function approximators would harm the performance of both, as the shared weights compete with each other, while the non-shared weights do not - meaning the 'meat' of the approximation is constrained to the non-shared parts (in some instances, a single FC layer). Is there any reason why we would expect that the representation necessary for value-approximation, would be exactly the same representation necessary for policy-approximation?

I ask because I'm considering merging my separate networks to a single network with two heads, but 1) while potentially improved performance would be nice, I'm not sure about the justification, and 2) I don't want to end up with higher variance as updates favouring a particular policy 'override' updates favouring a particular state-value. Perhaps this worry is relatively unfounded?

",reinforcementlearning,richard248,False,/r/reinforcementlearning/comments/b5wbtc/rationale_behind_policy_and_value_heads_as_a/
"""Inside Google’s Rebooted Robotics Program: In 2013, the company started an ambitious, flashy effort to create robots. Now, its goals are more modest, but the technology is subtly more advanced.""",1553639288,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b5w9zj/inside_googles_rebooted_robotics_program_in_2013/
"[N] ""OpenAI Five Finals"": live DoTA2 matches against world champs OG (+Blitz/Capitalist/ODPixel/Purge/Sheever) on 13 April 2019, 11:30AM PST/2:30PM EST",1553616640,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b5rlw2/n_openai_five_finals_live_dota2_matches_against/
Papers on safe imitation learning ?,1553589377,Could someone please refer me to any recent papers on safe imitation learning? I found a lot of work on safe RL but couldn't find much on this. Also what is state-of-the-art in safe imitation learning if someone is aware of it,reinforcementlearning,lifeadvicesponge,False,/r/reinforcementlearning/comments/b5n793/papers_on_safe_imitation_learning/
"For DQN, if the action with the maximum Q value is not allowed, then skip over that?",1553584189,"Typically, you'd have a NN where the output nodes give the Q values for its corresponding action 

&amp;#x200B;

but if such action is not allowed, then treat that node like it doesn't exist and choose the next allowable action with the greatest Q value? 

&amp;#x200B;

If many of them are not allowed regularly, how would that affect training? ",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/b5mlmd/for_dqn_if_the_action_with_the_maximum_q_value_is/
Learning to Paint with Model-based Deep Reinforcement Learning,1553577532,"Arxiv: [https://arxiv.org/abs/1903.04411](https://arxiv.org/abs/1903.04411)

Github: [https://github.com/hzwer/LearningToPaint](https://github.com/hzwer/LearningToPaint)

Abstract: We show how to teach machines to paint like human painters, who can use a few strokes to create fantastic paintings. By combining the neural renderer and model-based Deep Reinforcement Learning (DRL), our agent can decompose texture-rich images into strokes and make long-term plans. For each stroke, the agent directly determines the position and color of the stroke. Excellent visual effect can be achieved using hundreds of strokes. The training process does not require experience of human painting or stroke tracking data.

https://i.redd.it/atr00r38ceo21.png",reinforcementlearning,hzwer,False,/r/reinforcementlearning/comments/b5lpfl/learning_to_paint_with_modelbased_deep/
Learning to Paint with Model-based Deep Reinforcement Learning,1553576638,,reinforcementlearning,hzwer,False,/r/reinforcementlearning/comments/b5lkrr/learning_to_paint_with_modelbased_deep/
"""PEARL: Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables"", Rakelly et al 2019",1553550757,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b5h005/pearl_efficient_offpolicy_metareinforcement/
"""Monte Carlo Neural Fictitious Self-Play: Achieve Approximate Nash equilibrium of Imperfect-Information Games"", Zhang et al 2019",1553529140,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b5ccjx/monte_carlo_neural_fictitious_selfplay_achieve/
How would you design a reward function for sequential tasks ?,1553502922,"Hey ! 

&amp;#x200B;

I'm currently trying to train an agent in an environment where it has to perform sequential tasks. 

Let's say for instance that the agent has to physically reach for a target and then, carry it to a specific location that can change every episode. So far, with a negative, distance related reward for the first reaching part and a positive, distance related reward for the carrying part, the agent more or less succeeds in the task. It is rather efficient in the first reaching part, but barely moves towards the target when it has the object. To counteract this effect, I made sure the simulation sometimes starts from configurations where the agent already holds the object, but it didn't work as intended.

&amp;#x200B;

I'm wondering whether it would be more interesting to

* Train it in a curriculum fashion (first, master the reaching with a tailored reward and only then focus on the moving part), 
* Enhance the reward with bonus terms, such as the object's speed
*  Or something else I've not thought of. 

&amp;#x200B;

Any tips, ideas, advices ? Has anyone faced this type of problems before ? 

&amp;#x200B;

Thanks a lot ! ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/b580tn/how_would_you_design_a_reward_function_for/
Multitask Learning in DQN,1553458109,"Hello.
I have recently started learning about RL and have a project in mind to get some practical experience. But it involves multi task learning since I want to learn for multiple goals (one goal per episode though). I am going to use DQN but I am not sure how to incorporate multi task learning to it. Do I need to learn separate policy for all the goals? Or is there a way to condition the policy on given task input? 
Pointing to some resources (algorithm, paper, blog) will be helpful.
Thanks.",reinforcementlearning,HDidwania,False,/r/reinforcementlearning/comments/b50v1z/multitask_learning_in_dqn/
Representing non-fixed terminal states,1553447041,"Hi, I'm relatively new to reinforcement learning. I've been trying to implement td(0) to learn how to play a simple game, I've got the Sutton &amp; Barto textbook and have become a bit stuck. If our training agent is following a fixed policy, how do we represent terminal states which change in between episodes ?

Any help is much appreciated

Thanks",reinforcementlearning,HungrySurvivor,False,/r/reinforcementlearning/comments/b4ympg/representing_nonfixed_terminal_states/
[D] What is model free and model based learning?,1553396744,"While reading some blogposts I came across these terms model free and model based learning. Initially I thought its name was a result of usage of neural network models; model free (q-learning), model based (DQN) but it was not. 

Model free learning agent can be assumed as a child who crams up the lessons inorder to pass a exam and model based lerning agent is the child who understands without cramming up things. Is this right?",reinforcementlearning,begooboi,False,/r/reinforcementlearning/comments/b4rqhy/d_what_is_model_free_and_model_based_learning/
Where to start?,1553282001,"I have been learning and working on ML/DL for two years. Now, I want to start with reinforcement learning.
I am bit confused where to start learning it can any one share best resources available online for reinforcement learning.



 ",reinforcementlearning,m_aqeel,False,/r/reinforcementlearning/comments/b494jo/where_to_start/
1000 Feet overview of RL?,1553276215,"Hey all, I was wondering if anyone knew of any high-level overviews of RL and how everything ties together. E.g what you fit on one another (PPO can be fit onto the Actor and changes the default loss function/ updates etc). What can be used for continuous spaces, discrete spaces, and mixed action spaces?

&amp;#x200B;

E.g: an answer to another reddit post asking about ""[Conceptual Differences A2C and PPO](https://www.reddit.com/r/MachineLearning/comments/6unujm/d_conceptual_differences_a2c_ppo_reinforcement/dlu4cgt?utm_source=share&amp;utm_medium=web2x)"" which describes how there are 3 methods: policy-optimization, value-based methods, and actor-critic which aims to tie the two together, and then goes on to describe how PPO is an optimization method, and A2C is more like a framework.

&amp;#x200B;

There are so many terms flying around and the field seems to be changing so fast that I think this might be beneficial for myself and others if anyone knew of any",reinforcementlearning,ThrowawayTartan,False,/r/reinforcementlearning/comments/b47xxq/1000_feet_overview_of_rl/
"""Eighteen Months of RL Research at Google Brain in Montreal"", Marc Bellmare {GB}",1553274864,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b47o6j/eighteen_months_of_rl_research_at_google_brain_in/
largest possible number of actions for an RL model?,1553264544,"can anyone point me to examples of the deep rl models where the output layer has the most amount of working output neurons.  I don't care about the industry or application, I am trying to understand the limits of total freedom of a model. From all the papers I have read, the most successful examples I have seen  is  under 10 actions. And from my own experiments, I couldn't get any model to scale up past a few actions.  Do more traditional supervised models translate to RL in terms of output neurons? For example imagenet's output  layer has 1 thousand classes. I have not seen any successful RL model with 1 thousand possible actions.",reinforcementlearning,toisanji,False,/r/reinforcementlearning/comments/b45nf9/largest_possible_number_of_actions_for_an_rl_model/
Towards Characterizing Divergence in Deep Q-Learning,1553260320,,reinforcementlearning,ReinforcedMan,False,/r/reinforcementlearning/comments/b44wsq/towards_characterizing_divergence_in_deep/
[D] Evaluation policy for Q-learning agents with intrinsic reward,1553256829,"For methods which modify the reward to improve exploration like [Random Network Distillation](https://openai.com/blog/reinforcement-learning-with-prediction-based-rewards/), what is the proper way to evaluate their agents? For simpler exploration methods based on noise (e.g. epsilon-greedy or Gaussian noise), you can simply disable them in the evaluation phase but for these kinds of agents the Q-value itself is affected so I don't think you can easily get a ""pure"" version of the policy.",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/b44c3z/d_evaluation_policy_for_qlearning_agents_with/
"On ""Manipulation by Feel: Touch-Based Control with Deep Predictive Models"", Tian et al 2019 {BAIR}",1553189310,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b3tcag/on_manipulation_by_feel_touchbased_control_with/
"[N] PEARL: Meta-RL that is 20-100x faster than prior methods, with better final performance, using soft actor-critic and order-invariant context embedding",1553166245,,reinforcementlearning,smoke_carrot,False,/r/reinforcementlearning/comments/b3p6vq/n_pearl_metarl_that_is_20100x_faster_than_prior/
What are the standard tasks for evaluating reinforcement learning,1553145326,"Hi, I'm an undergraduate hoping to do an independent project in enforcement learning ultimately with the goal of presenting at a conference.

With that in mind I was wondering if there was a reference of common tasks for evaluation? I'm very much a beginner and will probably know more as I continue but I have a specific approach in mind and want to evaluate whether it's realistically applicable given existing task frameworks.",reinforcementlearning,Cartesian_Currents,False,/r/reinforcementlearning/comments/b3mn50/what_are_the_standard_tasks_for_evaluating/
Benchmarking TD3 and DDPG on PyBullet,1553137220,"[Here](https://github.com/georgesung/TD3) is a benchmark of TD3 and DDPG on the following [PyBullet environments](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#):
- HalfCheetah
- Hopper
- Walker2D
- Ant
- Reacher
- InvertedPendulum
- InvertedDoublePendulum

I simply used the [code from the authors of TD3](https://github.com/sfujim/TD3/), and ran it on the PyBullet environments (instead of MuJoCo environments). The TD3 and DDPG code were used to generate the results reported in the [TD3 paper](https://arxiv.org/abs/1802.09477).

*Motivation*:

I was trying to re-implement TD3 myself and evaluate it on the PyBullet environments, but soon realized there was no good benchmark to see how well my implementation was doing. When reading research papers, the algorithms are (almost?) always benchmarked on MuJoCo environments. As an individual, this is a problem:

- MuJoCo personal licenses are $500 USD per year for non-students.
- Even if I buy the license, the license is hardware-locked to 3 machines =( This means I cannot run MuJoCo experiments on AWS/GCP/etc. This problem also applies to the free personal student licenses, which are hardware-locked to 1 machine.

Fortunately, the authors of the TD3 paper have open-sourced their code, and IMO the code is very clearly written. I had some free Google Cloud credits lying around, so I decided to benchmark the TD3 authors' implementation of TD3 and DDPG on the PyBullet envs HalfCheetah, Hopper, Walker2D, Ant, Reacher, InvertedPendulum, and InvertedDoublePendulum -- the TD3 paper reports results from the MuJoCo version of those environments.

Hope this helps anyone in a similar situation!
",reinforcementlearning,georgesung,False,/r/reinforcementlearning/comments/b3ldq1/benchmarking_td3_and_ddpg_on_pybullet/
Assessing Generalization in Deep Reinforcement Learning,1553064243,,reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/b38p5f/assessing_generalization_in_deep_reinforcement/
"""Unmasking Clever Hans Predictors and Assessing What Machines Really Learn"", Lapuschkin et al 2019",1553034317,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b33udb/unmasking_clever_hans_predictors_and_assessing/
Google Brain SimPLe: Complete Model-Based Reinforcement Learning for Atari,1553013845,,reinforcementlearning,gwen0927,False,/r/reinforcementlearning/comments/b2zoz5/google_brain_simple_complete_modelbased/
"""Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset"", Zhang et al 2019",1553004310,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b2xt9w/atarihead_atari_human_eyetracking_and/
RL learning path,1552980758,"I know such questions have been asked before, but they focused too much on the RL part. I would like to know how should I start in order to get to a level that would allow me to enter in a research program.
Would it be a better idea to start revising the math, statistics and probability notions and then get into the rl part? Or should I just learn what I need as the notions are used in the books and courses about rl?
The first way seems the most logical path, but I would like to get more info on this.",reinforcementlearning,StefanGabriel,False,/r/reinforcementlearning/comments/b2uf9w/rl_learning_path/
Suggestion How to run Super-Mario-Bros with OpenAI baseline,1552949394,"Hi ,

I am working with different environments, Is there any suggestion in running mario with openAI baselines 

&amp;#x200B;

by using a custom simple code running this is not very difficult but using openAI baselines trying out to be a tricky one. I did try installing the pypi version ([https://pypi.org/project/gym-super-mario-bros/](https://pypi.org/project/gym-super-mario-bros/))  but didn't worked so far. 

&amp;#x200B;

 ",reinforcementlearning,alpha_ma,False,/r/reinforcementlearning/comments/b2pb20/suggestion_how_to_run_supermariobros_with_openai/
TF-Agents: 'official' RL library from and for TensorFlow,1552948034,"In case the 30 RL libraries we've already got was not enough, TensorFlow is now coming out with their own library, namely TF-agents:  
[Reinforcement Learning in TensorFlow with TF-Agents (TF Dev Summit '19)](https://www.youtube.com/watch?v=-TTziY7EmUA)  


[https://github.com/tensorflow/agents](https://github.com/tensorflow/agents)

&amp;#x200B;

While we're here, which library do you guys recommend for research? Either TFlow or PyTorch is fine; distributed is not a requirement, a flexible API is key. We've got so many: OpenAI/stable baselines, RLlib, RLGraph, Intel Coach, garage, DeepMind trfl, Google dopamine, and now TF-Agents god I don't know where to start.  
",reinforcementlearning,tsorn,False,/r/reinforcementlearning/comments/b2p1lj/tfagents_official_rl_library_from_and_for/
RLLib vs RLGraph ?!,1552923502,Both use Ray for distributed tasks. My use is mostly DQNs and PGs over weird tabular data (not video/audio). Which is better? I am doing basic production stuff not innovative research. ,reinforcementlearning,so_tiredso_tired,False,/r/reinforcementlearning/comments/b2k5sj/rllib_vs_rlgraph/
Which activation function are you using for actor network's output layer?,1552890242,"I'm specifically working on Deep Deterministic Policy Gradient and variants of it. 

&amp;#x200B;

For the Q network, since I do need to ""regress"" the ""target Q network""'s output value towards the target value (via minimizing the MSBE) (a process similar to DQN), it seems logical to use a linear activation function

&amp;#x200B;

But then, I also tried a linear function for the policy network, hoping that it might make the training faster since the extreme ends of (scaled and shifted) tanh function has gradient that's near zero. It does train fast, but it explodes. 

&amp;#x200B;

So then, which activation function for the policy network's output layer is the best? ",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/b2f8mf/which_activation_function_are_you_using_for_actor/
Why no importance sampling in DQN?,1552881282,"I understand why Q-learning does not need importance sampling in tabular case. Its update target dose not contain the bias introduced by the behavior policy. But in DQN, if we use supervised learning to train the network, the inputs (state-action pairs) are drawn from the distribution generated by the behavior policy. Unlike in tabular case, training on one state-action pair can affect other pairs when using a network. So if some state-action pairs are dominant under behavior policy, the supervised learning might tend to make more accurate prediction on these pairs and less accurate on other pairs which might be important under target policy. Wouldn't this be problematic?",reinforcementlearning,Pg-Lost,False,/r/reinforcementlearning/comments/b2e0xr/why_no_importance_sampling_in_dqn/
Gamblers problem does policy iteration work?,1552841607,,reinforcementlearning,henrikreddit,False,/r/reinforcementlearning/comments/b276ic/gamblers_problem_does_policy_iteration_work/
Why does Value Iteration work for the Gambler's Problem the way it does?,1552777288,"I was going through Sutton and Barto's ""Reinforcement Learning- An Introduction"". In Chapter 4, where they discuss Dynamic Programming techniques for solving basic RL problems, they discuss the Value Iteration algorithm and to demonstrate it they use the Gambler's Problem which is described as ... 

\&gt; A gambler has the opportunity to make bets on the outcomes of a sequence of coin flips. If the coin comes up heads, he wins as many dollars as he has staked on that flip; if it is tails, he loses his stake. The game ends when the gambler wins by reaching his goal of $100, or loses by running out of money. On each flip, the gambler must decide what portion of his capital to stake, in integer numbers of dollars. This problem can be formulated as an undiscounted, episodic, finite MDP. The state is the gambler’s capital, s belongs to the set {1,2,...,99} and the actions are stakes, a belongs to the set {0,1,...,min(s,100s)}. The reward is zero on all transitions except those on which the gambler reaches his goal, when it is +1.

[They get the following policy for p\_H = 0.4](https://imgur.com/a/uDJnV8I)

One of the exercises asks the reader ""Why does the optimal policy for the gambler’s problem have such a curious form? In particular, for capital of 50 it bets it all on one flip, but for capital of 51 it does not. Why is this a good policy?""

&amp;#x200B;

While looking for answers, I discovered Sutton's solution manual for the Chapter. He says that...

&amp;#x200B;

\&gt;In this problem, with p = 0.4, the coin is biased against the gambler. Because of this, the gambler want to minimize his number of flips. If he makes many small bets he is likely to lose. Thus, with a stake of 50 he can bet it all and have a .4 probability of winning. On the other hand, with stake of 51 he can do slightly better. If he bets 1, then even if he loses he still has 50 and thus a .4 chance of winning. And if he wins he ends up with 52. With 52 he can bet 2 and maybe end up with 54 etc. In these cases there is a chance he can get up to 75 without ever risking it all on one bet, yet he can always fall back (if he loses) on one big bet. And if he gets to 75 he can safely bet 25, possibly winning in one, while still being able to fall back to 50. It is this sort of logic which causes such big changes in the policy with small changes in stake, particularly at multiples of the negative powers of two.

&amp;#x200B;

Now, finally that I'm done with this wall of text, I present to you my question. How did the author interpret the final policy and arrive at the conclusion that he did?",reinforcementlearning,RealMatchesMalonee,False,/r/reinforcementlearning/comments/b1yiq8/why_does_value_iteration_work_for_the_gamblers/
What skills are you supposed to have to work as a Reinforcement Learning engineer/researcher?,1552725117,[removed],reinforcementlearning,TheHawkGriffith,False,/r/reinforcementlearning/comments/b1q9u7/what_skills_are_you_supposed_to_have_to_work_as_a/
[D] Random Network Distillation: Output layer for the target/prediction networks?,1552679384,"The paper (https://arxiv.org/pdf/1810.12894.pdf) doesn't say what the output layer of the RND networks look like... Do the target/prediction networks output a single neuron like a state-value function?

I feel like this is quite a glaring omission from the paper (unless I'm not reading properly and have missed it). Some further discussion here (https://towardsdatascience.com/reinforcement-learning-with-exploration-by-random-network-distillation-a3e412004402) states:

    &gt; These networks both get the states as input and output a vector. The output of the target network has nothing to do with the transitions, and is simply a random function of the game states.

I'm not sure if this helps, perhaps I just choose the same number of neurons as my action space, and then reduce to one value anyway. Has anyone implemented this recently and could point me in the right direction?",reinforcementlearning,richard248,False,/r/reinforcementlearning/comments/b1jcbs/d_random_network_distillation_output_layer_for/
Probability of picking a given action in REINFORCE,1552609065,"I am studying the REINFORCE method. I'm looking at this implementation: https://github.com/JamesChuanggg/pytorch-REINFORCE

I understand how the prob and log prob of the selected action are calculated with a discrete action space.

However I don't get the math in the case of a continuous action space: https://github.com/JamesChuanggg/pytorch-REINFORCE/blob/master/reinforce_continuous.py#L55

```
prob = normal(action, mu, sigma_sq)
log_prob = prob.log()
```

with `normal()` defined as:

```
def normal(x, mu, sigma_sq):
    a = (-1*(Variable(x)-mu).pow(2)/(2*sigma_sq)).exp()
    b = 1/(2*sigma_sq*pi.expand_as(sigma_sq)).sqrt()
    return a*b
```

Where does this formula come from?

Also, does this estimate the probability of picking precisely this value from the gaussian distribution? I don't understand how we can calculate the probability of picking a number (as opposed to an interval) from a distribution. ",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/b180na/probability_of_picking_a_given_action_in_reinforce/
"""The Bitter Lesson"": Compute Beats Clever [Rich Sutton, 2019]",1552602032,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b16pd4/the_bitter_lesson_compute_beats_clever_rich/
"""Simultaneously Learning Vision and Feature-based Control Policies for Real-world Ball-in-a-Cup"", Schwab et al 2019 {DM}",1552525447,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b0u1j0/simultaneously_learning_vision_and_featurebased/
"""A Generalized Framework for Population Based Training"", Li et al 2019 {DM} [PBT hyperparameter search]",1552525284,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b0u0hc/a_generalized_framework_for_population_based/
"""DeepMind and Google: the battle to control artificial intelligence"" [a history of DM]",1552523443,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b0tp5q/deepmind_and_google_the_battle_to_control/
Multi-agent gridworld environments,1552479790,"I've come across a couple of these environments but haven't had the time to work with any of them directly. There seems to be very little documentation on them and it seems quite difficult to customize. Has anyone had experience using any of these or is it more simple to build them from scratch? 

Thanks for your input",reinforcementlearning,hobbesfanclub,False,/r/reinforcementlearning/comments/b0ldf7/multiagent_gridworld_environments/
"""Stroke-based Artistic Rendering Agent with Deep Reinforcement Learning"", Huang et al 2019 {Megvii/Face++}",1552437604,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b0fjtb/strokebased_artistic_rendering_agent_with_deep/
"Fun fact: ""REINFORCE"" is an acronym",1552425443,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/b0d87b/fun_fact_reinforce_is_an_acronym/
[D] Are multiple agents (and creation of multiple trajectory samples for a particular policy) fundamental to Advantage Actor Critic + PPO?,1552414711,"It seems like the definitions of Advantage Actor Critic and A2C are quite muddled. To be clear---while these are not necessarily agreed upon by resources I find on the internet---my understanding is that:

* Advantage Actor Critic = A2C
* In A2C (with only one agent, if that matters), after each action-step we update the policy and value estimator that the next action-step uses
* The primary difference between this A2C and REINFORCE (with baseline) is the updates occur at each step, rather than at the end of each episode like in REINFORCE

I think it is correct to stay that A2C results in model updates that are very tightly-coupled with the current policy, i.e. it is very 'on-policy', thus for very long trajectories with very sparse reward (i.e., only at the end), it feels to me like A2C does very little exploration. For example, in my toy problem where I know that action A at each state is close-but-not-exactly-optimal, I see that the first episode takes action A in 80% of states, and from episode 2 onwards it takes action A in 100% of states.

Anyway, PPO is an interesting technique because we run multiple complete episodes, each using a different agent, and run minibatch gradient descent over the transitions of those trajectories. More agents means more trajectories, and therefore **more exploration**, at least early in the training when the policy is random. It seems like parallel agents, rather than being a performance boosting technique for the training, is actually potentially fundamental to the algorithm (that is: running one agent for 100 episodes likely performs worse than running 10 agents for 10 episodes, everything else equal).

Furthermore, by PPO only updating at the end of each episode and only using the transitions of one episode, we've returned to the supposedly inefficient REINFORCE, rather than A2C.

My question: if I can only run one 'agent' at a time, should I still sample multiple trajectories and pretend I have multiple agents, in order to enable the transition minibatch training?",reinforcementlearning,richard248,False,/r/reinforcementlearning/comments/b0b31p/d_are_multiple_agents_and_creation_of_multiple/
"""Backpropagation through time and the brain"", Lillicrap &amp; Santoro 2019",1552404303,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/b090nn/backpropagation_through_time_and_the_brain/
develop artificial general intelligence,1552401507,"here is a video on how to develop artificial general intelligence.

it is on youtube.

How to Keep Improving When You're Better Than Any Teacher - Iterated Distillation and Amplification.

it is on strong artificial intelligence research reddit.

I would like the labyia chabot which has it's websight to be improved with this.

it would be fun to talk to a artificial general intelligence on a websight.",reinforcementlearning,loopy_fun,False,/r/reinforcementlearning/comments/b08i12/develop_artificial_general_intelligence/
Scaling the A3C algorithm to multiple machines on Tensorflow.JS,1552393202,"Hello everyone :)

After spending some time playing with Reinforcement Learning for a project, some requirements led me to find a way to scale the A3C algorithm to multiple machines rather than limiting itself to multiple threads.

I understand my approach isn't the best and my code is not perfect, so feel free to let me know what you think and what I can improve :)

&amp;#x200B;

Here is the link to the post : [https://naifmehanna.com/2019-02-27-scaling-a3c-multiple-machines-tensorflowjs/](https://naifmehanna.com/2019-02-27-scaling-a3c-multiple-machines-tensorflowjs/)

&amp;#x200B;

Good reading :)",reinforcementlearning,naifmeh,False,/r/reinforcementlearning/comments/b074tj/scaling_the_a3c_algorithm_to_multiple_machines_on/
Completing my list of known RL algorithms!,1552390801,"Hi,

**Complete the list challenge!**

I have this fairly (in)complete list of known RL algorithms. The goal of this is to map which algorithms current exists.

I would be eternally grateful for additions to this list, Just the name is **good enough** *but i will certainly be happy for a link to the paper, additionally if its on-off policy and under which category the algorithm falls under (policy, value.. etc)*

    Dynamic Programming
    TD(λ)
    Q-Learning
    Dyna-Q
    SARSA
    Deep Q-Learning
    C51
    RAINBOW
    Generalized Advantage Estimation (GAE)
    Normalized Advantage Functions (NAF)
    Actor Critic using Kronecker-Factored Trust Region (ACKTR)
    Generative Adversarial Imitation Learning (GAIL)
    AlphaZero
    Actor-Critic with Experience Replay (ACER)
    Cross Entropy Method (CEM)
    REINFORCE
    Policy Gradients (PG)
    Deep Deterministic Policy Gradients (DDPG)
    Advantage Actor-Critic Algorithms 
    BFGS
    Distributed Prioritized Experience Replay (Ape-X)
    Importance Weighted Actor-Learner Architecture (IMPALA)
    Trust Region Policy Optimization (TRPO)
    Proximal Policy Optimization (PPO)
    Augmented Random Search (ARS)
    Evolution Strategies (ES)
    QMIX
    Advantage Re-Weighted Imitation Learnin (MARWIL)
    Soft Actor-Critic (SAC)

Also, if you find anything that should NOT be here, i'm grateful for input there as well :)",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/b06s5v/completing_my_list_of_known_rl_algorithms/
"RL Weekly 10: Learning from Playing, Understanding Multi-agent Intelligence, and Navigating in Google Street View",1552383168,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/b05s0q/rl_weekly_10_learning_from_playing_understanding/
Why not use JS divergence in TRPO ?,1552350917,I was reading about divergence metrics to compare 2 distributions and was wondering why the authors of TRPO didn't use Jensen Shannon divergence ? KL divergence gives a huge penalty for a sample more probable in q(x) than in p(x). whereas JS divergence relaxes such constraints. What this means is the new policy could possess a trajectory that was not common in the old trajectory. Am i missing something ? ,reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/b017hq/why_not_use_js_divergence_in_trpo/
"OA announces ""OpenAI LP"" [OA converting to a hybrid nonprofit/for-profit corporate model: original OA part-owner of new 'OpenAI-LP' corporation, a 'capped-profit' for-profit company]",1552320576,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/azv7sl/oa_announces_openai_lp_oa_converting_to_a_hybrid/
Policy-based model has oscillating final reward over episodes?,1552316798,"My understanding is that policy-based algorithms such as REINFORCE (with baseline/advantage), while being quite prone to local minima, should be rather deterministic - proceeding along a gradient to a minimum.

However, my results for this algorithm are showing an oscillation:

https://imgur.com/a/muh3MdV (note that the first run has fewer episodes, and that these are moving averages over 10 episodes IIRC)

I understand that the two runs look very different because they start with random initial conditions - for example, run 2 starts with a pretty good episode, and gets to the seemingly maximum reward quickly. However, once they reach a very good reward, why does the algorithm lose it? If the algorithm uses an neural network to output a probability map, then I understand that 'bad' actions have a (hopefully decreasing) probability of being taken, but I don't see why the next episode (it seems from the results) continues to take this 'bad' action, instead of correcting for it.

My main question is: how can I avoid this? Do you think it is a problem with exploration - too much, or too random? Perhaps REINFORCE is inherently susceptible to this? I was planning to implement Random Network Distillation to provide some systematic mechanism for exploration, but if even 'harmful' states are always going to be saved to future episodes, then perhaps increasing the tendency for exploration is going to be detrimental.

Any thoughts appreciated, thanks!

------
Some further details:
* I'm working on an unusual environment (not a game), where each action is a classification of an element in a set
* Input is features of each element, output is a probability map across the elements and their possible actions (so num_elements * num_classes)
* Termination is when all elements have been classified
* Each non-terminating state is thus a partial solution, so the action-space (num actionable elements) decreases as the state evolves
* Reward is returned by external analysis of the solution, centered on the mean reward from classifying all elements as class A, and normalized to the difference in reward when classifying all elements as class B (if num_classes is 2)
* Reward is thus sparse (0 reward until the termination state)",reinforcementlearning,richard248,False,/r/reinforcementlearning/comments/azuhxn/policybased_model_has_oscillating_final_reward/
"""MinAtar"": miniaturized version of Atari games on 10x10 grid",1552304676,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/azshb7/minatar_miniaturized_version_of_atari_games_on/
What is the current state of the art in Deep Reinforcement Learning?,1552302290,"This is a repost [of this question](https://www.reddit.com/r/reinforcementlearning/comments/8903jl/what_is_the_current_state_of_the_art_of_deep/) almost a year later.

How do you keep track of relevant new publications? How do you keep track of SotA algorithms?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/azs4me/what_is_the_current_state_of_the_art_in_deep/
"Why is Reward Engineering ""taboo"" in RL?",1552258177,"Reward engineering is an important part of supervised learning:

&gt; Coming up with features is difficult, time-consuming, requires expert knowledge. ""Applied machine learning"" is basically feature engineering.
— Andrew Ng

However my feeling is that tweaking the reward function by hand it is generally frowned upon in RL. I want to make sure I understand why.

One argument is that we generally *don't know* a priori what the best solution to an RL problem will be. So by tweaking the reward function, we may bias the agent towards what we *think* is the best approach, while it is actually sub-optimal to solve the original problem.

Another argument would be that it's conceptually better to consider the problem as a black box, as the goal is to develop a solution as general as possible. However this argument could also be made for supervised learning!

Am I missing anything?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/azllha/why_is_reward_engineering_taboo_in_rl/
The Effects of Memory Replay in Reinforcement Learning,1552224751,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/azfo9g/the_effects_of_memory_replay_in_reinforcement/
[D] How to test my RL algorithm against the Arcade Learning Environment?,1552220797,"Let's assume I have some ideas I want to test against the Arcade Learning Environment. I tried them out on OpenAi gym environments and they do well, but I feel that these problems are too simple to be meaningful.

What is the simplest way to run my algorithm against the full set of 57 Atari games, as is usually done in mainstream papers? How much time and money should I expect to spend per seed?

This seems to be the ""MNIST"" of RL, although more expensive than MNIST since RL algorithms are still generally so unpredictable.",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/azf6lh/d_how_to_test_my_rl_algorithm_against_the_arcade/
What is the intuition behind using reinforcement learning for tracking? And what other vision applications can see the benefits from RL,1552163035,,reinforcementlearning,SmartSpray,False,/r/reinforcementlearning/comments/az7nyl/what_is_the_intuition_behind_using_reinforcement/
OpenAI Baselines PPO2 doesn't work on MuJoCo environments,1552139515,"I'm looking in to using OpenAI baselines to run a bunch of experiments for comparing algorithms, and I've found that the PPO2 module doesn't work for MuJoCo environments.  I am attempting run it using commands like

python -m [baselines.run](https://baselines.run) \--env=HalfCheetah-v2 --alg-ppo2

I've found that it gets stuck on the return line for Model.train.  It can't run self.\_train\_op.  It just get's stuck forever and doesn't spit out any kind of error.  I've found that I can get it to work if I redefine self.\_train\_op by directly typing in the line 

self.\_train\_op = tf.train.AdamOptimizer(learning\_rate=LR, epsilon=1e-5).minimize(loss, var\_list=params)

bypassing the gradient clipping.  So I was thinking there was something wrong with the gradient clipping, however it does not work if I instead redefine self.\_train\_op with

self.\_train\_op = self.trainer.minimize(loss, var\_list=params)

which would allow it to use MPI.  This is weird because it doesn't even use MPI by default, so my line that does work should be doing the exact same thing as this one.  Also, for completeness' sake,

self.\_train\_op = tf.train.AdamOptimizer(learning\_rate=LR, epsilon=1e-5).apply\_gradients(grads\_and\_var)

doesn't work either, which is why I'm suggesting the problem is with the gradient clipping piece.

&amp;#x200B;

So, does it work for anyone else?  Anyone experience the same issue and found a solution?  Thanks!

",reinforcementlearning,TheJCBand,False,/r/reinforcementlearning/comments/az3oxf/openai_baselines_ppo2_doesnt_work_on_mujoco/
DeepMind relocating to expanded London HQ in 2020 (2-&gt;11 floors),1552065057,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aysnjm/deepmind_relocating_to_expanded_london_hq_in_2020/
is linear or soft clipping the best activation for output node for DDPG?,1552052681,"I've been using tanh activation for the output node for Deep Deterministic Policy Gradient, cause the agen'ts action and the reward is expected to not be so huge in magnitude. 

&amp;#x200B;

But in general, is linear or soft clipping the best activation? to allow for actions and rewards with huge magnitude? ",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/ayqf5k/is_linear_or_soft_clipping_the_best_activation/
Is there an attempt to combine K-FAC technique with Deterministic Policy RL?,1552040859,"Kronecker-Factored Approximate Curvature has shown to help improve the training accuracy and training time for neural networks in image classification tasks much much better than the common optimizers such as RMSprop. 

&amp;#x200B;

Recently, there was a successful combination of K-FAC with A2C algorithm, which is ACKTR which outperformed conventional A2C. 

&amp;#x200B;

These are for stochastic policy, but has there been an attempt to combine K-FAC optimizer with deterministic policy such as DDPG? ",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/ayorv6/is_there_an_attempt_to_combine_kfac_technique/
Should value function takes into account entropy regularization in maximum entropy RL?,1551961510,"We know that entropy regularization is used for promoting exploration. For A3C，entropy is used in loss and value function is estimate of accumulated external reward. For soft Q-learning, entropy is used as internal reward and value function is estimate of accumulated of sum of external reward and internal reward. Does this difference impact much? Is there any paper talking about the differences?",reinforcementlearning,linshiyx,False,/r/reinforcementlearning/comments/ayc2ik/should_value_function_takes_into_account_entropy/
What are the SOTA methods given perfect model of the environment?,1551949220,"Hi there,

&amp;#x200B;

I am currently researching a problem, where I have access to a  perfect model of the environment and can precisely calculate the (deterministic) transition to the next state and the reward given an action in any state. The state space is enormous though, and the number of potential actions relatively large, approximately 5000 actions in any given state.

&amp;#x200B;

It seems to me, that some state-value function approximation technique such as TD(lambda) could work, while e.g. DQN would fail, but I was wondering if anyone know other methods that are more well suited when given a perfect model of the environment.  Tips or links to research papers would be much appreciated!  


Thanks in advance :-)",reinforcementlearning,Peter_Emil,False,/r/reinforcementlearning/comments/ayakt9/what_are_the_sota_methods_given_perfect_model_of/
Reinforced Cross-Modal Matching &amp; Self-Supervised Imitation Learning for Vision-Language Navigation,1551888654,,reinforcementlearning,gwen0927,False,/r/reinforcementlearning/comments/ay0i72/reinforced_crossmodal_matching_selfsupervised/
[D] What libraries/frameworks do you use for casual reinforcement learning?,1551883745,,reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/axzm10/d_what_librariesframeworks_do_you_use_for_casual/
When can't we use off-policy training?,1551864207,"My understanding is that:

- Value-Based methods such as DQN, C51, Rainbow DQN **can** naturally be trained off-policy using a Replay Buffer, without having to account for any kind of off-policy correction.

- Deterministic Policy Gradient methods, such as DDPG or TD3 **can** be trained off-policy, as they don't integrate over actions. 

- Stochastic Policy Gradient methods **can't**, on principle, be trained off-policy, as they need to estimate the gradient using the current policy. However using importance sampling, you **can** correct that off-policiness, eg the way TRPO, ACER does.

So in the end... you can train from previous experiences in any case, right? Is importance sampling less efficient, in general, than ""true"" off-policy learning like we have with Value Function or DPG methods?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/axwxla/when_cant_we_use_offpolicy_training/
[D] State of the art Deep-RL still struggles to solve Mountain Car?,1551814431,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/axp63j/d_state_of_the_art_deeprl_still_struggles_to/
"""StreetNav: Learning To Follow Directions in Street View"", Hermann et al 2019 {DM}",1551813934,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/axp2no/streetnav_learning_to_follow_directions_in_street/
"""Efficient Reinforcement Learning with a Mind-Game for Full-Length StarCraft II"", Liu et al 2019 [learning in a simplifed turn-based SC2 with moves corresponding to learned options in Pang et al 2018, for transfer learning to the full game+options]",1551805442,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/axnfku/efficient_reinforcement_learning_with_a_mindgame/
Is CEM (Cross-Entropy Method) gradient-free?,1551791626,"I sometimes see CEM referred to as a gradient-free policy search method ([eg here](https://www.cs.upc.edu/~mmartin/URL/Lecture5.pdf)).

However, isn't CEM just a policy gradient method where instead of using an advantage function, we use `1` for elite episodes and `0` for the others?

This is what I get from the [Reinforcement Learning Hands-on](https://www.amazon.com/Deep-Reinforcement-Learning-Hands-Q-networks/dp/1788834240) book:

https://i.imgur.com/6yn4czZ.png

https://i.imgur.com/uwqhnrp.png",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/axl0p9/is_cem_crossentropy_method_gradientfree/
"""Autocurricula and the Emergence of Innovation from Social Interaction: A Manifesto for Multi-Agent Intelligence Research"", Leibo et al 2019",1551787314,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/axke7n/autocurricula_and_the_emergence_of_innovation/
Can you do partial minimization of MSBE for DDPG?,1551771999,"When talking about the original DDPG algorithm, you are to fully minimize mean squared bellman error before you use that Q function to update the policy.

Can I just run X number of gradient descent until I see some sign of convergence for minimizing MSBE? 

It's really hard to tell if the error is plateuing or not (which is a big problem in training a neural network in general) but i don't want to spend that much time training it either.

So can I just run a certain number of times using some optimizer like AdaDelta and then proceed to update the policy? 

Or is DDPG quite sensitive and unstable that you need to be so accurate? ",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/axikj7/can_you_do_partial_minimization_of_msbe_for_ddpg/
"RL Weekly 9: Sample-efficient Near-SOTA Model-based RL, Neural MMO, and Bottlenecks in Deep Q-Learning",1551759848,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/axgw20/rl_weekly_9_sampleefficient_nearsota_modelbased/
Neural MMO - A Massively Multiagent Game Environment,1551751980,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/axfpew/neural_mmo_a_massively_multiagent_game_environment/
"""Reinforcement learning in artificial and biological systems"", Neftci &amp; Averbeck 2019",1551735369,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/axcqax/reinforcement_learning_in_artificial_and/
Small-batch every step training vs Large-batch every n step training,1551734515,What are your experiences with this tradeoff ? ,reinforcementlearning,zQuantz,False,/r/reinforcementlearning/comments/axck60/smallbatch_every_step_training_vs_largebatch/
"RLgraph: Robust, incrementally testable reinforcement learning",1551733722,,reinforcementlearning,qu0d,False,/r/reinforcementlearning/comments/axcela/rlgraph_robust_incrementally_testable/
Transition from supervised NN to RL algorithm,1551731131,"Usually I create random agent which picks any legit action based on the data from the environment. Then I choose top N best games and train some simple neural network on it which will softmax action as response seeing input data from env.
[code sample](https://gist.github.com/bmwant/253596f4cf18a07a71d91c767f5800b3#file-carmax-py-L27)

How would you migrate from that approach to any RL algorithm and what is the good place to start?",reinforcementlearning,bmwant,False,/r/reinforcementlearning/comments/axbvua/transition_from_supervised_nn_to_rl_algorithm/
"""Efficient large-scale [ride-sharing] fleet management via multi-agent deep reinforcement learning"", Lin et al 2018 {Didi Chuxing} [discussion]",1551725574,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/axarh7/efficient_largescale_ridesharing_fleet_management/
"""Model-Based Reinforcement Learning for Atari"", Kaiser et al 2019 {GB} [considerably more sample-efficient than Rainbow DQN]",1551706530,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ax76wr/modelbased_reinforcement_learning_for_atari/
[Discussion] Masters in Operations Research,1551545299,"Hey people! I was just curious as to how a Masters in Operations Research compares to a Masters in Machine Learning or Computer Science for a PhD in reinforcement learning? I am quite new to reinforcement learning, but so far most of what I learnt is more related to OR than ML.",reinforcementlearning,Hari_a_s,False,/r/reinforcementlearning/comments/awjldm/discussion_masters_in_operations_research/
"Is Retrace (Munos, 2016) proved to converge with function approx. or only in tabular case?",1551500612,"It is shown in the paper that Retrace (lambda) is a constractive mapping this should imply convergence. Without further details, this should only mean in a tabular case. Would it hold true for function approximation case e.g. neural networks as well?",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/awdtru/is_retrace_munos_2016_proved_to_converge_with/
"Multiple Result images in a simple image for Paper, How ?",1551469198,"To show the flexibility and performance of either network or some model (policy agent) results of multiple data-sets have been put together in a simple image in recent papers also it saves a lot of space as well.

Any idea how do this in Latex ? or any other suggestions ...

&amp;#x200B;

The below image is from [A3C paper](https://arxiv.org/pdf/1602.01783.pdf)

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/ffnfbc112kj21.png",reinforcementlearning,alpha_ma,False,/r/reinforcementlearning/comments/aw8t66/multiple_result_images_in_a_simple_image_for/
Is it better to compare results between models based on total Training steps or Epochs?,1551457664,"Say I am trying to compare 2 different models - A3C and DQN - that have been training in an environment. 

The A3C model takes 6000 steps per epoch.

The DQN model takes 25,000 steps per epoch.

Is it better to compare these models via total training steps (eg/ 6 million steps), or via total trained epochs (eg/ 100 finished epochs).

Thanks for the help.

",reinforcementlearning,TheBrightman,False,/r/reinforcementlearning/comments/aw6skn/is_it_better_to_compare_results_between_models/
Has anyone ever interviewed for a position in Prowler.io?,1551456543,If someone knows their difficulty level and type of questions they ask and might want to share?,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/aw6lcn/has_anyone_ever_interviewed_for_a_position_in/
Does anybody implement mElo2 method mentioned in deepmind paper Re-evaluating Evaluation,1551411513,"According to the paper, mElo2 can handle cyclic games like rock-paper-scissors, and a reference implementation has presented in section E, but it doesn't say how to initialize matrix c. If we initialize it randomly,  elo scores will be exploded finally

&amp;#x200B;

paper: [https://arxiv.org/abs/1806.02643](https://arxiv.org/abs/1806.02643)

test code:

`import numpy as np`  


`def sigmoid(x):`  
 `if x &gt;= 2400.0:`  
 `return 1.0`  
 `elif x &lt;= -2400.0:`  
 `return 0.0`  
 `else:`  
 `return 1.0 / (1.0 + np.power(10, -1.0 * x / 400))`  


`def mElo2_update(i, j, p_ij, r, c):`  
`p_hat_ij = sigmoid(r[i] - r[j] + c[i, 0] * c[j, 1] - c[j, 0] * c[i, 1])`  
`delta = p_ij - p_hat_ij`  
`r_update = [16 * delta, -16 * delta]`  


   `c_update = [`  
`[+delta * c[j, 1], -delta * c[i, 1]],`  
 `[-delta * c[j, 0], +delta * c[i, 0]]`  
`]`  


   `r[i] += r_update[0]`  
`r[j] += r_update[1]`  


   `c[i, 0] += c_update[0][0]`  
`c[i, 1] += c_update[0][1]`  
`c[j, 0] += c_update[1][0]`  
`c[j, 1] += c_update[1][1]`  


`def predict_prob(i, j, r, c):`  
 `return sigmoid(r[i] - r[j] + c[i, 0] * c[j, 1] - c[j, 0] * c[i, 1])`  


`user_num = 10`  
`for i in range(1, user_num + 1):`  
 `for j in range(1, user_num + 1):`  
 `if i == j:`  
 `continue`  
 `print(""p_%d vs p_%d real prob: %f"" % (i, j, 1.0 * i / (i + j)))`  


`elo_r = np.zeros((user_num,))`  
`elo_c = np.zeros((user_num, 2))`  


`elo2_r = np.zeros((user_num,))`  
`elo2_c = np.ones((user_num, 2))`  


`match_num = 10000`  
`for _ in range(match_num):`  
`choices = np.random.choice(user_num, 2, replace=False)`  
`choices += 1`  
 `i, j = choices[0], choices[1]`  
`rand = np.random.random()`  
 `if rand &lt; 1.0 * i / (i + j):`  
`p_ij = 1.0`  
 `else:`  
`p_ij = 0.0`  
 `mElo2_update(i - 1, j - 1, p_ij, elo_r, elo_c)`  
`mElo2_update(i - 1, j - 1, p_ij, elo2_r, elo2_c)`  


`print(""####################"")`  


`print(""elo: %s"" % str(elo_r))`  
`for i in range(1, user_num + 1):`  
 `for j in range(1, user_num + 1):`  
 `if i == j:`  
 `continue`  
 `print(""p_%d vs p_%d elo prob: %f"" % (i, j, predict_prob(i - 1, j - 1, elo_r, elo_c)))`  


`print(""####################"")`  


`print(""elo2_r: %s"" % str(elo2_r))`  
`print(""elo2_c: %s"" % str(elo2_c))`  
`for i in range(1, user_num + 1):`  
 `for j in range(1, user_num + 1):`  
 `if i == j:`  
 `continue`  
 `print(""p_%d vs p_%d elo2 prob: %f"" % (i, j, predict_prob(i - 1, j - 1, elo2_r, elo2_c)))`

&amp;#x200B;",reinforcementlearning,vinowan,False,/r/reinforcementlearning/comments/aw02t0/does_anybody_implement_melo2_method_mentioned_in/
"""KataGo: Accelerating Self-Play Learning in Go"", Wu 2019 [adding auxiliary losses, exploration tweaks, global pooling, for much greater compute-efficiency in training to human-level]",1551392974,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/avx00z/katago_accelerating_selfplay_learning_in_go_wu/
Analysis of dataset with epsilon-greedy algorithm,1551391330,"Hi all,

I have a dataset in which subjects performed a 3-armed bandit task. I would like to fit data to an epsilon-greedy algorithm to see whether it can explain task performance.

Can anyone point me towards a resource that has the algorithm (written in python) that I'd be able to adapt to analyze my dataset using the features contained within (i.e., on each trial I have the action selected (A, B, or C), the feedback delivered (reward =1, no reward = 0) and the action selected on the next trial)?

I have found several examples of the algorithm but they simply simulate performance in an n-armed bandit whereas I was to take an existing dataset to see whether the algorithm can explain the performance in the task.

Many thanks,",reinforcementlearning,bigfuds,False,/r/reinforcementlearning/comments/avwp8b/analysis_of_dataset_with_epsilongreedy_algorithm/
How to give actions spatial arguments.,1551391272,So we are working on an rl agent. The environment has both spatial and non spatial actions. Currently what we do is have a policy network for action selected and a policy network for location to perform action. Is this a valid way to have it set up? It feels weird because the non spatial actions don't use the spatial network at all. Are there alternative ways to do this and can someone perhaps point me in the direction of some papers that discuss this. Thank you!,reinforcementlearning,sturdyplum,False,/r/reinforcementlearning/comments/avwos4/how_to_give_actions_spatial_arguments/
"""Long-Range Robotic Navigation via Automated Reinforcement Learning"": on Chiang et al 2018/Faust et al 2018/Francis et al 2019 {G}",1551381229,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/avuxd9/longrange_robotic_navigation_via_automated/
"How are ""rules"" generally encoded into RL agents?",1551379183,"Hi,

Generally whenever I'm coding up some RL agent I'm usually expecting to work in an environment where the set of actions that I can take never changes. Say I've got 10 possible actions at any point in time I can just have my output be of size 10 and use a softmax. Easy. However, if I consider a game of Chess there are cases where the agent loses the ability to make certain moves if it loses pieces OR the move that my agent selects is impossible, how exactly should I adjust my network such that it can't pick certain actions? 

Can someone point me to a repository which deals with this issue?

Thanks!

",reinforcementlearning,hobbesfanclub,False,/r/reinforcementlearning/comments/avuj6l/how_are_rules_generally_encoded_into_rl_agents/
"""Diagnosing Bottlenecks in Deep Q-learning Algorithms"", Fu et al 2019",1551367516,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/avsaro/diagnosing_bottlenecks_in_deep_qlearning/
[D] Stuck on a problem (soft actor-critic in a simple environment),1551308061,"Long post.

I am toying around in a simple environment. It is a number line with a 1D observation space and 1D action space in [-1, 1]. The agent starts around the origin and gets -0.01 reward when moving right and gets +1 reward when its position is &gt;90. Max steps is 100/episode.

I am using soft actor-critic for the agent. The problem is that its policy learns to output ""right"" when it is near position=90 but does not when it is near the origin. The optimal policy should be going right immediately but it does not learn this.

I think it's because the critic/Q network is not able to capture the miniscule difference in the value of going left vs going right when it is near the origin. Is there a way to deal with this? First thing I thought of was implementing some ""dueling network""-ish technique for DDPG/SAC, but it feels a little cumbersome.

[Image](https://i.imgur.com/VT0CBCo.png)

Different colors indicate different seeds. Left graph shows Q estimates (ignore red lines); solid lines are for action=+1 (right) and dashed lines are for action=-1 (left). You can see that the solid line is above the dashed line for &gt;50 for most seeds, but gets wonky when you get near 0. The right graph shows the outputs of the actor/policy networks. You can see that most of them learns to go right when &gt;50, but is less clear otherwise (actually, learns to go left :( ).",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/avjmi9/d_stuck_on_a_problem_soft_actorcritic_in_a_simple/
Jobs in RL without a PhD?,1551300490,"As a math MS student I picked up reinforcement learning as a fun hobby on the side. Since I don't want to pursue a PhD in any field I never thought that I'd have the chance to make a career out of it, and looking for job offers on sites like glassdoor and indeed it seems like I'm right: all the posts I saw were looking for experienced researchers.

&amp;#x200B;

Since it's now time for me to choose the topic for my graduate thesis I was wondering if it's the case to bother at all with RL, or maybe focus on some more accessible subfield of machine learning.

&amp;#x200B;

Do you know of someone who landed an RL job without a PhD?",reinforcementlearning,TylerPenderghast,False,/r/reinforcementlearning/comments/avi7yu/jobs_in_rl_without_a_phd/
Advantage Actor-Critic with Policy Network as PPO,1551290036,"Hi,

I want to try the PPO policy-network for reinforcement learning. It seems to me like the standard Actor-Critic algorithm would be good to use it in.

My understanding is that I need:
* A policy network that takes the state as input and outputs a probability map across states. This trains according to the clipped PPO loss function, together with the squared error loss from a state-value network, and an entropy bonus calculated on the policy network
* An 'old' policy network, that periodically takes its parameters from the policy network
* A state-value network, that takes the state as input and outputs a single average value for the expected future reward when in that state. This is necessary for the entropy bonus part of the PPO paper, as well as to calculate the advantage of a particular action
* A action-value network, that takes the state as input and outputs the expected reward for each possible action in that stat. This is necessary to act as the critic, sufficient to calculate the advantage.

That is 4 neural networks, or more like 3 which we actively train. 

Is this correct? It seems like most implementations of PPO have two networks (or 3 when including the old policy network). Thanks for any pointers...",reinforcementlearning,richard248,False,/r/reinforcementlearning/comments/avg966/advantage_actorcritic_with_policy_network_as_ppo/
epsilon-greedy in RL,1551285540,"A technique for tackling the Exploration-Exploitation Dilemma in Reinforcement Learning.

Read at: [https://prakhartechviz.blogspot.com/2019/02/epsilon-greedy-reinforcement-learning.html](https://prakhartechviz.blogspot.com/2019/02/epsilon-greedy-reinforcement-learning.html)",reinforcementlearning,prakhar21,False,/r/reinforcementlearning/comments/avffd6/epsilongreedy_in_rl/
Deep reinforcement Learning Nanodegree Program of Udacity,1551284910,"Hello guys,

Is there anyone who took ""Deep reinforcement Learning Nanodegree Program"" of Udacity ?   
I intend to join the course but still wonder if it worth the money.   
Would be appreciate to reiceive any advice or review.

  
Thanks a lot.  
",reinforcementlearning,nim8u5,False,/r/reinforcementlearning/comments/avfb8l/deep_reinforcement_learning_nanodegree_program_of/
"DQN learns a bit at first, but then worsens",1551267393,"Hi; idk if it's the right place to ask for help..?

I've implemented a quite standard DQN for a project in view to replicate DeepMind's original work on Atari. I did a lot of trial and error and finally got something that provided small results : I'm training the final version of my code on Pong; within the first 5 epoch (50k frames/epoch) it improved very nicely from -21 to -14 (30 episodes per test) but has now been training for 50 epoch and only oscillated downwards, reaching back some -20. (I'll try putting up the curve in coms)

Do you have any idea why this might happen; any similar experience ? I'll try summing up my parameters and methods :

* Original NN architecture (input: 4 84\*84 frames, 2 relu conv, 1 relu dense, output: n\_actions) which should be easier to train. I apply a mask to the output to select the actions during training. 
* Adam optimizer, learnrate=2.5\*10\^-4, optimizing MSE
* Using Keras and TensorFlow, using Nvidia GTX 1080
* Using OpenAI gym with [these wrappers](https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py), except the 4-frame stacking I did myself (theoretically working). Actions spammed for 4frames as in the paper
* 0.3\*10⁶ memory (my own limitations) filled with random behavior for 10k frames; minibatches of 32\*4 (agent selects 4 actions, then learns on bigger minibatch, for GPU parallelism)
* annealing exploration parameter epsilon linearly from 1 to .05 over 10⁶ frames (tried 1 to .1) then constant
* Reloading fixed-Q-targets as in the 2015 paper, every 4k frames
* Discount gamma at .99, test exploration at .05

If you have the time and kindness to try &amp; help, please don't hesitate to ask more about the setup. 

What bothers me is that it actually learns a bit, but at a bad level (reaching an average of 15 at Breakout) before degrading, which motivates my thinking that some bad hyper-parameter alchemy might be causing the problem...

Any thoughts ? Thanks in advance for any help.",reinforcementlearning,Naoshikuu,False,/r/reinforcementlearning/comments/avcjci/dqn_learns_a_bit_at_first_but_then_worsens/
PPO in Tensorflow?,1551251676,"Hey guys, I've seen a couple of implementations of the PPO algorithm in PyTorch floating around, but has anyone seen any implementations in Tensorflow? I'm trying to implement it by seem to have trouble applying the critic gradients to train the actor. Thanks in advance!",reinforcementlearning,shawnmanuel000,False,/r/reinforcementlearning/comments/aval9e/ppo_in_tensorflow/
Shared Autonomy via Deep Reinforcement Learning,1551215224,"Hello guys,

I've recently read the paper  ""Shared Autonomy via Deep Reinforcement Learning"" ([https://arxiv.org/abs/1802.01744](https://arxiv.org/abs/1802.01744)) and I was wondering if you guys know of any other paper going into this kind of field, because I could not find any. Additionally I would be very interested in another implementation (which I also could not find), because I found the papers very complex and difficult to understand.

Thanks for your help.

Jonas

&amp;#x200B;",reinforcementlearning,Fable67,False,/r/reinforcementlearning/comments/av4kzw/shared_autonomy_via_deep_reinforcement_learning/
Outcome of a Neural Network using standard Policy gradient method for continuous actions,1551215062,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/av4jsw/outcome_of_a_neural_network_using_standard_policy/
Monte Carlo actor critic algorithm,1551189054,"Has anyone implemented Monte Carlo on policy actor-critic algorithm or know a code base online and can share it?  
P.S: I am not sure if this is the right place to post this , sorry for that.  


https://i.redd.it/6dfwcx17zwi21.png",reinforcementlearning,gopal_chitalia,False,/r/reinforcementlearning/comments/auznxm/monte_carlo_actor_critic_algorithm/
Beginner RL Project Ideas,1551185199,"Hi all, I have been studying RL theory for a while, and now I would like to gain some practical experience in the field.

Can you suggest me some project ideas for newbies? For example, what was your first RL project? What should I make?

Thank you in advance! ",reinforcementlearning,pinkployd,False,/r/reinforcementlearning/comments/auz2oz/beginner_rl_project_ideas/
Question about n-step learning with DQN,1551170560,"Hello,

I'm reviewing the Rainbow paper and I'm not sure I understand how they can use DQN with multi-step learning, without doing any correction to account for off-policiness.

So. I understand how you can use 1-step update off-policy: the reward for a single transition doesn’t depend on the current policy, so you can reuse this experience in the future.

I also understand the point of n-step updates: it’s a trade-off between having high biais with 1-step update (as you get only limited information from a single transition) and having high variance with n-step update (as in that case both the policy and the environment can be stochastic, so you end up adding n random variables together).

What I do not get is how you can use n-step return off-policy, which is what the Rainbow DQN seems to do.  With n-step returns you are considering trajectories, and you can’t assume that these trajectories would have been taken if the agent was using the current policy. 

If I understood correctly, in the case of policy gradient this is dealt with using importance sampling, which will reduce the impact of policies which are further away from the current one. 

But I don’t see the equivalent of this for multi-step DQN?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/aux7a5/question_about_nstep_learning_with_dqn/
Application of Geotextile,1551166485,,reinforcementlearning,rohitgupta010,False,/r/reinforcementlearning/comments/auwouw/application_of_geotextile/
Tensorflow's normal distribution function gets slower and slower each time I sample...,1551107378,"So I'm using tf.distributions.normal to represent a policy, and when I run an experiment, the sample method gets slower and slower with each time step.  Has anyone else had this experience?  Is there a better way?",reinforcementlearning,TheJCBand,False,/r/reinforcementlearning/comments/aum7ud/tensorflows_normal_distribution_function_gets/
"I found two implementations of DDPG. Which one is ""better""?",1551091364,"There is one that seems like it's trying to train in an online learning fashion

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/6954o16v0pi21.png

where at each time step, you see a state, make an action, then sample a random minibatch, then update once 

&amp;#x200B;

And then there is this implementation 

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/fji2bay71pi21.png

where you play out entirely and given an entire episode, then for however many updates, you sample a random minibatch of that episode, then update. 

&amp;#x200B;

In terms of stability, training time....etc

&amp;#x200B;

which one is more common and which one is better? 

&amp;#x200B;",reinforcementlearning,qudcjf7928,False,/r/reinforcementlearning/comments/aujwl9/i_found_two_implementations_of_ddpg_which_one_is/
Anyone have any luck setting up mujoco on their machine?,1551072894,"I'm trying to work through Hands-on Reinforcement Learning with Python by Sudharasan Ravichandiran. I'm trying to setup the docker environment so I can run everything in the book. I've looked everywhere for the answer to this question and have gotten nowhere. I keep getting this error:   


&gt;Failed building wheel for mujoco-py  
&gt;  
&gt;Exception:  
&gt;  
&gt;You appear to be missing MuJoCo.  We expected to find the file here: /root/.mujoco/mjpro150  
&gt;  
&gt;This package only provides python bindings, the library must be installed separately  
&gt;  
&gt;Please follow the instructions on the README to install MuJoCo  
&gt;  
&gt;[https://github.com/openai/mujoco-py#install-mujoco](https://github.com/openai/mujoco-py#install-mujoco)  
&gt;  
&gt;Which can be downloaded from the website  
&gt;  
&gt;[https://www.roboti.us/index.html](https://www.roboti.us/index.html)""  
&gt;  
&gt;Which can be downloaded from the website  
&gt;  
&gt;[https://www.roboti.us/index.html](https://www.roboti.us/index.html)

&amp;#x200B;

I ran the code that he recommended already.  and that didn't fix it.   


The weird part is that I can go to /root/.mujoco/mjpro150 and see that mujoco is there with the license key.  
Please help, I've been trying to fix this for days. ",reinforcementlearning,ChemicalAffect,False,/r/reinforcementlearning/comments/auhjli/anyone_have_any_luck_setting_up_mujoco_on_their/
Summary: PlaNet – Arxiv Bytes – Medium,1551062925,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/aufzoe/summary_planet_arxiv_bytes_medium/
What do you think about the CARLA simulator?,1551062572,"Hey folks! I'm interested in a project in reinforcement learning and naturally I'm interested in self driving cars. I was wondering if anyone had experience using [CARLA](https://carla.readthedocs.io/en/latest/getting_started/) before? Is it a good simulator for someone with just a basic knowledge of RL? Any thoughts are greatly appreciated, thanks!",reinforcementlearning,rizzypillizy,False,/r/reinforcementlearning/comments/aufxmh/what_do_you_think_about_the_carla_simulator/
Recommendations for Reinforcement Learning Project,1551058286," Hey  guys I need some help selecting the broad area where I should focus my  attention for the next 2.5 months. I will be expected to do a reading  project and a final project in the topic of my choice in reinforcement  learning. I am using the link below as a rough breakdown for the field.

[https://spinningup.openai.com/en/latest/spinningup/keypapers.html](https://spinningup.openai.com/en/latest/spinningup/keypapers.html)

Can  someone please suggest what would be the easiest sub field to make a  project on. I will be expected to read multiple papers and write a  report on them for the reading project and come up with a novel idea for  the final project. Btw this is for a graduate level course. ",reinforcementlearning,sainoraider,False,/r/reinforcementlearning/comments/auf9cn/recommendations_for_reinforcement_learning_project/
Fully autonomous 1/18th scale race car for developers (Amazon race league),1551019808,,reinforcementlearning,fxidiot,False,/r/reinforcementlearning/comments/au8lv1/fully_autonomous_118th_scale_race_car_for/
An Intro to Reinforcement Learning - R Sutton &amp; A. Barto,1550967854,"Hello,

I'm quite interested by Reinforcement learning and thus reading the ""bible"" of the field as suggested by many: Reinforcement Learning An Introduction by R Sutton &amp; A Barto. Many exercices are proposed in the book and I'd like to discuss them. Anyone have read/is reading the book I can discuss with every now and then ? Maybe, after finishing the book we might share our discussion with the community ?

Let me know if interested !",reinforcementlearning,Cyalas,False,/r/reinforcementlearning/comments/au241v/an_intro_to_reinforcement_learning_r_sutton_a/
Disambiguating Q-Values,1550961312,"I'm working on Q-learning for an automated trading agent in Forex markets. I've hand picked 9-hours of trading to get the hang of certain parameters and how they affect overall learning.

My actions are to buy, do nothing and sell. When plotting the Q-values there are periods where all 3 are so close (the pairwise distances are nearly zero). 

Has anyone dealt with this kind of situation before ? Is there a way to post-process the Q-values in the network?

Thanks",reinforcementlearning,zQuantz,False,/r/reinforcementlearning/comments/au118h/disambiguating_qvalues/
[D] LeCun ISSCC19 slides : Use self supervising rather than RL,1550915159,,reinforcementlearning,yazriel0,False,/r/reinforcementlearning/comments/atttfw/d_lecun_isscc19_slides_use_self_supervising/
Is training a model on multiple seeds at once a good idea?,1550881623,"I am not sure how stupid this question is but lets say, if I am training an environment and setting the random seed, is it a good idea to choose a different seed at the start of every episode or I should select 1 seed and stick to it for the rest of the training and if not, how should a multi seed setting should be taken in account for optimal results?
",reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/atp0gk/is_training_a_model_on_multiple_seeds_at_once_a/
Yann LeCun Cake Analogy 2.0,1550860017,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/atkzxf/yann_lecun_cake_analogy_20/
DeepMind MuJoCo Multi-Agent Soccer Environment,1550834187," DeepMind released an [environment](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer) as part of their paper, [Emergent Coordinated Multi-Agent Behaviors through Competition](https://sites.google.com/view/emergent-coordination/home) (ICLR 2019)

DeepMind MuJoCo Multi-Agent Soccer Environment:

[https://github.com/deepmind/dm\_control/tree/master/dm\_control/locomotion/soccer](https://github.com/deepmind/dm_control/tree/master/dm_control/locomotion/soccer)

Emergent Coordination Through Competition:

[https://arxiv.org/abs/1902.07151](https://arxiv.org/abs/1902.07151)",reinforcementlearning,RhtSingh,False,/r/reinforcementlearning/comments/atgd3g/deepmind_mujoco_multiagent_soccer_environment/
"[N] Oriol Vinyals talk on ""AlphaStar: Mastering the Real-Time Strategy Game StarCraft II"" at Boston University, 11 March 2019",1550788293,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/at9g7a/n_oriol_vinyals_talk_on_alphastar_mastering_the/
[R] Recurrent Experience Replay in Distributed Reinforcement Learning,1550734353,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/at0d7i/r_recurrent_experience_replay_in_distributed/
[R] World Discovery Models,1550733737,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/at0a7t/r_world_discovery_models/
"[R] ""Deep Learning for Video Game Playing"" (updated version)",1550706854,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/asvs4g/r_deep_learning_for_video_game_playing_updated/
Is reinforcement learning usefull in a deterministic environment?,1550660135,"OpenAI Gym has many environments which are fully deterministic, like mountain car, lunar lander, the robotics environents etc.

&amp;#x200B;

In such an environment what are the advantages (and distadvantages) of using reinforcement learning instead of

\- a planning algorithm like A\* or Graphplan if there are descrete action/state spaces

\- a linear optimization algorithm like grad. descent (or simthing more elaborate) or a nonlinear optimization algorithm or an optimal control approach...

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,-50k4,False,/r/reinforcementlearning/comments/asn03h/is_reinforcement_learning_usefull_in_a/
TD3/DDPG time to obtain reasonable results.,1550658266,"Hi all,

&amp;#x200B;

I am using a personally modified version of TD3 to work in the parameterized action space ([https://arxiv.org/abs/1511.04143](https://arxiv.org/abs/1511.04143))  in a self-made environment. I've tried to find details of how long does it take to train the TD3 or DDPG methods but I couldn't find any, so I decided to ask reddit community to see if they could have any input on this. So you know, I am using a Geoforce GTX 1080 Ti with a i7, and I've been running my simulations for two weeks now and they are just in episode 300k, whereas TD3's and DDPG's paper train for up to 1M episodes, is this normal? Nvidia-smi show on average a 75% GPU-util utilization with my processes. The good part is that the overall reward is increasing that is why I haven't stopped them.

&amp;#x200B;

Many thanks!",reinforcementlearning,kashemirus,False,/r/reinforcementlearning/comments/asmrxq/td3ddpg_time_to_obtain_reasonable_results/
Can someone explain to me the running reward used here?,1550599464,"I was looking at the actor-critic implementation in pytorch (https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py) and for the life of me, I can't figure out where line 101 comes from.  

    running_reward = running_reward * 0.99 + t * 0.01)
I have read the implementation from Sutton and Barto (Section 13.5) and I am not quite sure what I am missing.",reinforcementlearning,14817102016,False,/r/reinforcementlearning/comments/asdbm1/can_someone_explain_to_me_the_running_reward_used/
I just posted my 6th video on Reinforcement learning tutorial series. We are quickly moving towards Deep RL...Don't miss the basics...,1550524016,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/as22rf/i_just_posted_my_6th_video_on_reinforcement/
how do i move q_target = to the main loop with this setup?,1550517788," 

Hi i have a ddpg algorithme, but the implementation has the q\_target defined in the init

      def __init__(self, a_dim, s_dim, a_bound, s):         target_update = [ema.apply(a_params), ema.apply(c_params)]      # soft update operation         a_ = self._build_a(self.S_, reuse=True, custom_getter=ema_getter)   # replaced target parameters         q_ = self._build_c(self.S_, a_, reuse=True, custom_getter=ema_getter)           self.a_loss = - tf.reduce_mean(q)  # maximize the q         self.atrain = tf.train.AdamOptimizer(LR_A).minimize(self.a_loss, var_list=a_params)           with tf.control_dependencies(target_update):    # soft replacement happened at here             q_target = self.R + GAMMA * q_             self.td_error = tf.losses.mean_squared_error(labels=q_target, predictions=q)             self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(self.td_error, var_list=c_params)   

and

      def learn(self):         indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)         bt = self.memory[indices, :]         bs = bt[:, :self.s_dim]         ba = bt[:, self.s_dim: self.s_dim + self.a_dim]         br = bt[:, -self.s_dim - 1: -self.s_dim]         bs_ = bt[:, -self.s_dim:] 		         critic_loss = self.sess.run(self.td_error, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})         critic_loss = float(critic_loss)         self.metrics[0] = critic_loss                        self.sess.run(self.atrain, {self.S: bs})         self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_}  

however i wanna use the variable ""Done"" to flag if the episode is resetting or not

and therefore if done = true i want it to be Q\_target = R instead of Q\_Target = R + Q\_ \* GAMMA as seen here

    q_target = self.R + GAMMA * q_  

however this is done in the init and i don't know how to move this to my main loop that looks like:

          for j in range(MAX_EP_STEPS): #Steps             s = env.reset() 				             a = ddpg.choose_action(s)			             s_, r, done = env.step(a)   			             kapper += 1			             if kapper == 10:                 kapper = 0                 print("""")                 print(ddpg.pointer)                 print(""printed pointer"")                 print(a[0])                 print(a[1])                 print(a[2]) 				             a = env.get_action()                             if a == 0:                 a = [1,0,0]             elif a == 1:                 a = [0,1,0]             elif a == 2:                 a = [0,0,1] 				             ddpg.store_transition(s, a, r / 10, s_) 				                     if ddpg.pointer &gt; MEMORY_CAPACITY:                 ddpg.learn()              s = s_             if env.StopEpisode:                 print("""")                 print(""Next Episode"")                 print("""")                 break  

Anyone has any idea how this should be done?

orignial implemntation is [https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow)

as you can see he doesn't work with the ""done"" variable. i need to have when done = true that he does q\_target = R and else q\_target = r + gamma \* q\_ but he defined it in the init so i can't edit it without losing the values of what the q\_target requires, any ideas how i should move it?

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/as0wn1/how_do_i_move_q_target_to_the_main_loop_with_this/
What are the current state of the art model-based reinforcement learning algorithms?,1550452519,"Just as the header says, what are some of the state of the art approaches / algorithms in model based reinforcement learning? Feel free to drop architectures names, research papers or even keywords that can help me find some of the more successful model based approaches.",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/arri1a/what_are_the_current_state_of_the_art_modelbased/
Reinforcement learning in trading,1550428207,"Simon Oulette chats about RL and trading strategies:

[https://www.youtube.com/watch?v=32NsZ7-Aao4](https://www.youtube.com/watch?v=32NsZ7-Aao4) \- Introduction to Bayesian Reinforcement Learning

[https://www.youtube.com/watch?v=OiVUONmhBxQ](https://www.youtube.com/watch?v=OiVUONmhBxQ) \- Reinforcement Learning in the Presence of Nonstationary Variables with Simon Ouellette

&amp;#x200B;",reinforcementlearning,wolfium,False,/r/reinforcementlearning/comments/arndhp/reinforcement_learning_in_trading/
Reinforcement Learning research groups in the US,1550427381,"coming from the very helpful post regarding **Reinforcement Learning research groups outside the US,** 

does anyone have a list of research groups in RL in the US? ",reinforcementlearning,abhijeetg12,False,/r/reinforcementlearning/comments/arn836/reinforcement_learning_research_groups_in_the_us/
Professors revolutionizing the field,1550343353,"What have been the most important breakthroughs for RL in the past couple of years for you guys (from academia), and who are the people pioneering the field according to you guys? ",reinforcementlearning,abhijeetg12,False,/r/reinforcementlearning/comments/arbs0p/professors_revolutionizing_the_field/
Introducing PlaNet: A Deep Planning Network for Reinforcement Learning,1550330365,,reinforcementlearning,dimis_d,False,/r/reinforcementlearning/comments/ar9o1g/introducing_planet_a_deep_planning_network_for/
Summary: World Models – Arxiv Bytes – Medium,1550320001,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/ar8dch/summary_world_models_arxiv_bytes_medium/
A Beginner's Guide to Deep Reinforcement Learning,1550257323,[https://skymind.ai/wiki/deep-reinforcement-learning](https://skymind.ai/wiki/deep-reinforcement-learning),reinforcementlearning,void_gear,False,/r/reinforcementlearning/comments/aqzxuz/a_beginners_guide_to_deep_reinforcement_learning/
Reinforcement learning for Rook placement problem?,1550226691,"I'm trying to solve the rook placement problem: placing n non-attacking rooks on a nxn board. The input is a board size n and the output is one possible placement of n rooks on the board. There could be a random noise to control the generation of different placement. But I'm just focusing on generating one for now.

It seems to me that there are several ways to solve this problem

\- From a random noise, generate a nxn matrix using a neural network similar to the generator of GAN. Then, I can use two hard attention layers to output the row indices and column indices of the rooks. Check if the row and column indices form a valid placement. Backprop. 

\- Learn a policy to place the rooks such that the placement is valid. 

As I'm not familiar with reinforcement learning, I'm not sure which one is better. After some researching, it seems that the first way is in essence RL as well as I need a differential argmax. ",reinforcementlearning,vic4ever,False,/r/reinforcementlearning/comments/aqvazs/reinforcement_learning_for_rook_placement_problem/
Controlling robotic arm with deep reinforcement learning,1550225883,,reinforcementlearning,pirate7777777,False,/r/reinforcementlearning/comments/aqv7xu/controlling_robotic_arm_with_deep_reinforcement/
Controlling robotic arm with deep reinforcement learning,1550195813,,reinforcementlearning,adamnemecek,False,/r/reinforcementlearning/comments/aqrj9u/controlling_robotic_arm_with_deep_reinforcement/
Montezuma with OpenAI baselines,1550141938,"Hi , 

I am trying to use Montezuma's environment in openAI baselines but even over the time-steps of 1e7 with ACER it doesn't learn anything.

&amp;#x200B;

While importing special wrapper for this environment from [large Scale Curiosity](https://github.com/openai/large-scale-curiosity)  doesn't help in learning anything and agent seems to do random actions all the time. 

&amp;#x200B;

I tried to search the implementation with uses OpenAI baselines with this environment but haven't found one. Each submission has its own flavor of code with its own kind of wrapper. Does it have any significance of using the special wrapper? and also can be embedded in baselines    ",reinforcementlearning,alpha_ma,False,/r/reinforcementlearning/comments/aqil6w/montezuma_with_openai_baselines/
Advantage Function,1550102396,"I was wondering about the advantage function and if it is always non-positive.  The advantage function is defined as Q(s,a)-V(s), and it seems to me that V(s) must be greater than or equal to Q(s,a) (aside from any errors due to function approximation).  

Just curious about this as this would mean that the function is not really the advantage of taking an action, but the disadvantage of taking a suboptimal action?

I'm just starting out on RL so any clarification on this would be very helpful.
Thanks
-Matthew.",reinforcementlearning,VirtualHat,False,/r/reinforcementlearning/comments/aqde3l/advantage_function/
Recommendations for training platform for RL for beginners,1550099398,"Hi

I'm new to reinforcement learning and I am trying to make my own implementations of algorithms such as DQN, A3C, A2C, PPO and testing them on some Atari games(Pong, Breakout, Space Invaders, etc.) using OpenAI gym. I made a DQN implementation and I trained it on FloydHub. It's very convenient and beginner friendly but also expensive and there is not much flexibility (as we cannot configure the kind of CPU and cores to use with the Tesla K80 GPU which I would need for A2C and A3C among other things). I tried looking at AWS and Google Cloud but they look too complicated to setup to begin with. Any suggestions? Any links for tutorials on setup step by step would be also be appreciated.",reinforcementlearning,owlesh,False,/r/reinforcementlearning/comments/aqcvop/recommendations_for_training_platform_for_rl_for/
Multi-agent Gridworld environment,1550094974,"Hi,

I'm currently working on a multi-agent grid-world problem where the objective is for several agents to pick up objects at pos x\_0, y\_0 and to deliver them at x\_1, y\_1. The size of the grid-world is arbitrary, and the same goes for the number of agents (specified by the user). Some notables:

* Agents cannot have the same objective, meaning that x\_0, y\_0 and x\_1, y\_1 will always be different for the agents.
* Agents have 5 actions, Up, Down, Left, Right, NOOP. The directional actions do have some ""ramp-up"" time (acceleration/deacceleration).
* The input (observation is) a vector of the following:
   * direction\_hint\_x,
   * direction\_hint\_y, 
   * distance\_border\_left, 
   * distance\_border\_right, 
   * distance\_border\_up, 
   * distance\_border\_down
   * proximity\_sensor: flattened 2x2 matrix where the value can be either 1 or 0. 1 means that there is another agent in this cell, 0 means its empty)

I've gotten this to work for a single agent, making it successful in collecting and delivering at the correct locations, but it seems to stagnate when i introduce multiple agents (All agents draw actions from the same policy)

&amp;#x200B;

The reward scheme so far is as follows:

* Movement yields **-0.001** (I've tried several)
* Pickup yields **1** (I've tried several positive ranges here as well)
* Delivery yields **10** (Same here, tried a bunch of them
* Crash in EITHER wall or other agent yields **-10.** This also causes terminal state to be reached

So to the behavior. I've tried a bunch of hyper-parameters but all seem to trigger the same behavior. The agent stagnates at approximately 1 000 steps at around 0 in mean average. At max, the agents' pickup 40 objects before a crash occurs.

The algorithm I've tried so far is PPO (Maybe there are other algorithms that would work better for such problem?, Off-policy?). Currently, I'm at default parameters from ray\[rllib\] baseline, which after 10 hours yields following results on a 1080TI

All assistance is appriciated :)

&amp;#x200B;

[Any obvious reasons to this silly stagnation](https://i.redd.it/tg8t4xehqeg21.png)",reinforcementlearning,Driiper,False,/r/reinforcementlearning/comments/aqc2d9/multiagent_gridworld_environment/
Emphatic weighting is biased under on-policy case?,1550051425,"I refer to the ""An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning"" (Sutton, 2016) paper in general. But the same idea also mentioned in his book in ""Emphatic TD"" section as well.

In the simpliest form, emphatic weighting is: **F(t) = γ ρ(t−1) F(t−1) + i(t)** where i(t) is state interest function, could be set to 1 that is we are interested in each state equally.

Consider an on-policy learning case, in this case there is initially *no need* for emphatic weighting at all because the semi-gradient target is guaranteed to be stable (with linear approximtaion, Sutton 2016).

But let say we will apply emphatic weighting anyway, consider an envirnoment with 4 contiguous states: 1 -&gt; 2 -&gt; 3 -&gt; 4. Then terminates at state 4. Very simple environment. 

Let say the target policy is going ""right"". Thus on-policy state distribution is 25% for all states.

Now consider that with emphatic weighting (let **γ = 1**), we could see that this F(t) will accumulate from 1 to 2 to 3 to 4. Since this F(t) is the scale of the gradient, SGD will generally learn to favor the **state** **4** four-time-more than the **state** **1** which contradicts to the on-policy distribution. 

In the paper mentioned this case exactly in its ""Conclusion"" section, but instead it said the case is in fact favored because it might get lower **MSVE**. I could not see how that could be the case.

&amp;#x200B;",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/aq4yzf/emphatic_weighting_is_biased_under_onpolicy_case/
[1901.07859] How do Mixture Density RNNs Predict the Future?,1550036668,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/aq39zc/190107859_how_do_mixture_density_rnns_predict_the/
"""ELF OpenGo: An Analysis and Open Reimplementation of AlphaZero"", Tian et al 2019 {FB}",1550030488,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aq2ec7/elf_opengo_an_analysis_and_open_reimplementation/
Ideas for Reinforcement Learning project in robotics ?,1550027539,"Hey there. Could someone recommend beginner-intermediate level RL projects focused in robotics or physical computing?

Some background: I'm a beginner at RL. I do have decent experience in deep learning though (especially supervised) and in embedded systems, robotics and open source hardware. 

Thanks in advance.",reinforcementlearning,void_gear,False,/r/reinforcementlearning/comments/aq1xyi/ideas_for_reinforcement_learning_project_in/
Ideas for Reinforcement Learning project in robotics ?,1550025842,"Hey there. Could someone recommend beginner-intermediate level RL projects focused in robotics or physical computing?

Some background: I'm a beginner at RL. I do have decent experience in deep learning though (especially supervised) and in embedded systems, robotics and open source hardware. 

Thanks in advance.",reinforcementlearning,void_gear,False,/r/reinforcementlearning/comments/aq1okv/ideas_for_reinforcement_learning_project_in/
Cheap (legged) robots compatible w/ RL?,1550018480,"Are there any cheap legged robots that can be used for RL? Something in the range of several hundred dollars, preferably around $100, and that you can buy on the internet like on Amazon. I've only delved into DRL with Atari and Mujoco, but I'm interested in training real legged robots as a side project.",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/aq0jn6/cheap_legged_robots_compatible_w_rl/
Why do we Nash averaging for AlphaStar ?,1550011959,"Is it mostly due to adversarial nature of the domain?

&amp;#x200B;

The multi-modal agent behaviour (which requires evolving separate agent?)

&amp;#x200B;

Or the very wide exploration ?",reinforcementlearning,so_tiredso_tired,False,/r/reinforcementlearning/comments/apzgnr/why_do_we_nash_averaging_for_alphastar/
"""At Scale"": Drago Anguelov talk on self-driving cars {Waymo} [active learning for labeling/sampling, NAS for car NN archs, imitation problems]",1549997625,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/apwrgx/at_scale_drago_anguelov_talk_on_selfdriving_cars/
Question on Robust Adversarial Reinforcement Learning,1549977164,"Hi all,   


I was reading this paper on RARL (Robust Adversarial Reinforcement Learning), by Lerrel Pinto, Abhinav Gupta et al. In the Proposed method section 3.3, the adversarial network update does not depend on the action of the protagonist. Why is this so ? Doesn't the action of the protagonist provide significant information to the adversary that is learning to modify the environment to make it harder for the protagonist ? ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/apt6us/question_on_robust_adversarial_reinforcement/
Expected advertising campaign profit with optimal stopping strategy,1549969183,"Hi, I am trying to figure out when to stop buying traffic based on previous conversions. Is dynamic programming a valid approach? Here is a prototype.

&amp;#x200B;

[https://github.com/Gaunt/optim/blob/master/optim.py](https://github.com/Gaunt/optim/blob/master/optim.py)",reinforcementlearning,gauntpp,False,/r/reinforcementlearning/comments/aps7es/expected_advertising_campaign_profit_with_optimal/
Question: State representation for a DQN Agent in a maze environment,1549923837,"Hey, I am training a DQN agent in a maze environment. The maze is  rectangular and the length of the maze can be 200 times the height of  the maze. The state of the agent can be represented by x, y on the maze.  Should I directly use x and y as inputs to the DNN? If not then how  should I transform them? Does the difference between the ranges of x and  y affect the performance of the DNN? I am sorry for a naive question. I  am new to RL and DNN.",reinforcementlearning,johnmillerhit,False,/r/reinforcementlearning/comments/aplvj1/question_state_representation_for_a_dqn_agent_in/
Tensorflow and PG implementations for Atari Gym envs?,1549894791,"I've been trying to find a policy gradient approach, that uses tensorflow, to solve Atari Gym environments with no luck. I've found loads that work with simpler OpenAI envs, such as cartpole-v0, but none yet that work on Atari gyms.

Can anyone point me in the right direction? Thanks a lot.

",reinforcementlearning,TheBrightman,False,/r/reinforcementlearning/comments/apgpea/tensorflow_and_pg_implementations_for_atari_gym/
[Discussion] Time-bound Markov states,1549884356,"In the second lecture of Dr.David Silver's course on RL, he provides an example of a student Markov  chain ([Image](https://i.imgur.com/VQSgZ8h.png)). A tiny nitpick is that the states are time dependent. Say Class 1  is from 10-11 and we cannot return to the state after 11. Is this  normally taken into consideration in Markov chains? How else can this  chain be modeled? Thanks for your help!",reinforcementlearning,Hari_a_s,False,/r/reinforcementlearning/comments/apfbl6/discussion_timebound_markov_states/
"RL Weekly 7: Obstacle Tower Challenge, Hanabi Learning Environment, and Spinning Up Workshop",1549882724,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/apf4tb/rl_weekly_7_obstacle_tower_challenge_hanabi/
Detailed discussion on solving multi-armed bandit problem.,1549879983,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/apeudh/detailed_discussion_on_solving_multiarmed_bandit/
PyTorch Implementation of the Hindsight Experience Replay (HER),1549845304,"Hi everyone, here is the PyTorch implementation of HER for the ""Fetch Env"": [https://github.com/TianhongDai/hindsight-experience-replay](https://github.com/TianhongDai/hindsight-experience-replay) The code is ported from the openai baseline. The performance is very similar to the original tensorflow code. Hope this can help you if you are using PyTorch!

&amp;#x200B;

![video](c06k05r84uf21)

&amp;#x200B;",reinforcementlearning,TianhongKobeFans,False,/r/reinforcementlearning/comments/apai3a/pytorch_implementation_of_the_hindsight/
I am building a free Reinforcement Learning online course. You might find it useful...,1549841594,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/ap9x5z/i_am_building_a_free_reinforcement_learning/
Proximal Policy Optimization with Beta distribution - Value goes to infinity,1549816478,"I am currently working on a Proximal Policy Optimization Problem where the algorithm is expected to output 7 values between 0 and 1 that are sampled from a beta-distribution. So my Problem is a continuous Problem where each of the 7 ""joints"" can be accelerated with a value between 0 and 1. The Network for the PPO has 7 policy heads (each head alpha and beta) and 1 value head.

[PPO Loss](https://cdn-images-1.medium.com/max/1600/1*T0D50EPz-oqGDn55uHv9IA.png)

My problem is that when learning, the value wreaks complete havock and increases to infinity. This makes the clipped loss utter bullshit because it doesnt even recognize the policy loss because the value loss is way bigger.

Moreover as the value loss is always positive (V\_targ-V\_pred)\^2 the loss can only increase in one direction.I have noticed that PPO2-baseline uses value clipping but that wont help me I guess, because it just clips the value in negative direction so it will always choose a big positive value increase.

Here is some code of my algorithm, maybe you can tell me if I am doing something wrong?Help is really appreciated!

    value=self.forward(states.float())[0]
    value=value.view([1,len(value)]).flatten()
    
    value_1=self.forward(states_1.float())[0]
    value_1=value.view([1,len(value)]).flatten()
    
    v_states=torch.stack(memory[""v_states""]).flatten()
    v_states_1=torch.stack(memory[""v_states_1""]).flatten()
    
    deltas, r_discs=self.compute_delta(value,value_1,v_states,v_states_1,rewards)
    
    old_advantage=0
    advantage_ests=torch.tensor([])
    for i in deltas:
        advantage_t=i+GAMMA*LAMBDA*old_advantage
        old_advantage=advantage_t
    advantage_ests=torch.cat((advantage_ests,advantage_t.reshape(1).float()))
    advantage_ests=torch.flip(advantage_ests,dims=[0])
    old_pred=old_policy.forward(states.float())[1:]
    
    old_pred=[i.view([2,len(i[0])]) for i in old_pred]
    new_pred=self.forward(states.float())[1:]
    new_pred=[i.view([2,len(i[0])]) for i in new_pred]
    
    ratios,entropia=self.calc_ratio(old_pred,new_pred,opti_action)
    
    surr1 = ratios.float() * advantage_ests.float()
    surr2 = torch.clamp(ratios.float(),1.0-epsilon,1.0+epsilon)*advantage_ests.float()
    
    action_loss=-torch.mean(torch.min(torch.mean(surr1),torch.mean(surr2)))
    value_loss= 0.5*torch.mean(value.float() - r_discs.float())**2
    entropy_loss=-torch.mean(entropia.float())
    
    loss=action_loss+C_1*value_loss+C_2*entropy_loss
    
    optimizer.zero_grad()
    loss.backward(retain_graph=True)
    optimizer.step()

&amp;#x200B;",reinforcementlearning,unieye,False,/r/reinforcementlearning/comments/ap5i7k/proximal_policy_optimization_with_beta/
Reinforcement Learning Project Ideas,1549736576,"For my RL class, we are tasked to hand in a project at the end of the term and I was looking for suggestions and ideas that I could do for my project.

&amp;#x200B;

&amp;#x200B;

I plan on spending about 30-40 hours on the project. Since this is an intro-level class, this is the first time we are introduced to the following concepts:

Multi-armed bandit problem  Markov Decision Problems  Dynamic Programming  Monte-Carlo solution methods  Temporal difference methods  Multi-period  Approximation methods  Policy gradient 

&amp;#x200B;

&amp;#x200B;

We are going to be programming in R (since our prof won't let us use python...), and my only idea is that I don't want to do a project related to finance.

&amp;#x200B;

&amp;#x200B;

Any ideas?",reinforcementlearning,QuietLie,False,/r/reinforcementlearning/comments/aouybk/reinforcement_learning_project_ideas/
Standalone PPO implementation in PyTorch,1549710070,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/aore9r/standalone_ppo_implementation_in_pytorch/
"""Weighted importance sampling, does not carry over easily to function approximation"" why is this the case?",1549610209,"Mahmood et al. (2014) said that ""weighted importance sampling, does not carry over easily to function approximation.""

I don't understand why that is the case apart from the fact the weighted IS is biased. But, many biased techniques are used in function approximation cases effectively, why is this an exception? 

Or it is just that at the time (2014) no one had tried this out?",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/aoe3zc/weighted_importance_sampling_does_not_carry_over/
Policy gradient neural network output in a continuous action space,1549609910,"While trying to solve the Berkeley Deep RL course's policy gradient homework, I'm unable to understand what the prompt below about continuous actions means:

    if discrete, the parameters are the logits of a categorical distribution
        over the actions
        sy_logits_na: (batch_size, self.ac_dim)
    if continuous, the parameters are a tuple (mean, log_std) of a Gaussian
        distribution over actions. log_std should just be a trainable
        variable, not a network output.
        sy_mean: (batch_size, self.ac_dim)
        sy_logstd: (self.ac_dim,)

[Link](https://github.com/berkeleydeeprlcourse/homework/blob/master/hw2/train_pg_f18.py#L122) to source.

Any pointers, please? Super grateful if you're able to explain this with an example.",reinforcementlearning,dudester_el,False,/r/reinforcementlearning/comments/aoe2tc/policy_gradient_neural_network_output_in_a/
Why does the Policy Gradient Theorem generalize to continuous action spaces?,1549593141,"The policy gradient is generally in the shape of the following:

https://i.redd.it/ospcg02s99f21.png

Where π represents the probability of taking action a\_t at state s\_t and A\_t is an advantage estimator. This makes perfect sense to me in discrete action spaces, however, I'm unsure why this still makes in continuous action spaces. 

&amp;#x200B;

In every application of policy gradients to continuous action spaces that I have seen π always evaluates a point on the PDF instead of actually representing a probability.  I was going through Sutton and Barto's policy gradient chapter and they brush over this and I didn't catch why it's justified to evaluate a point on the PDF

&amp;#x200B;

I've also posted a stackoverflow post [here](https://stats.stackexchange.com/questions/391405/why-does-the-policy-gradient-theorem-generalize-to-continuous-action-spaces)",reinforcementlearning,Data-Daddy,False,/r/reinforcementlearning/comments/aobu0z/why_does_the_policy_gradient_theorem_generalize/
"Dimensions of ""history"" in recurrent DPG in real code",1549588760,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/aob6h6/dimensions_of_history_in_recurrent_dpg_in_real/
Where to attain the needed mathematics knowledge without learning aspects of math that are unnecessary to know?,1549573385,"Whenever someone asks about what math is needed to understand this stuff people just give a very general answer like ""linear algebra"" or ""statistics"" or whatever the case is.

What I'm asking is where do I get what I need to know without learning a bunch of extra math that isn't going to matter? Inevitably if I just take a linear algebra course or something there's going to be a lot of stuff in there that I don't need to know for reinforcement learning, right? I don't want to waste my time on that stuff.

So what exactly do I need to know in math to be competent at reinforcement learning in the future? ",reinforcementlearning,Artemis225,False,/r/reinforcementlearning/comments/ao8kli/where_to_attain_the_needed_mathematics_knowledge/
Uber AI’s Go-Explore Tackles Hard-Exploration Problems,1549564771,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/ao6x94/uber_ais_goexplore_tackles_hardexploration/
"Which open-source projects, with some dependency on reinforcement learning, can I contribute to?",1549544041,"I would like to ask this generally so that it is applicable to anyone with a similar question on their mind.  


Specific scenarios can go in the comment section.",reinforcementlearning,dnk8n,False,/r/reinforcementlearning/comments/ao3fgt/which_opensource_projects_with_some_dependency_on/
PPO actor-critic style,1549482756,"Hello everyone! I am implementing proximal policy optimization (PPO) actor-critic style method [PPO](https://arxiv.org/pdf/1707.06347.pdf)to tune the gains of a controller. When sampling from action distribution, I will get random action from the learned distribution of the action learned by the actor through neural network. It is strange that at an instance I will get random point ( it can be positive or negative). I was wondering if my implementation is wrong or not? If not wrong, is there a way to have more correct/relevant action?",reinforcementlearning,Ali7422626,False,/r/reinforcementlearning/comments/anupv5/ppo_actorcritic_style/
Suggestion and Progress,1549459522,Any good resources for starting with Practical RL? Also I am trying to make a note of everything i learn in form og blogs. Please suggest if I go wrong anywhere. [https://prakhartechviz.blogspot.com/2019/02/introduction-to-reinforcement-learning.html](https://prakhartechviz.blogspot.com/2019/02/introduction-to-reinforcement-learning.html),reinforcementlearning,prakhar21,False,/r/reinforcementlearning/comments/anqo2h/suggestion_and_progress/
Ways to check for convergence in Q-networks,1549459480,"I am testing a novel hierarchical reinforcement learning architecture. I have utilized multiple online Q-networks for various goals. One for enemy avoidance, one for food collection, etc. While watching the agent, I can approximate when one of the networks converge because it consistently performs well in that specific task. I am using the same reward (the one given by the environment for all the goals). What are the ways of quantifying this convergence? I have tried checking for the losses in every network but it is not informative, and sometimes it does not decrease even when the agent behaves optimally. I have thought about checking average Q value difference across a number of steps but because I'm using an online network, I suspect that Q values will not settle much. I'm open to any insights about how one would check for converge of online q networks except reward maximisation?",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/anqnuf/ways_to_check_for_convergence_in_qnetworks/
Getting actual game score from Atari envs?,1549458703,"Hey all, I've ran into what is (hopefully) a simple problem with it comes to testing my DQN model.

The score I get when I train my model is not the same as the score on screen. For example; in breakout, the program might tell me the agent scored 50 points, but the score on the screen (env.render) may be more like 80. I think this is because I get a solid +1 reward for every brick broken. However, this is impractical as not every brick in breakout is worth 1 point so it leads the score to look very far off. 

Is there a simple way to get the actual game score from an Atari env? 

Thanks for the help.",reinforcementlearning,TheBrightman,False,/r/reinforcementlearning/comments/anqk34/getting_actual_game_score_from_atari_envs/
How to visualize gradients in a reinforcement learning setting with TensorBoard ?,1549454121,"Dear reader,

I am building a reinforcement learning agent in Keras which is supposed to learn to play PacMan. I wish to visualize the development of the gradients of my model using Histograms in TensorBoard. However I am having a hard time wrapping my head around, what exactly I am supposed to feed as validation data. I am using experience replay, so I also have a Memory filled with states. In addition, I am using a DDQN setup, so that the trainable model and the target model (which is used for inference) are not the same entity.

&amp;#x200B;

If I want to evaluate the gradients after every 500th episode, what would I use as validation data? I would want to see the gradients on the target model, because that is the one I also do inference on (right?). The loss is more or less unimportant for visualization, because deep reinforcement learning is like guiding a horse with a carrot on a stick (the carrot is just a little closer to our goal than the horse), is this analogy &amp; assumption right? (I am asking this because loss is only calculated in the training model and it feels like tracking the 'absolute distance between carrot and horse' does not make sense).

&amp;#x200B;

The TensorBoard callback object in Keras wants a set of validation data. I could set this every 500 episodes by wrapping the TensorBoard callback object, however I am not sure if it makes sense to evaluate the model's gradients with different states all the time (are the gradients still comparable?) or if i'd rather just feed it a single state (e.g. the start state). I am also unsure on what the 'targets' are supposed to look like when evaluating the target model gradients. No loss function is computed here, so what's the meaning of targets in that case? Is it fair to just feed np.zeros?

&amp;#x200B;

Thank you for your time",reinforcementlearning,sku-sku,False,/r/reinforcementlearning/comments/anpyzm/how_to_visualize_gradients_in_a_reinforcement/
OpenAI - Spinning Up in Deep RL Workshop [Video],1549451223,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/anpno1/openai_spinning_up_in_deep_rl_workshop_video/
Entropy bonus - Soft Actor Critic,1549380135,"Hello, 

I'm posting here because I can't wrap my head around how the entropy bonus is integrated in the value estimators in SAC, and am wondering if someone in here could explain it to me. 

&amp;#x200B;

In the [original paper](https://arxiv.org/abs/1801.01290), the entropy bonus is added in the state value estimator V only at the step t + 1.  The Q is then computed as the reward at time t + the state value of the next step (Modulo the entropy weight and the discount). In doing this however, I don't understand how the entropy bonus will be added at the first step at time 0 ? Do we simply skip it ? It would seem easier to me if we simply added the entropy bonus in the (state, action) value estimator Q at time t. 

&amp;#x200B;

In the [technical paper from the BAIR/Google post](https://bair.berkeley.edu/blog/2018/12/14/sac/), the V estimator is removed and we replace it by sampling an action to compute an expectation on the Qs. (Note that from my understanding, all it does is trade more variance for a bit more overall computation/memory efficiency). The entropy is thus computed in the Q, but still only at step t + 1, which I don't understand. Don't we miss the entropy bonus of the state at step 0 ? What's the point of doing this ? Why don't we compute the entropy at state t to add to the reward while updating the values at step t ? 

&amp;#x200B;

As the rest of SAC is well thought out and elegant, I really think there is something I'm missing. Could anyone help me understand ? ",reinforcementlearning,ReinforcedMan,False,/r/reinforcementlearning/comments/anf3mx/entropy_bonus_soft_actor_critic/
How to fairly compare 2 different RL methods?,1549368363,"I'm looking into comparing the performance of DQN vs a policy gradient approach (such as REINFORCE). Both of these models will be trained to play various Atari Gym environments. 

I want to eventually compare the score these two models get in these different environments but am struggling to think how to do this in a *fair way*. So far I have 2 schools of thought;

1) Keep all the hyperparameters as similar as possible (Learning rate, batch size ect). Keep the number of training steps the same.

2) Optimize and tweak each algorithm separately to get max performance. Keep the number of training steps the same.

Any help on this would be greatly appreciated. Thanks. ",reinforcementlearning,TheBrightman,False,/r/reinforcementlearning/comments/andgie/how_to_fairly_compare_2_different_rl_methods/
"""Automatic Local Rewriting for Combinatorial Optimization"", Chen &amp; Tian 2018",1549338466,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ana5kb/automatic_local_rewriting_for_combinatorial/
"""The Hanabi Challenge: A New Frontier for AI Research"", Bard et al 2019 {DM/GB} [cooperative board game environment stressing theory-of-mind; Impala/Rainbow baselines]",1549309191,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/an5dck/the_hanabi_challenge_a_new_frontier_for_ai/
Common bugs that make an agent perform worse after starting to do well?,1549294303,"I'm trying to train an agent using DDPG on the PyBullet version of HalfCheetah.  Initially, it seems like things are working well.  The sum of rewards for each episode starts out at around -1500, and it soon breaks into the positive region (\~300).  Shortly after becoming positive, it drops back down to -1500, which seems to correspond to purely random actions, and it just seems to stay there forever.

I was wondering if there are any common mistakes that might explain this type of behavior.  Thanks!",reinforcementlearning,TheJCBand,False,/r/reinforcementlearning/comments/an2qnl/common_bugs_that_make_an_agent_perform_worse/
Match-boxes can learn to play tic-tac-toe and defeat you everytime. Here is how,1549286234,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/an1kne/matchboxes_can_learn_to_play_tictactoe_and_defeat/
"Agents learns easy tasks, does not learn anything useful in harder tasks.",1549274350,"Hi,

I am developing an RL agent for Pacman. I am using the Berkeley Pacman framework ([http://ai.berkeley.edu/reinforcement.html](http://ai.berkeley.edu/reinforcement.html)) and adapted the knowledge from this ([https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814](https://towardsdatascience.com/advanced-dqns-playing-pac-man-with-deep-reinforcement-learning-3ffbd99e0814)), namely by rewiring ([https://github.com/keras-rl/keras-rl/blob/master/examples/dqn\_atari.py](https://github.com/keras-rl/keras-rl/blob/master/examples/dqn_atari.py)) to fit my needs.

&amp;#x200B;

The Architecture:

    self.model.add(Convolution2D(32, (3, 3), strides=(1, 1)))
self.model.add(Activation('relu'))
self.model.add(Convolution2D(64, (3, 3), strides=(1, 1)))
self.model.add(Activation('relu'))
self.model.add(Convolution2D(64, (3, 3), strides=(1, 1)))
self.model.add(Activation('relu'))
self.model.add(Flatten())
self.model.add(Dense(512))
self.model.add(Activation('relu'))
self.model.add(Dense(self.nb_actions)) #nb_actions))
self.model.add(Activation('linear'))

This can also be run with a training\_ and target\_model, just as DDQN suggests. A replay memory buffer of 300'000 memories is used.

&amp;#x200B;

My agent does converge on easy tasks (e.g. 'mediumGrid') in 5k-10k episodes (150k - 300k steps):

    %%%%%%%%
%P     %
% .% . %
%  %   %
% .% . %
%     G%
%%%%%%%%

However it does not only not converge, it looks like it gets worse and worse on harder tasks (e.g. 'smallClassic'):

    %%%%%%%%%%%%%%%%%%%%
%......%G  G%......%
%.%%...%%  %%...%%.%
%.%o.%........%.o%.%
%.%%.%.%%%%%%.%.%%.%
%........P.........%
%%%%%%%%%%%%%%%%%%%%

I use a linearly annealed epsilon 'restricted' greedy policy, which anneals epsilon from 1 to 0.1. The actual number of steps it takes to anneal depends on how long I expect it to train (e.g. 300k steps for mediumGrid, 2M steps for smallClassic).  The 'restricted' comes from the fact that we know which actions are allowed in the current state, so we get the 'greediest action that is actually allowed' (could that cause troubles?). 

&amp;#x200B;

What I tried/did \[In parantheses: (mediumClassic.converges, smallClassic.converges)\]:

\- Making random steps at the start of every episode (true, false)

\- Disabling DDQN (true, false)

\- Enabling DDQN (true, false)

\- Shaping rewards, so the agent gets more reward for every quarter of the beginning food he eats. e.g. if there is 20 food at the start, he gets extra 75 reward when he has eaten 5, 10 or 15

\- Rewriting the original Lambda layer to actual Layer classes, to make them serializable and saveable (true, false)

&amp;#x200B;

I include the run from last night (less reward is obviously bad):

smallClassic

around 60k episodes (2.5M steps)

no DDQN

shaped rewards

annealing in 2M steps

&amp;#x200B;

[Average reward](https://i.redd.it/6pxj4en3yie21.png)

&amp;#x200B;

One more thing:

I do save my models every 5000 steps, also saving the memory, steps, and everything (I hope) important and reload them after closing the program. This is do avoid a memory leak in the pacman implementation. However I get strange nudges in my scalars when plotting them with TensorBoard, just as if something was reinitialized. Could that cause damage? Any idea on what it could be?

&amp;#x200B;

&amp;#x200B;

[Nudges after reloading \(at 5000 Episodes\)](https://i.redd.it/qwa7yfb3xie21.png)

&amp;#x200B;

So to round it off with an actual question: Any ideas on how to proceed? Do you have an idea of an implementation error that could lead to the problems I have? Did you have such errors yourself? Any ideas on the nudges? 

&amp;#x200B;

Thank you for taking the time!",reinforcementlearning,sku-sku,False,/r/reinforcementlearning/comments/an08pe/agents_learns_easy_tasks_does_not_learn_anything/
best simple explore/exploit method?,1549274056,"Epsilon greedy exploration in Q-learning is defined as:

probability of taking the best action = epsilon

proability of taking any other action = (1/num\_of\_other\_actions)\*(1-epsilon)

then sample from this distribution.

&amp;#x200B;

Meanwhile in stochastic policy gradient methods, i see implementations on github that call sampling the probability distribution of the action space ""epsilon greedy"". I presume this isn't epsilon greedy. What is the correct name for this technique?

Is the Q-learning equivalent of this second method (taking a softmax over the Q-values and sampling from that as a probability distribution) a valid exploration technique?

&amp;#x200B;

If so, which of the two should I default to using (as a baseline before actually experimenting with exploration policies)?",reinforcementlearning,_llucid_,False,/r/reinforcementlearning/comments/an07pn/best_simple_exploreexploit_method/
Will policy gradient coverge to the optimal policy?,1549257304,"With non-linear function approximator as its policy network, it is by default to think that a reachable policy should be a locallly optimal one. 

**Thomas, P. S. (2014) argued however** that, I quote, ""We argue that with assumpions similar to those required by Q-learning and Sarsa, ascending the policy gradient results in convergence to a globally optimal policy as well.""

Becasue, for all suboptimal policies, their policy gradients are ""non-zero"".

&amp;#x200B;

**I doubt that.**

I think it is possible to show that for a given state ""s"" the policy gradient subject to that state under the policy could be non-zero. However, taking the expected gradient for all the trajectories under that policy, I think it is conceivable to think that there might be many zero gradient fixed-points. 

For one, It is not possible for a function approximator to remember the action distribution for all states independently, that is even at a fixed point the policy network might not yet reach the best possible action according to its critics (given it is perfect) for all states.

&amp;#x200B;

**Is that correct?**

&amp;#x200B;

Ref:

Thomas, P. S. (2014). Bias in Natural Actor-Critic Algorithms. Proceedings of Machine Learning Research, 32, 441–448.",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/amydva/will_policy_gradient_coverge_to_the_optimal_policy/
Easiest explanation of Thompson sampling to solve multi-armed bandit problem,1549228836,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/amu6f7/easiest_explanation_of_thompson_sampling_to_solve/
RL Reward Design,1549220225,"Hi,

&amp;#x200B;

I got an Algorithme (DDPG) to do package delivery in an environment, but i will save all that for now, since my question is fairly simple?

&amp;#x200B;

bonus = small bonus on how good of a route he took, just consider it as something around ca 0.1 till ca 0.3 extra points

&amp;#x200B;

So dropping the package gives 0.5 + bonus for goal and -0.5 + bonus (negative) for fail. so with this he should drop the package on the goal and not on anything else, also all the other steps have 0 reward except for the steplimit of -1 if he did 30 steps and still didn't make it. So what happend is that he only kept holding on the package till steplimit or epsilon instead of dropping it himself causing a prob of 0.01 outta 1 (softmax) for dropping the package, in all states and he kept on getting the -1 instead of the fail goal, so first of all i am confused why that behaviour elapsed, but also i heard some solutions, but i don't get those...

&amp;#x200B;

the solution says that i should increase all the rewards so that the expected future reward goes up, true, however isn't that just not relevant, since everything is in a ratio, so it shouldn't matter? another suggestion was having something like dropping fail -0.5 and dropping goal 1 so that the drop action is more likely to be chosen or something, but i tried that and that didn't work out aswel, also my algorithme saturates fairly quick if the rewards go outta -1 till 1 range.

&amp;#x200B;

Any information and suggestions are welcome, thanks

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/amsmi3/rl_reward_design/
New video in my Reinforcement Learning tutorial series...,1549196370,,reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/amp3dh/new_video_in_my_reinforcement_learning_tutorial/
[P] A 2d walker in the browser. Trained using tensorflow.js and DDPG.,1549176649,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/amnah6/p_a_2d_walker_in_the_browser_trained_using/
Sparlab is a training tool that allows fighting gamers to simulate battle scenarios and discover new combos. How can reinforcement learning be applied to show the most optimal action to take in any given scenario created by the end user? Any ideas on how to improve this software?,1549076657,,reinforcementlearning,realjohnward,False,/r/reinforcementlearning/comments/ama2i9/sparlab_is_a_training_tool_that_allows_fighting/
Can someone help me debug my Vanilla DQN algorithms in Tensorflow ?,1549037210,"Hello, I have coded the DQN algorithms [here](https://colab.research.google.com/drive/1gDPogAlbcMGNs5NAfuF_5C522ZdzWfk7) on Google Colab. However, I am still getting only a 9 mean reward on the Cartpole environment. Can someone please help ? I really don't know what's wrong. I have also referred to open AI baselines to build my network.",reinforcementlearning,karanchahal1996,False,/r/reinforcementlearning/comments/am3jcg/can_someone_help_me_debug_my_vanilla_dqn/
Summary: Learning Plannable Representations with Causal InfoGAN,1549005801,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/alzls0/summary_learning_plannable_representations_with/
"""The Evolved Transformer"", So et al 2019 {G} [NAS]",1548988461,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/alxa4g/the_evolved_transformer_so_et_al_2019_g_nas/
Why are correlated samples bad in DQN ?,1548969043,"I was reading the DQN paper where they talk about using the experience replay trick. The authors claim that correlated samples in the dataset, while playing atari games are bad and the ER trick , by randomly sampling from memory decorrelates the frames / states. Why is correlation bad ? ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/alua6f/why_are_correlated_samples_bad_in_dqn/
Online vs Offline Q-Learning with NN approximation,1548949656,"I have been running tests for the last week comparing different architectures. I have noticed that in a simple food collection task in a grid world environment; a Q network without experience replay, and without target network converges after 500 episodes, while a proper Q network with experience replay and target network fails to converge. Isn't the opposite supposed to happen? 

I update the primitive Q network with every &lt;s,a,r,d&gt; one at a time, while in the proper Q network I update it with the network with a batch of 16 experiences sampled randomly from experience replay. Also, the weights are copied from the main network to the target every 20 steps if that makes a difference. Effectively, the proper Q network trains on 16 times more data than the primitive one but performs worse. Is there something I'm missing or should I dive back in to check for bugs? (I'm pretty sure both implementations are bug-free but one never knows) ",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/alqr50/online_vs_offline_qlearning_with_nn_approximation/
Training an Off-Policy RL agent on data generated by trained PPO,1548932831,"Hey ! 

&amp;#x200B;

I've been reading a lot about TD3 and SAC algorithms and they both seem to have very nice features. However, when I apply them to  various control environments (such as BipedalWalker), they take quite a lot of time to reach acceptable performances. In contrast, PPO (even when using a single worker) reaches decent performances much much faster. 

&amp;#x200B;

For various reasons however, I do want an agent trained with these off-policy approach and I finally had an idea: 

Train a PPO agent -&gt; Generate ReplayBuffer of transitions using the trained agent -&gt; train the Off-Policy agent using this dataset.

While it sounded like a great idea, it is actually not giving any good results. The policy gets stuck to -50 for TD3 and doesn't learn much with sac. 

&amp;#x200B;

&amp;#x200B;

Do you guys have any idea why ? 

Thanks a lot !  ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/alofby/training_an_offpolicy_rl_agent_on_data_generated/
"""Go-Explore: a New Approach for Hard-Exploration Problems"", Ecoffet et al 2019 {Uber}",1548909096,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/allrqe/goexplore_a_new_approach_for_hardexploration/
"""See, feel, act: Hierarchical learning for complex manipulation skills with multisensory fusion"", Fazeli et al 2019",1548907281,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/alliua/see_feel_act_hierarchical_learning_for_complex/
"Can I use deterministic policy gradient methods, e.g. DDPG, for stochastic policy learning?",1548893983,"In particular, can I treat a ""stochastic policy over a finite action space of size n"" as a ""deterministic policy in the space of probability distribution in R\^n""? It seems to me that there is nothing is broken by making this mental translation except that the ""induced environment"" now has to take a stochastic action and spit out the next state, which is not hard based on the original environment. Is this legit? If yes, how does this ""deterministify then DDPG"" approach compare to, for example, A2C?

&amp;#x200B;

Thanks a lot.",reinforcementlearning,zhangxz1123,False,/r/reinforcementlearning/comments/aljjh7/can_i_use_deterministic_policy_gradient_methods/
Reward growth with state space growth,1548867813,"I've completed things like CartPole, GridWorld and so on. Learning from these is great but I sense that they are far fetched from ""real world"" RL problems.

In GridWorld, using a board of size 3x4, we have 11 states excluding that middle tile and including both terminal states. A reward of 1 at the desired terminal state and -1 at the failed terminal state worked very well and the models converged.

In the case where I expand that environment; I now have 10,000 states with two terminal states, one good and one bad.

Is a reward scheme of +1 and -1 still advised? Should I be increasing the reward at those terminal states so they can propagate back far enough to reach early stages?

Tldr; What is the relationship between state size and terminal state reward magnitudes?

Thanks!",reinforcementlearning,zQuantz,False,/r/reinforcementlearning/comments/alerto/reward_growth_with_state_space_growth/
Branching DQN: Simultaneous discrete actions for DQN,1548855589,"Hey ! 

&amp;#x200B;

I finished my implementation of BranchingDQN (based on [https://arxiv.org/pdf/1711.08946.pdf](https://arxiv.org/pdf/1711.08946.pdf)). Here's my repo if anyone is interested: [https://github.com/MoMe36/BranchingDQN](https://github.com/MoMe36/BranchingDQN)

As you can see, I had nice performances on BipedalWalker-v2 ! 

Enjoy (: ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/alcrja/branching_dqn_simultaneous_discrete_actions_for/
What's the right hardware for reinforcement learning?,1548850276,"Hi, I am a beginner in the field and I've been writing some agents (mainly variations of DQN from the rainbow paper) that I only tested on the OpenAI CartPole environment to check if the algorithms are correct. I figure it's time to give them some more challenging problems such as Atari games.

I only have a laptop so I decided to turn to AWS/GoogleCloud to get some computing power. I don't know what virtual machine would suit me best though. Every machine learning site says that you should spend a lot on GPU(s), though I don't think that this is the case for deep reinforcement learning. Even though you have to run a neural network, it usually is pretty small compared to the ones used in supervised learning, plus much of the work isn't done by the network itself (e.g. storing memories, computing value functions, environment interaction). I'm using PyTorch, and I observed that when using the GPU the learning process actually gets slower!

IMO I should go for a fast CPU and save on the GPU, if not skip it entirely. I'm interested in hearing opinions from more experienced people though, including how I could get the GPU to do more of the heavy lifting in my algorithms. ",reinforcementlearning,TylerPenderghast,False,/r/reinforcementlearning/comments/alc34d/whats_the_right_hardware_for_reinforcement/
Reinforcement Learning (RL) Tutorial with Sample Python Codes,1548849279,"Dynamic Programming (Policy and Value  Iteration), Monte Carlo, Temporal  Difference (SARSA, QLearning),  Approximation, Policy Gradient, DQN,  Imitation Learning, Meta-Learning,  RL papers, RL courses, etc.

**Tutorial Link:** [**https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo**](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)

**Table of Contents:**

* [What is Reinforcement Learning?](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#whatisRL)
* [Markov Decision Process](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#mdp)  

   * [Markov Property](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#markovproperty)
* [RL Components](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#RLcomponents)  

   * [Rewards](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#Rewards)
   * [State Transition Probability](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#StateTransitionProbability)
   * [Discount Factor](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#DiscountFactor)
   * [Return](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#Return)
   * [Value Function](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ValueFunction)
   * [Policy](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#Policy)
   * [State-Value Function](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#StateValueFunction)
   * [Action-Value Function](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ActionValueFunction)
   * [Planning vs RL](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PlanningRL)
   * [Exploration and Exploitation](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ExplorationandExploitation)
   * [Prediction &amp; Control Problem](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PredictionControlProblem)
* [Grid World](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#GridWorld)
* [Dynamic Programming Method (DP)](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#DP)  

   * [Policy Iteration](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PolicyIteration)  

      * [Policy Evaluation](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PolicyEvaluation)
      * [Policy Improvement](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PolicyImprovement)
   * [Value Iteration](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ValueIteration)
* [Monte Carlo (MC) Method](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MonteCarlo)  

   * [MC Calculating Returns](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MCCalculatingReturns)
   * [First-Visit MC](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#FirstVisitMC)
   * [MC Exploring-Starts](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MCExploringStarts)
   * [MC Epsilon Greedy](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MCEpsilonGreedy)
* [Temporal Difference (TD) Learning Method](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#TDLearning)  

   * [MC - TD Difference](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MCTDDifference)
   * [MC - TD - DP Difference in Visual](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MCTDDifferenceinVisual)
   * [SARSA (TD Control Problem, On-Policy)](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#SARSA)
   * [Q-Learning (TD Control Problem, Off-Policy)](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#Qlearning)
* [Function Approximation](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#FunctionApproximation)  

   * [Feature Vector](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#FeatureVector)
* [Open AI Gym Environment](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#OpenAIGym)
* [Policy-Based Methods](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PolicyBased)  

   * [Policy Objective Functions](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PolicyObjectiveFunctions)
   * [Policy-Gradient](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo/blob/master/PolicyGradient)
   * [Monte-Carlo Policy Gradient (REINFORCE)](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#REINFORCE)
* [Actor-Critic](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ActorCritic)  

   * [Action-Value Actor-Critic](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ActorValueActorCritic)
   * [Actor-Critic Algorithm:A3C](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#A3C)
   * [Different Policy Gradients](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#DifferentPolicyGradients)
* [Model-Based RL](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo/blob/master/ModelBasedRL)  

   * [Real and Simulated Experience](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#RealandSimulatedExperience)
   * [Dyna-Q Algorithm](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#DynaQ)
   * [Sim-Based Search](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#SimBased)
   * [MC-Tree-Search](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MCTreeSearch)
   * [Temporal-Difference Search](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#TemporalDifferenceSearch)
   * [RL in Games](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#RLinGames)
* [Deep Q Learning (Deep Q-Networks: DQN)](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#DQN)  

   * [Experience Replay](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ExperienceReplay)
   * [DQN in Atari](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#DQNAtari)
* [Imitation Learning](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ImitationLearning)  

   * [Dagger: Dataset Aggregation](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#Dagger)
   * [PLATO: Policy Learning with Adaptive Trajectory Optimization](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#PLATO)
   * [One-Shot Imitation Learning](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#OneShotImitation)
* [Meta-Learning](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#MetaLearning)
* [POMDPs (Partial Observable MDP)](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#POMDPs)
* [Resources](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#Resources)
* [Important RL Papers](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo#ImportantRLPapers)

Extra:   Image  Generation With AI: Generative Models Tutorial with  Python+Tensorflow  Codes (GANs, VAE, Bayesian Classifier Sampling,  Auto-Regressive Models,  Generative Models in RL)

**Tutorial Link**: [**https://github.com/omerbsezer/Generative\_Models\_Tutorial\_with\_Demo**](https://github.com/omerbsezer/Generative_Models_Tutorial_with_Demo)",reinforcementlearning,obsezer,False,/r/reinforcementlearning/comments/albylq/reinforcement_learning_rl_tutorial_with_sample/
Trust Region-Guided Proximal Policy Optimization,1548845271,,reinforcementlearning,RebornHugo,False,/r/reinforcementlearning/comments/albj5j/trust_regionguided_proximal_policy_optimization/
"Off-Policy Algorithms (SAC, TD3) on Reacher tasks ?",1548835014,"Hello fellows, 

&amp;#x200B;

I have been training on-policy agents on some reachers tasks. In these environments, each episode is 300 steps and the reward is 0.01 whenever the end effector is close enough to the target otherwise 0. Using PPO, this always yield nicely trained agents, able to perform the task efficiently. 

&amp;#x200B;

However, for various reasons I've wanted to swtich to off policy algorithms, such as TD3 and SAC. But, despite the triumphs described in the papers, I have troubles making these converge. 

What do you think ? Should I design a denser reward ? Any idea to make this work ? 

&amp;#x200B;

Thanks a lot ! ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/alaira/offpolicy_algorithms_sac_td3_on_reacher_tasks/
State-of-the-art algorithms mastering different environments,1548773588,"I was wondering what the current state-of-the-art is of algorithms that can perform well in different environments? E.g., the trained agent should be able to play pong and immediatly afterwards (without retraining) be able to play breakout. 

I looked around, but can only find examples of the same algorithm that can be trained on different environments (e.g., the Mnih 2013 paper uses the same algorithm trained on different environments).",reinforcementlearning,hooptjen,False,/r/reinforcementlearning/comments/al0qlv/stateoftheart_algorithms_mastering_different/
RL Reward Design Questions,1548767535,"Hi,

&amp;#x200B;

So i have 2 questions about the reward design in RL (DDPG), so i have a package carrying environment where the ai carries a package to x point, by holding the package he holds on the package and action dropping, drops the package. I've seen an issue in my reward design, since i have the action drop is either -0.5 if he didn't drop it on the goal and +1 if he dropped it on the goal, this causes the ai to carry the package until steplimit (-1), 30 steps, he kept getting only -0.5 and never converged to a diffrent policy, so what i think is that the ai doesn't wanna take the risk or something to drop it?  how can i encourage the ai to drop the package, i was thinking about giving it only a positive reward for example +0.5 if he failed and +1 on goal, but you should never give an ai positive points for failing i've been told, so anybody got any options?

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/akzvaa/rl_reward_design_questions/
Q-learning in the continuous time limit: a real problem or not?,1548735119,"This paper https://arxiv.org/abs/1901.09732 claims: *Empirically, we find that Q-learning-based approaches such as Deep Q- learning (Mnih et al., 2015) and Deep Deterministic Policy Gradient (Lillicrap et al., 2015) collapse with small time steps. Formally, we prove that Q-learning does not exist in continuous time.* 

To me this looks like a made-up issue: regardless of time discretization in the dynamics, one can always choose an arbitrary time discretization for the actions, by switching actions at these chosen times, and keeping them fixed in between. It seems to me, all the usual Q-learning machinery will work just fine. What do you think?",reinforcementlearning,underactuated,False,/r/reinforcementlearning/comments/akw8p4/qlearning_in_the_continuous_time_limit_a_real/
"""Hierarchical Reinforcement Learning for Multi-agent MOBA Game [King of Glory]"", Zhang et al 2019 {Vivo}",1548720918,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aku26w/hierarchical_reinforcement_learning_for/
Keras playing punchout,1548705147,"I created a bot that plays NES punchout using reinforcement learning. I wanted to learn about AI and python and this seemed like a good project, it uses BizHawk as an emulator, Keras and Gym. It is in a usable state but would appreciate any help validating some assumptions on the Keras object. Source code, setup instructions and Wiki available in:

[https://github.com/mkm-dz/punchoutAI](https://github.com/mkm-dz/punchoutAI)",reinforcementlearning,mkm_dz,False,/r/reinforcementlearning/comments/akr93t/keras_playing_punchout/
"[R] Obstacle Tower Environment: inspired by Montezuma's Revenge, a benchmark for hard problems in DeepRL",1548701209,,reinforcementlearning,elzobaba,False,/r/reinforcementlearning/comments/akqjai/r_obstacle_tower_environment_inspired_by/
What positions should I apply to?,1548697871,"I am third year undergrad. Reinforcement learning is my field of interest and ideally I want to end up working at some place like Deepmind. To take me closer to my goal, I want to for a masters after my undergrad. So I decided to look for summer research internships but most research internships seem to be looking for PhDs.


As a candidate,

My pros:

Have done research projects with two professors and have letters of recommendation from them (one in RL and another in distributed computing &lt;- working on a publication in this one)

Have done various other ML projects including one for a small startup and another winning a corporate competition

Studying in my country's best university (IIT Bombay/India)


Cons:

My major is not computer science, it is mechanical engineering

My major GPA is just 8.3/10 (though my grades in computer science courses are much better)



I was advised to mail professors directly but have only been offered non funded internships that way (I did get one paid offer but it was later rescinded because the professor became too busy).

Given my goals and my current skillset/profile, what do you recommend I do in the summer? Any internships that you think would perfect for a guy like me?",reinforcementlearning,ISeeCrabPeople,False,/r/reinforcementlearning/comments/akpxyi/what_positions_should_i_apply_to/
"""Learning to Drive Smoothly in Minutes: Reinforcement Learning on a Small Racing Car"", Antonin Raffin [learning Donkey Car simulation with SAC and VAE features]",1548691500,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/akou2p/learning_to_drive_smoothly_in_minutes/
How to properly train Q-matrix (Q-learning)?,1548684098,"First off, I'm new to Reinforcement Learning, so apologies if this is a trivial question. 

&amp;#x200B;

Basically, I am trying to implement Reinforcement Learning to calculate an optimal order policy (i.e. how much should be ordered each day) so as to minimize my total cost. I am using a Q-matrix where my states are dependant on how much stock I have each day and my actions will be how much to order (for example, action 1 = order 40 units, action 2 = order 50, etc.). 

&amp;#x200B;

However, the Q-matrix learning isn't working properly, because when I use my reward function as my cost, the values of the Q-matrix decrease with each iteration, which means when I'm exploiting (rather than exploring) I'll be choosing the maximum value in whatever row I'm in, but then that'll decrease as I do more iterations (since my reward returns a negative value). Am I doing something wrong? Here's the relevant part of the code:

&amp;#x200B;

    # Training
    
    # Ip - on-hand inventory
    # h - on-hand inventory cost
    # Im - lost orders
    # b - lost orders cost
    # T - no. of days
    
    def reward(t):
        return h*Ip[t]+b*Im[t]
    
    Q = np.matrix(np.zeros([9,9]))
    
    iteration = 0
    t = 0
    MAX_ITERATION = 500
    alp = 0.2 # learning rate (between 0 and 1)
    exploitation_p = 0.15 # exploitation probability (incresed after each iteration until it reaches 1)
    
    while iteration &lt;= MAX_ITERATION:
        while t &lt; T-1:
            if Ip[t] &lt;= 10:
                state = 0
            elif Ip[t] &gt; 10 and Ip[t] &lt;= 20:
                state = 1
            elif Ip[t] &gt; 20 and Ip[t] &lt;= 30:
                state = 2
            elif Ip[t] &gt; 30 and Ip[t] &lt;= 40:
                state = 3
            elif Ip[t] &gt; 40 and Ip[t] &lt;= 50:
                state = 4
            elif Ip[t] &gt; 50 and Ip[t] &lt;= 60:
                state = 5
            elif Ip[t] &gt; 60 and Ip[t] &lt;= 70:
                state = 6
            elif Ip[t] &gt; 70 and Ip[t] &lt;= 80:
                state = 7
            elif Ip[t] &gt; 80:
                state = 8
            
            rd = random.random()
            if rd &lt; exploitation_p:
                action = np.where(Q[state,] == np.max(Q[state,]))[1]
                if np.size(action) &gt; 1:
                    action = np.random.choice(action,1)
            else:
                av_act = np.where(Q[state,] &lt; 999999)[1]
                action = np.random.choice(av_act,1)
            action = int(action)
            rew = reward(t+1)
    
            if Ip[t+1] &lt;= 10:
                next_state = 0
            elif Ip[t+1] &gt; 10 and Ip[t+1] &lt;= 20:
                next_state = 1
            elif Ip[t+1] &gt; 20 and Ip[t+1] &lt;= 30:
                next_state = 2
            elif Ip[t+1] &gt; 30 and Ip[t+1] &lt;= 40:
                next_state = 3
            elif Ip[t+1] &gt; 40 and Ip[t+1] &lt;= 50:
                next_state = 4
            elif Ip[t+1] &gt; 50 and Ip[t+1] &lt;= 60:
                next_state = 5
            elif Ip[t+1] &gt; 60 and Ip[t+1] &lt;= 70:
                next_state = 6
            elif Ip[t+1] &gt; 70 and Ip[t+1] &lt;= 80:
                next_state = 7
            elif Ip[t+1] &gt; 80:
                next_state = 8
            
            if rd &lt; exploitation_p:
                next_action = np.where(Q[next_state,] == np.max(Q[next_state,]))[1]
                if np.size(next_action) &gt; 1:
                    next_action = np.random.choice(next_action,1)
            else:
                av_act = np.where(Q[next_state,] &lt; 999999)[1]
                next_action = np.random.choice(av_act,1)
            next_action = int(next_action)
    
            Q[state, action] = Q[state, action] + alp*(-rew+Q[next_state, next_action]-Q[state, action])
    
            t += 1
        if (exploitation_p &lt; 1):
            exploitation_p = exploitation_p + 0.05
        t = 0
        iteration += 1

Thanks in advance",reinforcementlearning,Mini_Miudo,False,/r/reinforcementlearning/comments/aknokt/how_to_properly_train_qmatrix_qlearning/
"RL Weekly 6: AlphaStar, Rectified Nash Response, and Causal Reasoning with Meta RL",1548673450,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/akmffb/rl_weekly_6_alphastar_rectified_nash_response_and/
Reinforcement learning tutorial series,1548669358,"Hey guys, I have started a tutorial series on youtube about reinforcement learning. I'll cover the topic in detail starting from multi-armed bandits to Policy gradient approaches. Then I'll also cover the modern approaches that use multiple ideas from different fields and blend it to have new reinforcement learning algorithms. So far I have 3 videos. Please give your feedback about the videos. This is my first time speaking in front of camera and editing for youtube. 

Channel name: AI Talks - Rituraj Kaushik

Channel: https://www.youtube.com/channel/UCwrblBV2g0m8SuG8jQbhjuA",reinforcementlearning,Riturajkaushik,False,/r/reinforcementlearning/comments/akm0t2/reinforcement_learning_tutorial_series/
Need Help understand Monte Carlo Tree Search:,1548658166,[removed],reinforcementlearning,Sharoon95,False,/r/reinforcementlearning/comments/akkwq5/need_help_understand_monte_carlo_tree_search/
Can't understand Monte Carlo Tree Search:,1548655713,"I have been setting my sights on Reinforced learning lately and happen to come across from Analytics Vidhya, they have mentioned a control flow how the Monte Carlo tree works.

in the first iteration of which:

1. We start from S0 from which two possible nodes are present, namely: S1 and S2
2. As we are traversing the tree for the first time, the score of both nodes are ZERO ( t = 0) and they have never been visited before (n = 0)
3. Therefore we Assign S2( t = inf ) so that that node is traversed in the second run and move to S1.
4. Once we step on the node S1, ( t = 0) ( n = 0 ). we do a Rollout....

now thats where I am stuck,

1. Why did we not simply rollout at S0, because it was not traversed or scored either.
2. WHAT EXACTLY is a rollout doing here.

I have a hard time understanding this, would appreciate your help.!",reinforcementlearning,Sharoon95,False,/r/reinforcementlearning/comments/akkmce/cant_understand_monte_carlo_tree_search/
Built a basic Cartpole balancing agent with Deep Q-Learning. #newbie_in_rl,1548652040,,reinforcementlearning,akshaynathr,False,/r/reinforcementlearning/comments/akk5ng/built_a_basic_cartpole_balancing_agent_with_deep/
optimal control of plant dynamics specified as a neural network,1548630997,"Hey all,   


I have a question regarding control of a plant, dynamics of which is specified as a neural network function f(s\_t-1,u) that takes in previous state and control, and outputs the next state. How can I go about designing an optimal controller for this plant, given an objective function J(s,u) ? ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/akh40n/optimal_control_of_plant_dynamics_specified_as_a/
Attention-LSTM initialization of the environment,1548630887,"My question concerns the use of attention with an LSTM to make sequential decision. Essentially, my agent has to make decisions at each time step and will receive a reward only at the end of the sequence based on the sequence of decisions and states observed. The number of time steps until the end of an episode is always known (fixed prior to the problem). 

I want to use an attention-LSTM that outputs the decision (a real value). With only an LSTM, my method is working, but the attention part could help the performance and will help to understand why each decision was picked. 

To the best of my knowledge, in my case, I need to use a window size of the past ‘T’ hidden states of my encoder with the attention model. This can be justified from the theoretical point of view of my problem. 

My main concern is how to start off the environment. Say I use a window of T = 10 attention weights. For the time-step t = 0,…,8, what do I do with the attention weights? Should I pad with zeros the missing hidden states such that some of the attention weights will be put to zero, and the sum of the weights still be 1? I have not been able to find a paper that has a similar set-up than mine, but I’m not at all familiar with the attention literature. I'm sure this is a common concern with many problems.

Also, if I do need to pad with zeros the missing hidden states, is this something that can be automatically done with deep learning libraries such as Tensorflow or Keras?

Thanks a lot, hopefully my question is clear. ",reinforcementlearning,ranirlol,False,/r/reinforcementlearning/comments/akh3an/attentionlstm_initialization_of_the_environment/
"""Construction of arbitrarily strong amplifiers of natural selection using evolutionary graph theory"", Pavlogiannis et al 2018 [creating subpopulations for more efficient selection]",1548617077,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/akf0gw/construction_of_arbitrarily_strong_amplifiers_of/
"Need, updated version from DQfD for LSTM models",1548616801,"I want to train a model from demonstrations. In the past I used DQfD with frame stacking. 

My problem is now, I would like to train a LSTM based model. DQfD performs after each timestamp a optimisation sampled from a ring buffer. I am very sure the random sampling from a ring buffer doesn't work with LSTMs.

&amp;#x200B;

Does anyone know a updated version of DQfD that can handel a LSTM model?",reinforcementlearning,XMasterDE,False,/r/reinforcementlearning/comments/akeyqb/need_updated_version_from_dqfd_for_lstm_models/
Any ideas for research in RL?,1548588041,"I am currently studying at the 3rd year of BSc CS. As a part of the course, I need to do a coursework on something.

I had a very interesting course on RL in the university and decided to make it the field of the coursework. The lecturer agreed to consult me on the work, but he also said that he didn't really have any ideas for the research. Other professors in the university said that they didn't have any time for me.

What are the cool things in RL that I can research? I guess that many people here have cool ideas that they don't have enough time for.",reinforcementlearning,PeterZhizhin,False,/r/reinforcementlearning/comments/akb8xs/any_ideas_for_research_in_rl/
Any RL finance environments ?,1548576637,"Hi ! 

&amp;#x200B;

Do you guys know any RL environment for training agents to trade stocks ? Or do I just have to create one myself, based on scrapped financial data ? 

&amp;#x200B;

Thanks ! (: ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/aka92e/any_rl_finance_environments/
DeepControl.ai: Front Page of Deep Reinforcement Learning and AGI,1548523734,,reinforcementlearning,deepcontrolai,False,/r/reinforcementlearning/comments/ak2w87/deepcontrolai_front_page_of_deep_reinforcement/
Faulty Reward Functions in the Wild,1548505936,,reinforcementlearning,dimber-damber,False,/r/reinforcementlearning/comments/ak0hid/faulty_reward_functions_in_the_wild/
Controlling running training session remotely,1548480691,"Someone posted their way of changing learning rate and monitoring the progress of a running training session remotely, but I was at work and just glanced at it without saving it and now I can't find it. Did anyone save it, or is the maker around?",reinforcementlearning,evanatyourservice,False,/r/reinforcementlearning/comments/ajy5sz/controlling_running_training_session_remotely/
"[R] ""Causal Reasoning from Meta-reinforcement Learning"", Dasgupta et al 2019",1548453854,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/aju8y4/r_causal_reasoning_from_metareinforcement/
"""PSRO_rN: Open-ended Learning in Symmetric Zero-sum Games"", Balduzzi et al 2019 {DM} [PBT]",1548444836,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ajso97/psro_rn_openended_learning_in_symmetric_zerosum/
"We are Oriol Vinyals and David Silver from DeepMind’s AlphaStar team, joined by StarCraft II pro players TLO and MaNa! Ask us anything",1548364959,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ajhezu/we_are_oriol_vinyals_and_david_silver_from/
"""AlphaStar: Mastering the Real-Time Strategy Game StarCraft II"" {DM} [AS architecture, training, progress curves, saved games]",1548362774,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ajgv2z/alphastar_mastering_the_realtime_strategy_game/
"DeepMind's ""AlphaStar"" StarCraft 2 demonstration livestream [begins in 1h from submission]",1548349214,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ajeg5m/deepminds_alphastar_starcraft_2_demonstration/
Conceptual confusion in backpropagation in Deep Q Network?,1548345901,"This might seem like a trivial question, but I was going through the code of Deep Q Network in pytorch as mentioned in the tutorial `https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#sphx-glr-download-intermediate-reinforcement-q-learning-py` and in there, there is the following code segment.
```
    # Compute V(s_{t+1}) for all next states.
    # Expected values of actions for non_final_next_states are computed based
    # on the ""older"" target_net; selecting their best reward with max(1)[0].
    # This is merged based on the mask, such that we'll have either the expected
    # state value or 0 in case the state was final.
    next_state_values = torch.zeros(BATCH_SIZE, device=device)
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()
    # Compute the expected Q values
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Compute Huber loss
    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))
```
In the above code in the line `next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()` there is a `detach` call for the gradient for picking the action with max value.
So my question: **We calculate the loss in DQN using the MSE or smooth_l1_loss between the Q value calculated for a given action for both `s'` and `s` and let `a` and `a'` be the actions that got maximum Q value from the network. So when we backpropagate the loss do we only backpropagate through the action `a` and 0 through other actions, if yes then why is there a detach in the code , Else , if we backprop the loss for action in the same way for `a`, isn't this step conceptually flawed because, we should only modify the action that affects the Q value.**


",reinforcementlearning,intergalactic_robot,False,/r/reinforcementlearning/comments/ajdv8t/conceptual_confusion_in_backpropagation_in_deep_q/
"DeepMind, StarCraft II, and The Next Big Thing in AI",1548342535,,reinforcementlearning,Inori,False,/r/reinforcementlearning/comments/ajdb49/deepmind_starcraft_ii_and_the_next_big_thing_in_ai/
"Generative Models Tutorial with Demo/Code (GANs, VAE, Bayesian Classifier Sampling, Auto-Regressive Models, Generative Model in Reinforcement Learning)",1548248692,"Generative   models are a subset of unsupervised learning that generate new   sample/data by using given some training data. There are different types   of ways of modelling same distribution of training data:   Auto-Regressive models, Auto-Encoders and GANs. In this tutorial, we are   focusing theory of generative models, demonstration of generative   models, important papers, courses related generative models. It will   continue to be updated over time.

Generative   Models Tutorial with Demo: Bayesian Classifier Sampling, Variational   Auto Encoder (VAE), Generative Adversial Networks (GANs), Popular GANs   Architectures (DCGAN, CycleGAN, Pix2Pix, PixelDTGAN, SRGAN, StackGAN,   TPGAN, 3DGAN, Age-cGAN, DiscoGAN, AnoGAN, IcGAN, MidiNet, etc. ),   Auto-Regressive Models (PixelRNN, PixelCNN), Important Generative Model   Papers, Courses, etc.

**Tutorial Link**: [**https://github.com/omerbsezer/Generative\_Models\_Tutorial\_with\_Demo**](https://github.com/omerbsezer/Generative_Models_Tutorial_with_Demo)

There are lots of applications for generative models:

* Image Denoising,
* Image Enhancement,
* Image Inpainting,
* Super-resolution (upsampling): SRGAN,
* Generate 3D objects: 3DGAN, [Video](https://youtu.be/HO1LYJb818Q)
* Creating Art:  

   * Create Anime Characters
   * Transform images from one domain (say real scenery) to another domain (Monet paintings or Van Gogh):CycleGAN
   * Creating Emoji: DTN
* Pose Guided Person Image Generation
* Creating clothing images and styles from an image: PixelDTGAN
* Face Synthesis: TP-GAN
* Image-to-Image Translation: Pix2Pix  

   * Labels to Street Scene
   * Aerial to Map
   * Sketch to Realistic Image
* High-resolution Image Synthesis
* Text to image: StackGAN
* Text to Image Synthesis
* Learn Joint Distribution: CoGAN
* Transfering style (or patterns) from one domain (handbag) to another (shoe): DiscoGAN
* Texture Synthesis: MGAN
* Image Editing: IcGAN
* Face Aging: Age-cGAN
* Neural Photo Editor
* Medical (Anomaly Detection): AnoGAN
* Music Generation: MidiNet
* Video Generation

Extra: Reinforcement Learning Tutorial:

[**https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo**](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)",reinforcementlearning,obsezer,False,/r/reinforcementlearning/comments/aizhy7/generative_models_tutorial_with_democode_gans_vae/
"[R] ""Understanding Multi-Step Deep Reinforcement Learning: A Systematic Study of the DQN Target"", Hernandez-Garcia et al 2019",1548238796,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/aiyc9m/r_understanding_multistep_deep_reinforcement/
Policy gradient optimal constant baseline confusion,1548234896,"The formula for the optimal constant baseline confuse me. The formula shown here is from this paper: [http://www-clmc.usc.edu/publications/P/peters-NN2008.pdf](http://www-clmc.usc.edu/publications/P/peters-NN2008.pdf) section 3.2.  The optimal baseline is a value for each parameter of the gradient, and b\^h is the computation for one of those baseline values.

What i dont get is why this can be anything other than just: \\sum\_{l=0}\^H a\_l r\_l

The sum of gradients that is squared is equivalent in the numerator and denominator, and it is just a scalar. **So why dont they just cancel out and we get \\sum\_{l=0}\^H a\_l r\_l  for every parameter?**

![img](fmg88hdj25c21 ""Optimal baseline for parameter h"")",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/aixxiu/policy_gradient_optimal_constant_baseline/
Sampling from Stochastic Policy,1548222815,"Hello,

&amp;#x200B;

I was reading the Spinning Up in RL's Vanilla Policy Gradient code and I have a question. To sample an action from the policy network (which is a multi perceptron), a multinomial distribution is used. And the code uses a simple line like this to sample an action from this policy.
```
 action = tf.squeeze(tf.multinomial(logits,1), axis=1)
```
where logits is the output of the MLP (4 actions to be precise)

&amp;#x200B;

I am unclear as to why we use a multinomial distribution to sample actions instead of just taking the max value out of the logits. 

I have read online and the aim to do this is to balance exploration and exploitation because the policy is stochastic and thus gives a random element while training. However, won't this lead to the network not learning anything if we use a random sample from a distribution. 

Or I might be unclear of what exactly ""sampling from a distribution"" means. Is this sampling random or somehow biased towards the higher probability actions ? 

&amp;#x200B;

I would be really grateful if someone can answer this query. 

&amp;#x200B;

Thanks !

&amp;#x200B;",reinforcementlearning,karanchahal1996,False,/r/reinforcementlearning/comments/aiwlgb/sampling_from_stochastic_policy/
"""The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions"", Garcia-Garcia et al 2019",1548212859,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aiv6xe/the_robotrix_an_extremely_photorealistic_and/
Understanding MADDPG: Multi Agent Actor-Critic with Experience Replay,1548198286,,reinforcementlearning,forgaibdi,False,/r/reinforcementlearning/comments/aisu4y/understanding_maddpg_multi_agent_actorcritic_with/
"Oriol Vinyals discusses DM SC2 progress as of early November 2018, BlizzCon ""What's Next"" panel discussion: imitation-learning agent which solves micro and has learned camera control &amp; macro",1548180321,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aipjab/oriol_vinyals_discusses_dm_sc2_progress_as_of/
DeepMind schedules StarCraft 2 demonstration on YouTube: Thursday 24 January 2019 at 6PM GMT / 1PM ET / 10AM PT,1548173617,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aiocrt/deepmind_schedules_starcraft_2_demonstration_on/
Multi Agent Systems Frameworks,1548158385,"Hi,

I am looking to implement a simulation for a problem that I am working on which involves a large number of noncooperative agents and looking for a framework that will help me write this simulation.

When looking for frameworks online I found that most of them weren't updated in a long time which made me worry a little bit.

Is there any new frameworks (in any programming language) that fit these settings and are easy to use or do researchers usually implement this stuff by themselves?

Thanks in advance.",reinforcementlearning,liormoshe,False,/r/reinforcementlearning/comments/aim5r8/multi_agent_systems_frameworks/
[R] Fully Convolutional Network with Multi-Step Reinforcement Learning for Image Processing,1548154171,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/ailp9c/r_fully_convolutional_network_with_multistep/
Is there any application where DQN can do better than A3C/PPO?,1548104621,"Or with the policy gradient, A3C/PPO can always do better than DQN. Is that true?",reinforcementlearning,deepbluesome,False,/r/reinforcementlearning/comments/aiew5b/is_there_any_application_where_dqn_can_do_better/
Is there any applications where DQN can do better than A3C or PPO?,1548102904,"Or for anything DQN can do, A3C can do better?",reinforcementlearning,deepbluesome,False,/r/reinforcementlearning/comments/aieguq/is_there_any_applications_where_dqn_can_do_better/
Video analysis with deep reinforcement learning.,1548086822,"I just have the basic understanding of deep reinforcement learning. So I was wondering if it be possible to design one for video analysis.
So we have a video of poses of people. Can we train the model to predict whether the person is standing or running?
Now, we may have some hand engineered methods for prediction but, I want it to be scaled to a large no of people.
If the first task of prediction using a single entity is possible, then I shall try to implement the second idea using an open source implementation.
Can you tell me if the idea is feasible?
Thank you in advance. ",reinforcementlearning,crazy_lazy_life,False,/r/reinforcementlearning/comments/aibhs3/video_analysis_with_deep_reinforcement_learning/
ANYmal dog learned by RL,1548064854,"Wow this is huge! ANYmal scientists from ETH Zurich used TRPO to teach theirs mechanical dog to move and standing up after falling. 

[https://www.youtube.com/watch?v=aTDkYFZFWug](https://www.youtube.com/watch?v=aTDkYFZFWug)

[http://robotics.sciencemag.org/content/4/26/eaau5872](http://robotics.sciencemag.org/content/4/26/eaau5872)

Can't wait to see more RL based application in real world!",reinforcementlearning,WhichPressure,False,/r/reinforcementlearning/comments/ai8q07/anymal_dog_learned_by_rl/
"RL Weekly 5: Robust Control of Legged Robots, Compiler Phase-Ordering, and Go Explore on Sonic the Hedgehog",1548062225,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/ai8ggo/rl_weekly_5_robust_control_of_legged_robots/
RL Conferences,1548061336,"Hi,

Lastly I found AAMAS conference in Montreal. When I read last year agenda I noticed that it was mostly about reinforcement learning. Was anyone on this conference? Did you like it?

Do you guys know any other RL conferences?

Best regards!",reinforcementlearning,WhichPressure,False,/r/reinforcementlearning/comments/ai8d83/rl_conferences/
Fetch robotics - policy gets much worse as updates are processed,1548024027,"I am training HER and TD3 on the pick and place fetch robotics environment

&amp;#x200B;

I don't understand why, as soon as the gradient updates are passed to the value and policy network, the exploration policy looks far, far worse in a **systematic** way

&amp;#x200B;

As soon as the first update, the policy starts systematically pulling **away** from the table, over many runs 

&amp;#x200B;

I would expect the policy is be very bad, in a random sense, but instead of being random the policy is updated in such a way that it **pulls away** from the table consistently...

&amp;#x200B;

Is this a sign of some error in the code? ",reinforcementlearning,Spiritual_Doughnut,False,/r/reinforcementlearning/comments/ai3asa/fetch_robotics_policy_gets_much_worse_as_updates/
[R] Depth-Limited Solving for Imperfect-Information Games NIPS18,1548019078,,reinforcementlearning,yazriel0,False,/r/reinforcementlearning/comments/ai2gc8/r_depthlimited_solving_for_imperfectinformation/
SOTA,1548006541,I have been on a break from literature review on RL and would like to catch up. What are the current State of the Art (SOTA) methods for discrete and continuous state and action space RL problems? And any interesting new works to look for ?!  Thanks ! ,reinforcementlearning,guruji93,False,/r/reinforcementlearning/comments/ai08qt/sota/
Deep Reinforcement Learning with TensorFlow 2.0,1547993630,,reinforcementlearning,Inori,False,/r/reinforcementlearning/comments/ahxya8/deep_reinforcement_learning_with_tensorflow_20/
"[R] Real robot trained via simulation and reinforcement learning is capable of running, getting up and recovering from kicks",1547921749,,reinforcementlearning,P_a_t_RICK,False,/r/reinforcementlearning/comments/ahojwp/r_real_robot_trained_via_simulation_and/
[N] Stable-Baselines 2.4.0 released: Soft Actor-Critic (SAC) and easy policy customization,1547903610,,reinforcementlearning,araffin2,False,/r/reinforcementlearning/comments/ahlyvy/n_stablebaselines_240_released_soft_actorcritic/
Standard environments for exploration research with continuous actions?,1547891008,"AFAIK for discrete actions, the harder tasks in Atari (e.g. Montezuma's Revenge) or maybe some variants of VizDoom (e.g. My Way Home scenario) are often used to evaluate RL algorithms focused on exploration (please add if you know more). However, I do not know much for continuous environments. I think Mujoco is the most often used environment framework, but most if not all of the commonly used envs have dense rewards (from Hopper to Humanoid). I'm sure you can modify existing environments to change the reward function to something more sparse, but are there commonly used/available environments for exploration in continuous action spaces?",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/ahkrce/standard_environments_for_exploration_research/
An Intuitive Explanation of the Theory and PyTorch Implementation Guide for Soft Actor-Critic (State of the Art RL Algo),1547825911,"Hope this helps you guys

[https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665](https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665)",reinforcementlearning,vaishak2future,False,/r/reinforcementlearning/comments/ahbnjd/an_intuitive_explanation_of_the_theory_and/
"""ReJOIN: Towards a hands-free query optimizer through deep learning""",1547823681,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ahb9pn/rejoin_towards_a_handsfree_query_optimizer/
"In DQN, what is the real reason we don't backpropagate through the target network?",1547787263,"Actually, if we are to backpropagate through the target network, there is no use for the target network anymore.

Let's say we don't use any target network. We hence enforce only the ""temporal difference"" allowing gradients to flow through both ways i.e. q(s,.) and q(s',.).

This completes the gradient and prevents known instabilities of DQN. While this training is much more stable, it is not widely used.

**My question is why not? Where is the catch?**

&amp;#x200B;

More information (from Sutton 2018):

\- DQN seems to use something called ""semi-gradient"" which is unstable in a sense that it has no convergence guarantee

\- Since the value can diverge, a target network is used to mitigate this

\- However, there is also a ""full-gradient"" version of TD learning which has a convergence guarantee.

\- In the book, there is no conclusive evidence to show which objective is better than other.",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/ah6y01/in_dqn_what_is_the_real_reason_we_dont/
Amplifying the Imitation Effect for Reinforcement Learning of UCAV’s Mission Execution,1547784923,[https://arxiv.org/pdf/1901.05856.pdf](https://arxiv.org/pdf/1901.05856.pdf),reinforcementlearning,bluediary8,False,/r/reinforcementlearning/comments/ah6mp4/amplifying_the_imitation_effect_for_reinforcement/
"""Learning from Dialogue after Deployment: Feed Yourself, Chatbot!"", Hancock et al 2019 {FB}",1547775871,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ah5b07/learning_from_dialogue_after_deployment_feed/
"""Evolving super stimuli for real neurons using deep generative networks"", Ponce et al 2019",1547756507,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ah20kz/evolving_super_stimuli_for_real_neurons_using/
"Learning agile and dynamic motor skills for legged robots (Science) [Sim-to-real, Curriculum learning, RL for continuous control, System ID]",1547690573,,reinforcementlearning,p-morais,False,/r/reinforcementlearning/comments/agssak/learning_agile_and_dynamic_motor_skills_for/
"[D] While ML compute doubles every 3.5 months, RL compute doubles every 2 months",1547682493,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/agrjcu/d_while_ml_compute_doubles_every_35_months_rl/
"[D] While ML compute doubles every 3.5 months, RL compute doubles every 2 months.",1547681726,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/agrewb/d_while_ml_compute_doubles_every_35_months_rl/
"[N] ""A Poker-Playing Robot Goes to Work for the Pentagon"" [Libratus use in wargaming]",1547660020,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/agnikz/n_a_pokerplaying_robot_goes_to_work_for_the/
"""SageDB: a learned database system""",1547659940,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/agni18/sagedb_a_learned_database_system/
[D] Go Explore VS _Sonic the Hedgehog_,1547648108,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aglivl/d_go_explore_vs_sonic_the_hedgehog/
"""AutoPhase: Compiler Phase-Ordering for High Level Synthesis with Deep Reinforcement Learning"", Haj-Ali et al 2019",1547606484,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aggkfa/autophase_compiler_phaseordering_for_high_level/
"""ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware"", Cai et al 2018",1547597033,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/agf4np/proxylessnas_direct_neural_architecture_search_on/
How to change episode length and reward parameters in Spinningup RL?,1547585720,"I would like to play with modified Openai RL tasks, using Spinning Up RL. For example, in the Pendulum environment, I would like to increase the episode length (say, to 500 or 1000 from the current 200) and the action penalty coefficient (say, to 1 from the current 0.001). What would be the easiest way to do it? Do I need to create my own environment?",reinforcementlearning,underactuated,False,/r/reinforcementlearning/comments/agd6j4/how_to_change_episode_length_and_reward/
"""Looking Back at Google’s Research Efforts in 2018"", Jeff Dean",1547585172,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/agd30d/looking_back_at_googles_research_efforts_in_2018/
"""AutoML: Automating the design of machine learning models for autonomous driving"" {G} [AutoAutoML?]",1547583802,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/agcu9j/automl_automating_the_design_of_machine_learning/
"""Sim2Real – Using Simulation to Train Real-Life Grasping Robots""",1547518992,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ag3k57/sim2real_using_simulation_to_train_reallife/
"""Designing neural networks through neuroevolution"", Stanley et al 2019 {Uber/Sentient} [review]",1547500006,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ag0ecj/designing_neural_networks_through_neuroevolution/
"""Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation"", Li et al 2019 {G}",1547496720,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/afzt4b/autodeeplab_hierarchical_neural_architecture/
Menace - An AI-based Tic Tac Toe in python,1547483701,,reinforcementlearning,Hsankesara,False,/r/reinforcementlearning/comments/afxj3m/menace_an_aibased_tic_tac_toe_in_python/
Using evolution strategies to solve RL envs ?,1547478120,"Hey ! 

&amp;#x200B;

Can someone with a normal pc configuration report success when using evolution strategies to solve RL envs such as BipedalWalker ? I was really interested by these works, but when reading papers from Uber and OpenAI, I realized the used hundreds of cores to do their stuff. I've also  been using hardmaru implementations but they seems to take a long time to converge or even improve a little. 

&amp;#x200B;

Anyone has some tips ? 

&amp;#x200B;

Thanks ! (:   ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/afwlqm/using_evolution_strategies_to_solve_rl_envs/
"RL Weekly 4: Generating Problems with Solutions, Optical Flow with RL, and Model-free Planning",1547467856,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/afv7w9/rl_weekly_4_generating_problems_with_solutions/
Reinforcement Learning in the Wild and Lessons Learned,1547439825,"Hi folks,

I am new to Reddit and I am glad to find this subreddit. I worked at Samsung for 4 years on applied RL problems and I am sharing in this post my lessons learned of applying RL in the wild [https://medium.com/@mohamadtweets/reinforcement-learning-in-the-wild-and-lessons-learned-c4b7aa04c3a5](https://medium.com/@mohamadtweets/reinforcement-learning-in-the-wild-and-lessons-learned-c4b7aa04c3a5). Please let me know what you think, what I got right, and what I missed. Thanks.

*Processing img h5w1edmseba21...*",reinforcementlearning,mohamadc,False,/r/reinforcementlearning/comments/afs1ju/reinforcement_learning_in_the_wild_and_lessons/
"""DRC: An investigation of model-free planning"", Guez et al 2019 {DM} [learning implicit planning in LSTM CNNs; &gt;Ape-X scores on 3/5 ALE]",1547433030,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/afr26g/drc_an_investigation_of_modelfree_planning_guez/
Gaussian Mixtures for DDPG-likes?,1547420069,"I am currently looking into Soft Actor Critic and its [implementation](https://github.com/haarnoja/sac/tree/master/sac), but I'm having trouble understanding how they can use Gaussian mixture policies. If it's a simple Gaussian policy, I can see how you can use the reparameterization trick and just plug the output into the Q function etc. but does that work with GMMs? I don't think there is a reparameterization thing for sampling the mixture part, so my best guess is that you would have to use policy gradients to account for that. What is the correct understanding for this? Thanks",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/afp0wc/gaussian_mixtures_for_ddpglikes/
Recreating grid-world game design from a publication.,1547414219,"Hi, I'm trying to recreate grid-world game presented here [https://arxiv.org/abs/1707.06600](https://arxiv.org/abs/1707.06600) \[""A multi-agent reinforcement learning model of common-pool resource appropriation""\] to later apply a number of multi-agent RL algorithms to it. But I'm really struggling with creating the game to which I could easily connect a number of algorithms (DQN, MADDPG, LOLA). I know how to code but don't exactly know how to build the game from scratch ensuring a number of RL algorithms could be easily connected to it.

&amp;#x200B;

Should I build the environment using OpenAI gym? Should I use something else? I would really appreciate any tips, links and help. Thanks!",reinforcementlearning,mw_molino,False,/r/reinforcementlearning/comments/afo0bm/recreating_gridworld_game_design_from_a/
Calculating Loss in a continuous Action space for PPO,1547406373,"Can someone explain to me how i calculate the Loss for a PPO-Algorithm when having a continuous action space? I need to calculate the Propability of old Policy divided by Probability of new Policy, but how do I adapt this to continuous action that is sampled from a normal distribution? ",reinforcementlearning,unieye,False,/r/reinforcementlearning/comments/afmk5x/calculating_loss_in_a_continuous_action_space_for/
"Curiosity, Reward Sign Bias, and Political Orientation in Reinforcement Learning",1547326883,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/afbno1/curiosity_reward_sign_bias_and_political/
"""Malthusian Reinforcement Learning"", Leibo et al 2018 {DM}",1547315496,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/af9qjr/malthusian_reinforcement_learning_leibo_et_al/
"""StrokeNet: A Neural Painting Environment"", Zheng et al 2018 [SPIRAL with a learned/differentiable painting 'world model']",1547235330,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aeyz2i/strokenet_a_neural_painting_environment_zheng_et/
Mini-Push Environment with Hindsight Experience Replay in TF Eager [w/ Colab Notebook],1547225476,"I recently experimented with Hindsight Experience Replay with DDPG with TensorFlow Eager. Since many environments used in papers require millions of samples, I tried to create a similar task to the Fetch Push (pushing a box in a goal location) but in a grid world, solvable in significantly fewer episodes. In the notebook it's also possible to see how, without HER, the task is much harder. 

You should be able to run the code in Colab.

[https://github.com/normandipalo/mini-push-for-her](https://github.com/normandipalo/mini-push-for-her)",reinforcementlearning,dantehorrorshow,False,/r/reinforcementlearning/comments/aex9pg/minipush_environment_with_hindsight_experience/
How was the code base for open-ai baselines designed ?,1547219112,"How do the programmers for big libraries like open-ai baselines or tensorflow structure their code ? When I try to read their code, I see that they have split their code into various classes, utilized different data structures, structured their code into different files and sorted them in a particular order. Not to forget the extensive documentation they've provided for all functions with examples. When I start with a programming task, I somehow donot seem to think about all these factors. How do I develop that skill ? Is this what is called production level code ?   


I have never worked in a company before. I've mostly spent my time in school writing academic code. Any insights will be appreciated. Thanks ! ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/aew7vt/how_was_the_code_base_for_openai_baselines/
What are some situations when GAE doesn't​ work?,1547210378,[removed],reinforcementlearning,hr375,False,/r/reinforcementlearning/comments/aeuzgo/what_are_some_situations_when_gae_doesnt_work/
"[R] An Atari Model Zoo for Analyzing, Visualizing, and Comparing Deep Reinforcement Learning Agents (Uber AI/Google Brain/OpenAI)",1547209285,,reinforcementlearning,aiismorethanml,False,/r/reinforcementlearning/comments/aeuum6/r_an_atari_model_zoo_for_analyzing_visualizing/
"Ilya Sutskever (OpenAI): ""I see no worthy games for RL besides Dota2""",1547186508,"So in his [2018 talk](https://www.youtube.com/watch?v=w3ues-NayAs) Ilya Sutskever, co-founder of OpenAI was asked what other games are harder then Dota 2. He said that after Dota 2 he doesn't see any game worthy. That's weird, it's not like we ""solved"" all complex games. For example:

1. Strategy games. DeemMind is working on Starcraft 2 for 2-3+ years. They reported literally 0 progress on it. Must be really hard, hence the silence. 
2. What about ""primitive"" SNES / Sega games. Where are agents, that are able to beat the WHOLE game with a good score? There are none. 
3. What about any modern AAA game? Where are the agents that can beat any modern game? There are none.

Those are all very complex RL problems. I was very surprised he thinks that Dota 2 is the hardest games can be.

&amp;#x200B;

What do you think?

&amp;#x200B;",reinforcementlearning,Roboserg,False,/r/reinforcementlearning/comments/aese8o/ilya_sutskever_openai_i_see_no_worthy_games_for/
Are bandits problems constrained to discrete action spaces?,1547130106,"I'm looking at [this](https://github.com/tensorflow/models/tree/master/research/deep_contextual_bandits) implementations, and all datasets given as examples were using discrete actions.",reinforcementlearning,quazar42,False,/r/reinforcementlearning/comments/aejjuu/are_bandits_problems_constrained_to_discrete/
"""Continual Match Based Training in Pommerman: Technical Report"", Peng et al 2018 [1st place]",1547085656,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aee7jq/continual_match_based_training_in_pommerman/
"""Model-Predictive Policy Learning with Uncertainty Regularization for Driving in Dense Traffic"", Henaff et al 2018",1547082422,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aedpyo/modelpredictive_policy_learning_with_uncertainty/
"How to systematically stop training for ""convergence"" with DDPG?",1547070227,,reinforcementlearning,JourneyTheMan,False,/r/reinforcementlearning/comments/aebqze/how_to_systematically_stop_training_for/
"DDPG for discrete Actions, plausible, but how?",1547063168,"Hello,

&amp;#x200B;

so i am trying to get my my ddpg network implementation to work with a discrete action space, i've read that it is possible.

&amp;#x200B;

this is how my network looks like:

&amp;#x200B;

Actor:

        def _build_a(self, s, reuse=None, custom_getter=None):
            trainable = True if reuse is None else False
            with tf.variable_scope('Actor', reuse=reuse, custom_getter=custom_getter):
                net = tf.layers.dense(s, 50, activation=tf.nn.relu, name='l1', trainable=trainable)
                mid = tf.layers.dense(net, 3, activation=tf.nn.tanh, name='mid', trainable=trainable)
                a = tf.layers.dense(mid, 3, activation=tf.nn.softmax, name='a', trainable=trainable)
                return a 

Critic:

        def _build_c(self, s, a, reuse=None, custom_getter=None):
            trainable = True if reuse is None else False
            with tf.variable_scope('Critic', reuse=reuse, custom_getter=custom_getter):
    #            s = tf.layers.batch_normalization(self.S, trainable=trainable)
                n_l1 = s.shape[1]
                w1_s = tf.get_variable('w1_s', [s.get_shape()[1], n_l1], trainable=trainable)
                w1_a = tf.get_variable('w1_a', [3, n_l1], trainable=trainable)
                b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)
                net = tf.nn.relu(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)
                return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a) 

It's a setting i made up myself, i am pretty sure this is not the right way by going over this...

&amp;#x200B;

Could anybody inform/advise me? Thanks

&amp;#x200B;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/aeaiq7/ddpg_for_discrete_actions_plausible_but_how/
"""Creating a Model Zoo of ALE Atari-Playing Agents"": investigating learned representations of the self, visual input, and similar state-spaces/strategies in A2C/ES/Deep GA/DQN/Ape-X/Rainbow {Uber/GB/OA} [Such et al 2018]",1547060262,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aea0j2/creating_a_model_zoo_of_ale_atariplaying_agents/
"""V-Fuzz: Vulnerability-Oriented Evolutionary Fuzzing"", Li et al 2019",1547051649,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ae8ind/vfuzz_vulnerabilityoriented_evolutionary_fuzzing/
Hot topics in RL research (autonomous driving preferred),1546979416,What are some hot topics of research in RL (preferably in Autonomous Driving) that are worth investigating. I would appreciate links.,reinforcementlearning,mahjoobi,False,/r/reinforcementlearning/comments/adykq5/hot_topics_in_rl_research_autonomous_driving/
POET: Endlessly Generating Increasingly Complex and Diverse Learning Environments and their Solutions through the Paired Open-Ended Trailblazer,1546974048,,reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/adxn5f/poet_endlessly_generating_increasingly_complex/
[R] Off-Policy Deep Reinforcement Learning without Exploration,1546964136,,reinforcementlearning,lifeadvicesponge,False,/r/reinforcementlearning/comments/advry1/r_offpolicy_deep_reinforcement_learning_without/
[P] Leela Chess Zero - an open-source distributed project inspired by Deepmind’s AlphaZero [review of recent changes and progress],1546958219,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aduqx9/p_leela_chess_zero_an_opensource_distributed/
2DConvs in non-visual (image-based) Environments,1546941102,"Why would someone use 2DConvolutional layers in a non-visual environment? For instance,[in this example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/reinforcement_learning/rl_portfolio_management_coach_customEnv), a 2DConvolutional network is used to process the env's observation - inputs - which are, matrices, comprised of rows of   (open, close, high, low). Is the convolution supposed to sum these values as a form of decreasing their complexity? Why not simply consider the ladder or the differences between the indicators (open/close, high/low) as a form of simplifying the data and feeding that onto the network?",reinforcementlearning,Unless13,False,/r/reinforcementlearning/comments/adsr40/2dconvs_in_nonvisual_imagebased_environments/
RL internship interview questions,1546938444,"I have an interview coming up for a RL research internship. You can easily find lists of ""machine learning"" interview questions, but not specific to RL. Any insights into possible questions? I'm preparing for things like explaining the Bellman equation, TD vs. monte carlo, policy gradient theorem, etc. (not coding questions).",reinforcementlearning,jurniss,False,/r/reinforcementlearning/comments/adshx7/rl_internship_interview_questions/
[R] CS294-112 - Chelsea Finn - Meta reinforcement learning [2nd half includes overview of latest Developments],1546938212,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/adsh3p/r_cs294112_chelsea_finn_meta_reinforcement/
Off-Policy Method are so slow !,1546935489,"Hey fellow RL enthusiasts,

&amp;#x200B;

Recently I read the BAIR post concerning their Soft-Actor-Critic agent and its outstanding capabilities. I decided to benchmark it against my usual PPO implementation. As a matter of fact, I also added a TD3 implementation in order to evaluate it as well. I'm using BipedalWalker-v2 as common environment. 

Even though on-policy algorithms are denoted as  very sample inefficient, I find that in terms of wall-clock performances, PPO litteraly crushes the other two algorithms.

&amp;#x200B;

Do you guys have experiences in benchmarking those ? What are your thoughts ?

&amp;#x200B;

 Thanks (: ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/ads7g6/offpolicy_method_are_so_slow/
[Discussion] Why neural networks used in reinforcement learning is more shadow than image classification?,1546929381,"Most of the baseline deep RL methods such as DQN and PPO only use shadow NN as approximation. Generalization method like BN, dropout are not work for RL tasks. Is there some empirical or theoretical analysis about that? 
Imagination-like methods like WorldModel maybe out of this discussion.",reinforcementlearning,VectorChange,False,/r/reinforcementlearning/comments/adrj98/discussion_why_neural_networks_used_in/
"for the Reinforcement Learning, what's the input and output? please an example",1546904751,,reinforcementlearning,alexchauncy,False,/r/reinforcementlearning/comments/adnwf6/for_the_reinforcement_learning_whats_the_input/
"""Analogues of mental simulation and imagination in deep learning"", Hamrick 2019",1546877348,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/adj3y1/analogues_of_mental_simulation_and_imagination_in/
How to choose the length of trajectories in non-episodic on-policy (deep) reinforcement learning?,1546865727,"I'm looking for some heuristics or even better research regarding a good choice for the length of trajectories when the environment is non-episodic.   
Specifically I am interested in the case of Proximal Policy Optimization (PPO).  
Is the choice always specific to the environment and if so, how? Which characteristics of the environment determine the right choice of the episode length?   
",reinforcementlearning,skervim,False,/r/reinforcementlearning/comments/adhern/how_to_choose_the_length_of_trajectories_in/
PPO with continuous actions,1546853480,"Does anyone know what the loss is for PPO with continuous actions?

&amp;#x200B;

I know the loss for discrete action PPO is [given](https://spinningup.openai.com/en/latest/algorithms/ppo.html) by:

*Processing img zh4v8hkpzy821...*

But I can't seem to find an explanation of what happens to the ratio of action probabilities when the actions are continuous? Do you just use the ratio of actions instead of action probabilities?

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,__data_science__,False,/r/reinforcementlearning/comments/adg447/ppo_with_continuous_actions/
"RL Weekly 3: Learning to Drive through Dense Traffic, Learning to Walk, and Summarizing Progress in Sim-to-Real",1546852140,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/adfyxl/rl_weekly_3_learning_to_drive_through_dense/
[P] My PPO doesn't learn and I don't know why...,1546848486,"Hi,

I have recently started to dabble a bit in (deep) RL and pytorch.

I wanted to implement PPO to solve OpenAI Gym's Pendulum.
My implementation is more or less based on the pseudocode from
[this paper](https://arxiv.org/pdf/1804.02717.pdf) .

I know my code is not the best documented, I will try to fix that in the next days. 

If there is anything unclear, feel free to ask.

[You can find the code here](https://github.com/Mufabo/Learning-Reinforcement-Learning/tree/master/PPO)",reinforcementlearning,oFabo,False,/r/reinforcementlearning/comments/adflyj/p_my_ppo_doesnt_learn_and_i_dont_know_why/
Multi-Agent training in parallel,1546828148,"In MARL training, using a for-loop to update agents is time-consuming, and I found OpenAi used a for-loop to train MADDPG. Are there any best practice to train MARL in parallel?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/add0ch/multiagent_training_in_parallel/
(CartPole) Actor-Critic not learning - using v(s) as baseline works fine.,1546818373,"I'm desperately looking for help/advice with implementing this AC algorithm on CartPole-v0. Im trying to implement the basic batch AC algorithm below, which is from Sergej Levines RL course (link below algorithm).

When using v(s+1) and v(s) to evaluate advantage as shown below, the algorithm does not learn. In fact it seems more like its trying to get as small reward as possible. If i use just v(s) as baseline, it works fine. 

For both actor and critic i use simple one hidden layers networks of size 10. I'm not using regularization of any form, should i do this? For the critic i use the tf.mean\_squared\_error loss function.

&amp;#x200B;

Here is a paste, the interesting part starts at 142: [https://pastebin.com/PkHhB9e9](https://pastebin.com/PkHhB9e9) . Thanks in advance for any suggestions!

&amp;#x200B;

[http:\/\/rail.eecs.berkeley.edu\/deeprlcourse\/static\/slides\/lec-6.pdf slide 13](https://i.redd.it/n5odd1l31w821.png)",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/adbh2w/cartpole_actorcritic_not_learning_using_vs_as/
[Course] UMass Amherst Reinforcement Learning CS687 (Fall 2018),1546805896,,reinforcementlearning,a9entropy2,False,/r/reinforcementlearning/comments/ad9df0/course_umass_amherst_reinforcement_learning_cs687/
"""End-to-End Differentiable Physics for Learning and Control"", de Avila Belbute-Peres et al 2018",1546789749,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/ad6nnb/endtoend_differentiable_physics_for_learning_and/
can't figure out which reinforcement algorithm is the best fit,1546780761,"Hi, I'm currently working on a reinforcement learning project in which two roombas play a game against eachother. The rules are quite simple: the green roomba should get on the top of the environment (green wins the game) while the red roomba tries to occupy greens position (red wins the game). I made a simple gym environment to virtualise the environment before making it in real life as a way to find a suitable algorithm faster. But so far I haven't been able to figure which algorithm is the best fit. I think Q learning will not work because the optimal Q values change all the time since the position of the other roomba (which changes all the time) plays a significant role. So I was thinking about deep Q learning in which a neural network uses for instance the roombas positions as input and gives a certain action as output. But how do I make the network look for the 'highest reward action'? And does anyone have a better suggestion concerning an alogrithm/approach to this problem? ",reinforcementlearning,Vincentvbc,False,/r/reinforcementlearning/comments/ad5ie3/cant_figure_out_which_reinforcement_algorithm_is/
Is it possible to calculate Safety Stock (in inventory management) using RL?,1546775163,"I've never used RL before, but I have to get into it now. I'm developing a code in Python and I was told by my supervisor that I should run my current code 10 times for each k (a variable in my current safety stock formula) so that my program learns to calculate the safety stock, but I'm not sure how that makes sense?

&amp;#x200B;

I wanna calculate my safety stock using RL so that my objective function is the lowest possible (I've got a lot of formulas inbetween to calculate all this but this is the main idea). I've been reading about RL, of course, and I don't quite understand what would be my states, my actions, etc.?

&amp;#x200B;

I was trying to run the code 30 times (10 for each k) and compare the objective function value with the previous one, if it's higher I assign a negative reward to it, if it's higher I assign a positive reward to it, but again, I'm stuck as I don't know what my actions/states and such would be.

&amp;#x200B;

Any thoughts?",reinforcementlearning,Mini_Miudo,False,/r/reinforcementlearning/comments/ad4ylf/is_it_possible_to_calculate_safety_stock_in/
DeepTraffic: competition for controlling 2D cars on a highway {MIT},1546715462,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/acx4e7/deeptraffic_competition_for_controlling_2d_cars/
"""How a Feel-Good AI Story Went Wrong in Flint: A machine-learning model showed promising results, but city officials and their engineering contractor abandoned it."" [difficulties implementing RL algorithms in the real world]",1546713458,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/acwsgw/how_a_feelgood_ai_story_went_wrong_in_flint_a/
Optimal stopping time with DQN,1546709982," 

The optimal stopping time problem that I have is the following:

\- At each step, the agent has a binary set of possible actions, say {0,1}.

\- If the agent chooses action {0}, he receives no reward and move on to the next state. 

\- As soon as the agent takes the action {1}, the game ends and he receives a positive reward based on the current state. This is the only case when a non-null reward is received. 

\- The reward function is known in advance and solely based on the current state once the action {1} is taken. Note: it is not trivial to know when to optimally take the action {1} and end the game. The state distribution is highly stochastic.

\- There’s a maximum number of steps known in advance. Once you’ve reached the end of the episode, if the agent never chose the action {0}, he must take the action {1}.

I want to train an agent in this environment to learn how to optimally choose the action {1} for every state. 

So far, I’ve tried a standard DQN agent with an epsilon-greedy policy. The issue I quickly ran into is that my agent won’t explore a lot of the environment as it will randomly choose the action {1} in the first few steps. 

I would like to know if you have any suggestions on how to improve the exploration task in this environment, or if you have any other suggestions for this particular type of task.

Thanks a lot.",reinforcementlearning,ranirlol,False,/r/reinforcementlearning/comments/acw82w/optimal_stopping_time_with_dqn/
Suggestions on Q learning code tutorial,1546701593,"Hi,
I just watched the first two lectures of DeepRL boot camp which introduces MDP, and Q learning.
Looking for some implementation tutorials on basic  Q-learning. Can you please suggest good tutorials.",reinforcementlearning,akshaynathr,False,/r/reinforcementlearning/comments/acuylp/suggestions_on_q_learning_code_tutorial/
Reinforcement Learning on StreetFighter2 (MD 1993) with Tensorflow &amp; Bizhawk,1546662023,"In this project, I set up an online DRL training environment for Street Fighter 2 (Sega MD) on Bizhawk and with the same method, we could start training models for any other games on Bizhawk.

More on [my github](https://github.com/RuochenLiu/StreetFighter2-DeepRL-Model-on-Bizhawk). Please leave a  🌟  if you like it.

Cheers!

&amp;#x200B;

https://i.redd.it/tkp0qkf26j821.jpg",reinforcementlearning,HeyRaaay,False,/r/reinforcementlearning/comments/acqmz6/reinforcement_learning_on_streetfighter2_md_1993/
"Questions about continuous action spaces in PPO, other methods",1546657505,"Title basically explains the situation. I have many questions about how continuous action policies differ from discrete, and I have done some searching for papers on the topic, for policy gradient in general and not just PPO. It seems to me that many papers are lax on details about how they implement it, what kind of distribution they use to decide the action, which distribution parameters are decided by the policy and which are hyperparameters, how they handle boundaries, how to apply entropy penalty to increase exploration, and so on.

Are there any good papers or Medium posts or something on continuous action policies that contain all of these details? Am I an idiot who missed some obvious sources of information?

Thank you.",reinforcementlearning,LookAliveStayAlive,False,/r/reinforcementlearning/comments/acq1ej/questions_about_continuous_action_spaces_in_ppo/
Can't figure out how to use REINFORCE/Monte Carlo algorithm for continuous environments,1546653017,"Hi, I've been trying to solve the OpenAI Reacher-v1 problem for the past few weeks but I keep running into hurdles after hurdles. I have done a bunch of tutorials implementing different algorithms but it seems that each environment used it discrete (CartPole) when Reacher-v1 is continuous. This is making it extremely difficult to figure out how to actually solve a continuous problem. I've read that the Monte Carlo/REINFORCE algorithm is probably the easiest for beginners so I researched that and looked up examples but again, they use the discrete [FrozenLake](https://harderchoices.com/2018/04/04/monte-carlo-method-in-python/) environment. Can someone help me figure out how I can do this? I feel like I'm just going in circles trying to work on this...",reinforcementlearning,Spaceman776,False,/r/reinforcementlearning/comments/acpe65/cant_figure_out_how_to_use_reinforcemonte_carlo/
[D] Question about continuous action space for PPO and in general,1546642052,[removed],reinforcementlearning,IntercourseThwomp,False,/r/reinforcementlearning/comments/acnp92/d_question_about_continuous_action_space_for_ppo/
Using a node in the input layer of neural net as a switch for underlying policy ?,1546634954,"I wanted to ask about the possibility of the above mentioned point. Given I have a state representation for my problem, and I pass all the state values to a  neural net at the input layer, but suppose a single node or a subset of node in the input layer can be set to represent the kind of policy followed for eg ",reinforcementlearning,intergalactic_robot,False,/r/reinforcementlearning/comments/acmhge/using_a_node_in_the_input_layer_of_neural_net_as/
Trying other distributions instead of gaussian in continuous control,1546616148,"Hi,   


Has anyone tried using other differentiable probability distributions like the student-t distribution for continuous control using a stochastic policy ? I have so far only seen a gaussian probability distribution.   


Thanks !",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/acj971/trying_other_distributions_instead_of_gaussian_in/
Openai Retro hard to solve game,1546610796,"Hey there, Recently I got into Openai's retro, and using the stable-baselines and the baselines that Openai provides I wanted to ""solve"" a game. While some games may be easier to solve than other , Mortal Kombat 3 is not. I 've spend quite an extensive time on this particular game, and it seems that none of the algorithm seem to ""solve"" it. I've even tried Curiosity learning and still no improvement. Please help! If you have or can solve this games, please shoot me a a message or email at rockierocklol @ gmail .  Thank you!",reinforcementlearning,hlsafin,False,/r/reinforcementlearning/comments/acigm6/openai_retro_hard_to_solve_game/
Question regarding continuous control in VPG.,1546569821,"Hi all,   


I'm trying to understand continuous control stochastic policy gradient method like VPG. I understood the fact that for a discrete action space, one would use a distribution like the multinomial distribution and estimate a probability for each class of action. Whereas in continuous action space, one would go for a gaussian probability distribution to model the mean(simplest case where we hold the variance constant).  


&amp;#x200B;

[This is the pdf of a gaussian distribution with mean u and variance epsilon. Equation 1](https://i.redd.it/kwdh4la7kb821.png)

&amp;#x200B;

[Equation 2](https://i.redd.it/9tjxveiakb821.png)

We then backpropagate :

&amp;#x200B;

[Equation 3](https://i.redd.it/w4kjx16ikb821.png)

How do I get the first line of equation 2 ? What does f(s\_t) and a\_t mean ?   ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/acdj7d/question_regarding_continuous_control_in_vpg/
What category of problems are well suited for RL,1546337853,"It is understandable that RL is goal oriented learning, that learns by interaction but when it comes to a specific goal, what category of problems are well suited for RL?  


Taking the example of a supervised setting machine learning, we can it's for regression and classification  etc ",reinforcementlearning,alpha_ma,False,/r/reinforcementlearning/comments/abh5kh/what_category_of_problems_are_well_suited_for_rl/
"UC Berkeley and Berkeley AI Research published all materials of CS 188: Introduction to Artificial Intelligence, Fall 2018.",1546303601,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/abd9tc/uc_berkeley_and_berkeley_ai_research_published/
"""Learning to Coordinate Multiple Reinforcement Learning Agents for Diverse Query Reformulation"", Nogueira et al 2018 {G}",1546290558,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/abbabb/learning_to_coordinate_multiple_reinforcement/
Solving the N-Puzzle with Reinforcement Learning,1546210188,"Link to the game: [https://en.wikipedia.org/wiki/15\_puzzle](https://en.wikipedia.org/wiki/15_puzzle)

I've been following this [course](https://www.udemy.com/artificial-intelligence-reinforcement-learning-in-python/) and wanted to try what I've learned on the 9-Puzzle.

I implemented the game in Python.

I am using a Monte Carlo method with epsilon greedy to explore the space and learn a Q function.

I posted everything to [this](https://github.com/zQuantz/NPuzzle_RL) GitHub repo if anyone wants to check my implementation or try their own!

I find that my algorithm still makes a lot of out of bounds moves even after 200k games played. To visualize this, I've plotted the number of games that ended in an out of bounds error (wrong move) and the number of games that ended with a completed puzzle. The orange line is the proportion of completed games; I feel like the line should be converging near 1 such that out of bounds moves have been learned after 200k games. 

Therefore, is there something I'm doing wrong ? Is there something that I can change or optimize in the learning process?

Note: Do not set the initial difficulty of the puzzle to over 15 or else your Q function is not likely to converge in a reasonable amount of time. 

Any feedback or other implementations are greatly appreciated :)

&amp;#x200B;

https://i.redd.it/5a8xcxhauh721.png",reinforcementlearning,zQuantz,False,/r/reinforcementlearning/comments/ab0sad/solving_the_npuzzle_with_reinforcement_learning/
Help applying OpenAI CartPole code to OpenAI Reacher?,1546205515,"Hi, I am following [this](https://pythonprogramming.net/openai-cartpole-neural-network-example-machine-learning-tutorial/) tutorial series and got it to work for the OpenAI Gym CartPole problem. I am trying to understand the code and apply it to the OpenAI Reacher environment but I am having trouble with this line of code near the end of the linked page:

    action = np.argmax(model.predict(prev_obs.reshape(-1, len(prev_obs), 1))[0])

Reacher takes two values for action while CartPole only needs one so this line only gives one value. I can't seem to figure out how to modify it to provide two values based on the training model. Can someone help me figure out how to do this? Thanks!

    ",reinforcementlearning,Spaceman776,False,/r/reinforcementlearning/comments/ab024k/help_applying_openai_cartpole_code_to_openai/
Soft Actor Critic—Deep Reinforcement Learning with Real-World Robots,1546203233,"Soft actor-critic is a step towards feasible deep RL with real-world robots. Work still needs to be done to scale up these methods to more challenging tasks.

[https://bair.berkeley.edu/blog/2018/12/14/sac/](https://bair.berkeley.edu/blog/2018/12/14/sac/)",reinforcementlearning,obsezer,False,/r/reinforcementlearning/comments/aazozp/soft_actor_criticdeep_reinforcement_learning_with/
"The Tea Time Talks with Rich Sutton (May 28, 2018)",1546202110,,reinforcementlearning,woodsja2,False,/r/reinforcementlearning/comments/aazioh/the_tea_time_talks_with_rich_sutton_may_28_2018/
"“Robustify” RL: Uber, Go-Explore, and Research as RL with SOTA rewards",1546199206,,reinforcementlearning,djangoblaster2,False,/r/reinforcementlearning/comments/aaz245/robustify_rl_uber_goexplore_and_research_as_rl/
"""Explore, Exploit, and Explode — The Time for Reinforcement Learning is Coming"", Yuxi Li",1546191637,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aaxvuf/explore_exploit_and_explode_the_time_for/
Policy gradient: How much better is the optimal constant baseline versus compared to the simple average total reward one?,1546103800,"By better i mean in learning curve terms. Its often mentioned as being a minor difference in lectures, but is it so small that one could hardly see it on a learning curve? Has anyone tried actually implementing and comparing?

To be clear i am talking about the simple baseline: b\_avg = 1/N \\sum\_i R(tau)

Versus optimal:                                                               b = (grad log pi R(tau)) / grad log pi

&amp;#x200B;",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/aamxfo/policy_gradient_how_much_better_is_the_optimal/
"""How the Artificial Intelligence Program AlphaZero Mastered Its Games"" [the New Yorker explains Zero and LeelaZero]",1546102869,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aamsi0/how_the_artificial_intelligence_program_alphazero/
Best-supported RL environment right now for trying out algorithms?,1546087962,"I'm thinking of something like OpenAI Gym, with a variety of different tasks and a relatively easy to use in python in coordination with neural nets.

&amp;#x200B;

Is OpenAI Gym good these days or is there something better?",reinforcementlearning,TheMoskowitz,False,/r/reinforcementlearning/comments/aakzps/bestsupported_rl_environment_right_now_for_trying/
"""Learning Unsupervised Learning Rules"", Metz et al 2018 {GB}",1546056382,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aahtpo/learning_unsupervised_learning_rules_metz_et_al/
"""BASE: Bayesian Meta-network Architecture Learning"", Shaw et al 2018",1546049880,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aagzt7/base_bayesian_metanetwork_architecture_learning/
"""How People Initiate Energy Optimization and Converge on Their Optimal Gaits"", Selinger et al 2018",1546048343,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/aagsm6/how_people_initiate_energy_optimization_and/
Why isn't my epsilon-greedy agent learning anything?,1545992445,"Hello, I've been trying to implement a Q-learning agent to play the game of snake. There are many examples of deep Q-learning agents doing this on github but I couldn't find any with simple Q-learning and as such I wanted to give it a go.  


Now my concern was that it might need deep learning to actually accomplish anything but I don't see why since given any state, there are 4 possible actions (left, down, up, right) and I have a 10x10 grid where for each gridspace I record a triple of booleans (True/False for is snake head in this square, True/False for is snake body in this square and True/False for is there food in this square). This gives me a state space with size 10\*10\*2\^3 = 800 and a Q-table with 800\*4=3200 values, right? To me it seems like if I train for e.g. half a million episodes, the behaviour should improve.  


My agent is following an epsilon-greedy policy whereby it starts behaving randomly 80% of the time (exploration) and then every time it dies, epsilon is changed slightly so that by the end of training it is behaving greedily 100% of the time.  


The code is pretty simple (I hope!) and is available here along with instructions on how to run it (there is also a single player mode which can take you back to your Nokia 3210 days!): [https://github.com/davecerr/snake](https://github.com/davecerr/snake)

&amp;#x200B;

As you can see, you can actually watch the training in progress and what we see is the agent quickly finds out how to get 2 bits of food or so before dying and then doesn't improve much thereafter.

&amp;#x200B;

Any suggestions as to why that might be the case? Or any suggestions on how I can improve this?  


Thank you very much for your help.

&amp;#x200B;",reinforcementlearning,errminator,False,/r/reinforcementlearning/comments/aa95hw/why_isnt_my_epsilongreedy_agent_learning_anything/
"Sparlab is an open-source project that allows fighting gamers to create their own bots to train against, to experiment with various combos, and more. How can reinforcement learning be applied to optimize the user's training experience? (Github link in comments)",1545968700,,reinforcementlearning,realjohnward,False,/r/reinforcementlearning/comments/aa6lc2/sparlab_is_an_opensource_project_that_allows/
Policy gradient: Average total reward baseline VS standardizing total rewards,1545936754,"The simple average total reward baseline, b = 1/N R(\\tau\_i) makes a huge improvement on the learning curve for the basic policy gradient. This technique seem to me to be very similar to just standardizing the R(\\tau)'s  ( subtract mean, div by std), but from my tests (on cartpole) the baseline technique yields faster learning, and a more smooth learning curve than the standardized R(\\tau) technique. It also seems to be very good at remaining near the optimum. 

What i dont get is why the standardizing is not better - it seems to have all the same benefits as the baseline, and the only difference is the smaller numbers multiplied onto the gradients which should yield lower variance.   
But my tests does not confirm this - avg baseline is better consistently.

Does anyone have any insights?",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/aa1ymw/policy_gradient_average_total_reward_baseline_vs/
First steps in reinforcement learning,1545935338,,reinforcementlearning,gfrison,False,/r/reinforcementlearning/comments/aa1qd3/first_steps_in_reinforcement_learning/
How do you solve a problem using reinforcement learning?,1545875832,"Hi, I am new to reinforcement learning and have been reading/watching a lot of introductory material (David Silver lectures, etc) to help me get started. I just joined a research group and my professor asked me to solve the OpenAI Reacher-v1 problem using any algorithm as a introduction, but I am really confused on how to apply what I learned to an actual environment. I went through a tutorial for the OpenAI CartPole problem and, apart from applying reinforcement learning algorithms, it was really straightforward. 

I have the environment setup on my computer and know how to get the observation data but I don't know what to do with that information, nor what actions I can apply. Can someone help me understand how to get started on applying RL to an actual problem like this?",reinforcementlearning,Spaceman776,False,/r/reinforcementlearning/comments/a9ur7i/how_do_you_solve_a_problem_using_reinforcement/
Best update moment in off-policy algorithms ?,1545838204,"Hey ! 

&amp;#x200B;

Quick question about off-policy algorithm (TD3, DDPG, SAC). Is there any specific reason for updating at each step instead of updating several times at the end of the episode ? 

Thanks !",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/a9pfp5/best_update_moment_in_offpolicy_algorithms/
All you need to Completely learn RL - Courses &amp; Practice with solutions.,1545831342,"Good morning, afternoon, or evening, 😁

&amp;#x200B;

I have been learning for the last few months and wanted to share with you the most useful resources I found.

&amp;#x200B;

All you would need as **Courses &amp; Practice exercises with solutions** can be found in the amazing work done in the blog and corresponding GitHub repository in the following links:

*Blog post:* [http://www.wildml.com/2016/10/learning-reinforcement-learning/](http://www.wildml.com/2016/10/learning-reinforcement-learning/)*Repository:* [https://github.com/dennybritz/reinforcement-learning](https://github.com/dennybritz/reinforcement-learning)

&amp;#x200B;

Additionally, here is a link to a **Q-Learning simulator**:  [Q-learning Simulator](https://www.mladdict.com/q-learning-simulator).

&amp;#x200B;

Hope this helps.Feel free to add any useful content you stumbled upon in the comments. 😊",reinforcementlearning,hapaahamza,False,/r/reinforcementlearning/comments/a9ols4/all_you_need_to_completely_learn_rl_courses/
[D] Quick question: difference between REINFORCE w/ GAE and A2C?,1545816506,,reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/a9n857/d_quick_question_difference_between_reinforce_w/
How Neural Networks Work- Simply Explained,1545717584,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/a9cjhf/how_neural_networks_work_simply_explained/
Would anyone be willing to take a look at my DDPG implementation?,1545671144,"Hi, all.  I'm trying to implement DDPG based on the [original paper](https://arxiv.org/abs/1509.02971).  I've also taken a look at several example implementations on github to help fill in some implementation details.  However, I'm still not really getting any good results.  Rewards don't seem to improve with training.

I was wondering if anyone here would be able to take a look at [my code](https://github.com/TheJCBand/ddpg) to see if there are any glaring issues.  This is really my first time using Tensorflow.  Thanks!",reinforcementlearning,TheJCBand,False,/r/reinforcementlearning/comments/a96o6s/would_anyone_be_willing_to_take_a_look_at_my_ddpg/
Hands-on AI Agents Developer Guide with code on X-mas sale ($5),1545667168,,reinforcementlearning,ai_energy,False,/r/reinforcementlearning/comments/a963ng/handson_ai_agents_developer_guide_with_code_on/
Hands-on AI Agents Developer Guide with code on X-mas sale ($5),1545666329," ""If you are someone wanting to get a head start in this direction of building intelligent agents to solve problems and you are looking for a structured yet concise and hands-on approach to follow, you will enjoy this book and the [code repository](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym). The chapters in this book and the accompanying code repository are aimed at being simple to understand and easy to follow. While simple language is used everywhere possible to describe the algorithms, the core theoretical concepts including the mathematical equations are laid out with brief and intuitive explanations as they are essential for understanding the code implementation and for further modifications and tailoring by the readers.

&amp;#x200B;

[Guide to develop agents using PyTorch and OpenAI Gym](https://i.redd.it/7rb3ammjx8621.png)

 

The book takes the readers through the step-by-step process of building intelligent agent algorithms using deep reinforcement learning starting from implementation of the building blocks for configuring, training, logging, visualizing, testing and monitoring the agent. **The book walks the reader through agent implementations in PyTorch to solve a variety of tasks and problems including: classical AI problems and console games like the Atari games, and complex problems like autonomous driving in the CARLA driving simulator**. In the closing chapters, the book provides an overview of the latest learning environments and learning algorithms along with pointers to more resources that will help the readers to take their hands-on skills to the next level. The[ code repository](https://github.com/PacktPublishing/Hands-On-Intelligent-Agents-with-OpenAI-Gym/) for the book on GitHub provides all the necessary code, scripts and instructions to follow through the book successfully.

### Learning outcome

* Get introduced to intelligent agents and learning environments
* Understand the basics of Reinforcement Learning (RL) &amp; deep RL
* Get started with OpenAI gym &amp; PyTorch for deep RL
* Implement building blocks to configure, train, log, visualize &amp; test agents
* Implement deep Q learning agent to solve discrete optimal control tasks
* Learn to create custom learning environments for real-world problems
* Implement deep actor-critic agent to drive a car autonomously in CARLA
* Resources to take your intelligent agent development skills to next level

""  
 ",reinforcementlearning,ai_energy,False,/r/reinforcementlearning/comments/a95zjv/handson_ai_agents_developer_guide_with_code_on/
What is the best way to learn practical RL?,1545651861,"I did David Silver's online course, Berkeley Intro to AI by Pieter Abeel and  Anca Dragan. 

Now I want to implement RL algorithms, write code to solve gym environments, but I am not finding any guide as to how to do this.   
I did ML and DL courses, which set me up to implement my own DL projects, I have no such concrete RL resource. All courses out there are theoretical. 

How did you start? Any resources recommended? ",reinforcementlearning,l0gicbomb,False,/r/reinforcementlearning/comments/a94c5s/what_is_the_best_way_to_learn_practical_rl/
"RL Weekly 2: Tuning AlphaGo, Macro-strategy for MOBA, Sim-to-Real with conditional GANs",1545646532,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/a93une/rl_weekly_2_tuning_alphago_macrostrategy_for_moba/
Computer Vision - RL: Learning Acrobatics by Watching YouTube,1545643129," In this work, they present a framework for learning skills from videos (SFV). By combining state-of-the-art techniques in [computer vision](https://akanazawa.github.io/hmr/) and [reinforcement learning](https://xbpeng.github.io/projects/DeepMimic/index.html), their system enables simulated characters to learn a diverse repertoire of skills from video clips. I think it's awesome:)

[https://bair.berkeley.edu/blog/2018/10/09/sfv/](https://bair.berkeley.edu/blog/2018/10/09/sfv/)",reinforcementlearning,obsezer,False,/r/reinforcementlearning/comments/a93jow/computer_vision_rl_learning_acrobatics_by/
How to Diagnose a Failing RL Agent?,1545631549,"I write code for an algorithm and start training a Deep RL agent, keeping track of rewards per episode. After millions of steps it hasn't learnt anything. I tweak hyperparameters like crazy and keep trying. It still doesn't learn. How do I diagnose? Specifically these six questions, 

1. How do I check for blunders in the code, i.e., if the algorithm has been implemented correctly? I checked the algorithm with a trivial environment, it worked. But that doesn't mean much. 
2. How do I know if my agent is exploring enough? 
3. How do I know my agent has not saturated at a local maxima ( wrt rewards)  and won't improve any further? 
4. How do I know if my agent simply needs a lot more time to train and I shouldn't be too hasty in killing the training after seeing no improvement? 
5. What are some useful variables to keep track of to know what's going wrong? For eg. I keep track of rewards. epsilon (in an annealing epsilon-greedy policy). 
6. If it's a solved problem, I know I can keep trying till I get the established results, but if it's a new problem, when should I realize this algorithm isn't working and give up on it, instead of spending half my life on it?",reinforcementlearning,shura04,False,/r/reinforcementlearning/comments/a92ezc/how_to_diagnose_a_failing_rl_agent/
How to extend the REINFORCE algorithm to continuous action space ?,1545612444,"Hey all, Thanks for your help.  


 I've been reading about Deep Reinforcement learning from this source : [https://spinningup.openai.com/en/latest/spinningup/rl\_intro3.html](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html) and I have tried implementing the REINFORCE algorithm for cart pole balancing in the discrete open AI environment.  However, I want to take it up a step and learn how to extend this algorithm to the continuous action space. I know that there exists actor critic methods like DDPG for continuous control, but I want to learn how to do it using the REINFORCE algorithm. Can someone point me to how to get started on this ? ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/a904sv/how_to_extend_the_reinforce_algorithm_to/
"""The Bayesian Superorganism III: externalised memories facilitate distributed sampling"", Hunt et al 2018",1545596626,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a8xzrl/the_bayesian_superorganism_iii_externalised/
[P] Trouble getting OpenAI Roboschool to work,1545526554,"Hi, I am new to RL and was asked to solve OpenAI Gym Reacher-v1 using Roboschool as an introduction. However, I am having trouble importing roboschool into Python. I am using Mac OS and did the whole ""pip install roboschool"" but when I run ""import roboschool"" I get 
    Segmentation fault: 11

&amp;nbsp;

Does anyone know what I could be doing wrong? Their [Git](https://github.com/openai/roboschool/blob/master/README.md) doesn't seem to have much info either (though I am dyslexic when it comes to setting up this stuff). Can someone help me out?",reinforcementlearning,Spaceman776,False,/r/reinforcementlearning/comments/a8qcjs/p_trouble_getting_openai_roboschool_to_work/
NueroIPS 2018 Talks on Reinforcement Learning,1545525543,"NuerIPS has released some 2018 recordings of talks. Here are the ones that are about or include some RL papers.

RL Keynote talks:

- [Joelle Pineau (McGill University/Facebook) delivers a talk entitled Reproducible, Reusable, and Robust Reinforcement Learning.](https://www.facebook.com/nipsfoundation/videos/2120856364798049/)

RL sessions talks:

- [Session 1: Room 517 C/D on reinforcement learning](https://www.facebook.com/nipsfoundation/videos/1689968097774203/)
- [Session 1: Room 220 E on reinforcement learning](https://www.facebook.com/nipsfoundation/videos/745243882514297/)

Session which seem to include RL papers:

- [Session 1: Room 220 C/D including reinforcement learning](https://www.facebook.com/nipsfoundation/videos/412716066331630/)
- [Session 2: Room 220 C/D including reinforcement learning](https://www.facebook.com/nipsfoundation/videos/357066758186895/)
- [Session 2: Room 220 E including reinforcement learning](https://www.facebook.com/nipsfoundation/videos/265425524142328/)
- [Session 2: Room 220 C/D including reinforcement learning](https://www.facebook.com/nipsfoundation/videos/1914579825246403/)
- [Session 2: Room 220 C/D (again?) including reinforcement learning](https://www.facebook.com/nipsfoundation/videos/198568127715251/)
- [Session 2: Room 517 C/D (again?) including reinforcement learning](https://www.facebook.com/nipsfoundation/videos/333047997528187/)

",reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/a8q7y9/nueroips_2018_talks_on_reinforcement_learning/
Interesting article showing how amoebas can find optimal policies.,1545523563,,reinforcementlearning,fxidiot,False,/r/reinforcementlearning/comments/a8pyb2/interesting_article_showing_how_amoebas_can_find/
Pieter Abbeel: Deep Reinforcement Learning (podcast),1545499673,,reinforcementlearning,The_Amp_Walrus,False,/r/reinforcementlearning/comments/a8mmb2/pieter_abbeel_deep_reinforcement_learning_podcast/
[R] Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation,1545498955,"Check out my paper with Yoav Goldberg (ICLR rejected with 7,7,4): [https://arxiv.org/abs/1806.07377](https://arxiv.org/abs/1806.07377)

**We show that:**

* RL-over-pixels really overfits, agents fail completely with small visual changes.
* Fine-tuning a trained agent is harder than training from scratch.

**We propose:**

* Do transfer learning by analogy: train an unaligned GAN to map between old and new domain, and feeding the agent with the GAN's mapping.  This is much more sample efficient than training from scratch or fine-tuning.
* A more realistic metric evaluation for unaligned GANs.

**We also have two cool videos:**

Transferring between the original and modified versions of Breakout - [https://youtu.be/4mnkzYyXMn4](https://youtu.be/4mnkzYyXMn4)

Transferring between level 1 and levels 2,3,4 of a car racing game called Road Fighter - [https://youtu.be/khtS-VjpOEA](https://youtu.be/khtS-VjpOEA)

&amp;#x200B;",reinforcementlearning,Shani_Gamrian,False,/r/reinforcementlearning/comments/a8mimo/r_transfer_learning_for_related_reinforcement/
"""MetaMimic: One-Shot High-Fidelity Imitation: Training Large-Scale Deep Nets with RL"", Le Paine et al 2018 {DM} [ResNet-34]",1545497596,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a8mbka/metamimic_oneshot_highfidelity_imitation_training/
Question on Dueling Network Architecture for Deep Reinforcement Learning,1545451006,"Having read the paper of [Dueling Network Architecture for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581), I feel very confused by the Eqn 8 and 9 that solve the issue of identifiability, why can it work and why it makes sense?

My question: is this issue can not help boosting learning for only it can help to build this Dueling Network?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/a8hm51/question_on_dueling_network_architecture_for_deep/
Use two neural nets when fitting value function to use as critic/baseline?,1545441621,"In this lecture by levine at \~ 27min, 30 sec. he talks about fitting the state value function using labels  y\_it = r\_t + Vhat (s\_t+1) instead of the rewards to go. He states that we substitute in ""previous V hat pi for the previous pi"" - does he mean using two neural nets so that we update one of them each time (alternating between them each update)? 

&amp;#x200B;

[https://youtu.be/Tol\_jw5hWnI?t=1649](https://youtu.be/Tol_jw5hWnI?t=1649)",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/a8ghs9/use_two_neural_nets_when_fitting_value_function/
Help understanding how to use RL,1545428661,"Hi, I just joined a research project at my school on RL. I have never done anything involving it before so I am a bit overwhelmed but I've been watching some of David Silver's lectures to get introduced to it. 

&amp;nbsp;

My advisor asked me to solve the Gym.OpenAI Reacher-v1 environment using any RL algorithm but I am unsure what this exactly means and how to get started. My advisor is away until after the holidays so won't be able to answer questions but I was hoping to at least get started on this. Can someone please help me understand what is being asked?
",reinforcementlearning,Spaceman776,False,/r/reinforcementlearning/comments/a8eq92/help_understanding_how_to_use_rl/
[MetaRL] [R] Transferring Knowledge across Learning Processes,1545421862,,reinforcementlearning,Independent_Engine,False,/r/reinforcementlearning/comments/a8dp2n/metarl_r_transferring_knowledge_across_learning/
Actor Critic direct methods,1545412692,"Hey,
I am looking for what exactly direct methods are in the context of actor critic. I am asking here because the results I get on Google are very sparse ans confused me because some referred to the direct policy estimation while others were talking about the bellman equation. Could someone briefly explain what direct method means in this case or at least point me in the right direction?

Thanks in advance",reinforcementlearning,salah3,False,/r/reinforcementlearning/comments/a8c4h4/actor_critic_direct_methods/
[D] Best RL paper you read in 2018 and why?,1545412017,"Per the [discussion](https://www.reddit.com/r/MachineLearning/comments/a6cbzm/d_what_is_the_best_ml_paper_you_read_in_2018_and/) in the /r/MachineLearning, I thought it might be useful to discuss some of our favorite RL-focused papers this year.",reinforcementlearning,pdxdabel,False,/r/reinforcementlearning/comments/a8c0ck/d_best_rl_paper_you_read_in_2018_and_why/
Help with Sarsa algorithm,1545376192,"Hello,

* I'm trying to build an agent to play the game of snake. I've *finally* built a PyGame interface that allows me to run my agent and watch what happens as well. Since SARSA is an online learning technique, I expect that if I watch enough episodes (1 episode = 1 snake lifetime), I should see some improvement.
* I just watched 40,000 episodes and at the end, the snake was still behaving poorly - the top score was 3 bits of food being eaten but tbh this seemed to be out of luck rather than any trained behaviour!
* So then the question is why? Firstly, let me explain a bit about the setup:

&amp;#x200B;

1. state is a tuple: (snake\_x - food\_x, snake\_y - food\_y, bottom\_wall, top\_wall, right\_wall, left\_wall). My reason for this is that I thought it would be useful for the snake to receive the signed x and y separations as this may help it learn whether it should be going up/down/left/right. The 4 wall values are booleans that are True if the snake's head is in a square next to a wall. This will hopefully allow it to learn to avoid crashing into the edge of the map.
2. actions are up/down/left/right
3. I have a Q\_table and for each state, there is a dictionary containing actions and their associated long term reward. Every time a state is visited, Q\[state\] = (1 - alpha) \* Q\[state\] + alpha \* (reward + gamma \* max(Q\[next\_state\]))
4. I am using an epsilon-greedy policy whereby I generate a random number between 0 and 1. If it is bigger than epsilon, I make a random action and if it is less than epsilon, I pick the argmax of the Q values for the current state.

&amp;#x200B;

Information on the values I have chosen:

&amp;#x200B;

1. epsilon = 0.2 meaning that I start off with random moves 80% of the time. From the very first episode onwards, this increases at a steady rate until it eventually equals 1 for the final episode i.e. we should be using a fully greedy policy at this point as we should have converged on the optimal policy
2. alpha = 0.3. This controls the weighting given to the Q value updates. I am giving only a 30% weighting to the new information coming in - since everything is initialised as 0, I am concerned this is too low but when trying with 0.7, my results didn't improve much.
3. gamma = 0.1. This is the discount factor. I am not 100% sure I implemented this correctly - I believe if I apply it as I have described above, then it will propagate through different episodes with the appropriate power i.e. the Q\_value from 2 moves ago will be weighted by gamma\^2 - can someone confirm? As I understand it, this value is controlling how focussed we are on immediate vs long term rewards (gamma = 0 meaning we only ever look one step ahead whilst gamma = 1 means everything in history is considered equally). Again I am concerned this is too low but when changing to 0.9 there wasn't a noticeable improvement either.
4. When I ran the experiment again with alpha = 0.7 and gamma = 0.9, the snake seemed to improve slightly in the sense that it reached a high score of 6 pretty quickly but sadly this seemed to be more of a one-off and most of the time the snake length was stuck on 1).
5. Rewards. This is the bit that is most up for debate!!! Currently, I reward the agent as follows:

* \-500 for hitting edge of map
* 1000 \* snakelength for hitting a target
* \-15 for every step where neither of the above is achieved
* for every step where the manhattan distance between snake and target decreases, I award the manhattan distance
* for every step where the manhattan distance between snake and target increases, I subtract the manhattan distance
* I am pretty inexperienced with this - what do people think about these numbers?

&amp;#x200B;

As mentioned above, my experiments have been fairly inconclusive (running on laptop). However, I am using a 10x10 map meaning that I have 100\*99\*2\^4 = 158,400 possible game states (2\^4 for boolean wall values and 100\*99 since food can't be generated on top of existing snake position). Therefore, testing just 40,000 episodes is unlikely to be enough training data. I will probably pay to run this on AWS but before I fork out my hard earned cash, I was hoping that I could get some community feedback about my efforts so far. Am I going in the right direction - any comments/criticisms? n particular, I'd be interested in hearing of any suggested improvements to my state space as well as any values that might be worth tracking to check if the snake is indeed learning stuff!!! Thanks!

&amp;#x200B;

* Update: I just ran a few more tests and found that if I record the snake's average lifespan, this increases steadily through the episodes suggesting that it does learn to avoid the walls which is good! However, I also record the time taken for the snake to reach the target and this increases **substantially** over even a small number of trials (after 400 episodes the average time taken for a snake to find a bit of food is &gt;300 moves). I am a bit unsure whether this is good or bad - it probably should go up if the snake is living longer but at the same time we want it to be eating food not just wandering randomly. Any suggestions on how to read this?

&amp;#x200B;",reinforcementlearning,errminator,False,/r/reinforcementlearning/comments/a87hsq/help_with_sarsa_algorithm/
Are the gym Mujoco environments Stochastic Or Deterministic?,1545372904,"I've been looking at the xml files and they mention randomness but I also am not sure how these files are parsed so it could be irrelevant. 

&amp;#x200B;

ex: [https://github.com/openai/gym/blob/master/gym/envs/mujoco/assets/ant.xml](https://github.com/openai/gym/blob/master/gym/envs/mujoco/assets/ant.xml)

&amp;#x200B;

In chapter 4 of Mujoco's guide([http://www.mujoco.org/book/programming.html](http://www.mujoco.org/book/programming.html)) they say that the simulators are deterministic unless is added.  ",reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/a873ym/are_the_gym_mujoco_environments_stochastic_or/
"""The Pommerman team competition or: How we learned to stop worrying and love the battle"", Borealis AI writeup [5th place]",1545357218,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a855s4/the_pommerman_team_competition_or_how_we_learned/
"""Nevergrad: An open source Python3 tool for derivative-free optimization"" {FB} [CMA-ES, particle swarm, FastGA, SQP etc]",1545348704,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a83yid/nevergrad_an_open_source_python3_tool_for/
[D] Resources and questions on (sequential) generative models,1545344321,,reinforcementlearning,mostly_rnd_questions,False,/r/reinforcementlearning/comments/a83858/d_resources_and_questions_on_sequential/
Sim-to-Real via Sim-to-Sim: Data-efficient Robotic Grasping via Randomized-to-Canonical Adaptation Networks,1545329666,,reinforcementlearning,mostly_rnd_questions,False,/r/reinforcementlearning/comments/a80p7w/simtoreal_via_simtosim_dataefficient_robotic/
Question on policy update equation [Reinforcement Learning Sutton and Barto],1545325167,"Hi all,   
Thanks for your help. I understood this equality definition (bellman equation) for the value function . 

[Value Function Equation \(Sutton and Barto pg. 60\)](https://i.redd.it/468kmfczqg521.png)

This equation tells how to compute the value of a particular state in an MDP. It relates how the value of the current state relates to the other nearby states it is connected to. It tells nothing about how to update a particular state. What I dont understand is how the authors jump to the update equation from this bellman equation. 

&amp;#x200B;

[Update equation. S&amp;B pg. 60](https://i.redd.it/rb6gw37crg521.png)

How does the equality sign hold in this case ? In the previous equation, the policies in the LHS and RHS are the same. whereas, in the second equation the policies are different (k+1 and k). Also, can someone tell me why this equation is true ? ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/a7zwj8/question_on_policy_update_equation_reinforcement/
"""Hierarchical Macro Strategy Model for MOBA Game AI"", Wu et al 2018 {Tencent}",1545276767,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a7twp9/hierarchical_macro_strategy_model_for_moba_game/
Question from a RL novice: What is an easy library to use?,1545250852,"Hello, all. After studying for long enough and learning of amazing projects like OpenAI and its game simulations, I wanted to start implementing basic reinforcement learning algorithms myself on some games.

The only problem is (at least from my angle), that a lot of coding and technical know-how seems to go into even the most basic reinforcement learning projects. One example guide (out of many I found) is here, https://keon.io/deep-q-learning/

This reminds me of how deep learning novices first learn by using matrix algebra with numpy to create the networks themselves, but are soon shown tensorflow or keras so they can get on their way. In your opinions, what are the easiest packages that similarly automate reinforcement learning? Is there any consensus, like there is generally for tensorflow for regular deep learning?",reinforcementlearning,mcoolio2654,False,/r/reinforcementlearning/comments/a7q5b8/question_from_a_rl_novice_what_is_an_easy_library/
Best reinforcement learning algorithm with limited computational resources,1545249516,"I've been reading through a lot of papers trying to figure out what the best algorithm out there is. It seems like TRPO/PPO and NPG have great performance. But in these papers, the agent has usually been trained for a massive amount of simulation runs. E.g. in the TRPO paper some agents were trained for 200M simulation episodes. Are these also the fastest learning algorithms? I'm looking for an algorithm that I can easily run on my 1080TI and in a timeframe, such that my landlord doesn't come by complaining about the electricity bill. Any suggestions?",reinforcementlearning,neusbal,False,/r/reinforcementlearning/comments/a7pxgt/best_reinforcement_learning_algorithm_with/
How to get started learning Reinforcement Learning,1545248821,After seeing what OpenAI did with Dota2 and what Deepmind did with Go I am pretty interested in learning more about RL. Assuming I have the prerequisite math background what are some good starting points for papers/books/course notes that are recommended?,reinforcementlearning,funemployedeecs,False,/r/reinforcementlearning/comments/a7ptnn/how_to_get_started_learning_reinforcement_learning/
NeurIPS 2018 Through the Eyes of First-Timers,1545248636,,reinforcementlearning,Yuqing7,False,/r/reinforcementlearning/comments/a7pskh/neurips_2018_through_the_eyes_of_firsttimers/
[R] Uncertainty in Neural Networks: Bayesian Ensembling,1545245344,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/a7p9gv/r_uncertainty_in_neural_networks_bayesian/
Why don't we use the sum of discounted rewards with off policy algorithms ?,1545235458,"Dear fellow RL enthousiasts, 

I'm more used to on policy methods, but recently, I've been working with off-policy approach, such as DDPG, TD3 or SAC. In all of the implementations I've gone through, the authors use the reward signal as such, ie: they store R as given after the environment step. I'm wondering why people do not wait until the end of the episode, to compute the discounted reward ? Indeed, for the case of a sparse reward, that would make more sense, right ? 

&amp;#x200B;

What do you guys think ? 

Thanks ! 

&amp;#x200B;",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/a7npjj/why_dont_we_use_the_sum_of_discounted_rewards/
Solution to exercise 3.16 in the Sutton Barto RL book,1545210434,"Hello, I am trying t work through the exercises in the RL book. Can someone please tell me the solutions to this  question:

Now consider adding a constant c to all the rewards in an episodic task,

such as maze running. Would this have any effect, or would it leave the task unchanged

as in the continuing task above? Why or why not? Give an example.

&amp;#x200B;",reinforcementlearning,karanchahal1996,False,/r/reinforcementlearning/comments/a7kspk/solution_to_exercise_316_in_the_sutton_barto_rl/
Resources to study Hierarchical Reinforcement Learning,1545199185,"My advisor has asked me to look into methods of HRL to solve a problem involving sub policies, She herself has little knowledge about HRL and resources. Can someone provide a list of possible resources from where I can start studying HRL.",reinforcementlearning,intergalactic_robot,False,/r/reinforcementlearning/comments/a7jorm/resources_to_study_hierarchical_reinforcement/
"""Bayesian Optimization in AlphaGo"", Chen et al 2018 {DM} [hyperparameter optimization of runtime play: +90-300 Elo; insight into Zero]",1545164798,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a7ev1p/bayesian_optimization_in_alphago_chen_et_al_2018/
Using Age to compute Human Baseline,1545158562,"How is the human baseline computed ? Do we consider age when we are trying to compute the human baseline ?  I mean taking age into account helps us would help measure the accuracy and the progress of our models, for very complex games, very young kids are probably struggling as much as the next RL Agent. ",reinforcementlearning,operman18,False,/r/reinforcementlearning/comments/a7dv34/using_age_to_compute_human_baseline/
"""2018 AI Alignment Literature Review and Charity Comparison""",1545156357,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a7dicb/2018_ai_alignment_literature_review_and_charity/
What’s the deal with Neural Architecture Search?,1545153695,,reinforcementlearning,yoavz,False,/r/reinforcementlearning/comments/a7d2ri/whats_the_deal_with_neural_architecture_search/
Do Q-values inform us about the quality of a state?,1545145794,"Does a combination (sum, average or combination) of all q values in a state inform us about the value/quality of that state? I know in more vanilla DQNs suffer from q-explosion or in the best case overestimation, even a recent nips paper showed that there are other biases (delusion bias) at play in q-values as well and we know that those biases are not uniformly spread across the state-action space, but do the q values still offer some insight into the value of a state?",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/a7buz5/do_qvalues_inform_us_about_the_quality_of_a_state/
[D] Pytorch parallelism,1545145013,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/a7bqs1/d_pytorch_parallelism/
Google's RL library Dopamine got rejected by ICLR 2019,1545135279,"

Link here: https://openreview.net/forum?id=ByG_3s09KX",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/a7aji9/googles_rl_library_dopamine_got_rejected_by_iclr/
Aren't off-policy algorithms with deterministic policies the same as on-policy algorithms with stochastic policies?,1545069446,"Am I missing something, or are off-policy algorithms with deterministic policies the same in practice as on-policy algorithms with stochastic policies?  Why is it valuable to distinguish the two?",reinforcementlearning,TheJCBand,False,/r/reinforcementlearning/comments/a7242o/arent_offpolicy_algorithms_with_deterministic/
RL Weekly 1: Soft Actor-Critic Code Release; Text-based RL Competition; Learning with Training Wheels,1545062516,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/a711gf/rl_weekly_1_soft_actorcritic_code_release/
PyTorch Deep RL Implementations code review,1545052661,"I have written some PyTorch implementations of RL algorithms here: [https://github.com/p-christ/Deep\_RL\_Implementations](https://github.com/p-christ/Deep_RL_Implementations)

&amp;#x200B;

Please let me know what you think of this and if you have any ideas on how i could improve the repository?",reinforcementlearning,1243141deep_rl_14141,False,/r/reinforcementlearning/comments/a6zq9m/pytorch_deep_rl_implementations_code_review/
[D] MIT AI: Deep Reinforcement Learning (Pieter Abbeel Interview) [Video],1545001492,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/a6txfa/d_mit_ai_deep_reinforcement_learning_pieter/
REINFORCEMENT LEARNING AND OPTIMAL CONTROL by Dimitri P. Bertsekas,1544981831,,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/a6qz5c/reinforcement_learning_and_optimal_control_by/
Reinforcement Learning Tutorial With Demo,1544956867,"I summarized reinforcement learning topics and I emphasized important points in the RL tutorial. I am continuing to develop repo. It may be beneficial for everyone. 

[https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)

&amp;#x200B;",reinforcementlearning,obsezer,False,/r/reinforcementlearning/comments/a6o5gi/reinforcement_learning_tutorial_with_demo/
"""Exploration in the wild"", Schulz et al 2018 [Deliveroo dataset: 1,613,967 meal orders 30,552 restaurants by 195,333 customers]",1544894769,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a6gyp3/exploration_in_the_wild_schulz_et_al_2018/
Policy gradients variance - direction and magnitude?,1544894682,"I'm a bit confused about the whole variance problem of policy gradients. Although i cant find a resource that explicitly mention it, it seems to me that there are two types of variance: variance of the gradient directions and variance of the magnitudes. 

The magnitude variance is reduced with the baselines/value function/reward to go/discounting tricks, but these improvements does not affect the direction - right?   
It seems to me that the only way of reducing the variance of the gradient direction is to base the estimate on more sample.

Am i missing something? 

&amp;#x200B;",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/a6gy9z/policy_gradients_variance_direction_and_magnitude/
"""Exploration in the wild"", Schulz et al 2018 [Deliveroo dataset: 1,613,967 meal orders 30,552 restaurants by 195,333 customers]",1544894569,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a6gxp6/exploration_in_the_wild_schulz_et_al_2018/
[R] Deconfounding Reinforcement Learning in Observational Settings,1544848013,,reinforcementlearning,____jelly_time____,False,/r/reinforcementlearning/comments/a6ca08/r_deconfounding_reinforcement_learning_in/
"""Soft Actor Critic: Deep Reinforcement Learning with Real-World Robots"", Haarnoja et al 2018 {BAIR/GB} [source code release]",1544835301,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a6arla/soft_actor_critic_deep_reinforcement_learning/
"""How AI Training Scales"": a simple gradient-noise rule for predicting maximum minibatch sizes {OA} [on ""An Empirical Model of Large-Batch Training"", McCandlish et al 2018]",1544811323,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a67bd6/how_ai_training_scales_a_simple_gradientnoise/
Looking for ideas for a semester long uni project,1544792462,"I'm currently trying to come up with good ideas for a semester long project in RL. In the course we get a short intro to RL and how things like q-learning and sarsa work. It is then expected that we go beyond this knowledge in our projects. Something that uses DQN would be an example, but it can be anything really as long as it is more complicated than q-learning/sarsa. There are no restrictions on what problem/environment we can use for our RL solution. I already talked to the prof to get some ideas, but that wasn't very helpful.       

So right now my problems are:     

* What environment/problem setting am I going to use?       
* What specific part/algorithm of RL do I want to focus on?

My goal for the course is to really take a deep dive into RL (as deep as possible for only one semester), so I'm not afraid to put in work.     
    
Feeling a little lost and overwhelmed with these decisions, so I would appreciate any pointers, help or discussion! 
",reinforcementlearning,foldo,False,/r/reinforcementlearning/comments/a64m1u/looking_for_ideas_for_a_semester_long_uni_project/
Easiest Continuous Control Task?,1544778676,I'm trying to test an algorithm (DDPG variant) and it's not training on environments like Inverted Pendulum. I don't know if the code is correct or if I just need to tune some hyperparameters so I want to train it on a very easy task to verify that my code has no bugs. What is the easiest env with continuous state and action space on OpenAI gym? Or anywhere else if Gym doesn't have anything like that.,reinforcementlearning,shura04,False,/r/reinforcementlearning/comments/a639fr/easiest_continuous_control_task/
"""IRLAS: Inverse Reinforcement Learning for Architecture Search"", Guo et al 2018",1544757112,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a60up6/irlas_inverse_reinforcement_learning_for/
"""InstaNAS: Instance-aware Neural Architecture Search"", Cheng et al 2018",1544744785,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a5z64y/instanas_instanceaware_neural_architecture_search/
Selected 2018 sim2real robotics papers,1544744483,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a5z4i3/selected_2018_sim2real_robotics_papers/
"“Neural Ordinary Differential Equations.” I think this paper is pretty interesting, and would eventually become useful for continuous-time continuous state/action scenarios. Thoughts?",1544670655,,reinforcementlearning,Shobu711,False,/r/reinforcementlearning/comments/a5phn1/neural_ordinary_differential_equations_i_think/
Best RL papers at NeurIPS 2018,1544663463,"Which RL work from NeurIPS did you find interesting. Let's gather the best pieces of work from the conference here.

&amp;#x200B;

Some of my favorites:

\- [Non-delusional Q-learning and value iteration](https://papers.nips.cc/paper/8200-non-delusional-q-learning-and-value-iteration)

\- [Policy Optimization via. Important Sampling](https://arxiv.org/abs/1809.06098)",reinforcementlearning,MrDoOO,False,/r/reinforcementlearning/comments/a5oik2/best_rl_papers_at_neurips_2018/
"""Adapting Auxiliary Losses Using Gradient Similarity"", Du et al 2018 {DM}",1544659493,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a5nyom/adapting_auxiliary_losses_using_gradient/
Sutton and Barto 2nd Edition: print version not identical to final online version,1544649485,"There is at least one difference; e.g. there is a difference in the first sentence immediately after equation (12.1) on page 288. Have others noticed other differences?

I wonder why MIT Press printed a different version that the final online edition.

&amp;#x200B;",reinforcementlearning,zcra,False,/r/reinforcementlearning/comments/a5mg65/sutton_and_barto_2nd_edition_print_version_not/
"Advice for training recurrent policies, Please share your folk wisdom",1544644050,"I think this is common wisdom in the RL community, but it's only dawned on me yesterday: training recurrent policies is quite tricky/hard. The reason why many papers use frame stacking is, presumably because they couldn't get a recurrent policy to converge, or did not know how to train one (recurrent DQN is fairly new). The reason why many papers use auxiliary losses (eg: DeepMind Lab) is that the reward signals from RL are potentially too sparse to train something like an LSTM. The auxiliary loss presumably can guide the LSTM into learning something useful in a way that sparse RL training signals can't on their own.

I've been working on a 3D environment of my own, something similar to VizDoom, and couldn't get a simple ConvNet + LSTM policy to get above 60% success rate, in an environment that's fairly tiny and should (it seems) be easy to train in. I was wondering if there were people here with experience training recurrent models with RL, and if you had advice to share. What worked for you, what didn't? I'm also curious as to how to set the coefficient for an auxiliary loss. Any guidelines for this? Please come forward and share some of the RL folk wisdom that is often absent from papers and texbooks.",reinforcementlearning,maximecb,False,/r/reinforcementlearning/comments/a5llaa/advice_for_training_recurrent_policies_please/
[P] Support for multi-agent reinforcement learning in Ray RLlib 0.6.0,1544643401,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a5lhk2/p_support_for_multiagent_reinforcement_learning/
[R] Deep Reinforcement Learning and the Deadly Triad,1544628436,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/a5j3sf/r_deep_reinforcement_learning_and_the_deadly_triad/
Reinforcement Learning predictions 2019,1544605282,"What does 2019 and beyond hold for:

- Meta learning
- Model-based learning
- Curiosity based exploration
- Multi-agent RL
- Temporal abstraction &amp; hierarchical RL
- Inverse RL, Demonstrations, Imitation
- Will compute keep doubling every 3.5 months
- OpenAI &amp; Deepmind
- RL deployed to real world tasks like self driving cars
- and all other RL topics

This is your chance to read the predictions of random redditors, and to make your own predictions :p 

If you want you predictions to be formal, consider putting them on predictionbook.com, [example prediction](https://predictionbook.com/predictions/2818).",reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/a5gi3u/reinforcement_learning_predictions_2019/
Could someone explain to me the Policy update in the Soft-Actor Critic algorithm ?,1544602128,"Hello y'all ! 

&amp;#x200B;

I'm having troubles understanding the policy update in the SAC algorithm ([https://arxiv.org/abs/1801.01290](https://arxiv.org/abs/1801.01290)) where it relies on minimizing something involving the KL divergence. Could someone please shed some lights ? 

&amp;#x200B;

Thanks a lot ! ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/a5g830/could_someone_explain_to_me_the_policy_update_in/
Difference between Value iteration and policy iteration,1544544334,"I am a beginner and I have started to read the book ""Reinforcement Learning: An Introduction"" by Richard S. Sutton and Andrew G. Barto and I have a doubt in the value iteration and policy iteration topic. 

As far as I understood, in value iteration, for every time step an action is taken and a value function is calculated and repeats till convergence.

In policy iteration, an action is randomly chosen initially and the entire policy is calculated if the calculated policy is not optimal then the initial action is changed and again the new policy is calculated and the process continues till the convergence. 

is this right? or I have understood is totally wrong. I kindly ask you for a simple explanation.",reinforcementlearning,Matlab_Begin,False,/r/reinforcementlearning/comments/a5838t/difference_between_value_iteration_and_policy/
Is there a comprehensive ablation study on techniques used in DQN papers?,1544518609,"I have finished reading classic DQN papers (DQN2013, DQN2015, DDQN, PER, Dueling, NoisyNet, Rainbow), and will be implementing them this month with PyTorch (1.0!!!). I have done some research in GitHub for existing repositories, and although most repos implement the core of the algorithm similarly, they differ in the details. (ex. Huber Loss, Epsilon Decay)

I thought implementing and performing an ablation study of such techniques would be a nice personal project for December, but I wanted to make sure I'm not being repetitive.

Any links and partial results would be appreciated! Thanks :)",reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/a556k9/is_there_a_comprehensive_ablation_study_on/
what if agents don't know what actions can be performed?,1544503331,"I just thought of this: if you have a MDP but unknown action space that varies for each state, what are some algorithms out there addressing this problem?",reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/a53mks/what_if_agents_dont_know_what_actions_can_be/
"""ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst"", Bansal et al 2018 {G/Waymo} [imitation learning for self-driving cars: RNNs for trajectory planning + prediction of object trajectories, tested on test track]",1544483415,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a50xmj/chauffeurnet_learning_to_drive_by_imitating_the/
N-step DQN implementation issue,1544478956,"I'm having many issues with how to properly implement an N-step DQN. I wanted to try this after reading the Rainbow (2017) paper, but their implementation is using an n-step distributional loss and I would like to use the standard MSE loss in a multi-step way.

 

Anyone has a suggestion of an implementation of a paper that would explain? Thanks a lot for your help. 

I found one implementation on this subreddit posted a few months ago, but I'm having trouble understanding some parts of the code: [https://www.reddit.com/r/reinforcementlearning/comments/8r6j3t/deeprltutorials\_pytorch\_implementation\_of\_dqns/](https://www.reddit.com/r/reinforcementlearning/comments/8r6j3t/deeprltutorials_pytorch_implementation_of_dqns/). ",reinforcementlearning,ranirlol,False,/r/reinforcementlearning/comments/a509jw/nstep_dqn_implementation_issue/
[D] Key Papers in Deep RL,1544474274,,reinforcementlearning,pistachiorodriguez,False,/r/reinforcementlearning/comments/a4zir5/d_key_papers_in_deep_rl/
A review of recent reinforcment learning applications to healthcare,1544465270,,reinforcementlearning,svpadd2,False,/r/reinforcementlearning/comments/a4y2mq/a_review_of_recent_reinforcment_learning/
Examples of structuring curriculum learning in practice?,1544460113,"I am new to curriculum learning in RL and am trying to understand how this is structured in practice.

I'm looking for some paper references, repos, and some high level examples.

From what I gathered, it sounds like there are 2 (not exclusive) approaches to making a curriculum:

1. The dynamics of the game stay the same and the rewards slowly shifted from an dense reward function to the sparse reward function, the one that is available at test time. For example, for a simple ""go to target"" task, one could create lessons by changing the reward function (R_t) on each lesson in the following form:
	Lesson 1: *R_t = W_1 * (last_dist_to_target - dist_to_target) + W_2 * forward_velocity + float(at_target == 1)*
	Lesson 2: *R_t = W_1 * (last_dist_to_target - dist_to_target ) + float(at_target== 1)*
	Lesson 3: *R_t = float(at_target== 1)*
	With some weighting scheme W.
2. The dynamics of the game are slowly changed. Using UnityML [curriculum learning](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Training-Curriculum-Learning.md) as an example, one could increase the difficulty of the environments. 
![Alt-text](https://github.com/Unity-Technologies/ml-agents/raw/master/docs/images/curriculum.png)
3. Some combination of 1. and 2.

What I'm not sure about is how one deals with the problems induced by both approaches.
In 1., the weighting scheme W must properly balanced, often using domain knowledge. For instance, if W_2 is too large, this motivates higher speeds rather than getting to the goal and an agent might end up zooming around the state-space rather than actually approaching the goal. With more complicated games with many contributing factors to the reward function ([like OpenAI5's reward function](https://gist.github.com/dfarhi/66ec9d760ae0c49a5c492c9fae93984a)), such unbalancing becomes more likely and potentially much harder for a researcher to spot.

In 2., the example in UnityML of a wall getting higher does seem like a good application but it also seems easy because the wall is changed gradually along a continuum. In more complicated environments, aspects of the environment may instead be discrete (enemies on/off). 

Does anyone have any experience with these problems or know of references exploring these problems?",reinforcementlearning,Serious__Joker,False,/r/reinforcementlearning/comments/a4x965/examples_of_structuring_curriculum_learning_in/
improving tic tac toe model,1544447058,"For this [tic tac toe model](https://drive.google.com/file/d/1h8mzqdztbyM-9pg_eJZkfkuMHKs_WtKB/view?usp=sharing) , how do I achieve the following model improvement for the [original kaggle kernel](https://www.kaggle.com/dhanushkishore/a-self-learning-tic-tac-toe-program) ?

* Try using a **deeper** neural network, and trying **different learning rates**, to see if it improves the results
* Figure out if the **Opponent** (and training **experience**) being used currently can be improved

I believe this is a reinforcement TD learning model where the concept of number of convolution layers is not there. Besides, reinforcement learning uses discount factor instead of learning rates for current or future results  


And as for tapping on opponent winning experience, I am not sure how I am going to modify the [training code section](https://dhanushkishore.github.io/a_self_learning_tic-tac-toe_player/#Training-the-Program). Any help ?",reinforcementlearning,promach,False,/r/reinforcementlearning/comments/a4vhcj/improving_tic_tac_toe_model/
Is there a formal test to see if system is a valid Markov Decision Process?,1544411564,"Having the markov property means the behavior depends only on the current observation. In other words, if you have `a_{t} and s_{t}`, you can forget about the history of transitions.

    a_{t-1}, a_{t-2}, .... a_{0} and s_{t-1}, s_{t-2}...s_{0}

Is there a formal test for determining if a system is a Markov Decision Process? I found this python module [pymdptoolbox](https://github.com/sawcordwell/pymdptoolbox), but I'm trying to understand what it is theoretically testing for. If one were to feed a bunch of transitions for my system, could I determine whether or not it is in fact a MDP?

&amp;#x200B;

I'm thinking about this in the context of reinforcement learning and modern controls. The system is a tractor trailer with state space equations written as such:

    A = np.array([[0,         0,    0],
                  [-0.1974, 0.1974, 0],
                  [0,      -2.0120, 0]])
    
    B = np.array([[-0.3505],
                 [-0.0100],
                 [0]]
    
    C = np.array([[1, 0, 0],
                  [0, 1, 0],
                  [0, 0, 1]])
    
    D = np.array([[0],
                  [0],
                  [0]])
    
    \dot{x} = Ax + Bu
    y = Cx + Du

&amp;#x200B;",reinforcementlearning,JourneyTheMan,False,/r/reinforcementlearning/comments/a4rrve/is_there_a_formal_test_to_see_if_system_is_a/
"""Meta-Learning: Learning to Learn Fast"", Lilian Weng [metric learning, MANN &amp; meta networks, MAML/REPTILE]",1544404933,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a4qwva/metalearning_learning_to_learn_fast_lilian_weng/
"DDPG is supposed to be off-policy, but doesn't do well at test time when training only on expert policy",1544387664,"I'm using the Deep Deterministic Policy Gradient (DDPG) to back up a tractor-trailer along a specified path. I'm comparing it's performance to a Modern Control method using the Linear Quadratic Regulator (LQR). The states (or inputs) to the networks are the error from the path. Basically, the inputs to both the LQR and DDPG are the same. Currently I am training on one track and testing on the same track to figure out hyperparameters, rewards, etc. 

I found training with purely the Ornstein-Uhlenbech noise is possible, but takes \~15758 episodes. When I use an expert policy (which is basically the LQR) 50% of the time, it can learn within 1938 episodes. When I use the LQR as actions 100% of the time (so basically overfitting like supervised learning), the learned policy performs poorly while testing. 

My question is the following: If DDPG is off-policy (learning over someone else's shoulder), why does the algorithm need to train on samples of both the output of the network and the expert policy?

There is a difference of population distributions between the LQR expert policy and the networks, but shouldn't this not affect it because DDPG is an off-policy reinforcement learning algorithm? Theoretically, what about the DDPG algorithm would make it so one cannot train in a supervised learning manner? Is it due to the algorithm maximizing the future expected reward or something?",reinforcementlearning,JourneyTheMan,False,/r/reinforcementlearning/comments/a4ogtt/ddpg_is_supposed_to_be_offpolicy_but_doesnt_do/
Looking for someone experienced to ask some theoretical/practical questions?,1544368664,"What the title says. I am working on a deep-rl architecture but run into problems and looking to bounce some ideas off of someone. I have some theoretical knowledge but have successfully implemented various supervised networks and some simple toy rl networks. Anyone willing to lend a critical mind?

Edit: Suggestions for a place where I can ask some quick questions are also welcome. ",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/a4lney/looking_for_someone_experienced_to_ask_some/
Code example for beginner,1544326196,"Hi guys, 

&amp;#x200B;

I'm a control engineer, and recently studying the reinforcement learning. 

I have a strong preference to apply the reinforcement learning to my research area. 

Recently, I have taken the course by David Silver in Youtube (2015), but I can't fully understand everything. 

I consider the practice for coding the algorithm using Python. 

&amp;#x200B;

Can you recommend the well organized material or lecture for my case? 

I know that there are so many example in github, but I would like to  practice step by step. 

&amp;#x200B;

Please share your precious experiences when you were a beginner.

&amp;#x200B;",reinforcementlearning,advl29,False,/r/reinforcementlearning/comments/a4hnnz/code_example_for_beginner/
"""RL under Environment Uncertainty"", Abbeel 2018 NIPS slides",1544316075,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a4ge47/rl_under_environment_uncertainty_abbeel_2018_nips/
[Book] Modeling Agents with Probabilistic Programs (Bayesian RL),1544299020,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/a4dy50/book_modeling_agents_with_probabilistic_programs/
[R] ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware,1544286618,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a4c3gt/r_proxylessnas_direct_neural_architecture_search/
"""Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures"", Uesato et al 2018 {DM} [reusing errors from training as seeds]",1544212321,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a43edd/rigorous_agent_evaluation_an_adversarial_approach/
Spinning Up a Pong AI With Deep Reinforcement Learning,1544195122,,reinforcementlearning,pirate7777777,False,/r/reinforcementlearning/comments/a40ooy/spinning_up_a_pong_ai_with_deep_reinforcement/
"""AlphaZero: Shedding new light on the grand games of chess, shogi and Go"" [DM releases followup paper on AlphaZero, +100 shogi games, +100 chess games, and video discussion]",1544124573,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a3rbm3/alphazero_shedding_new_light_on_the_grand_games/
"""Interactions Between Learning and Evolution"", Ackley &amp; Littman 1992",1544121313,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a3qrd0/interactions_between_learning_and_evolution/
New benchmark from OpenAI: Quantifying Generalization in Reinforcement Learning,1544116738,,reinforcementlearning,TallCitizen,False,/r/reinforcementlearning/comments/a3pzhi/new_benchmark_from_openai_quantifying/
OpenAI gym: how to get pixels in classic control environments without opening a window?,1544093106,"I want to train `MountainCar` and `CartPole` from pixels but if I use `env.render(mode='rgb_array')` the environment is rendered in a window, slowing everything down.

I am using `gym 0.10.9`.

(Basically same question as this one on StackOverflow: https://stackoverflow.com/questions/43536034/openai-gym-how-to-get-pixels-in-cartpole-v0)",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/a3mwfn/openai_gym_how_to_get_pixels_in_classic_control/
Is there a better algorithm than PPO or SAC ?,1544083838,"Hey ! 

&amp;#x200B;

Well, pretty much self contained. Do you guys think that there could be a better algorithm than Proximal Policy Optimization or Soft-Actor-Critic ? 

Thanks ! ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/a3m0xz/is_there_a_better_algorithm_than_ppo_or_sac/
[D] Reinforcement learning: Fast and slow - Matthew Botvinick (Video),1544071432,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/a3kmuj/d_reinforcement_learning_fast_and_slow_matthew/
What is the current state of the art when it comes to stock market predictions using Reinforcement Learning?,1544050999,,reinforcementlearning,jackraddit,False,/r/reinforcementlearning/comments/a3hpwo/what_is_the_current_state_of_the_art_when_it/
Policy Gradients- Agent learns to always select the same action,1544041635,"I'm trying to train an agent using REINFORCE algorithm to remove noisy sentences from a dataset. The actions available to the agent are - selecting a sentence, and, not selecting a sentence. The goal for the agent is to eliminate noisy sentences from the dataset by not selecting them. In other words, the agent has to select all clean sentences and not select noisy sentences. I'm using a single layer neural network with sigmoid activation to model the policy. I'm using a CNN to provide the reward. 

&amp;#x200B;

The issue is that the always selects the same action- selecting the sentence. When training starts, the agent is a bit random, but within a 1000 steps of training, the policy always converges. I also tried different reward functions-

&amp;#x200B;

1) Constant positive reward- the agent learns to select all the sentences.

2) Constant negative reward- the agent learns to deselect all the sentences.

3) Random positive reward- the agent learns to select all the sentences.

4) Random negative reward- the agent learns to deselect all the sentences.

5) Random reward (positive and negative rewards) - the agent learns to select all the sentences.

&amp;#x200B;

Can someone help me figure out what exactly is happening?",reinforcementlearning,shubhamjha97,False,/r/reinforcementlearning/comments/a3g7qe/policy_gradients_agent_learns_to_always_select/
"""How to train your MAML: A step by step approach"" [implementing MAML]",1544037941,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a3flko/how_to_train_your_maml_a_step_by_step_approach/
[R] QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning,1543976569,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/a37r7g/r_qmix_monotonic_value_function_factorisation_for/
Bias/Variance and variable length trajectories clarification (policy gradients),1543954593,"Looking for clarification on a few things:

&amp;#x200B;

1. When we say that Policy Gradient methods have high variance, does that only refer to very noisy gradient estimates?
2. How would one actually compute the variance of such gradients?
3. If we imaging a 0 variance, some bias situation, does that mean our gradient estimates are ""stable"", but that they are biased away from taking us to the ""correct"" way?
4. In most litterature, the standard policy gradient (before variance reduction techniques) is an expectaion of a sum of gradients of log probabilities times the reward sum. The gradients are summed up to T timesteps, indicating all our sample trajectories have the same length - is that also done in practice, or is it just to simplify the math? seems weird to halt mid trajectory because T timestep has passed.",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/a34egi/biasvariance_and_variable_length_trajectories/
"""DISCERN: Unsupervised Control Through Non-Parametric Discriminative Rewards"", Warde-Farley et al 2018 {DM} [HER/auxiliary losses]",1543952049,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a33zfr/discern_unsupervised_control_through/
How to deal with both discrete and continuous controls together in DRL?,1543949161,"Say I have a robot. I would like to control its movement by deterministic parameters. But I have a 4-state switch I would like to control with a stochastic parameter.

How should I design the model?

1. try to use 0-1 for the switch.  E.g. \[0,0.25\] for state\_0, \[0.25, 0.5\] for state\_1, etc.
2. use multi-agent model

I am wondering if these two methods are doable?

Or is there any better solution?",reinforcementlearning,deepbluesome,False,/r/reinforcementlearning/comments/a33ii3/how_to_deal_with_both_discrete_and_continuous/
"Reproducing results of Domain Randomization using Fetch robot, ROS and Gazebo",1543928722,"At The Construct we have finished the code, simulations, and training to reproduce the results of the amazing paper ""Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World"", by many of the OpenAI team.

In [today's ROS LIVE-Class](https://www.youtube.com/watch?v=XKJe--u8xHI) we're going to show how to do it with ROS and Gazebo simulation of Fetch, and we will PROVIDE THE FULL RUNNING CODE to all the attendants (notebook with instructions, simulation, Python3 environments, training datasets, and many much ROS code). Waiting for you at [18:00 CET on Youtube](https://www.youtube.com/watch?v=XKJe--u8xHI) (free attendance).",reinforcementlearning,roboticist101,False,/r/reinforcementlearning/comments/a30i1q/reproducing_results_of_domain_randomization_using/
Fast Cash Offer for Houses Spring TX,1543919219,,reinforcementlearning,northhoustonhomebuye,False,/r/reinforcementlearning/comments/a2zi3m/fast_cash_offer_for_houses_spring_tx/
[Course] CS885 Reinforcement Learning - Spring 2018 - University of Waterloo,1543915080,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/a2z44b/course_cs885_reinforcement_learning_spring_2018/
Continuous Control with Deep Reinforcement Learning on TurtleBot3 Burger - DDPG,1543899891,"I'm coming here to spread my project to the world. I am making a DDPG with Pytorch to run on a Gazebo simulation of the TurtleBot3 Burger:
- https://youtu.be/NhGxEC3g7sU

I intend to to implement on the real robot. When I do this I will post another video on youtube.

The link of my github project:

- https://github.com/dranaju/project

P.S.: I implemented everything in a Docker container. So if you want to test I can post a tutorial on how to make this.",reinforcementlearning,dranaju,False,/r/reinforcementlearning/comments/a2xfre/continuous_control_with_deep_reinforcement/
"""Macro action selection with deep reinforcement learning in StarCraft"", Xu et al 2018 {Bilibili} [RNN Ape-X]",1543890133,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a2w2k2/macro_action_selection_with_deep_reinforcement/
"""Macro action selection with deep reinforcement learning in StarCraft"", Xu et al 2018 [RNN Ape-X]",1543890088,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a2w2b0/macro_action_selection_with_deep_reinforcement/
"'AlphaFold': ""De novo structure prediction with deep-learning based scoring"", Evans et al 2018 abstract {DM} [supervised learning of protein structure using DRAW-generated samples as data augmentation, evolutionary hyperparameter tuning]",1543875653,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a2ttj5/alphafold_de_novo_structure_prediction_with/
Trying to reproduce Pursuit Algorithm from 'Reinforcement Learning - An Introduction',1543869969,"Hello dear redditors,

&amp;#x200B;

as the title states, I am trying to reproduce the findings from the book  'Reinforcement Learning - An Introduction', especially those from page 44, where a pursuit method is compared to an epsilon-greedy method. I get comparable results, however the findings in the book converge to much higher '% Optimal Action' than mine.

Are there any obvious flaws to my code or could it be something internal to the random functions?

[Results for the comparison of e-greedy and pursuit method](https://i.redd.it/015tegr2k4221.png)

&amp;#x200B;

    import numpy as np
    from matplotlib import pyplot
    import random
    
    class Bandit:
        def __init__(self, mean, std):
            self.mean = mean
            self.std = std
    
        def __init__(self):
            self.mean = np.random.normal(0, 1)
            self.std = 1
    
        def sample(self):
            return np.random.normal(self.mean, self.std)
    
    def main():
        Nbandits = 10
        Nepochs = 2000
        Nplays = 1000
        epsilon = 0.1
        beta = 0.01
    
        pursuit_best_pick = [0] * Nplays
        epsilon_greedy_best_pick = [0] * Nplays
    
        for epoch in range(1, Nepochs):
            bandits = [Bandit() for _ in range(Nbandits)]
    
            pursuit_values = [0] * Nbandits
            epsilon_greedy_values = [0] * Nbandits
    
            pursuit_best_pick_this_epoch = [0] * Nplays
            eps_best_pick_this_epoch = [0] * Nplays
    
    
            best_bandit = np.argmax([b.mean for b in bandits])
    
            pi = [1/Nbandits] * Nbandits
    
            for play in range(1, Nplays):
                # Pick a random according to our policy
                pursuit_action = np.random.choice(Nbandits, 1, p=pi)[0]
                epsilon_greedy_action = np.argmax(epsilon_greedy_values) if np.random.uniform(0, 1) &gt; epsilon else np.random.choice(range(Nbandits))
    
                # Save the relative best picks for this epoch 
                pursuit_best_pick_this_epoch[play] = pursuit_best_pick_this_epoch[play-1] + 1/play * ((1 if pursuit_action == best_bandit else 0) - pursuit_best_pick_this_epoch[play-1])
                eps_best_pick_this_epoch[play] = eps_best_pick_this_epoch[play-1] + 1/play * ((1 if epsilon_greedy_action == best_bandit else 0) - eps_best_pick_this_epoch[play-1])
    
                # Update the average of relative best picks
                pursuit_best_pick[play] = pursuit_best_pick[play] + 1/epoch * (pursuit_best_pick_this_epoch[play] - pursuit_best_pick[play])
                epsilon_greedy_best_pick[play] = epsilon_greedy_best_pick[play] + 1/epoch * (eps_best_pick_this_epoch[play] - epsilon_greedy_best_pick[play])
    
                # Sample the according bandit
                pursuit_value = bandits[pursuit_action].sample()
                epsilon_greedy_value = bandits[epsilon_greedy_action].sample()
    
                # Update the average values
                pursuit_values[pursuit_action] = pursuit_values[pursuit_action] + 1/play * (pursuit_value - pursuit_values[pursuit_action])
                epsilon_greedy_values[epsilon_greedy_action] = epsilon_greedy_values[epsilon_greedy_action] + 1/play * (epsilon_greedy_value - epsilon_greedy_values[epsilon_greedy_action])
                        
                # PURRRSUIT:
                # Find the greedy action
                greedy_action = np.argmax(pursuit_values)
    
                # Update in favor of the greedy action
                pi = [pi[a] + beta * ((1 if a == greedy_action else 0) - pi[a]) for a in range(Nbandits)]
    
        pyplot.plot(epsilon_greedy_best_pick, label=""Epsilon Greedy"")
        pyplot.plot(pursuit_best_pick, label=""Pursuit"")
        pyplot.legend(loc='upper left')
        pyplot.show()
    
    if __name__ == ""__main__"":
        main();

&amp;#x200B;",reinforcementlearning,sku-sku,False,/r/reinforcementlearning/comments/a2sslm/trying_to_reproduce_pursuit_algorithm_from/
An Open Source Tool for Scaling Multi-Agent Reinforcement Learning,1543863400,,reinforcementlearning,rayspear,False,/r/reinforcementlearning/comments/a2rp04/an_open_source_tool_for_scaling_multiagent/
Fully Decentralized Multi-Agent Reinforcement Learning with Networked Agents,1543825852,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/a2mspj/fully_decentralized_multiagent_reinforcement/
"""Deep Counterfactual Regret Minimization"", Brown et al 2018 [poker CFR]",1543811667,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a2l6ja/deep_counterfactual_regret_minimization_brown_et/
"""An Introduction to Deep Reinforcement Learning"", Francois-Lavet, Henderson, Islam, Bellemare, Pineau 2018 [DQN, PG, exploration, benchmarking, POMDPs, transfer &amp; meta-learning, multi-agent RL]",1543807209,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a2kldk/an_introduction_to_deep_reinforcement_learning/
[R] A Closer Look at Deep Policy Gradients,1543801321,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a2jrwy/r_a_closer_look_at_deep_policy_gradients/
Reading material for model-based deep RL,1543800973,"I'm an undergrad now starting work in model-based deep RL. I've only read the ""Planning and Learning with Tabular Methods"" chapter by the standard RL book(Sutton and Barto) and some introductory slides I found online(Berkeley, UCL)  but I feel like I've only scratched the surface and can't seem to find anything else that goes deeper. Should I just start reading papers? If so, do you have any recommendations? ",reinforcementlearning,WalkingCook1e,False,/r/reinforcementlearning/comments/a2jq41/reading_material_for_modelbased_deep_rl/
"If you had to give an introduction to RL with only 20 minutes time, what things would you talk about?",1543785384,"I feel like it is very hard to give a reasonable introduction with such a short time limit, but I have to somehow make it work.     
Things that I'm considering so far:   


* Where RL belongs in machine learning (differences to supervised and unsupervised learning)
* RL Interface (The famous picture from sutton)
* An intuitive example
* Exploitation vs Exploration
* Large Statespaces/Why do we need function approximation
      
&amp;nbsp;   
I would really like work up to Q-Learning but it seems impossible in 20 minutes. How would you guys approach this task? What things would you talk about?     
Any input is greatly appreciated!",reinforcementlearning,__Seven_Costanza_,False,/r/reinforcementlearning/comments/a2he6f/if_you_had_to_give_an_introduction_to_rl_with/
A2C converging and only going in one direction,1543781889,"Hey,  
I am fairly new to reinforcement learning and tried to implement the A2C algorithm but am now encountering an issue.  
My goal environment to solve is Continuous Cartpole but it gets stuck at some point only going left or right. I had success with Continuous Mountain Car but am now struggling to get this to work on continuous cartpole as I already spent countless hours researching this issue. I would be very happy if someone could tell me if this is a known issue with A2C (or actor critic in general) and CartPole or if there are tricks to apply.",reinforcementlearning,salah3,False,/r/reinforcementlearning/comments/a2gt02/a2c_converging_and_only_going_in_one_direction/
"Temporal Value Transfer - Agent with soft memory of past observations/actions. When memory similarity rises above a threshold during a rollout, some of the current reward is assigned to the era that memory's from",1543677262,,reinforcementlearning,AlexCoventry,False,/r/reinforcementlearning/comments/a23rrs/temporal_value_transfer_agent_with_soft_memory_of/
[R] Noisy Natural Gradient as Variational Inference,1543668533,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/a22rwd/r_noisy_natural_gradient_as_variational_inference/
Environments for Lifelong Reinforcement Learning,1543657997,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/a21u4u/environments_for_lifelong_reinforcement_learning/
Implementing Policy Gradient for Atar (near deterministic policy at the beginning),1543625235,"Im trying to make my PG algorithm -  which work on simpler environments like Cartpole - work on Atari (PongDeterministic-v4 for starters)

But when using the algorithm on Pong with the DQN network architecture (conv(16) -&gt; conv(32)-&gt;FC(256) -&gt; softmax) with  the 80 x 80 x 4  preprocessed frames as input, the policy is very near deterministic at initialization. This means the agent never learns since it gets -21 reward for all episodes, and avg baseline b = 1/n sum R(tau) = -21, so the advantage is 0. 

I could of course try another baseline (or none), but i feel like the problem is the near deterministic initial state instead.

I have tried putting in a batch of 80 x 80 x 4 ones, and for this input the actions are about equiprobable, so the weight initialization seems fine. **So it must be the four very similar frames that makes one action extremely likely, is this perfectly normal for an agent on Atari to start with this near-deterministic behaviour?**

I also tried SpaceInvadersDeterministic-v4, and there it actually managed to pick some different actions due to more moving game objects i guess (but still each action was picked with near probability 1). But since the action probs were so close to one, and because it would always get a positive reward, it would quickly converge to a terrible behaviour, and to a state similar in pong where the advantage would be 0.

(using relu nonlinearities on all but logits layer, tried with both default param init and he init)

tl:dr: the question in bold 

Thanks a lot for any input :)

&amp;#x200B;",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/a1yaql/implementing_policy_gradient_for_atar_near/
"""Visual Model-Based Reinforcement Learning as a Path towards Generalist Robots"" [on Ebert et al 2018]",1543622713,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a1xyvo/visual_modelbased_reinforcement_learning_as_a/
Common things that happen when RL starts to learn?,1543612464,"Hi,

&amp;#x200B;

 i am working with my RL now already quite a bit (DDPG) and when i am training it (talking about first 200 episodes or so) i see (of course) strange behaviour from the agent, i know this is normal, but technical there are some strange things that i don't understand if this is normal or that i should change something?

&amp;#x200B;

i am mostly talking about the sigmoid values, they either climb all very low/high or only 1 goes high and the others low. i got told that it's climbing specific actions (even on negative rewards) cause thats how he starts of learning!? i can't tell from my ai if it's just ""the start of learning and have more patience"" or that it is acctually a problem i need to fix?! i tried longer runs, but no diffrent results

&amp;#x200B;

what do you think?

&amp;#x200B;

Jan ",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/a1wjnn/common_things_that_happen_when_rl_starts_to_learn/
Using Neural architecture search technique to discover new SVM kernel function for MNIST,1543605549,,reinforcementlearning,a7b23,False,/r/reinforcementlearning/comments/a1vig9/using_neural_architecture_search_technique_to/
LunarLanderContinuous and Policy Gradients,1543599075,"I have tried several implementations of VPG of the LunarLanderContinuous-V2 environment in OpenAI gym. They all fail to solve the environment (get &gt; 200 mean score), since they don't seem to be able to learn how to stop moving (which gives 100 pts. in the end).   


It seems that it requires staying still for &gt;20 frames (not firing any engines) - forcing trained agent with mean reward of 150+ to stay still last 25 frames does the trick. At the same time solving LunarLander-V2 discrete version is relatively easy, with agent quickly reaching &gt; 200 reward.   


This looks suspicious to me, especially since the underlying physics engine in the same. Can anyone report similar findings with their PG implementations, or provide any comments on the matter?",reinforcementlearning,maksai,False,/r/reinforcementlearning/comments/a1ui4i/lunarlandercontinuous_and_policy_gradients/
Reproduce experiments from DeepMind and OpenAI in Unity with my new tutorial 'Getting Started with Marathon Environments' to get you going with Reinforcement Learning and Continuous Control benchmarks in #Unity3d + ML-Agents. All open source!,1543558178,,reinforcementlearning,soho-joe,False,/r/reinforcementlearning/comments/a1pu98/reproduce_experiments_from_deepmind_and_openai_in/
Any difference between return and cumulative reward in RL?,1543520235,What is the difference between a return and cumulative reward in the context of reinforcement learning?,reinforcementlearning,underactuated,False,/r/reinforcementlearning/comments/a1kkn5/any_difference_between_return_and_cumulative/
[R] The Importance of Sampling in Meta-Reinforcement Learning,1543507681,,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/a1in64/r_the_importance_of_sampling_in_metareinforcement/
linktext reddit przypomina: Ciekawa treść i dobry tytuł gwarantami sukcesu dla Twojego linka. url https://deepsense.ai/driverless-car-or-autonomous-driving-tackling-the-challenges-of-autonomous-vehicles/ x Driverless car or autonomous driving? Tackling the challenges of autonomous vehicles,1543488743,,reinforcementlearning,AnnaKow,False,/r/reinforcementlearning/comments/a1gc6b/linktext_reddit_przypomina_ciekawa_treść_i_dobry/
"[R] ""Experience Replay for Continual Learning"", Rolnick et al 2018",1543476611,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/a1f97q/r_experience_replay_for_continual_learning/
Understanding the impact of entropy in policy learning,1543464955,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/a1dygf/understanding_the_impact_of_entropy_in_policy/
AWS DeepRacer,1543440005,"[https://aws.amazon.com/deepracer/](https://aws.amazon.com/deepracer/)

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,mimoralea,False,/r/reinforcementlearning/comments/a1ahqs/aws_deepracer/
Bad local minima in robotics DRL: tapping a table to slightly move a ball instead of using a joystick to move the ball,1543433431,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a19gzm/bad_local_minima_in_robotics_drl_tapping_a_table/
[D] Main Deep Reinforcement Learning implementations?,1543415325,"Here are the implementations I'm aware of:

||**Repo**|**Stars**|**Framework**|**a2c**|**acer**|**acktr**|**ddpg**|**dqn**|**her**|**ppo**|**trpo**|**vpg**|**cem**|**gae**|**td3**|**sac**|**Algos path**|
:--|:--|--:|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|:--|
|**OpenAI**|[https://github.com/openai/baselines](https://github.com/openai/baselines)|6161|TF|x|x|x|x|x|x|x|x||||||[https://github.com/openai/baselines/tree/master/baselines](https://github.com/openai/baselines/tree/master/baselines)|
|**RLLAB**|[https://github.com/rll/rllab](https://github.com/rll/rllab)|1926|Theano||||x|||x|x|x|x||||[https://github.com/rll/rllab/tree/master/rllab/algos](https://github.com/rll/rllab/tree/master/rllab/algos)|
|**Udacity**|[https://github.com/udacity/deep-reinforcement-learning](https://github.com/udacity/deep-reinforcement-learning)|1671|PyTorch||||x|x|||||x||||[https://github.com/udacity/deep-reinforcement-learning](https://github.com/udacity/deep-reinforcement-learning)|
|**RL-Adventure**|[https://github.com/higgsfield/RL-Adventure-2](https://github.com/higgsfield/RL-Adventure-2)|1477|PyTorch|x|x||x||x|x||||x|x|x|[https://github.com/higgsfield/RL-Adventure-2](https://github.com/higgsfield/RL-Adventure-2)|
|**ikostrikov**|[https://github.com/ikostrikov/pytorch-a2c-ppo-acktr](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr)|1022|PyTorch|x||x||||x|||||||[https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/algo](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/tree/master/algo)|
|**ShangtongZhang**|[https://github.com/ShangtongZhang/DeepRL](https://github.com/ShangtongZhang/DeepRL)|962|PyTorch|x|||x|x||x|||||||[https://github.com/ShangtongZhang/DeepRL/tree/master/deep_rl/agent](https://github.com/ShangtongZhang/DeepRL/tree/master/deep_rl/agent)|
|**Stable Baselines**|[https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)|419|TF|x|x|x|x|x|x|x|x||||||[https://github.com/hill-a/stable-baselines/tree/master/stable_baselines](https://github.com/hill-a/stable-baselines/tree/master/stable_baselines)|
|**Vel**|[https://github.com/MillionIntegrals/vel](https://github.com/MillionIntegrals/vel)|185|PyTorch|x|x||x|||x|x||||||[https://github.com/MillionIntegrals/vel/tree/master/vel/rl/algo](https://github.com/MillionIntegrals/vel/tree/master/vel/rl/algo)|

Do you know of any other?

There should be a website running convergence and runtime benchmarks for those. Some kind of ""Deep RL That Matters"" but updated continuously. Anything like that out there?",reinforcementlearning,MasterScrat,False,/r/reinforcementlearning/comments/a16o4h/d_main_deep_reinforcement_learning_implementations/
Request to join a study group,1543343862,"I am a software engineer who has recently started picking up on Reinforcement Learning. I am interested in this field, have implemented some deep algorithms:

[https://github.com/AmreshVenugopal/drlnd\_continous\_control](https://github.com/AmreshVenugopal/drlnd_continous_control)

[https://github.com/AmreshVenugopal/drlnd\_competition\_and\_collaboration](https://github.com/AmreshVenugopal/drlnd_competition_and_collaboration)

[https://github.com/AmreshVenugopal/DRLND\_Navigation\_Project](https://github.com/AmreshVenugopal/DRLND_Navigation_Project)

That said I am directionless and need to work with someone who isn't. If you have a paper to publish, a project with some clarity, contact me and I'll put in the effort to make things work. I am not asking for a full-time job offer, but a group committed into learning/teaching/experimenting/research and I'll be happy to offer what I can do best.",reinforcementlearning,ltbringer,False,/r/reinforcementlearning/comments/a0xn09/request_to_join_a_study_group/
State representation,1543338626,"Hello everyone, I just started to learn about reinforcement learning and I would like to apply it to some 2d grid games like vindinium ([https://github.com/ornicar/vindinium](https://github.com/ornicar/vindinium)) or bomberman.   


I understand that there are more than few choices when talking about state representation. 

First is to just take a raw map and create 3d tensor with shape WxHxC, where C is number of different objects in the scene. 

Second is to create a handcrafted set of features. From my point of understanding, this way we would help the agent a lot and spare the time for him trying to learn a good feature representation. Also, I believe that with smart choice of features I would help the agent to generalize better. Similar situations at the map should have similar feature representations, that way we would reduce the state space etc.

&amp;#x200B;

I would like to reduce the training time as much as I can, so it seems to me that I should go with handcrafted features. I have a couple of questions now.

Where could I find a good ideas for feature representation in these kind of games? I understand that it depends on the game, but also believe there are some things that are unique and best practice.

Should I use reward in every timestep or just at the end of the game? Any advices for reward design?

Is there any algorithm that performs particularly well in this problem?

&amp;#x200B;

I've already read some tutorials and papers but please share any materials regarding the application of reinforcement learning to 2d grid games and competitive self-play.",reinforcementlearning,j_saric,False,/r/reinforcementlearning/comments/a0wsrb/state_representation/
"""DiCE: The Infinitely Differentiable Monte Carlo Estimator"" [discussion &amp; PyTorch demo]",1543285452,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a0qctv/dice_the_infinitely_differentiable_monte_carlo/
Uber claims solution of entire _Montezuma's Revenge_ game (reaching 2m points &amp; level 159) and progress on _Pitfall_ with 'Go-Explore',1543270957,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a0oa5d/uber_claims_solution_of_entire_montezumas_revenge/
"What is the best exploration / exploitation strategy among Thompson Sampling, UCB, EXP3, etc?",1543264571,"Is there some evaluation between the different approaches for the exploration exploitation tradeoff?

As I understand there are many approaches that can be shown to be asymptotically optimal, but how they fare on practical problems doesn't make the comparison any less interesting! (their regret will still grow proportionally to different constants).",reinforcementlearning,antonosika,False,/r/reinforcementlearning/comments/a0n8my/what_is_the_best_exploration_exploitation/
[P] Reaver: StarCraft II Deep Reinforcement Learning Agent in Python Keras,1543246386,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/a0kb68/p_reaver_starcraft_ii_deep_reinforcement_learning/
[R] Learning to Learn without Forgetting By Maximizing Transfer and Minimizing Interference (Meta-ER),1543151162,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/a08c83/r_learning_to_learn_without_forgetting_by/
Aren't Model Free RL techniques actually using MDP/POMDP as models?,1543105504,Am I missing something or are model free rl techniques actually using mdp/pomdp as a model of the environment? I accept that it is a very general model but still a model that comes with the constraints and assumptions similar to more specific ones. ,reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/a03v3j/arent_model_free_rl_techniques_actually_using/
"Why don't policies over large action spaces also have to ""optimize""?",1543080680,"I'm reading [Continuous control with deep reinforcement learning](https://arxiv.org/abs/1509.02971). They say:

&gt;DQN cannot be straightforwardly applied to continuous domains since it relies on a finding the action that maximizes the action-value function, which in the continuous valued case requires an iterative optimization process at every step.

I think I know what they mean, partly: when you do Q-learning, you input a state into the network, and get a vector of action values back for that state. Then, you have to do an argmax over them to find the best one, which is an O(N) operation. Right?

On the other hand, using a policy, I input a state and get back a probability distribution of how much I should choose each action. But (at least in the discrete case), isn't that also an O(N) operation? If I have an action space of 1000 actions, it seems like calculating the softmax of all of them (what seems like the typical policy network output for discrete action spaces, right?) involves summing all of them, even if that's happening internally.

It seems like the same thing would apply to continuous action spaces too, unless we assume that the policy is outputting a normal probability distribution or something else.

What am I missing here? thanks for any tips.",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/a00c3v/why_dont_policies_over_large_action_spaces_also/
Why do rewards start to drop after a certain number of episodes?,1543046354,"I am working on a small project where I have a single robotic arm (Unity ML-agents) Reacher. The objective is to train the arm to move in a 3d space and touch a particular target which is part of the environment.

I have implemented a DDPG Algorithm for this, according to the [paper](https://arxiv.org/abs/1509.02971) for continuous control tasks it is suitable provided there is a sufficient number of examples provided.



State space: 33
===

The observation space consists of 33 variables corresponding to the position, rotation, velocity, and angular velocities of the arm.

Action space: 4
===

Each action is a vector with four numbers, corresponding to torque applicable to two joints. Every entry in the action vector should be a number between -1 and 1.


Network Architecture:
===
### Actor
```
Model(
    (fc1): Linear(in_features=33, out_features=128, bias=True)
    (fc2): Linear(in_features=128, out_features=128, bias=True)   
    (fc3): Linear(in_features=128, out_features=4, bias=True)   
    (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)   
    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
```

### Critic
```
Critic(
  (fcs1): Linear(in_features=33, out_features=128, bias=True)
  (fc2): Linear(in_features=132, out_features=128, bias=True)
  (fc3): Linear(in_features=128, out_features=1, bias=True)
  (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
)
```

The average reward over 100 episodes so far:
```
Episode 100	Average Score: 3.42	Time: 19.618900299072266
Episode 200	Average Score: 13.52	Time: 20.195945739746094
Episode 300	Average Score: 27.03	Time: 20.565974235534668
Episode 400	Average Score: 25.36	Time: 20.646285295486453
Episode 500	Average Score: 27.59	Time: 20.215036630630493
Episode 600	Average Score: 25.32	Time: 20.783222675323486
Episode 621	Average Score: 24.42	Time: 20.148203372955322
```

Why are the rewards receeding?

If there are details that are necessary to answer this question and I have missed them, please mention in the comments and I'll add them.
",reinforcementlearning,ltbringer,False,/r/reinforcementlearning/comments/9zwr0r/why_do_rewards_start_to_drop_after_a_certain/
DeepMind Reinforcement Lectures (YouTube),1543043147,,reinforcementlearning,fxidiot,False,/r/reinforcementlearning/comments/9zwgjj/deepmind_reinforcement_lectures_youtube/
"""SOS: Stable Opponent Shaping in Differentiable Games"", Letcher et al 2018",1543022543,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9zu46e/sos_stable_opponent_shaping_in_differentiable/
[Course] Advanced Deep Learning and Reinforcement Learning course at UCL by DeepMind,1542980952,"[https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ\_K2RZs](https://www.youtube.com/playlist?list=PLqYmG7hTraZDNJre23vqCGIVpfZ_K2RZs)

This course, taught originally at UCL and recorded for online access, has two interleaved parts that converge towards the end of the course. One part is on machine learning with deep neural networks, the other part is about prediction and control using reinforcement learning. The two strands come together when we discuss deep reinforcement learning, where deep neural networks are trained as function approximators in a reinforcement learning setting.  

The deep learning stream of the course will cover a short introduction to neural networks and supervised learning with TensorFlow, followed by lectures on convolutional neural networks, recurrent neural networks, end-to-end and energy-based learning, optimization methods, unsupervised learning as well as attention and memory. Possible applications areas to be discussed include object recognition and natural language processing.  

The reinforcement learning stream will cover Markov decision processes, planning by dynamic programming, model-free prediction and control, value function approximation, policy gradient methods, integration of learning and planning, and the exploration/exploitation dilemma. Possible applications to be discussed include learning to play classic board games as well as video games.  
",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9zoety/course_advanced_deep_learning_and_reinforcement/
"""Can computer science algorithms show us how to live better, or is that a false hope?"" [a discussion with Brian Christian of _Algorithms to Live By_: explore vs exploit, optimal stopping, simulated annealing]",1542944525,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9zkre6/can_computer_science_algorithms_show_us_how_to/
[D] Help need in understanding Monte Carlo first visit vs every visit policy evaluation,1542941055,"I was going through Richard Sutton's RL book. While reading policy evaluation using Monte Carlo methods, I could not comprehend the difference between first visit MC and every visit MC.

Can someone help me understand the method? I would be very thankful if the example mentioned in this presentation is used: https://www.kth.se/social/files/58b941d5f276542843812288/RL04-Monte-Carlo.pdf ([slide 17](https://imgur.com/a/ZVxEnil) )",reinforcementlearning,seeking_fin_guru,False,/r/reinforcementlearning/comments/9zkdjb/d_help_need_in_understanding_monte_carlo_first/
[D] Help need in understanding Monte Carlo first visit vs every visit policy evaluation.,1542940455,,reinforcementlearning,seeking_fin_guru,False,/r/reinforcementlearning/comments/9zkb3k/d_help_need_in_understanding_monte_carlo_first/
Multi-agent environments,1542895160,"Which are the most popular multi-agent environments?

There is any multi-agent environment which includes mazes and in negative case which framework would you suggest to implement one?

These are the ones that seem to be more popular:
- [openai-multiparticle](https://github.com/openai/multiagent-particle-envs)
- [sisl](https://github.com/sisl/MADRL)








",reinforcementlearning,HeavyGradient,False,/r/reinforcementlearning/comments/9zebb0/multiagent_environments/
"Are you interested in Reinforcement Learning and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1542866300,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/9zbea7/are_you_interested_in_reinforcement_learning_and/
What does the Policy Gradient Theorem give us that Score Function Gradient Estimator does not?,1542850307,"Im studying policy gradient methods, and i get that we can use the score function gradient estimator method to derive a formula for the gradient of the expected total reward of trajectories. And that we can estimate it by sampling trajectories and compute an empirical mean. 

&amp;#x200B;

What i dont get is why do we need the policy gradient theorem - it seems to say pretty much the same thing, but derived in a different way (chap 13 - [http://incompleteideas.net/book/bookdraft2017nov5.pdf](http://incompleteideas.net/book/bookdraft2017nov5.pdf)) 

&amp;#x200B;

Can someone help me understand the relationship between the two? ",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/9z9i6l/what_does_the_policy_gradient_theorem_give_us/
A collection of 70+ trained RL agents using Stable Baselines,1542845697,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/9z8xi7/a_collection_of_70_trained_rl_agents_using_stable/
[R] Scalable agent alignment via reward modeling – DeepMind Safety Research,1542844478,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9z8rrs/r_scalable_agent_alignment_via_reward_modeling/
How much of the learning is happening in the convnet part of a typical DQN?,1542840404,"Something that surprised me when I first read the Atari Deepmind papers was that, although the Q NN *is* somewhat ""deep"", most of the layers are part of a convnet; there are only 1 or 2 fully connected layers at the end right before the output. 

I know it's using the convnet to take the game's raw pixels and get useful info out of them, but how much of the learning of the Q values for states is happening in the convnet section as well?

Put another way: most of the games they looked at are fairly simple in the sense that any of their states could be uniquely specified by a pretty small set of numbers. Let's say that instead of making the DQN learn from the raw pixels as input, we first ran the frames through an image processing function that accurately returns the values of all the relevant state features. Would the NN then need only the same 1 or 2 FC layers? Or, is some of the Q function approximation happening in the convnet as well?
",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9z881x/how_much_of_the_learning_is_happening_in_the/
Other RL Environments,1542818534,"Aside from the obvious OpenAI Gym, are there other open RL environment collections you have used?",reinforcementlearning,ph03n1x333,False,/r/reinforcementlearning/comments/9z4ttw/other_rl_environments/
Spinning Up an Open AI course on RL,1542806098,,reinforcementlearning,WrinkledTime,False,/r/reinforcementlearning/comments/9z3473/spinning_up_an_open_ai_course_on_rl/
Why does the agent refuse to go for the big reward?,1542805473,"So I am trying to use a dqn like architecture (convnet q estimation with replay buffer) to train a reinforcement learning agent: to teach it to pick up a key and go for the door in a 4 by 4 gridworld. The key gives +0.5 reward, while the door gives +1.0 reward if the agent goes there after picking up the key . (The key simply disappears when the agent moves on top and grants the agent +0.5). If the agent moves on top of the door without going to the key first nothing happens. Every step -0.05 is given as incentive to finish quicker. Is it possible for the agent to learn this behavior with this architecture? I do not have any kind of memory system and the samples from the replay buffer are taken randomly to train the network, but considering the key disappears after it gets picked up, the state changes so I don't see any reason why it should not learn.

&amp;#x200B;

After training, the agent picks up the key and stays there until the maximum amount of time before episode reset, accumulating negative reward continously instead of going to the door. What am I doing wrong here? 

&amp;#x200B;

Thank you!  ",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/9z31i3/why_does_the_agent_refuse_to_go_for_the_big_reward/
[R] Practical Bayesian Learning of Neural Networks via Adaptive Subgradient Methods,1542798503,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9z2b0t/r_practical_bayesian_learning_of_neural_networks/
Can somebody explain this? Everything converges to 1 or 0.,1542753760,"I have implemented a PPO agent to solve a partially observable problem and this is what happens after around 30 episodes.

My agent is a Clipped PPO agent using Generalised Advantage Estimation in an Actor Critic Framework, it generates 512 steps in 8 parallel environments (for a total of 4096 data points) and then learns in randomised mini-batches of 128 steps on the whole dataset for 10 epochs. 

The hyperparameters are as follows:

Clip Param - 0.2  
GAE Lambda - 0.8  
GAE Gamma - 0.998

&amp;#x200B;

The networks structure is quite simple, the policy network has some 2D Conv layers with ReLU activations and finally a dense softmax layer. The value network has the same 2D conv layers, but at the end has 3 Dense layers with ReLU activation and finally a dense layer with a Sigmoid activation.

&amp;#x200B;

One other thing I noticed is that the episodes seem to get longer, the 1st only took 68 seconds, the 10th, 247 seconds and the 30th has gone up to 918 seconds.

&amp;#x200B;

Any help is much appreciated.

&amp;#x200B;

https://i.redd.it/fse9f98dbkz11.png",reinforcementlearning,olafgarten,False,/r/reinforcementlearning/comments/9yx8if/can_somebody_explain_this_everything_converges_to/
"Summary of ""Exploration By Random Network Distillation""",1542747388,"I wrote a summary of OpenAI's recent paper ""Exploration By Random Network Distillation"". Their model introduces a new approach to develop curiosity in RL agents using 2 neural networks (fixed and predictor) that learn previously-visited state and give smaller rewards for visiting them again.

[https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/](https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/)

I'd love to get your feedback!",reinforcementlearning,ranihorev,False,/r/reinforcementlearning/comments/9yw968/summary_of_exploration_by_random_network/
"summary of ""Exploration By Random Network Distillation""",1542746588,"I wrote a summary of OpenAI's recent paper ""Exploration By Random Network Distillation"". Their model introduces a new approach to develop curiosity in RL agents using 2 neural networks (fixed and predictor) that learn previously-visited state and give smaller rewards for visiting them again. 

[https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/](https://www.lyrn.ai/2018/11/20/curiosity-driven-learning-exploration-by-random-network-distillation/)

I'd love to get your feedback!",reinforcementlearning,ranihorev,False,/r/reinforcementlearning/comments/9yw4h7/summary_of_exploration_by_random_network/
Schmidhuber's new blog post on Unsupervised Adversarial Neural Networks and Artificial Curiosity in Reinforcement Learning,1542701295,,reinforcementlearning,milaworld,False,/r/reinforcementlearning/comments/9yq6nk/schmidhubers_new_blog_post_on_unsupervised/
"""PlaNet: Learning Latent Dynamics for Planning from Pixels"", Hafner et al 2018 {GB}",1542670510,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9ymbt6/planet_learning_latent_dynamics_for_planning_from/
"""Natural Environment Benchmarks for Reinforcement Learning"", Zhang et al 2018 {FB} [more on brittleness of RL-learned policies]",1542656883,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9yk7ei/natural_environment_benchmarks_for_reinforcement/
Announcing the 2019 Uber AI Residency,1542614523,,reinforcementlearning,akshaynathr,False,/r/reinforcementlearning/comments/9yesi9/announcing_the_2019_uber_ai_residency/
"""Woulda, Coulda, Shoulda: Counterfactually-Guided Policy Search"", Buesing et al 2018 {DM}",1542561156,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9y7odk/woulda_coulda_shoulda_counterfactuallyguided/
"""Reward learning from human preferences and demonstrations in Atari"", Ibarz et al 2018 {DM/OA}",1542557975,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9y77sh/reward_learning_from_human_preferences_and/
[1811.01768v1] A Biologically Plausible Learning Rule for Deep Learning in the Brain (Q-AGREL),1542508642,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9y2gtl/181101768v1_a_biologically_plausible_learning/
Chances at a PhD in top Institutes like Berkeley,1542479907,"I just got my first paper ever accepted at nips 2018 workshop track. I am a Masters student and I worked alone on the paper.
I was wondering whether this would improve my chances at securing a PhD at top Institutes in the world especially Berkeley. 
Or there are other things I could still do increase my chances.
Thanks!",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/9xys21/chances_at_a_phd_in_top_institutes_like_berkeley/
Basic Policy gradients,1542452691,[removed],reinforcementlearning,KarthikMgk,False,/r/reinforcementlearning/comments/9xvl06/basic_policy_gradients/
[P] A library to organize experiments,1542420518,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/9xskqy/p_a_library_to_organize_experiments/
Question about Prioritized Replay implementation in an Actor-Critic model,1542416171,"I am stuck at evaluating the TD-error.

According to the original paper that looks at simple DDQN the error is:

`δ_j = R_j + γ_jQ_target (S_j , arg max_a Q(S_j , a)) − Q(S_j−1, A_j−1)`

But in my case I have the critic and the actor, and the actor has the main and the target models. So which Q-values do I use?

&amp;#x200B;

My current thought process is to just use the loss generated by the critic for that specific batch the model it's being trained on.",reinforcementlearning,steamingironkettle,False,/r/reinforcementlearning/comments/9xs291/question_about_prioritized_replay_implementation/
"A game environment with a moving target can still be stationary, right?",1542405176,"Let's say you have a game where your agent is in some enclosed space and gets rewarded for getting to a target, which it is given the position of. So, the state vector at any given time would be (agent position, target position).

However, the target changes position randomly every N seconds. Would this be a stationary environment?

My current understand is that it would be, because even though the target changes position, the distribution of states doesn't change, and the reward structure $R_as$ doesn't change either. Is that right?

I'm trying to think about what one should do with the learning rate for a problem like this -- if a game is stationary, then typically you want to decrease the LR over time, right?",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9xqmhi/a_game_environment_with_a_moving_target_can_still/
(REINFORCE) All probabilities becoming 1,1542381665,"I trying to replicate [this paper's](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/AAAI2018Denoising.pdf) results. The paper addresses the problem of Noise Reduction in Relation Extraction using Distant Supervision by removing noisy sentences from bags. The paper uses a Policy Gradients based approach(REINFORCE algorithm).

For some reason, when I try to train the model, the probability of selecting sentences becomes 1 (Attached screenshot). This happens after only a few steps (less than 1000 steps). I've followed the exact algorithm presented in the paper, same model architecture and hyperparameters. Also, looking at their code, I realized that they were sampling actions thrice for each episode and averaging the rewards which were then used as baseline. I've implemented this too.

Can someone help me figure out this weird behaviour?

https://i.redd.it/ib6ldcwqmpy11.png",reinforcementlearning,shubhamjha97,False,/r/reinforcementlearning/comments/9xn1y8/reinforce_all_probabilities_becoming_1/
PPO is a great algorithm but seems to lack consistency,1542364783,"After running PPO on several setting, environments and seeds, I am under the impression that whenever you have too few workers, PPO fails to learn and succeed in the environment. 

&amp;#x200B;

What are you thoughts ? ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/9xl3i1/ppo_is_a_great_algorithm_but_seems_to_lack/
Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?,1542341695,,reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9xirbo/are_deep_policy_gradient_algorithms_truly_policy/
"[R] Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control {UWashington, OpenAI}",1542256329,[https://arxiv.org/abs/1811.01848](https://arxiv.org/abs/1811.01848),reinforcementlearning,enigmatic_17,False,/r/reinforcementlearning/comments/9x7uva/r_plan_online_learn_offline_efficient_learning/
[D] Any thoughts on OpenAI's 'Key Papers in Deep RL' for Spinning Up?,1542239469,"Link to full list: http://spinningup.openai.com/en/latest/spinningup/keypapers.html
Any additional papers that you would recommend? Any on that list that you are surprised were included?",reinforcementlearning,AurelianTactics,False,/r/reinforcementlearning/comments/9x5myq/d_any_thoughts_on_openais_key_papers_in_deep_rl/
"""The Bayesian Superorganism I: collective probability estimation"", Hunt et al 2018",1542208563,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9x0y41/the_bayesian_superorganism_i_collective/
[D] Is Q learning unstable with multi-layer linear nets?,1542185633,"Linear Q learning is stable, but I am not sure what would happen if as an exercise linear layers would be stacked. Is it specifically nonlinearities that make it unstable or is it the nesting of layers?",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9wyeqv/d_is_q_learning_unstable_with_multilayer_linear/
Reinforcement Learning Master's Programs [Looking for Fall of 2020],1542175644,"Hey everyone,  


I'm wondering as to which graduate schools have really good Reinforcement Learning Master's Programs. I've gone through some of the papers published in well known conferences and have come up with this list of schools that have good RL programs:  


University of Alberta  
McGill University  
University of Massachusetts Amherst  
University of Michigan  
Duke University  
University of Texas at Austin  
University College London  


The above universities have researches who are well known in the RL field. If there's any other university which you think I've missed out, please do let me know. Looking to apply to these universities for a Master's degree in RL.",reinforcementlearning,vavantoo,False,/r/reinforcementlearning/comments/9wxfip/reinforcement_learning_masters_programs_looking/
"""PLCBC: Sample-Efficient Policy Learning based on Completely Behavior Cloning"", Zou et al 2018",1542164793,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9ww55v/plcbc_sampleefficient_policy_learning_based_on/
[1811.01132] VIREL: A Variational Inference Framework for Reinforcement Learning,1542160544,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9wvksu/181101132_virel_a_variational_inference_framework/
Profile of Karl Friston and free energy minimization,1542133617,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9wrmv1/profile_of_karl_friston_and_free_energy/
How are the states represented in Deepmind's model for playing Atari ?,1542131700,"I was wondering how the states are represented there since I don't think we can have a fixed representation for this kind of applications.  
I tried to read the [original paper](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf), but can't seem to get how the states are represented.   
I am kind of new to reinforcement learning, having started only back in September for the purpose of my final project, so my question may seem obvious to some, but I need a bit of help :)",reinforcementlearning,naifmeh,False,/r/reinforcementlearning/comments/9wrbvy/how_are_the_states_represented_in_deepminds_model/
"""IW-ES: Importance Weighted Evolution Strategies"", Campos et al 2018 [importance sampling for off-policy updates]",1542128166,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9wqrup/iwes_importance_weighted_evolution_strategies/
Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient,1542113919,"An AAAI 2019 paper,

4416: Robust Multi-Agent Reinforcement Learning via Minimax Deep Deterministic Policy Gradient
Shihui Li (Carnegie Mellon); Yi Wu (UC Berkeley)*; Xinyue Cui (Tsinghua University); Honghua Dong (Tsinghua University); Fei Fang (Carnegie Mellon); Stuart Russell (UC Berkeley)

Currently not available.",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9wou38/robust_multiagent_reinforcement_learning_via/
Combining RL and training simulators.,1542101976,"a Masters student in HCI here.

is there any work which combines RL and training simulators(Virtual training simulators). My first thought here was to think of policy learning as a framework for Gamifying the elements in the training simulator or a serious game.  Any leads/ thoughts on this, is highly appreciated. 

&amp;#x200B;",reinforcementlearning,truecriminal,False,/r/reinforcementlearning/comments/9wno4x/combining_rl_and_training_simulators/
Why don't we wrapping genesis emulator?,1542089482,"Hi.

When experiment RL(reinforcement learning) by gym, i found module \`nes-py\` that wrap NES emulator by python.

I expect to find pip that wrap genesis(MegaDriver) emulator too. But, I can't find it.

If somebody know that module, teach me. And any advice is ok :)",reinforcementlearning,keicoon15,False,/r/reinforcementlearning/comments/9wmi1h/why_dont_we_wrapping_genesis_emulator/
"""ViBe: Learning from Demonstration in the Wild"", Behbahani et al 2018 {Latent Logic} [curriculum learning w/GAIL]",1542083457,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9wltu7/vibe_learning_from_demonstration_in_the_wild/
"""Controllable Neural Story Generation via Reinforcement Learning"", Tambwekar et al 2018",1542075942,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9wkw9t/controllable_neural_story_generation_via/
"""Q-AGREL: A Biologically Plausible Learning Rule for Deep Learning in the Brain"", Pozzi et al 2018",1542069617,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9wk12r/qagrel_a_biologically_plausible_learning_rule_for/
"""The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale"", Kuznetsova et al 2018 {Google} [9.2m images, 30.1m tags, 15.4M bounding boxes on CC-BY Flickr photos]",1542064721,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9wjcq9/the_open_images_dataset_v4_unified_image/
Simple multi-agent environments in POMDP,1542056653,"Any recommendations on multi-agent co-operative environments, similar to say, Starcraft, but not so complex to benchmark algorithms?

I tried testing on the multi-agent-particle-env, the one tested on the MADDPG paper, but would like something more well suited for partially observable agents.",reinforcementlearning,FatherCannotYell,False,/r/reinforcementlearning/comments/9wi4gx/simple_multiagent_environments_in_pomdp/
[D] Opinions/views on successor representations?,1542042633,"I find this approach really interesting; ""caching"" the reward function and transition function instead of the usual values, positioning itself as sort of in between model-free and model-based RL. There's also the connections to place cells in the brain and its possible uses in exploration. However, it still doesn't seem as popular and well-known as the classic approaches. I'd like to hear what people think about this line of research.",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/9wfu2d/d_opinionsviews_on_successor_representations/
[R] Modular Architecture for StarCraft II with Deep Reinforcement Learning {Berkeley},1541975140,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9w84u3/r_modular_architecture_for_starcraft_ii_with_deep/
"""Optimization-based locomotion planning, estimation, and control design for the Atlas humanoid robot"", Kuindersma et al 2016",1541955972,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9w5ern/optimizationbased_locomotion_planning_estimation/
[D] In TD learning is there a way to have the network predict the scale of the reward and take advantage of that?,1541927444,"When applied to square error, [natural gradient](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;rep=rep1&amp;type=pdf) methods essentially have that scaling constant (see page 6/36) in front of them. Given that I want to do RL with NG, it seems sensible to imagine that the network itself should predict the scale of the reward and take advantage of that when doing updates. Surely it should be possible to do more than just predict the mean. Manual reward scaling is complete nonsense because even with NG it would act as a multiplier on the critic's learning rate.

Had any work been done towards that end?",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9w2gny/d_in_td_learning_is_there_a_way_to_have_the/
How would you approach an infinite grid with sub goals?,1541881179,"I was looking at a problem where there is an infinite grid(the agent can only see a small area around it) and the agent has to collect items as rewards(collecting is just being in the same cell) but it must first collect something to hold the item i.e. item A will give a high reward but before getting to item A it needs to collect item B to hold item A. Each item B can hold 1 of item A.

I was wondering if anyone knows of any work applicable here? I was looking at [value iteration networks](https://arxiv.org/abs/1602.02867).",reinforcementlearning,canthisgetanyharder,False,/r/reinforcementlearning/comments/9vxi6q/how_would_you_approach_an_infinite_grid_with_sub/
"""Model Mis-specification and Inverse Reinforcement Learning""",1541867607,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9vvn2i/model_misspecification_and_inverse_reinforcement/
[R] Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor,1541789738,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9vn6ct/r_soft_actorcritic_offpolicy_maximum_entropy_deep/
How to start in RL?,1541782177,"I don’t know where or how to start in Reinforcement learning, i didn’t find any useful courses so far, so How can i start in Reinforcement learning? ",reinforcementlearning,hazemessamm,False,/r/reinforcementlearning/comments/9vm1u9/how_to_start_in_rl/
Reproducing OpenAI Domain Randomization with Fetch robot (Gazebo+ROS),1541762505,"Hi,

we have achieved to reproduce the results of the amazing paper [Domain Randomization for Transferring Deep Neural Networks from Simulation to the Real World](https://arxiv.org/abs/1703.06907) made by many people of OpenAI lab. Here a couple of videos: [video1](https://www.youtube.com/watch?v=G1PeaPF2Wds), [video2](https://www.youtube.com/watch?v=5nKFLej7cGM)

We have used ROS and Gazebo simulation to reproduce the results and created a [ROSject](https://www.rosjects.com) that allows to anybody reproduce the same results off-the-shelf (let me know if interested on having it and I will share it with you).

The thing is that, at present, we only reproduced it on simulation. Since we have no real robot, we cannot test it in the real robot. We are looking for somebody with a Fetch robot that would like to partner with us and do the test. Anybody there?

Also, anybody wanting to have the ROSject with the whole code and tutorial, just let me know and I will share it with you.",reinforcementlearning,roboticist101,False,/r/reinforcementlearning/comments/9vjmx6/reproducing_openai_domain_randomization_with/
[R] Efficient Eligibility Traces for Deep Reinforcement Learning,1541753139,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9viv7r/r_efficient_eligibility_traces_for_deep/
Just lost my copy of Sutton and Barto in LaGuardia Terminal C,1541725456,"This is so sad. Alexa play despacito.

(if anyone sees it, feel free to keep it :( )",reinforcementlearning,whymauri,False,/r/reinforcementlearning/comments/9vfwxr/just_lost_my_copy_of_sutton_and_barto_in/
Dumb question: why are bootstrapping methods biased and MC methods not?,1541718813,"I keep reading that MC methods (i.e., where you wait until the end of an episode before doing any updates, and then update all the state-action pairs that were visited in that episode and update their values with sums of real collected rewards) are ""unbiased"", whereas bootstrapping methods like SARSA or Q-learning are ""biased"", because you update the values with approximations of the target Q value.

I think I kind of have an intuitive grasp of what they mean, correct me if I'm wrong here: Bootstrapping methods are passing back real collected rewards to update ""earlier"" Q values (over many episodes), but via having to update several Q values in between those states and terminal/later states. So if Q1 is updated by Q2 and the reward R12, and Q2 is updated by Q3 and the reward Q23, Q1 only gets the info about Q3 through Q2, which is definitely more indirect and prone to error.

On the other hand, in MC, every Q visited in an episode gets an actual sum of rewards that was experienced, so it only gets ""real"" info. So they usually say that MC has less bias but more variance.

The thing I'm confused about is, I get that stuff above, but I don't see why MC necessarily has to be less biased. When the agent is learning, it will make many poor choices at first, and luck (from eps-greediness, environment, etc) is a big factor as well. So it seems like sometimes MC will mess up, end an episode with some uncharacteristic reward sequence, and then update all the visited Q's with a value that seems ""wrong"". So why doesn't that count as bias too?

",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9vf1jo/dumb_question_why_are_bootstrapping_methods/
Spinning Up in Deep RL from OpenAI,1541698467,[https://blog.openai.com/spinning-up-in-deep-rl/](https://blog.openai.com/spinning-up-in-deep-rl/),reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9vc1p6/spinning_up_in_deep_rl_from_openai/
[D] Specification gaming examples in AI,1541693997,,reinforcementlearning,julian88888888,False,/r/reinforcementlearning/comments/9vbdo5/d_specification_gaming_examples_in_ai/
[R] Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning,1541652113,"[https://arxiv.org/abs/1811.00796](https://arxiv.org/abs/1811.00796)

&amp;#x200B;",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9v70sw/r_automated_theorem_proving_in_intuitionistic/
[R] Are Deep Policy Gradient Algorithms Truly Policy Gradient Algorithms?,1541648908,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9v6nid/r_are_deep_policy_gradient_algorithms_truly/
Monte carlo policy gradient based deep reinforcement learning with discounted rewards.(Lunar Lander V2 environment),1541647418,,reinforcementlearning,charan_1996,False,/r/reinforcementlearning/comments/9v6h26/monte_carlo_policy_gradient_based_deep/
[R] Zap Meets Momentum: Stochastic Approximation Algorithms with Optimal Convergence Rate,1541601345,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9uzxdj/r_zap_meets_momentum_stochastic_approximation/
Custom GridWorld Environment,1541590433,"I require a custom gridworld environment that has multiple sub goals, such as key collection before opening doors, as well as enemies. Both the gym, and the unity gridworlds lack these, although the unity one seems easier to modify. Do you know of any library that I can use to create custom gridworld environments or should I create everything from ground up?",reinforcementlearning,oyuncu13,False,/r/reinforcementlearning/comments/9uyntw/custom_gridworld_environment/
Does n-step return has bias reduction effect with no discount?,1541585907,"It is shown in Sutton's 2nd edition that n-step return with a discount &lt; 1 does have a bias reduction guarantee. 

If the discount = 1, **error reduction property of n-step returns** does not apply, do we still have any other reason to favor n-step return? ",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/9uy9oj/does_nstep_return_has_bias_reduction_effect_with/
Machine Learning Algorithm for Everyone,1541522245,"Guys, we are happy to share that we started creating Machine Learning Algorithms videos, in which we will explain each and every algorithm Have a look and give your valuable feedback to us, please.

[https://youtu.be/Zt83JnjD8zg](https://youtu.be/Zt83JnjD8zg)",reinforcementlearning,adarsh_adg,False,/r/reinforcementlearning/comments/9upvj4/machine_learning_algorithm_for_everyone/
Designing a correct reward function,1541519430,"I am a beginner in this field and I started with simple tabular Q learning approach. The task I started with was the vehicle1 (agent) should stop as soon as the Vehicle2(vehicle ahead) is within the safety distance in order to avoid a collision.

So I rewarded as follows,

&amp;#x200B;

if(collision ==1)

reward = -10;

elseif((Relative\_distance &lt;= (Vehicle2\_velocity \*3)) &amp;&amp; ( vehicle1\_velocity&lt;= Vehicle2\_velocity))

reward = 30 - ((Vehicle2\_velocity \*3)- Relative\_distance);

elseif((Relative\_distance &gt; (Vehicle2\_velocity \*3)) &amp;&amp; ( vehicle1\_velocity&gt; Vehicle2\_velocity )) 

reward = 0.01;

else

reward = -1;    

end

end

&amp;#x200B;

could any one please tell if this is the right way to design a reward function? I am a little confused with the reward function concept, because, in this case the agent gets all the information what to do, so eventually it will learn to brake and avoid collision, and the actual policy can be visualized just by looking at this reward function, so why do we need reinforcement learning in this case when we can directly use an algorithmic way to achieve the same output, isn't it just complicating a simple behavior? or my understanding of reward function designing is just wrong. Kindly help.

&amp;#x200B;",reinforcementlearning,Matlab_Begin,False,/r/reinforcementlearning/comments/9upg0p/designing_a_correct_reward_function/
Summary: Value Prediction Networks(VPN) – Arxiv Bytes – Medium,1541474609,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9ukrhl/summary_value_prediction_networksvpn_arxiv_bytes/
"""Automated Theorem Proving in Intuitionistic Propositional Logic by Deep Reinforcement Learning"", Kusumoto et la 2018 {PN} [graph NNs]",1541454798,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9ui12n/automated_theorem_proving_in_intuitionistic/
Help with implementing a paper,1541322176,"For the past few days, I've been trying to implement this paper and replicate the results of this [paper](https://www.microsoft.com/en-us/research/wp-content/uploads/2017/11/AAAI2018Denoising.pdf). The paper uses a standard REINFORCE algorithm, and uses two policy networks- a normal policy network and a target network. The actions are sampled according to the probabilities given by the target network, and the normal policy network is what's updated via backpropogation after each episode. After every epoch, the parameters of the target network are updated in the direction of the parameters of the normal policy network by a small amount.

When I implement this paper, after the first 300-400 episodes, the output probabilities of the policy network become 1 for all the states and then since the loss function uses log probabilities, the loss becomes nan and the policy network's weights become nan. Can someone tell me how this can be fixed?

Also, since REINFORCE is an on-policy algorithm, why are the actions being sampled according to a different policy than the one that is being updated?",reinforcementlearning,shubhamjha97,False,/r/reinforcementlearning/comments/9u2292/help_with_implementing_a_paper/
Which Policy Gradients works the best?,1541306127,"Hi All,

I'm relatively new to RL. Seeing that the standard policy gradient, even with optimal baselines, still yields not so good gradients even with high batch sizes due to high variance.

What are the best policy gradients (trpo, ppo?) that converge in the shortest # of timesteps/iterations given that everything else is held constant (like batch_size, learning rate, etc)?",reinforcementlearning,GodoftheDrow,False,/r/reinforcementlearning/comments/9u0w0s/which_policy_gradients_works_the_best/
Is openai gym really open source? The most important mujoco environment is not easy to install.. whats the work around?,1541269270,,reinforcementlearning,guruji93,False,/r/reinforcementlearning/comments/9twdm5/is_openai_gym_really_open_source_the_most/
Does having a reward function that punishes for time taken naturally encourage exploration?,1541180388,"Let me start off by saying that I'm specifically *not* talking about ""reward shaping"" here (where you ""lead"" the agent to a goal by giving it more rewards as it gets closer to a goal, when you really just want it to get to the goal).

In a few toy examples I've coded, I do something like give a reward of +1 if it reaches the goal, and -0.01 for each time step it takes otherwise. So taking Mountain Car as the example, it only gets the +1 once it reaches the top of the hill, and -0.01 otherwise.

I've found that it seems like this naturally encourages exploration, even if you use an entirely greedy policy. If the car just sits at the bottom of the valley, it begins racking up those small punishments, so any movement away from that gives access to higher Q values. 

Is this right? Is this a second motivation for giving a small punishment for each time step, aside from just trying to make it solve the problem faster?
",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9tma5g/does_having_a_reward_function_that_punishes_for/
"""Horizon: Facebook's Open Source Applied Reinforcement Learning Platform"", Gauci et al 2018 {FB} [production applications of DRL beyond advertising/hyperparameters: recommendations, notifications, video bitrate settings]",1541172865,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9tl56p/horizon_facebooks_open_source_applied/
"""SDRL: Interpretable and Data-efficient Deep Reinforcement LearningLeveraging Symbolic Planning"", Lyu et al 2018",1541172636,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9tl3z4/sdrl_interpretable_and_dataefficient_deep/
MAMEToolkit: Python wrapper around MAME for RL agents playing arcade games (Street Fighter III demo),1541171513,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9tky5c/mametoolkit_python_wrapper_around_mame_for_rl/
Have anyone solved BipedalWalkerHardcore ?,1541168807,"Hi,

Have anyone solved   [BipedalWalkerHardcore](https://gym.openai.com/envs/BipedalWalkerHardcore-v2) env yet?

Do you know which algorithms is best for this environments? How big should be network? How long should training take?  How to choose hyper parameters? I am trying to solve it with PPO and it's not going well...

I will be grateful for any help!",reinforcementlearning,WhichPressure,False,/r/reinforcementlearning/comments/9tkk52/have_anyone_solved_bipedalwalkerhardcore/
[R] Learning to Dress: Synthesizing Human Dressing Motion via Deep Reinforcement Learning,1541123311,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/9tfttn/r_learning_to_dress_synthesizing_human_dressing/
Arcade Game Reinforcement Learning Python Library,1541114707,"I created a Python library as part of my final year dissertation which enables you to train RL algorithms against any arcade game supported by [MAME](https://www.mamedev.org/).

I have just gotten around to setting it up as a proper [github repository](https://github.com/M-J-Murray/MAMEToolkit). I thought it could be useful to others doing research in Reinforcement Learning. It works similar to OpenAI's gym, but it should be able to support more complex games.

The library itself doesn't actually come with any games, only an example implementation of Street Fighter III Third Strike: Fight for the Future. I didn't include any ROM's just in case there could be any legal issues. However, the repository contains all the information you need to get starting implementing your own games.

If you do manage to implement any games and would like to share it with the community, just contact me and I'll be happy to add it to the repository.

If you have any further questions about implementing your own games, or find any bugs, or have any advice, I'd be more happy to help.",reinforcementlearning,M-J-Murray,False,/r/reinforcementlearning/comments/9teov1/arcade_game_reinforcement_learning_python_library/
Horizon: Facebook’s Open Source Applied Reinforcement Learning Platform,1541112365,,reinforcementlearning,last_useful_man,False,/r/reinforcementlearning/comments/9teddl/horizon_facebooks_open_source_applied/
Papers with concatenating input observation with noise vector for exploration,1541089481,"I think I've seen/heard of this kind of approach somewhere, but I can't recall the exact paper(s). Does anyone know?

(I'm not talking about adding noise to the weights or action output)",reinforcementlearning,seann999,False,/r/reinforcementlearning/comments/9taxvf/papers_with_concatenating_input_observation_with/
"""Differentiable MPC for End-to-end Planning and Control"", Amos et al 2018",1541085532,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9tac6j/differentiable_mpc_for_endtoend_planning_and/
Toy environment representing hard exploration?,1541085059,"Hello! With all these new works on exploration methods, I wanted to know if there are some toy environments that require directed exploration to solve. (The Atari environments are too ""expensive"" for me to test my 5-min ideas on) I can think of simple tabular representations like a maze, but I want something with higher dimensional observation space. Any suggestions?

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/9ta9jn/toy_environment_representing_hard_exploration/
A Gameboy Supercomputer,1541042090,,reinforcementlearning,kmrocki,False,/r/reinforcementlearning/comments/9t5ww6/a_gameboy_supercomputer/
What exactly was the Deepmind DQN improvement over Neural Fitted Q iteration?,1541029309,"I was reading through [the big Deepmind DQN/Atari paper](http://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) again, because I generally understand the concept, but wanted to remember what exactly the novel breakthrough was. 

Interestingly, they mention that people used Q networks before, citing [this paper](http://ml.informatik.uni-freiburg.de/former/_media/publications/rieecml05.pdf), but say:

&gt;While other stable methods exist for training neural networks in the reinforcement learning setting,such as neural fitted Q-iteration , these methods involve the repeated training of networks de novo on hundreds of iterations. Consequently, these methods, unlike our algorithm, are too inefficient to be used successfully with large neural networks.

So I looked at that paper. They show the main algorithm in Fig 1. The differences I see between NFQ and the Deepmind paper are: 1) NFQ calculates the TD error using the same Q network for the ""current"" and target Q values, whereas the Deepmind uses the ""frozen"" network technique, and 2) NFQ uses all the samples it can, every time, whereas Deepmind just uses minibatches.

I don't know what the DM paper meant by ""these methods involve repeated training of networks de novo on hundreds of iterations"" means, though... That sounds like it could just as easily describe the DM DQN's, right?

One other difference I just noticed in the cartpole part is that, if I'm reading this correctly:

&gt;Transition samples were generated by starting the pole in an upright position and then applying random control signals until failure.

it seems like the NFQ paper is saying that the experiences were generated by random actions, not by using an eps-greedy policy based on the Q values being learned, like the DM paper did.

Anyway, I'd love some clarification if anyone can tell me. The DM paper is really impressive, but I don't quite understand what they mean about the NFQ method being too inefficient.
",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9t49gv/what_exactly_was_the_deepmind_dqn_improvement/
"Deep Learning and Reinforcement Learning Summer School, Toronto 2018 - Video Lectures",1541010319,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9t1flm/deep_learning_and_reinforcement_learning_summer/
[D] Reinforcement Learning with Prediction-Based Rewards from OpenAI,1541008186,"Papers mentioned:

[https://arxiv.org/abs/1810.12894](https://arxiv.org/abs/1810.12894)

[https://arxiv.org/pdf/1808.04355](https://arxiv.org/pdf/1808.04355)

Blog:

[https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/](https://blog.openai.com/reinforcement-learning-with-prediction-based-rewards/)",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9t13qx/d_reinforcement_learning_with_predictionbased/
"""Contingency-Aware Exploration in Reinforcement Learning"", Anonymous 2018 [also good results on _Montezuma's Revenge_]",1541003303,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9t0cd3/contingencyaware_exploration_in_reinforcement/
RL Control Optimal Policy Stability,1540982601,"I am using an A2C agent (Baseline's implementation) to estimate the optimal policy on a subset of a problem. In essence, I am able to achieve the optimal policy to that subset using Dynamic Programming, which yields a fairly stable policy, meaning, one where each selected action resembles - to a certain extent - that of the previous period's. This is different when certain states are achieved, which I will explain. 

However, the RL agent just spits out absolutely crazy and hectic policy! One day it selects the minimum action, and the next day the maximum (example), in a clearly non-long term planning fashion. Is this normal? Should I simply iterate more and more to hopefully, achieve stability?

&amp;#x200B;

The environment is as follows:

observation = \[time, errors\]  
action = actiont

The observation is comprised of time and errors. Time is deterministic of course, which decays one day at a time, and the environment is reset when time reaches 0. Errors are simple: the action (in percent) dictates the probability which the environment can record an error at the time-step, or the environment to reset early with a very negative reward, or proceed normally - which are basically normal distribution probabilities. Actions go from 0 to 3 (3k actions with a 0.001 step).

When the env reaches 0, a reward is given based on the number of errors (if &lt;4 its okay, if more, then each unit further increases the negative reward).

&amp;#x200B;

Am I doing something wrong or it this simply RL in a nutshell?",reinforcementlearning,Kalooca,False,/r/reinforcementlearning/comments/9sxk58/rl_control_optimal_policy_stability/
Reinforcement learning for humanoid robot locomotion,1540979436,"Hi, I would like to try teaching a humanoid robot to walk, but I'm very new to reinforcement learning and machine learning in general. Hence, I would like some advice on the techniques I should use to train the robot.
From the tutorials I've read, it appears that deep Q-learning is impractical for this case due to the large number of actions that can be taken by the robot at any instant. It seems like policy gradients could work, but may I ask if anyone has an opinion on the matter? Thanks in advance for the help guys.",reinforcementlearning,techman007,False,/r/reinforcementlearning/comments/9sx9h1/reinforcement_learning_for_humanoid_robot/
"Mobile app development Plano, TX ?",1540967002,,reinforcementlearning,kairosmeTX,False,/r/reinforcementlearning/comments/9sw8ml/mobile_app_development_plano_tx/
What is the state of the art for mujoco continuous control tasks?,1540965442,I'm looking to benchmark a project of mine and it seems to me that state of the art is Mujoco but I'd just like to double check in case I have missed something,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9sw3m0/what_is_the_state_of_the_art_for_mujoco/
"""Introducing AdaNet: Fast and Flexible AutoML with Learning Guarantees"": TensorFlow-based hyperparameter optimization framework {Google}",1540929918,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sriqe/introducing_adanet_fast_and_flexible_automl_with/
Network architectures used for Atari in PPO paper?,1540924276,"Im trying to reproduce the PPO algorithm exactly as it is was used for the Atari benchmarks in the paper. But i have trouble figuring out the exact network architectures used when producing the results in table 6. (link to paper below).

The paper does not mention the architecture of the critic at all - is it just implicit some ""standard"" critic that you would know if you have been in the field for some time? 

The paper does however mention something about the actor network in section 6.4: ""For all three algorithms, we used the same policy network architecture as used in \[Mni+16\]."" (A3C paper).

Now, the A3C paper section 8 states: ""The agents used the network architecture from (Mnih et al., 2013)."" (2013 DQN paper). Do they mean for both actor and critic? Futhermore it states: ""The model used by actor-critic agents had two set of outputs – a softmax output with one entry per action representing the probability of selecting the action, and a single linear output representing the value function."" I read this as the actor and critic is the same network except for the output layer?

&amp;#x200B;

TLDR; I cant figure out the exact architectures of the actor critic PPO implementation for atari.

&amp;#x200B;

PPO paper: [https://arxiv.org/pdf/1707.06347.pdf](https://arxiv.org/pdf/1707.06347.pdf)

A3C paper: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)",reinforcementlearning,AppelsinJuice32,False,/r/reinforcementlearning/comments/9sqnz3/network_architectures_used_for_atari_in_ppo_paper/
"""Model-Based Active Exploration"", Shyam et al 2018 {NNAISENSE}",1540916294,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9spfld/modelbased_active_exploration_shyam_et_al_2018/
"""Quality Diversity Through Surprise"", Gravina et al 2018",1540915693,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9spcai/quality_diversity_through_surprise_gravina_et_al/
Help with exercise,1540915257,"I'm currently stuck in one of the exercises in the 2nd edition intro to RL book: [https://imgur.com/a/PtByEjy](https://imgur.com/a/PtByEjy)

I worked according to the hint in the book which says: Write the RE as an expectation over possible states s of the expectation of the squared error given that St = s. Then add and substract the true value of state s from the error(before squaring), grouping the substracted true value with the return and the added true value  with the estimated value. Then if you expand the square the **most complex term will turn out to be zero**.

I worked according to those instructions but I really don't get why the complex expression is going to turn out as zero, would love to get an explanation.

&amp;#x200B;

Thanks in advance.

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,liormoshe,False,/r/reinforcementlearning/comments/9sp9x7/help_with_exercise/
[R] Multi-Agent Common Knowledge Reinforcement Learning,1540911713,[https://arxiv.org/pdf/1810.11702.pdf](https://arxiv.org/pdf/1810.11702.pdf),reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9soq2u/r_multiagent_common_knowledge_reinforcement/
RL agents discovering the Inverse Kinematics ?,1540894451,"Hello ! 

&amp;#x200B;

I'm training RL agents using PPO for various [manipulation tasks](https://www.youtube.com/watch?v=hAxGpOzFCGE). Clearly, this task is achivable using analytical methods, such as the inverse Jacobian. Using RL, the agents usually succeed but I was wondering whether there was any connection, link, relation with the analytical methods ? 

I mean, given that the agent's solutions is expressed in a space with as many dimensions as it has weights, it seems hard to believe that its model related to theory-grounded, physically-guided solutions such as the inverse jacobian, right ? 

&amp;#x200B;

What do you think  ? 

Thanks a lot ! ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/9smipc/rl_agents_discovering_the_inverse_kinematics/
Openai summer internship,1540884554,"So I had applied for the openai summer internships 2019 and I got a rejection yesterday. I was wondering what does someone have to do to get an acceptance in such programs.

I have been working on maintaining a pytorch repository with Reinforcement Learning algorithms implemented. (Link Down below)
I am also working on some research dealing with intrinsic rewards using Empowerment (calculated using mutual information estimation). More information in the paper below.

Github Profile: https://github.com/navneet-nmk
Pytorch-RL: https://github.com/navneet-nmk/pytorch-rl
My Website: https://navneet-nmk.github.io/
Preprint of the paper I am working on: https://arxiv.org/pdf/1810.05533.pdf

Any suggestions are welcome!",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/9slp0h/openai_summer_internship/
Reward function in infinite state space,1540877685,"In several implementations of RL continuous tasks, I've seen reward functions in one part using something like (z-z_des)/z_max. I guess that is fine if you know the boundaries of your environment because you would know the z_max value, but what happens if you don't know those boundaries? How the reward function would suppose to look like then?",reinforcementlearning,page47250,False,/r/reinforcementlearning/comments/9sl3vo/reward_function_in_infinite_state_space/
"""RLgraph: Flexible Computation Graphs for Deep Reinforcement Learning"", Schaarschmidt et al 2018",1540865275,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sjq5q/rlgraph_flexible_computation_graphs_for_deep/
"""Hierarchical Deep Multiagent Reinforcement Learning"", Tang et al 2018",1540864897,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sjoe7/hierarchical_deep_multiagent_reinforcement/
"""Deep Imitative Models for Flexible Inference, Planning, and Control"", Rhinehart et al 2018",1540864829,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sjo1j/deep_imitative_models_for_flexible_inference/
"""Intrinsic Social Motivation via Causal Influence in Multi-Agent RL"", Jaques et al 2018",1540863904,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sjjv1/intrinsic_social_motivation_via_causal_influence/
"""TarMAC: Targeted Multi-Agent Communication"", Das et al 2018",1540855650,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sigsc/tarmac_targeted_multiagent_communication_das_et/
What environments to report on?,1540849227,"I am currently doing some work to improve the sample efficiency of the general class of policy gradients. Unfortunately I am very new to the field (It is incredibly exciting, yet head scratching!) and I am not sure what is the normal test suite when showing experimental results on incremental improvement of the algorithms.

I am familiar with the openAI baselines and the atari test suite, however, are there any other standards? Iterating over the pixel data takes a lot of time and I was wondering if there is middle ground above the toy examples and below learning from pixel. Do people report on learning from RAM?

Thank you very much for the insight.",reinforcementlearning,LupusPrudens,False,/r/reinforcementlearning/comments/9shkgj/what_environments_to_report_on/
What are your best tips for debugging RL problems?,1540846761,"I've done a few RL toy problems, but I'm still pretty new to the field. In each of the problems I've done, there has been some point where it seems like I've implemented everything correctly, the environment is working correctly, etc, but it's still just not working, or is, with some really strange problem.

RL seems to be harder to debug than any other type of programming I've done before. There's an element of randomness usually. It often takes a while (in the run) for the problem to manifest, so it's hard to pinpoint exactly *where* something is going wrong. Lastly, stuff just takes a while to even run, so my ""attempt solution/code/evaluate"" loop takes a long time, which makes it even harder.

Does anyone have any tips? The things I've figured out so far are to log everything feasible, and to try to isolate things to find the problem, but those are pretty general tips. I've found some help a few times from reading relevant papers, but that's rarer.

Do any experts in the field have any tips?",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9sh77q/what_are_your_best_tips_for_debugging_rl_problems/
"""One-Shot Hierarchical Imitation Learning of Compound Visuomotor Tasks"", Yu et al 2018 {BAIR}",1540837951,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9sfucu/oneshot_hierarchical_imitation_learning_of/
What are the major recent papers in LSTD (Least Squares Temporal Difference Learning)? Is there any work that relates LSTD to graphical models?,1540834514,I am looking into LSTD literature but as a newbie confused on what to read first. What should I read to get overall current state of the LSTD approaches. Also is there any paper that explores connection between LSTD and graphical model based approacehs? ,reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/9sf9mu/what_are_the_major_recent_papers_in_lstd_least/
"Reinforcement Materials Market Expected to Reach $22,826 Million, Globally, by 2022",1540810555,[removed],reinforcementlearning,srushtih,False,/r/reinforcementlearning/comments/9sc1j5/reinforcement_materials_market_expected_to_reach/
[D] Questions about PPO algorithm,1540795455,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9sas32/d_questions_about_ppo_algorithm/
[R] Neural MMO: A Massively Multiplayer Game Environment For Intelligent Agents,1540771627,,reinforcementlearning,AurelianTactics,False,/r/reinforcementlearning/comments/9s86d4/r_neural_mmo_a_massively_multiplayer_game/
"""DGN: Graph Convolutional Reinforcement Learning for Multi-Agent Cooperation"", Jiang et al 2018",1540737650,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9s3m5x/dgn_graph_convolutional_reinforcement_learning/
Dynamic Programming in Reinforcement Learning,1540725076,,reinforcementlearning,vector_machines,False,/r/reinforcementlearning/comments/9s2fhh/dynamic_programming_in_reinforcement_learning/
Projected Bellman error in linear value function approximation is non-convex?,1540707469,"This is in reference to Sutton's Reinforcement Introduction book (second edition). Chapter ""Off-policy methods with approximation"" &gt; ""Linera value-function geometry"".

**Linear value function space** is a smaller **subspace** of the true value function space.

**Value error projection** (squared distance) seems to be convex. This is well-known I think.

Sutton suggests that minimimum **projected Bellman error** is not achievable via iterative process. To me that means the geometry of this projected Bellman error is not convex, having many local minima.

*Note: Bellman error is actually the expected TD error, the direction you should take to get closer to the true value function.* 

***Projected*** *Bellman error is presented bacause in the case of function approximation you cannot optimize exactly to that becasue approximation function lives in a subspace. Even though the direction is right, but you will not actually get there. You will get only closest to there on your subspace.*

That seems questionable that such a thing would happen in the linear function approxmiation case. Or, it is the case that linear function under this projected Bellman error is indeed non-convex.

I ask here for a confirmation if that is the case.

&amp;#x200B;

&amp;#x200B;

[This is from Sutton's book page 217 \(second edition, final\), Is it illegal to upload this by the way?](https://i.redd.it/ko8zrosncvu11.png)",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/9s16yc/projected_bellman_error_in_linear_value_function/
"[R] ""BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop"" {UM} [Find that RL methods are insufficient, 3+ orders of magnitude improvement required]",1540705882,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9s12i3/r_babyai_first_steps_towards_grounded_language/
Training RL models in continuous action space using OpenAI Gym. How long should this take?,1540671850,"Hi,

I'm testing out some toy problems and need some environments to train on quickly. For discrete environments on here you can generally see some reasonable performance increase with a relatively high learning rate after a few hundred iterations. Does anyone have any experience or idea how long it takes on average to train on the continuous control or lunar landing environments? 

My estimates are telling me that it'll take about 24 hours at least with the model I'm running which seems a bit much given there's just 2 dimensional output.

Thanks!",reinforcementlearning,hobbesfanclub,False,/r/reinforcementlearning/comments/9rxfcx/training_rl_models_in_continuous_action_space/
[R] Optimizing Agent Behavior over Long Time Scales by Transporting Value,1540616782,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/9rrt97/r_optimizing_agent_behavior_over_long_time_scales/
"""Learning Representations in Model-Free Hierarchical Reinforcement Learning"", Rafati &amp; Noelle 2018",1540604736,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9rqmp7/learning_representations_in_modelfree/
What is correct definition of the true Q-value of the policy?,1540598975,"I'm looking through [GAE paper](https://arxiv.org/abs/1506.02438) and can't convince myself that formula 2 is correct.

&amp;#x200B;

The formula in the paper says:

`Q^{pi}(s_t,a_t) = \mathbb{E}_{s_{t+1:\infty}, a_{t+1:\infty}} \sum_{l=0}^{\infty}r_{t+l}`

While I think it should be:

`Q^{pi}(s_t,a_t) = \mathbb{E}_{s_{t:\infty}, a_{t+1:\infty}} \sum_{l=0}^{\infty}r_{t+l}`

&amp;#x200B;

In other words, my understanding of Q-value for a policy is that it's the sum of rewards, where:

* **all** rewards are expected over environment randomness
* **only following** rewards are expected over policy randomness

While the paper implies that Q-value for a policy is the sum of rewards where:

* **only following** rewards are expected over environment randomness
* **only following** rewards are expected over policy randomness
* the first reward isn't expected over any randomness

Can anyone help me understand this? Thanks!",reinforcementlearning,spring_stream,False,/r/reinforcementlearning/comments/9rq04z/what_is_correct_definition_of_the_true_qvalue_of/
"""Less competitive"" graduate programs",1540591167,"Hi, I'm looking into graduate programs with reinforcement learning (my specific interest is towards more general AI, so exploration, model learning, continual learning, hierarchical RL, etc), but as many know top universities like Stanford, CMU, or UCB are very competitive, probably more so nowadays. Does anyone know any ""low-key"" research labs/graduate programs doing any of the aforementioned topics?",reinforcementlearning,ta719619,False,/r/reinforcementlearning/comments/9rp1zo/less_competitive_graduate_programs/
What is continuous-state in RL,1540524433,"Hi, there, what is continuous-state in RL. Does it mean the space is continuous? For example, a state space `s_t` has 6 dimensions, and the value of each dimension is continuous. So, the state space is continuous. Is it right?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9rh7e8/what_is_continuousstate_in_rl/
[1810.02541v2] PPO-CMA: Proximal Policy Optimization with Covariance Matrix Adaptation,1540524056,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9rh5wa/181002541v2_ppocma_proximal_policy_optimization/
"""Learned optimizers that outperform SGD on wall-clock and validation loss"", Metz et al 2018 {GB}",1540483066,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9rbknq/learned_optimizers_that_outperform_sgd_on/
"""Episodic Curiosity through Reachability"", Savinov et al 2018 {GB/DM} [avoiding entropy traps of prediction error by distance measure to recent observations]",1540411901,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9r38oq/episodic_curiosity_through_reachability_savinov/
Personalized vitamins in bone health,1540341433,[removed],reinforcementlearning,Bin1SuraT,False,/r/reinforcementlearning/comments/9quv7k/personalized_vitamins_in_bone_health/
Question about transfer learning in deep RL,1540331666,"I’m currently working on a project where for a small state space, there exist a numerical scheme to find the optimal policy. The issue is that this numerical solution comes at the expense of imposing some constraints on the state-space dimensions not to fall in the curse of dimensionality. 

I would like to add a few states (2-3 add. states) to make my model more realistic and use deep RL to solve it. My plan is for my deep agent to first learn from the optimal agent in the initial state space (smaller one). This would be a classification task (only supervised learning) where the deep agent needs to match the optimal agent's actions. 

Then, I can increase the state-space dimension and adapt my deep agent by changing the dimension of the first layer of the Nnet. I believe that this method would be much more efficient in terms of computation compared to starting from nothing (random initial policy). 

I’m not really familiar with the transfer learning literature with deep RL. Does anyone have some references that I can look up for my type of problem? I’m sure many papers have already been published with a similar framework as mine. I’m particularly interested in how to train my deep agent after the supervised learning step: should I fix the parameters of every layer except for the first one? Should I start by fixing most of the layers and gradually start retraining all of them?

Thank you very much for your help. ",reinforcementlearning,ranirlol,False,/r/reinforcementlearning/comments/9qtjy8/question_about_transfer_learning_in_deep_rl/
Summary: Proximal Policy Optimization(PPO) – Arxiv Bytes – Medium,1540278675,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9qmk4n/summary_proximal_policy_optimizationppo_arxiv/
"""Optimization of Molecules via Deep Reinforcement Learning"", Zhou et al 2018",1540266610,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9qlafo/optimization_of_molecules_via_deep_reinforcement/
"""Fluid Annotation: An Exploratory Machine Learning–Powered Interface for Faster Image Annotation"" {G}",1540241069,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9qht9k/fluid_annotation_an_exploratory_machine/
"""Learning Complex Goals with Iterated Amplification"" {OA} [""Supervising strong learners by amplifying weak experts"", Christiano et al 2018]",1540234670,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9qgtqj/learning_complex_goals_with_iterated/
"""Evolution"" by Keiwan is an awesome physics simulation web-app, check it out! (link in comments)",1540223763,,reinforcementlearning,julian88888888,False,/r/reinforcementlearning/comments/9qf3xe/evolution_by_keiwan_is_an_awesome_physics/
"""ProMP: Proximal Meta-Policy Search"", Rothfuss et al 2018",1540146017,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9q5s1h/promp_proximal_metapolicy_search_rothfuss_et_al/
"Official Python3 TF implementation of ""ProMP: Proximal Meta-Policy Search"" [extends MAML, E-MAML, DiCE]",1540145965,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9q5rt9/official_python3_tf_implementation_of_promp/
Proximal Policy Optimization with experience replay,1540143958,"Hi!

I was studying the Berkeley reinforcement learning classes, and it's explained that any policy optimization algorithms need to do sample weighting if they are going to use experience replay.

Thing is, the frameworks I've seen (Tensorforce and Nervana Coach) use experience replay normally, and it doesn't look like they are weighted. At least I didn't find where they do it, if they do.

I also didn't find any mention of experience replay in the original paper.

Did I misunderstood something? Should I disable experience replay?

I also have an issue of PPO forgetting things on all frameworks I test. It doesn't matter the size of the network, so it doesn't look like it's an issue of the capacity of the network. Maybe it's because the experience replay is indeed buggy? But if I disable it, then wouldn't it unlearn old things anyway (since the batch will not be representative of the dataset anymore)? It sounds like I'm missing something.

Does anyone have an idea? Thanks!",reinforcementlearning,Sohakes,False,/r/reinforcementlearning/comments/9q5hxf/proximal_policy_optimization_with_experience/
This is a blog explaining the behind the scenes of Q learning.(how agent is exactly learning about the Taxi V2 environment). Please give your feedback .I am a starter so please correct my mistakes.,1540140990,,reinforcementlearning,charan_1996,False,/r/reinforcementlearning/comments/9q5327/this_is_a_blog_explaining_the_behind_the_scenes/
WBE and DRL: a Middle Way of imitation learning from the human brain,1540063325,"Most deep learning methods attempt to learn artificial neural networks from scratch, using architectures or neurons or approaches often only very loosely inspired by biological brains; on the other hand, most discussions of ['whole brain emulation'](https://www.fhi.ox.ac.uk/brain-emulation-roadmap-report.pdf ""'Whole brain emulation: a roadmap', Sandberg &amp; Bostrom 2008"") assume that one will have to learn every or almost every neuron in large regions of or the entire brain from a specific person, and the debate is mostly about how realistic (and computationally demanding) those neurons must be before it yields a useful AGI or an 'upload' of that person.

Highlighted by /u/starspawn0 a year ago ([""A possible unexpected path to strong A.I. (AGI)""](https://www.reddit.com/r/thisisthewayitwillbe/comments/59lu26/a_possible_unexpected_path_to_strong_ai_agi/)), there's an interesting vein of research which takes the middle way of treating DL/biological brains as [a kind of imitation learning](https://www.reddit.com/r/reinforcementlearning/search?q=flair%3AI&amp;restrict_sr=on), where human brain activity such as fMRI, EEG, or eyetracking, is taken as being itself as being some kind of rich dataset or oracle to learn better algorithms from, to learn to imitate, or [meta](https://www.reddit.com/r/reinforcementlearning/search?q=flair%3AMetaRL&amp;restrict_sr=on)-learn new architectures which then train to something *similar* to the human brain:

- [""Interpretable Semantic Vectors from a Joint Model of Brain- and Text-Based Meaning""](https://www.cs.cmu.edu/%7Eafyshe/papers/acl2014/jnnse_acl2014.pdf), Fyshe et al 2014
- ["" Improving sentence compression by learning to predict gaze""](https://arxiv.org/abs/1604.03357), Klerke et al 2016; [""Gaze-guided Image Classification for Reflecting Perceptual Class Ambiguity""](https://dl.acm.org/citation.cfm?id=3266090), Ishibashi et al 2018 
- [""Exploring Semantic Representation in Brain Activity Using Word Embeddings""](http://www.aclweb.org/anthology/D16-1064), Ruan et al 2016
- [""Deep Learning Human Mind for Automated Visual Classification""](https://arxiv.org/abs/1609.00344), Spampinato et al 2016
- ["" Mapping Between fMRI Responses to Movies and their Natural Language Annotations""](https://arxiv.org/abs/1610.03914), Vodrahalli et al 2016
- [""Using Human Brain Activity to Guide Machine Learning""](https://arxiv.org/abs/1703.05463), Fong et al 2017
- [""Towards Deep Modeling of Music Semantics using EEG Regularizers""](https://arxiv.org/abs/1712.05197), Raposo et al 2017
- [""Deep reinforcement learning from human preferences""](http://papers.nips.cc/paper/7017-deep-reinforcement-learning-from-human-preferences), Christiano et al 2017; ["" Brain Responses During Robot-Error Observation""](https://arxiv.org/abs/1708.01465), Welke et al 2017; ""[ The signature of robot action success in EEG signals of a human observer: Decoding and visualization using deep convolutional neural networks""](https://arxiv.org/abs/1711.06068), Behncke et al 2017

- [""Predicting Driver Attention in Critical Situations""](https://arxiv.org/abs/1711.06406), Xia et al 2017
- [""Using Human Brain Activity to Guide Machine Learning""](https://arxiv.org/abs/1703.05463), Fong et al 2017
- [""Neural Encoding and Decoding with Deep Learning for Dynamic Natural Vision""](https://academic.oup.com/cercor/advance-article/doi/10.1093/cercor/bhx268/4560155), Wen et al 2018 
- [""Visceral Machines: Reinforcement Learning with Intrinsic Rewards that Mimic the Human Nervous System""](https://arxiv.org/abs/1805.09975), McDuff &amp; Kapoor 2018 
- [""A Neurobiological Cross-domain Evaluation Metric for Predictive Coding Networks""](https://arxiv.org/abs/1805.10726), Blanchard et al 2018 (see also [""Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks""](https://www.biorxiv.org/content/early/2018/02/12/240614), Rajalingham et al 2018/[""Taking a machine's perspective: Human deciphering of adversarial images""](https://arxiv.org/abs/1809.04120), Zhou &amp; Firestone 2018)

Human preferences/brain activations are themselves the reward, or the distance between neural activations for a pair of images represents their semantic distance and a classification CNN is penalized accordingly, or the activation statistics become a target in hyperparameter optimization/neural architecture search ('look for a CNN architecture which when trained in *this* dataset produces activations with similar distributions as *that* set of human brain recordings looking at said dataset'), and so on.

Given steady progress in brain imaging technology, the extent of recorded human brain activity will escalate and more and more data will become available to imitate/optimize based on. As human brain architecture must be fairly generic, learning to imitate data from many different brains may usefully reverse-engineer architectures. Given the already demonstrated power &amp; efficiency of DL without any such help, and the compute requirement of even optimistic WBE estimates, it seems quite plausible that a DL learning to imitate (but not actually copying or 'emulating' in any sense) a human brain could achieve AGI long before any WBE does (which must struggle with the major logistics challenge of scanning a brain in any way and then computing it), and it might be worth thinking about this kind of approach more.",reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9pwy2f/wbe_and_drl_a_middle_way_of_imitation_learning/
Blockchain and AI - Possible Synergy?,1540056567,,reinforcementlearning,lepton99,False,/r/reinforcementlearning/comments/9pvz40/blockchain_and_ai_possible_synergy/
Need help in UML Diagrams for RL Project,1539960412,"For our undergrad final year project, I am building an AI agent that would use RL to gather objects in a warehouse environment in ViZDoom. For our reports, we need to draw all the UML diagrams. I have no idea where to start with these (also couldn't find any stuff on Google), so It'd be great if someone can share some resources of an RL project's UML diagrams, so I can refer those to make mine. Thanks in advance!",reinforcementlearning,yculcarnee,False,/r/reinforcementlearning/comments/9pkhmz/need_help_in_uml_diagrams_for_rl_project/
How to evaluate self play agent performance during training?,1539916652,"When playing zero sum games, all agent will end up with zero sum return. How to evaluate the agent's performance during training and validate whether the agent have learned some valuable skills?",reinforcementlearning,BoscoTsang,False,/r/reinforcementlearning/comments/9pfrs8/how_to_evaluate_self_play_agent_performance/
DeepMind Open-Sources RL Library TRFL,1539900273,,reinforcementlearning,trcytony,False,/r/reinforcementlearning/comments/9pdr7v/deepmind_opensources_rl_library_trfl/
Berkeley AI Research Open-Sources DeepMimic,1539898903,,reinforcementlearning,gwen0927,False,/r/reinforcementlearning/comments/9pdkh3/berkeley_ai_research_opensources_deepmimic/
CFP: AAAI Workshop on Reinforcement Learning in Games,1539886136,"Date: January 27th or 28th, 2019  
Location: Honolulu, Hawaii  
Submission deadline: November 5th, 2018  
Web site: [http://aaai-rlg.mlanctot.info/](http://aaai-rlg.mlanctot.info/)  
 

**Description**  
 

Games provide an abstract and formal model of environments in which  multiple agents interact: each player has a well-defined goal and rules  to describe the effects of interactions among the players. The first  achievements in playing these games at super-human level were attained  with methods that relied on and exploited domain expertise that was  designed manually (e.g. chess, checkers). In recent years, we have seen  examples of general approaches that learn to play these games via  self-play reinforcement learning (RL), as first demonstrated in  Backgammon. While progress has been impressive, we believe we have just  scratched the surface of what is capable, and much work remains to be  done in order to truly understand the algorithms and learning processes  within these environments.  
 

The main objective of the workshop is to bring researchers together to  discuss ideas, preliminary results, and ongoing research in the field of  reinforcement in games.   
 

We invite participants to submit papers, based on but not limited to, the following topics: 

* RL in one-shot games
* RL in turn-based and Markov games
* RL in partially-observable games
* RL in continuous games
* RL in cooperative games
* Deep RL in games
* Combining search and RL in games
* Inverse RL in games
* Foundations, theory, and game-theoretic algorithms for RL
* Opponent / teammate modeling
* Ad-hoc teamwork
* Analyses of learning dynamics in games
* Evolutionary methods for RL in games
* RL in games without the rules 

**Structure and Submission Instructions**  
 

RLG is a 1 full-day workshop. It will start a 60 minute mini-tutorial  covering a brief tour of the history and basics of RL in games, 2-3  invited talks by prominent contributors to the field, paper  presentations, a poster session, and will close with a discussion panel.  
 

Papers must be between 4-8 pages in the AAAI submission format, with the  eighth page containing only references. Papers will be submitted  electronically using Easychair. Accepted papers will not be archival,  and we explicitly allow papers that are concurrently submitted to,  currently under review at, or recently accepted in other conferences /  venues.  
 

Easychair link: [https://easychair.org/conferences/?conf=aaai19rlg](https://easychair.org/conferences/?conf=aaai19rlg)  
 

**Programme Committee**  
 

* David Balduzzi
* Yngvi Bjornnsson
* Michael Bowling
* Noam Brown
* Michael Buro
* Jakob Foerster
* Matthieu Geist
* Johannes Heinrich
* Thomas Hubert
* Emilie Kaufmann
* Ed Lockhart
* Viliam Lisy
* Michael Littman
* Matej Moravcik
* Martin Mueller
* Alex Peysakhovich
* Olivier Pietquin
* Bilal Piot
* Tom Schaul
* Bruno Scherrer
* Gerald Tesauro
* Julian Togelius
* Karl Tuyls
* Vinicius Zambaldi  
 

**Organizers**  
 

Marc Lanctot (DeepMind)  
Julien Perolat (DeepMind)  
Martin Schmid (DeepMind)",reinforcementlearning,sharky6000,False,/r/reinforcementlearning/comments/9pbq6z/cfp_aaai_workshop_on_reinforcement_learning_in/
"DeepMind Releases TRF - ""a library of reinforcement learning building blocks""",1539805935,"\- \[Announcement\]([https://deepmind.com/blog/trfl/](https://deepmind.com/blog/trfl/))

\- \[github\]([https://github.com/deepmind/trfl/](https://github.com/deepmind/trfl/))

&amp;#x200B;

Thought it would be very relevant to the sub. They cite a blog post from \[OpenAI\]([https://blog.openai.com/openai-baselines-dqn/](https://blog.openai.com/openai-baselines-dqn/)) wherein they mention

&amp;#x200B;

\&gt;some of the most popular open-source implementations of reinforcement  learning agents and finding that six out of 10 “had subtle bugs found by  a community member and confirmed by the author”.",reinforcementlearning,iamquah,False,/r/reinforcementlearning/comments/9p209l/deepmind_releases_trf_a_library_of_reinforcement/
Reinforcement learning interview questions,1539789411,"I know that there are no RL-only positions, but still some AI-Research position requires good understanding of RL. Does anyone has a list of questions/topics need to be covered. 
Or maybe you can share your experience from the last interview",reinforcementlearning,zkid18,False,/r/reinforcementlearning/comments/9ozjyw/reinforcement_learning_interview_questions/
"Use hidden layer outputs, but Critic, the end representation, does it work?",1539782937,"My intention is to use the intermediate hidden layer of a separate network, ""B"", as an input for an Actor-Critic model.

The separate network, ""B"" will be trained from another Critic-B. Feeding the intermediate layer of, B, as an input to Critic-B, will result in a really large input space for the Critic-B. Would it be possible, to Critic, the end representation of Critic-B, maybe as some kind of a distribution (Gaussian?), instead of directly doing it on a hidden layer?

Any thoughts, suggestions, experiences are welcome!

Thanks!",reinforcementlearning,FatherCannotYell,False,/r/reinforcementlearning/comments/9oyo1p/use_hidden_layer_outputs_but_critic_the_end/
[R] Exploration by random distillation (predicting outputs of a random network) (new Sota on Montezuma),1539759192,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9ow9f5/r_exploration_by_random_distillation_predicting/
"""Better Safe than Sorry: Evidence Accumulation Allows for Safe Reinforcement Learning"", Agarwal et al 2018 {CMU}",1539734754,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9ot9p1/better_safe_than_sorry_evidence_accumulation/
"Official Python TensorFlow implementation of ""Large-Scale Study of Curiosity-Driven Learning"" (Burda et al 2018) {OA}",1539733959,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9ot64p/official_python_tensorflow_implementation_of/
Impact of discretization of continuous states and action space,1539702994,"I am new to RL and this filed is just blowing my mind day by day and I am wanting to learn more. While literature survey I found that there are recent algos such as fitted Q, ddpg, DQN which can handle large continuous state and action space, but what if I would like to start with a simple tabular q approach just to understand the Q matrix and stuff. So if I discretize the continuous state and action space, what would be the impact on the agent's performance. What parameters will be affected or what exactly would happen?",reinforcementlearning,Matlab_Begin,False,/r/reinforcementlearning/comments/9oojwu/impact_of_discretization_of_continuous_states_and/
Designing a reward function,1539702007,"I am new to RL and I would like to know how will the design of the reward function impact the performance of the agent? In some article I read that scaling the rewards between -1 and 1 helps in faster learning. for example if r=1 if goal state and -1 for bad state and 0 otherwise, rather than 100 for goal state, -500 for bad state and -10 otherwise. Could some please explain how this works? and also are there any guidelines or steps in general to follow while designing a reward function? ",reinforcementlearning,Matlab_Begin,False,/r/reinforcementlearning/comments/9ooe7o/designing_a_reward_function/
Actor-Critic vs Model-Based RL,1539641588,"In the most classical sense the critic can only evaluate a single step and cannot model the dynamics, while model based RL also learns dynamics / forward model.

However, what happens when a critic is based on an RNN/LSTM model? that could predict multistep outcomes? Is the line blurry then or there is some distinction that still set these two concepts apart?",reinforcementlearning,lepton99,False,/r/reinforcementlearning/comments/9ohfle/actorcritic_vs_modelbased_rl/
Empowerment Driven Exploration using Mutual Information Estimation,1539587198,"Have a look at this paper:

[https://arxiv.org/abs/1810.05533](https://arxiv.org/abs/1810.05533)

&amp;#x200B;

You can find the corresponding code here : [https://github.com/navneet-nmk/pytorch-rl](https://github.com/navneet-nmk/pytorch-rl)

&amp;#x200B;

Blog Post: [https://navneet-nmk.github.io/2018-08-26-empowerment/](https://navneet-nmk.github.io/2018-08-26-empowerment/)

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/9oafy0/empowerment_driven_exploration_using_mutual/
PPO struggling at MountainCar whereas DDPG is solving it very easily. Any guesses as to why?,1539567798,"I am using the stable baselines implementations of both algorithms (I would highly recommend it to anyone doing RL work!) using the default hyperparameters for DDPG and both the atari hyperparameters and the default ones for PPO.  


PPO is can not solve mountain car where as DDPG can. Any guesses as to why? 

Anyone has a set of hyperparameters that solve mountain car for PPO?

&amp;#x200B;

Thank you in advance",reinforcementlearning,LupusPrudens,False,/r/reinforcementlearning/comments/9o8ez0/ppo_struggling_at_mountaincar_whereas_ddpg_is/
Noobie QN: Facing trouble in merging codes,1539542116,"Hi,

I found an awesome post on Q-learning which is implemented on taxi-v2, I was able to understand the code, but since the author had separated the training and evaluationg codes. Im finding difficult merging them to make a complete sense of it. Help needed

&amp;#x200B;

P.s: I just started on RL

    """"""Training the agent""""""
    
    import random
    from IPython.display import clear_output
    
    # Hyperparameters
    alpha = 0.1
    gamma = 0.6
    epsilon = 0.1
    
    # For plotting metrics
    all_epochs = []
    all_penalties = []
    
    for i in range(1, 100001):
        state = env.reset()
    
        epochs, penalties, reward, = 0, 0, 0
        done = False
        
        while not done:
            if random.uniform(0, 1) &lt; epsilon:
                action = env.action_space.sample() # Explore action space
            else:
                action = np.argmax(q_table[state]) # Exploit learned values
    
            next_state, reward, done, info = env.step(action) 
            
            old_value = q_table[state, action]
            next_max = np.max(q_table[next_state])
            
            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)
            q_table[state, action] = new_value
    
            if reward == -10:
                penalties += 1
    
            state = next_state
            epochs += 1
            
        if i % 100 == 0:
            clear_output(wait=True)
            print(f""Episode: {i}"")
    
    print(""Training finished.\n"")
    
    

When I merge them, The training part of the code runs,Evaluation part doesnt run

the code is here: [https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/](https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/)

    """"""Evaluate agent's performance after Q-learning""""""
    
    total_epochs, total_penalties = 0, 0
    episodes = 100
    
    for _ in range(episodes):
        state = env.reset()
        epochs, penalties, reward = 0, 0, 0
        
        done = False
        
        while not done:
            action = np.argmax(q_table[state])
            state, reward, done, info = env.step(action)
    
            if reward == -10:
                penalties += 1
    
            epochs += 1
    
        total_penalties += penalties
        total_epochs += epochs
    
    print(f""Results after {episodes} episodes:"")
    print(f""Average timesteps per episode: {total_epochs / episodes}"")
    print(f""Average penalties per episode: {total_penalties / episodes}"")

&amp;#x200B;",reinforcementlearning,KarthikMgk,False,/r/reinforcementlearning/comments/9o5468/noobie_qn_facing_trouble_in_merging_codes/
Empowerment driven exploration,1539501697,"For the past year I have been dabbling with the idea of using empowerment as an intrinsic reward. I know that it is nothing new but I wanted to have a system which was very easy and intuitive to implement, similar to what was achieved by Curiosity driven Exploration using Self Supervised Learning. 

&amp;#x200B;

[https://navneet-nmk.github.io/2018-08-26-empowerment/](https://navneet-nmk.github.io/2018-08-26-empowerment/)

&amp;#x200B;

I believe with better computational resources, I have a chance of having a system which could go toe to toe with the SOTA on Montezuma's revenge. 

&amp;#x200B;

Thanks !",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/9o0z5g/empowerment_driven_exploration/
Multi Agent RL Question,1539455677,"Hi everybody,

I was recently thinking about the case of learning agent using RL algorithms in a network of agents where the communication is implicit. Can learning in this kind of platform work?

To be clear my assumptions about the problem settings is that any agent can see the agents around him up to some radius r but there isn't any direct communication between them.

My initial thought is that because there isn't explicit communication and our state must depend on the other agents that are part of the environment that is around our learning agent the state **is unstable**, meaning that it can change in any moment due to movements of the other agents, this will cause our state to change in any moment which can be troublesome for a learning agent.

Is learning in these settings feasible? If so can anyone explain or refer to some work in these settings?",reinforcementlearning,liormoshe,False,/r/reinforcementlearning/comments/9nw25r/multi_agent_rl_question/
[D] Need help in answering questions from reviewer,1539450616,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/9nvd2e/d_need_help_in_answering_questions_from_reviewer/
Model-Based Reinforcement Learning on Roboschool,1539356114,,reinforcementlearning,andri27-ts,False,/r/reinforcementlearning/comments/9nky5h/modelbased_reinforcement_learning_on_roboschool/
Code review: One-step Actor-Critic,1539325022,"Here is my implementation of Sutton &amp; Barto's One-Step Actor-Critic: [https://pastebin.com/UuPxd5uz](https://pastebin.com/UuPxd5uz)

The algorithm is presented below:

&amp;#x200B;

https://i.redd.it/vq9k73zp5pr11.png

Are there mistakes in my implementation of the algorithm? Are there ways I can improve it?",reinforcementlearning,SuspiciousMariachi,False,/r/reinforcementlearning/comments/9nhu12/code_review_onestep_actorcritic/
Can RL be used for fault detection?,1539318605,"I've been searching papers on that topic, but have found close to zero results.

Does someone have some insight on the matter?",reinforcementlearning,page47250,False,/r/reinforcementlearning/comments/9nh8eg/can_rl_be_used_for_fault_detection/
Using a RNN to build the agent state 'a la POMDP' and on-policyness,1539270853,"Recently, u/tigerneil posted the [Principles of Deep RL by David Silver](https://www.reddit.com/r/reinforcementlearning/comments/9h60h2/principles_of_deep_rl_by_david_silver/) slides here. I had a question regarding principle #5 'State is subjective' where the agent state is actually the hidden state of a RNN taking as input the last action, the current observation, and the current reward.

Do you know any paper experimenting with this kind of architecture? I wonder about how you would train it / use it online. I have a lot of questions about the implications of this RNN.

For instance, can you use an off policy method for training? Using information about the last actions and rewards would introduce on-policyness in the behaviour, wouldn't it? Or is the structure independent of the algorithm and you can you just use any stream of action/observation/reward to train the network off-policy?

Also, what about non-episodic environments? For backprop, you would need to split the stream into finite sequences of action/observation/reward. But you would not need to reset the value of the agent internal state. Assuming I can use an off-policy method like DQN to train the agent, here is what I think I should do:

* Initialize the 'online' internal state when constructing the agent
* For inference, feed the 'online' internal state as recurrent input to the network. Never reset this 'online' internal state.
* For training, backprop on finite sequences of action/observation/reward stored in replay buffer. For each sequence, the initial 'offline' internal state could be also stored in the replay buffer.

Am I on the right track with this implementation idea? More generally, any advice when dealing with that kind of architecture?",reinforcementlearning,thibault_c,False,/r/reinforcementlearning/comments/9nau3p/using_a_rnn_to_build_the_agent_state_a_la_pomdp/
Issue with MC return estimate when truncating episode before termination.,1539269249,"Hello everyone,

I'm having an issue with computing return estimates, and it seems like it would be such a common issue that hopefully someone might be able to shed some light.  I'll try to strip my problem to the bare minimum.

The context is that I'm trying to use some policy gradient variants.  The environments that I'm using can be either episodic of continuing, but in either case I truncate the running episode after a certain number of time-steps (even if the environment is episodic, there is no guarantee that we reach a terminal state for the given current policy).  This means that my MC estimates of the overall return are intrinsically biased whenever I truncate an episode before it reaches a terminal state.

As an attempt to solve this, I switch to an n-step TD target, where n is the number of steps after which I truncate, so I need to implement a value function approximator for my states.  My issue is that, while the early states will be easily trainable due to the abundance of time-steps before truncation, i.e. the feedback loop between different estimated values is loose,

v(s_0) &lt;- v(s_0) + \alpha ( \sum_i \gamma^i r_i + gamma^T v(s_T) - v(s_0) )

The latter states will have a much tighter feedback loop which makes the whole estimation unstable, sometimes breaking the whole learning procedure.

v(s_{T-1}) &lt;- v(s_{T-1}) + \alpha ( r_{T-1} + \gamma v(s_T) - v(s_{T-1}) )

This is essentially the same issue often incurred when trying to use a TD(0) target, and my main issue in all this is that it seems a bit off that I'm incurring in these kinds of errors when to begin with, as I was just interested in using an *unbiased* MC estimate of the episode return.

Does anyone know how else I could approach this issue?",reinforcementlearning,BigBlindBais,False,/r/reinforcementlearning/comments/9nalvz/issue_with_mc_return_estimate_when_truncating/
Paper search for RL + hyperparameter optimization.,1539268721,"Hey everyone,

&amp;#x200B;

I am searching through the literature for hyperparameter optimization applied to reinforcement learning algorithms. I am aware of (hopefully) most of the work in hyperparameter optimization for supervised learning, but am having some difficulty finding that work applied to RL. 

&amp;#x200B;

So far I have seen these guys:  
1. Henderson et al. - Deep Reinforcement Learning that Matters: [https://arxiv.org/abs/1709.06560](https://arxiv.org/abs/1709.06560)

2. Xingping et al. - Hyperparameter Optimization for Tracking With Continuous Deep Q-Learning: [http://openaccess.thecvf.com/content\_cvpr\_2018/html/Dong\_Hyperparameter\_Optimization\_for\_CVPR\_2018\_paper.html](http://openaccess.thecvf.com/content_cvpr_2018/html/Dong_Hyperparameter_Optimization_for_CVPR_2018_paper.html)

&amp;#x200B;

Are there any papers on hyperparameters and RL that you would recommend? ",reinforcementlearning,yngtodd,False,/r/reinforcementlearning/comments/9naj6a/paper_search_for_rl_hyperparameter_optimization/
Deep Q-Learning and Experience Replay: Article about the relation between tabular and function approximated Q-learning algorithm,1539258840,,reinforcementlearning,rewardsignal,False,/r/reinforcementlearning/comments/9n9a0w/deep_qlearning_and_experience_replay_article/
Deep Q-Learning and Experience Replay: Article about the relation between tabular and function approximated Q-learning algorithm,1539258209,[https://rlenv.directory/blog/deep-q-learning](https://rlenv.directory/blog/deep-q-learning),reinforcementlearning,rewardsignal,False,/r/reinforcementlearning/comments/9n97oz/deep_qlearning_and_experience_replay_article/
OpenAi Gym custom environment [help],1539255421,"So I've made my own environment, registered it, etc and used agents on it however the result of the training is usually a random infinite loop of bad moves,  I think it could be that the observation calculation is wrong but I'm not sure...

The game is simple, a 5x5 grid with a gold chest placed in a random cell and the player spawns in the middle of the bottom row. The player simply has to land on the gold chest and they win.

I figured the action space is discrete(4) -&gt; (Up, down, left right) and the observation space is discrete(600) -&gt; (25 possible locations for place and 24 locations for chest). Currently the observation is calculated from the player's co-ordinates multiplied with the chest's co-ordinates, but I'm pretty sure it should be something else... any ideas?",reinforcementlearning,ToxXxiiK,False,/r/reinforcementlearning/comments/9n8wcm/openai_gym_custom_environment_help/
"In Dyna-Q, in the ""planning"" stage, are you supposed to keep a ""frozen"" Q like in experience replay?",1539200799,"I'm trying to implement a very simple Dyna-Q algorithm to learn it. For the environment I'm just having my agent solve a maze. I'm implementing [this algorithm, as seen in Sutton and Barto](https://imgur.com/9xvvx8C). My Q is just a 2D torch tensor, Q(s,a), and I'm using torch to do the gradient descent.

Here's an example of my code:

    def dynaQ(self, N_steps=100):

        self.initEpisode()
        for i in range(N_steps):
            #Get current state, next action, reward, next state
            s = self.getStateVec()
            a = self.epsGreedyAction(s)
            r, s_next = self.iterate(a)
            #Get Q values, Q_next is detached so it doesn't get changed by the gradient
            Q_cur = self.Q[s, a]
            Q_next = torch.max(self.Q[s_next]).detach().item()
            TD0_error = (r + self.params['gamma']*Q_next - Q_cur).pow(2).sum()
            #SGD
            self.optimizer.zero_grad()
            TD0_error.backward()
            self.optimizer.step()
            #Add to experience buffer
            e = Experience(s, a, r, s_next)
            self.updateModel(e)

I'm seeing some confusing results though. The one-step TD learning part (steps a-e in the S&amp;B algorithm) are working perfectly, solving the maze quickly every time. Now, when I add the part where it gets experiences from the list and tries to update Q more, it seems like the Q_cur and Q_next values keep growing, and it almost never reaches the goal.

My code for that immediately follows the above in each loop:

    for j in range(N_plan_steps):

        xp = self.experiences[randint(0,len(self.experiences)-1)]
        Q_cur0 = self.Q[xp.s, xp.a]
        Q_next0 = torch.max(self.Q[xp.s_next]).detach().item()
        TD0_error0 = (xp.r + self.params['gamma']*Q_next0 - Q_cur0).pow(2).sum()

        self.optimizer.zero_grad()
        TD0_error0.backward()
        self.optimizer.step()

I'm not really sure what the problem is... nowhere in S&amp;B or the David Silver lectures do they mention to use a ""frozen""/""target"" Q (which I know is crucial for experience replay), but I saw it in some random person's code.

Could anyone confirm this, or have any other tips I could try? thanks!",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9n2t7a/in_dynaq_in_the_planning_stage_are_you_supposed/
"TensorFlow ActiveQA: ""Open Sourcing Active Question Reformulation with Reinforcement Learning"" {Google}",1539195180,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9n1yoj/tensorflow_activeqa_open_sourcing_active_question/
[1809.11169] Propagation Networks for Model-Based Control Under Partial Observation,1539193006,,reinforcementlearning,LazyOptimist,False,/r/reinforcementlearning/comments/9n1mhp/180911169_propagation_networks_for_modelbased/
CS 598 Statistical Reinforcement Learning (F18) from a theoretical view,1539188346,[http://nanjiang.cs.illinois.edu/cs598/](http://nanjiang.cs.illinois.edu/cs598/) ,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9n0wx7/cs_598_statistical_reinforcement_learning_f18/
DOUBLY REPARAMETERIZED GRADIENT ESTIMATORS FOR MONTE CARLO OBJECTIVES,1539187068,"[https://sites.google.com/view/dregs](https://sites.google.com/view/dregs)

[https://github.com/google-research/google-research/tree/master/dreg\_estimators](https://github.com/google-research/google-research/tree/master/dreg_estimators)

[https://drive.google.com/file/d/1XmNbCn4-AuzobofdHzFaAwb1nLTHX-s-/view](https://drive.google.com/file/d/1XmNbCn4-AuzobofdHzFaAwb1nLTHX-s-/view)",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9n0q08/doubly_reparameterized_gradient_estimators_for/
"""Relational Forward Models for Multi-Agent Learning"", Tacchetti et al 2018",1539181606,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9mzxje/relational_forward_models_for_multiagent_learning/
DDPG for flight simulator landing,1539161988,"Hey

I'm currently involved in a small university project to teach a flight simulator (flight gear) how to land via reinforcement learning.

We do not have much experience with reinforcement learning, so we thought we ask about your opinions here.

The action space and observation space both are continuous, therefore we need to apply a more advanced algorithm. We decided to use the following approach:

Algorithm: Deep Deterministic Policy Gradient  
Frameworks: TensorFlow &amp; Keras  
Separation: We might separate the algorithm in three parts: Landing, descent and flying to a point suitable for descent

&amp;#x200B;

Do you think our project is destined to fail this way or could it work?

&amp;#x200B;

Thanks a lot!  
",reinforcementlearning,A3Ohh,False,/r/reinforcementlearning/comments/9mxsdx/ddpg_for_flight_simulator_landing/
Reinforcement Learning for Improving Agent Design,1539156756,,reinforcementlearning,milaworld,False,/r/reinforcementlearning/comments/9mxdjh/reinforcement_learning_for_improving_agent_design/
Realizing Learned Quadruped Locomotion Behaviors through Kinematic Motion Primitives,1539149999,,reinforcementlearning,abhik_singla,False,/r/reinforcementlearning/comments/9mws8t/realizing_learned_quadruped_locomotion_behaviors/
"""CAML: Fast Context Adaptation via Meta-Learning"", Zintgraf et al 2018",1539138336,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9mvjp6/caml_fast_context_adaptation_via_metalearning/
DeepMind planning to release StreetLearn (DRL agent for navigating Google Maps using only photos) for research purposes,1539137180,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9mvens/deepmind_planning_to_release_streetlearn_drl/
RL vs Planning,1539111138,"While designing a model, I've been coming up against this question a lot and there isn't really a way to proceed if I avoid this question.

&amp;#x200B;

What is the difference between RL and planning? Googling has only made me more confused.

Consider the example:

If you have a sequence which can be generated using a Finite State Machine (FSM), is learning to produce a sequence (which can be represented using the FSM) RL? Or is it planning? 

Is it RL when the FSM is not known, but the agent has to learn the FSM from supervision using sequences? Or is it planning? 

Is planning the same as the agent learning a policy ? 

The agent needs to look at sample sequences and learn to produce them given a starting state. ",reinforcementlearning,wavelander,False,/r/reinforcementlearning/comments/9mrwqf/rl_vs_planning/
"""Learning Acrobatics by Watching YouTube"" {BAIR}",1539101718,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9mqjfn/learning_acrobatics_by_watching_youtube_bair/
Looking for suggestions to implement a paper for the ICLR Reproducibility Challenge,1539092573,"Hi r/reinforcementlearning  

I am looking to implement one of the papers under submission at ICLR 2019 for the [ICLR 2019 Reproducibility Challenge](https://reproducibility-challenge.github.io/iclr_2019/). There was a great post summarising the papers which mention the keyword 'Reinforcement Learning' or 'Game' in the ICLR submissions : [ICLR '19 submitted papers with keyword Reinforcement Learning / Game](https://old.reddit.com/r/reinforcementlearning/comments/9jjniz/iclr_19_submitted_papers_with_keyword/). Now I am looking for some help to chose one of these papers for the reproducibility challenge since I can't possibly go through 100 papers and decide for myself.    

Here are some the requirements for me to be able to implement them
* Anything which requires massive hardware resources (think of something like OpenAI Five or Ape-X DQN) is out of the question. I do have access to some 1080Tis at my lab but that's about it.
* Any paper which requires some other previously trained models (for example I read **What Would pi\* Do?: Imitation Learning via Off-Policy Reinforcement Learning** and it requires me to have a DQN trained on Lunar Ladder and Recurrent World Model trained on Car Racing to act as experts for demonstrations) will be harder to replicate simply because I have no way of ensuring that me training the model from scratch will give same results. Also if a paper compares with too many models which are not easily available it will be harder to implement in the time frame of the challenge.  

It would be really helpful if you all can chip in with possible papers for me to implement which you have read and think can be feasibly done in 2 months time. I can give about one hour everyday reading/writing code and have access to a 4x1080Ti machine for running. I am also applying for grad school in the meanwhile so time is a constraint for me.   

Here is the complete list of submission for ICLR 2019 on OpenReview : https://openreview.net/group?id=ICLR.cc/2019/Conference",reinforcementlearning,lifeadvicesponge,False,/r/reinforcementlearning/comments/9mp93e/looking_for_suggestions_to_implement_a_paper_for/
Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning,1539059084,,reinforcementlearning,dezzion,False,/r/reinforcementlearning/comments/9mm3sd/qmap_a_convolutional_approach_for_goaloriented/
Deep Reinforcement Learning Doesn't Work Yet (Feb 2018),1539058554,,reinforcementlearning,last_useful_man,False,/r/reinforcementlearning/comments/9mm1t5/deep_reinforcement_learning_doesnt_work_yet_feb/
[1810.01112v1] The Dreaming Variational Autoencoder for Reinforcement Learning Environments,1539054914,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9mlnax/181001112v1_the_dreaming_variational_autoencoder/
"""Generalization and Regularization in DQN"", Farebrother et al 2018",1539049630,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9ml0l9/generalization_and_regularization_in_dqn/
Constructing a markov state,1539027150,"After passing through the elementary literature of reinforcement learning I still don't know how can I build a state that holds the markov property for a given problem, it seems like in all the literature they just assume that the state holds this property and we can jump right ahead to compute the optimla policy.  Can anybody here reference material about how do we usually construct a state that doesn't break the markov property or real thought process of constructing a markov state for a given problem?",reinforcementlearning,liormoshe,False,/r/reinforcementlearning/comments/9mi35c/constructing_a_markov_state/
Normalizing states in a financial application.,1539025027,"I'm currently trying to train a Deep Q-network for a financial application. I have a simulation environment from which I am sampling trajectories of a stock price. Each episode consists of exactly 10 steps where each step is one trading day, i.e. the stock price at the end of the day for 10 consecutive days. I end up receiving a single reward at the end of those 10 days that depends on the stock price at that time.
I’m interested in a way to normalize the stock price. The only information I was able to find is from this post: https://discuss.pytorch.org/t/normalization-of-input-data-to-qnetwork/14800
i.e. you compute a running average and variance of each state and Z-normalize each new state with those. My question is: would it make more sense to have a running average and variance for each step AND each day? In my problem, this would mean that I would have to keep a running average and variance for each of the 10 trading days.

Thank you for your help, I'm quite new to RL.",reinforcementlearning,ranirlol,False,/r/reinforcementlearning/comments/9mhrqr/normalizing_states_in_a_financial_application/
"""Where Did My Optimum Go?: An Empirical Analysis of Gradient Descent Optimization in Policy Gradient Methods"", Henderson et al 2018",1538967402,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9mb7ti/where_did_my_optimum_go_an_empirical_analysis_of/
3D collision avoidance using reinforcement learning,1538942230,"I am now starting a project where I will work with 3D collision avoidance for autonomous underwater vehicles (AUVs) in the Arctic. The end goal is robust collision avoidance of submerged ice and icebergs in unknown environments, using sonars. I find reinforcement learning interesting, however my knowledge on this is very limited. I am wondering if someone here could say if techniques from reinforcement learning could be suitable for this application?

The idea is to train the AUV in a simulator.",reinforcementlearning,PhDizzleFoShizzle,False,/r/reinforcementlearning/comments/9m86nw/3d_collision_avoidance_using_reinforcement/
"""Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"", Yi et al 2018",1538865359,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9m08yw/neuralsymbolic_vqa_disentangling_reasoning_from/
"""Unsupervised Learning via Meta-Learning"", Hsu et al 2018",1538864453,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9m04we/unsupervised_learning_via_metalearning_hsu_et_al/
How can I draw such figure?,1538813811,"How can I draw such figure depicting deviation and the average bold line?

https://i.redd.it/kt4b2hgsxiq11.png",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9ludz6/how_can_i_draw_such_figure/
"DeepMind 2017 financial statistics: $260m labor, $433m expenses total, $71m revenue",1538785132,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9lrqlx/deepmind_2017_financial_statistics_260m_labor/
Different equations for minimising Bellman Error for the last time step,1538772628,"I am confused regarding the correct update rule for the last time step of an epoch in Q learning, based on trying different alternatives empirically.

In the special case when:
The epoch ends if and only if we are in a terminal state, then it seems plausible to assume the Q values for this states to be zero (no reward can ever be gained from them).

However, from Arthur Juliani's blog post with Tabular Q learning in the Frozen Lake environment he does not follow the above, but lets the Q values for the terminal states to remain the same during the entire training (see: https://gist.github.com/awjuliani/9024166ca08c489a60994e529484f7fe#file-q-table-learning-clean-ipynb)

And, if I change the update rule from:
$Q(s, a) = Q(s, a) + \alpha ((r + \gamma max_a Q(s', a) - Q(s, a))$

To:
$Q(s, a) = Q(s, a) + \alpha (r - Q(s, a))$

Then the it does not learn to solve the environment anymore.

I don't see why this should even make a difference, any advice is appreciated.

",reinforcementlearning,antonosika,False,/r/reinforcementlearning/comments/9lq3wp/different_equations_for_minimising_bellman_error/
When is greedy the best exploration choice for contextual bandits?,1538730905,"Hi all,

this might be some very specific questions about contextual bandits and the exploration/exploitation tradeoff. I'm sorry, if I'm just throwing in something, but maybe someone can help me with the following:

There are a bunch of rather new papers  


[Mostly Exploration-Free Algorithms for Contextual Bandits](https://arxiv.org/pdf/1704.09011.pdf)  


[Smoothed Analysis of the Greedy Algorithm for the Linear Contextual Bandit Problem](https://arxiv.org/pdf/1801.03423.pdf)  


[A Contextual Bandit Bake-off](https://arxiv.org/pdf/1802.04064.pdf)  


[The Externalities of Exploration and How Data Diversity Helps Exploitation](https://arxiv.org/pdf/1806.00543.pdf)

which indicate that sometimes, when the contexts are diverse enough, we don't need to perform explicit exploration and can just pick our choices greedily.

Did I understand correctly, that the proofs just hold for linear models?

Is this basically about the smallest eigenvalue of the data's covariance matrix fulfilling a certain criterion?

I have a dataset and apply different regression models and it seems like greedy outperforms all explorations methods I tested. It would be good, if I could support this observation with something formal instead of just something empirical. How can I approach this?

Do you guys have any experience in this topic?

Your help is appreciated.",reinforcementlearning,temporary_idiot_420,False,/r/reinforcementlearning/comments/9lkowh/when_is_greedy_the_best_exploration_choice_for/
Holodeck - a High Fidelity Simulator for Reinforcement Learning,1538707321,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9lih8u/holodeck_a_high_fidelity_simulator_for/
"""Optimal Completion Distillation for Sequence Learning"", Sabour et al 2018 {GB}",1538678671,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9lenzx/optimal_completion_distillation_for_sequence/
"""AlphaSeq: Sequence Discovery with Deep Reinforcement Learning"", Shao et al 2018",1538676607,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9lecth/alphaseq_sequence_discovery_with_deep/
"""Few-Shot Goal Inference for Visuomotor Learning and Planning"", Xie et al 2018",1538615037,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9l7962/fewshot_goal_inference_for_visuomotor_learning/
Stuck with Open ai gym taxi v2,1538514036,"I have attempted the Open ai gym taxi v2 task. To qualify, the solution must get a reward of 9.7 on an average over 100 episodes, I am stuck at 8.5-8.9 and it doesn't appear that I can improve any further with the current agent (tried over 4 hours). 

&amp;#x200B;

This is [what it looks like](https://medium.com/@amresh.venugopal/attempting-open-ais-taxi-v2-using-the-sarsa-max-algorithm-70a4de8c8c9c), this is my post which covers how it works and visualisations for rewards/episodes. What I don't understand is: The agent always seems to trace the shortest paths, but still suffers from poor rewards? What am I doing wrong? ",reinforcementlearning,ltbringer,False,/r/reinforcementlearning/comments/9kul2z/stuck_with_open_ai_gym_taxi_v2/
"At what point should I stop building my own algorithms and start using packages like gym, etc?",1538502586,"I've been learning some RL from books and videos and have been building things by hand using python/numpy/torch/etc. 

But often, when I look at tutorials, they're using some pretty hefty packages like openai gym, which I'm sure saves a lot of time. I've been able to solve simpler problems so far, but I definitely spend a ton of time debugging my own messes and stuff that's probably not really that helpful for actually learning RL.

Additionally, I'm sure that if I ever do this professionally, they probably wouldn't say ""okay, implement A3C by hand"" (...right?).

At what point should I say, okay, I've done enough low level stuff myself, time to use other peoples' stuff? ",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9ksver/at_what_point_should_i_stop_building_my_own/
[R] Agent-based modelling for trading - resources,1538466761,I'm looking for resources on agent-based models typically used for trading. I'm happy to look through blog posts and video lectures but research papers would be most preferred.,reinforcementlearning,grupiotr,False,/r/reinforcementlearning/comments/9koq1m/r_agentbased_modelling_for_trading_resources/
"""Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow"", Peng et al 2018",1538446144,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9kmq3e/variational_discriminator_bottleneck_improving/
"[R] Unsupervised stroke-based drawing agents! + scaling to 512x512 sketches (""Unsupervised Image to Sequence Translation with Canvas-Drawer Networks"", Frans &amp; Cheng 2018 {Autodesk})",1538433027,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9kl2by/r_unsupervised_strokebased_drawing_agents_scaling/
Solutions of Reinforcement Learning An Introduction Sutton 2nd,1538382798,"Hello:

I am learning the Reinforcement Learning through the book written by Sutton

However, I have a problem about the understanding of the book.

When I try to answer the Exercises at the end of each chapter, I have no idea.  I think that's terrible for I have read the book carefully.

Could anyone give me some hints in the Exercises, (e.g.  Exercises 2.2)?

Or, is there a solutions manual of this book?

I will be appreciative.",reinforcementlearning,huayangshiboqi,False,/r/reinforcementlearning/comments/9keko0/solutions_of_reinforcement_learning_an/
"""Self-Supervised Generalisation with Meta Auxiliary Learning"", Anonymous 2018",1538364098,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9kctnr/selfsupervised_generalisation_with_meta_auxiliary/
[R] Backpropamine - training self-modifying neural networks with differentiable neuromodulated plasticity,1538364038,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9kcteq/r_backpropamine_training_selfmodifying_neural/
"""Evo-NAS: Evolutionary-Neural Hybrid Agents for Architecture Search"", Anonymous 2018",1538272553,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9k2i5d/evonas_evolutionaryneural_hybrid_agents_for/
Reinforcement learning for trading,1538267184,I am learning to program and code in python but I am still quite new at it. I have been trading for some time now. I was just wondering if there are any links out there would I could get an example of a reinforcement learning model that I could use for trading. I am hoping to start using it for some of my trading. Thanks for the help!,reinforcementlearning,akozzy18,False,/r/reinforcementlearning/comments/9k1xh1/reinforcement_learning_for_trading/
Markov Decision Processes can be made really easy to understand...,1538264953,"I am a random Maths BSc taking his Maths Master's and I am also taking plenty of courses on ML and AI and all those things.

&amp;#x200B;

https://i.redd.it/hey0sirvl9p11.png

Recently I started reading about Markov Decision Processes (I had some prior knowledge of Markov Chains) and then started writing about them in my blog! I started doing all this because of the [\#100DaysOfMLCode](http://mathspp.blogspot.com/2018/09/pledging-to-do-100-days-of-machine.html) initiative I am taking part in.

The [first blog post](http://mathspp.blogspot.com/2018/09/markov-decision-processes-basics.html) was on the formal definition of MDP and I gave a really clear example, the [second post](http://mathspp.blogspot.com/2018/09/markov-decision-processes-02-how.html) I wrote was mainly about the discount factor and using it to compute the expected reward of some policies.

&amp;#x200B;

You are all welcome to read the post(s) and then leave your honest feedback as a comment!",reinforcementlearning,RojerGS,False,/r/reinforcementlearning/comments/9k1or2/markov_decision_processes_can_be_made_really_easy/
Two hands-on books on RL - what are the differences?,1538251876,"I'm looking for a hands-on book with sample code to complement the Sutton &amp; Barto book.  Specifically interested in implementations in OpenAI gym.  I found two but they sound so similar (both published in June 2017 by Packt, both have good reviews so far on amazon) that I can't decide between them:

1. Deep Reinforcement Learning Hands-On, by Maxim Lapan

2. Hands-On Reinforcement Learning with Python, by Sudharsan Ravichandiran",reinforcementlearning,billtubbs,False,/r/reinforcementlearning/comments/9k04cx/two_handson_books_on_rl_what_are_the_differences/
What resources would you recommend for a reinforcement learning newbie,1538223296,"Hello guys,
I believe that RL is the future and I want to get into it now. I am a newbie and I am trying to start from scratch. Are there any resources you could recommend that I can use to build up my expertise slowly but gradually? Please note Udacity is too expensive and the stuff looks abstract. I have taken some ML classes on udemy but I haven’t found any good AI courses there",reinforcementlearning,byronkats,False,/r/reinforcementlearning/comments/9jw54d/what_resources_would_you_recommend_for_a/
[R] Boltzmann Weighting Done Right in Reinforcement Learning,1538210764,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9jv1t9/r_boltzmann_weighting_done_right_in_reinforcement/
"""What Would pi* Do?: Imitation Learning via Off-Policy Reinforcement"", Anonymous 2018",1538183625,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9jslyq/what_would_pi_do_imitation_learning_via_offpolicy/
Deep Exploration via Bootstrapped DQN Implementation,1538173728,"So I'm a Master's student taking a deep learning course this semester. As part of our course, I've been asked to do a project by the end of the semester where I have to work on Ian Osband's paper of Deep Exploration via Bootstrapped DQN. 

Unfortunately, however, we won't be covering any reinforcement learning in the course. So I have to teach myself the material. I am allowed to look at any code or resources online to complete the project. Does anyone have an idea on how I should get started? I do have a background in probability models and stochastic processes so I understand the theory of Markov decision processes and Q learning. However, I am not sure about how to get started with the code and implementing the research paper. Would appreciate your input!

Heres' the paper for reference (https://papers.nips.cc/paper/6501-deep-exploration-via-bootstrapped-dqn.pdf)",reinforcementlearning,platinumbjj,False,/r/reinforcementlearning/comments/9jrh7o/deep_exploration_via_bootstrapped_dqn/
[D] Would updating the actor using TD and the critic using MC be a good idea?,1538157128,"The reason for that would be to keep the variance away from the actor and have the critic soak it up. That way the net could potentially find a ideal balance between bias and variance as it would have the aspects of both methods.

I never saw this brought up before, but there is no reason why the critic update should be the same as the modulation term for the actor. Has this been tested?",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/9jp6k8/d_would_updating_the_actor_using_td_and_the/
[P] ICLR 2019 Papers | A Super Cool Analysis and Visualization,1538108701,,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9jjwlr/p_iclr_2019_papers_a_super_cool_analysis_and/
ICLR 2019 Papers | A Super Cool Analysis and Visualization,1538107200,,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9jjqn1/iclr_2019_papers_a_super_cool_analysis_and/
ICLR '19 submitted papers with keyword Reinforcement Learning / Game,1538106378,"[https://openreview.net/group?id=ICLR.cc/2019/Conference](https://openreview.net/group?id=ICLR.cc/2019/Conference)

ICLR '19 keyword: Reinforcement Learning

1. Cross-Task Knowledge Transfer for Visually-Grounded Navigation  
2. Algorithmic Framework for Model-based Deep Reinforcement Learning with Theoretical Guarantees  
3. Hierarchical Deep Reinforcement Learning Agent with Counter Self-play on Competitive Games  
4. Competitive experience replay  
5. Rigorous Agent Evaluation: An Adversarial Approach to Uncover Catastrophic Failures 
6. TarMAC: Targeted Multi-Agent Communication  
7. An Active Learning Framework for Efficient Robust Policy Search  
8. Reinforced Pipeline Optimization: Behaving Optimally with Non-Differentiabilities  
9. Universal Successor Features for Transfer Reinforcement Learning  
10. Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning  
11. Multi-agent Deep Reinforcement Learning with Extremely Noisy Observations  
12. Knowledge Representation for Reinforcement Learning using General Value Functions  
13. Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning  
14. Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences  
15. Causal Reasoning from Meta-learning  
16. Reinforcement Learning with Perturbed Rewards  
17. A new dog learns old tricks: RL finds classic optimization algorithms  
18. On Inductive Biases in Deep Reinforcement Learning  
19. Learning to Navigate the Web  
20. Exploration in Policy Mirror Descent  
21. Actor-Attention-Critic for Multi-Agent Reinforcement Learning  
22. Adapting Auxiliary Losses Using Gradient Similarity  
23. Visual Semantic Navigation using Scene Priors  
24. Reinforced Imitation Learning from Observations  
25. Unsupervised Control Through Non-Parametric Discriminative Rewards  
26. Amortized Bayesian Meta-Learning  
27. Learning Actionable Representations with Goal Conditioned Policies  
28. Understanding &amp; Generalizing AlphaGo Zero  
29. Learning and Planning with a Semantic Model  
30. Learning To Simulate  
31. Safe Policy Learning from Observations  
32. DHER: Hindsight Experience Replay for Dynamic Goals  
33. Variance Reduction for Reinforcement Learning in Input-Driven Environments  
34. DOM-Q-NET: Grounded RL on Structured Language  
35. End-to-End Hierarchical Text Classification with Label Assignment Policy  
36. MuMoMAML: Model-Agnostic Meta-Learning for Multimodal Task Distributions  
37. Transferring Knowledge across Learning Processes  
38. Double Neural Counterfactual Regret Minimization  
39. Importance Resampling for Off-policy Policy Evaluation  
40. The Implicit Information in an Initial State  
41. Distributed Deep Policy Gradient for Competitive Adversarial Environment  
42. Successor Options : An Option Discovery Algorithm for Reinforcement Learning  
43. Inducing Cooperation via Learning to reshape rewards in semi-cooperative multi-agent reinforcement learning  
44. Rating Continuous Actions in Spatial Multi-Agent Problems  
45. Discovering General-Purpose Active Learning Strategies  
46. Learning To Solve Circuit-SAT: An Unsupervised Differentiable Approach  
47. AD-VAT: An Asymmetric Dueling mechanism for learning Visual Active Tracking  
48. Value Propagation Networks  
49. SIMILE: Introducing Sequential Information towards More Effective Imitation Learning  
50. Evolutionary-Neural Hybrid Agents for Architecture Search  
51. Batch-Constrained Reinforcement Learning  
52. Deterministic Policy Gradients with General State Transitions  
53. Imitative Models: Perception-Driven Forecasting for Flexible Planning and Control  
54. Transfer and Exploration via the Information Bottleneck  
55. Surprising Negative Results for Generative Adversarial Tree Search  
56. Learning to Search Efficient DenseNet with Layer-wise Pruning  
57. Learning powerful policies and better generative models by interaction  
58. Learning Abstract Models for Long-Horizon Exploration  
59. Environment Probing Interaction Policies  
60. An investigation of model-free planning  
61. Variational Discriminator Bottleneck: Improving Imitation Learning, Inverse RL, and GANs by Constraining Information Flow 
62. Composing Entropic Policies using Divergence Correction  
63. Learning Heuristics for Automated Reasoning through Reinforcement Learning  
64. Recall Traces: Backtracking Models for Efficient Reinforcement Learning  
65. Deep Online Learning Via Meta-Learning: Continual Adaptation for Model-Based RL  
66. Guided Evolutionary Strategies: Escaping the curse of dimensionality in random search  
67. SSoC: Learning Spontaneous and Self-Organizing Communication for Multi-Agent Collaboration  
68. Constraining Action Sequences with Formal Languages for Deep Reinforcement Learning  
69. Shrinkage-based Bias-Variance Trade-off for Deep Reinforcement Learning  
70. Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning  
71. Lyapunov-based Safe Policy Optimization  
72. From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following  
73. Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning  
74. Learning State Representations in Complex Systems with Multimodal Data  
75. Learning Goal-Conditioned Value Functions with one-step Path rewards rather than Goal-Rewards  
76. Distilled Agent DQN for Provable Adversarial Robustness  
77. Temporal Difference Variational Auto-Encoder  
78. Learning Exploration Policies for Navigation  
79. HR-TD: A Regularized TD Method to Avoid Over-Generalization  
80. Plan Online, Learn Offline: Efficient Learning and Exploration via Model-Based Control  
81. Perception-Aware Point-Based Value Iteration for Partially Observable Markov Decision Processes  
82. Multi-Objective Value Iteration with Parameterized Threshold-Based Safety Constraints  
83. Neural Predictive Belief Representations  
84. Information asymmetry in KL-regularized RL  
85. Success at any cost: value constrained model-free continuous control  
86. Neural MMO: A massively multiplayer game environment for intelligent agents  
87. Optimal Control Via Neural Networks: A Convex Approach  
88. The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution  
89. Probabilistic Planning with Sequential Monte Carlo  
90. Deconfounding Reinforcement Learning  
91. Guided Exploration in Deep Reinforcement Learning  
92. COLLABORATIVE MULTIAGENT REINFORCEMENT LEARNING IN HOMOGENEOUS SWARMS  
93. Dopamine: A Research Framework for Deep Reinforcement Learning  
94. Deep Reinforcement Learning of Universal Policies with Diverse Environment Summaries  
95. Hierarchical Reinforcement Learning with Limited Policies and Hindsight  
96. Backpropamine: training self-modifying neural networks with differentiable neuromodulated plasticity  
97. Convergent Reinforcement Learning with Function Approximation: A Bilevel Optimization Perspective  
98. SNAS: stochastic neural architecture search  
99. Expressiveness in Deep Reinforcement Learning  
100. Boltzmann Weighting Done Right in Reinforcement Learning  

ICLR '19 keyword: Game

1. Hierarchical Deep Reinforcement Learning Agent with Counter Self-play on Competitive Games  
2. Competitive experience replay  
3. Neural MMO: A massively multiplayer game environment for intelligent agents  
4. Generating Multi-Agent Trajectories using Programmatic Weak Supervision  
5. Understanding &amp; Generalizing AlphaGo Zero  
6. Fast Exploration with Simplified Models and Approximately Optimistic Planning in Model Based Reinforcement Learning  
7. Self-Tuning Networks: Bilevel Optimization of Hyperparameters using Structured Best-Response Functions  
8. Q-map: a Convolutional Approach for Goal-Oriented Reinforcement Learning  
9. Learning Abstract Models for Long-Horizon Exploration  
10. CoDraw: Collaborative Drawing as a Testbed for Grounded Goal-driven Communication 
11. Evaluating GANs via Duality  
12. Double Neural Counterfactual Regret Minimization  
13. Exploration using Distributional RL and UCB  
14. Surprising Negative Results for Generative Adversarial Tree Search  
15. Optimal Attacks against Multiple Classifiers  
16. Learning to Control Visual Abstractions for Structured Exploration in Deep Reinforcement Learning  
17. Adapting Auxiliary Losses Using Gradient Similarity  
18. A Direct Approach to Robust Deep Learning Using Adversarial Networks  
19. Constraining Action Sequences with Formal Languages for Deep Reinforcement Learning  
20. Unsupervised Control Through Non-Parametric Discriminative Rewards  
21. Safe Policy Learning from Observations  
22. Multi-objective training of Generative Adversarial Networks with multiple discriminators 
23. Mimicking actions is a good strategy for beginners: Fast Reinforcement Learning with Expert Action Sequences  
24. Visualizing and Discovering Behavioural Weaknesses in Deep Reinforcement Learning 
25. Reinforcement Learning with Perturbed Rewards  
26. The Body is not a Given: Joint Agent Policy Learning and Morphology Evolution  
27. Beyond Winning and Losing: Modeling Human Motivations and Behaviors with Vector-valued Inverse Reinforcement Learning  
28. Exploration by random distillation  
29. Solving the Rubik's Cube with Approximate Policy Iteration  
30. Towards Consistent Performance on Atari using Expert Demonstrations  
31. Stable Opponent Shaping in Differentiable Games  
32. Probabilistic Program Induction for Intuitive Physics Game Play  
33. Transfer Learning for Related Reinforcement Learning Tasks via Image-to-Image Translation  
34. Beyond Games: Bringing Exploration to Robots in Real-world  
35. Negotiating Team Formation Using Deep Reinforcement Learning  
36. COLLABORATIVE MULTIAGENT REINFORCEMENT LEARNING IN HOMOGENEOUS SWARMS  
37. Evolving intrinsic motivations for altruistic behavior  
38. Probabilistic Recursive Reasoning for Mutli-Agent Reinforcement Learning  
39. Fatty and Skinny: A Joint Training Method of Watermark Encoder and Decoder  
40. Recurrent Experience Replay in Distributed Reinforcement Learning  
41. Boltzmann Weighting Done Right in Reinforcement Learning  
42. Information-Directed Exploration for Deep Reinforcement Learning  
43. Efficient Exploration through Bayesian Deep Q-Networks  
44. Emergent Coordination Through Competition  
45. Learning Finite State Representations of Recurrent Policy Networks  
46. Expressiveness in Deep Reinforcement Learning  
47. DEEP ADVERSARIAL FORWARD MODEL  
48. Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile  
49. Learning Representations in Model-Free Hierarchical Reinforcement Learning  
50. Episodic Curiosity through Reachability  
51. Characterizing Vulnerabilities of Deep Reinforcement Learning  
52. Contingency-Aware Exploration in Reinforcement Learning  
53. A Variational Inequality Perspective on Generative Adversarial Networks  
54. Intrinsic Social Motivation via Causal Influence in Multi-Agent RL  
55. Trajectory VAE for multi-modal imitation  
56. Learning Physics Priors for Deep Reinforcement Learing  
57. TTS-GAN: a generative adversarial network for style modeling in a text-to-speech system  
58. Generative Adversarial Models for Learning Private and Fair Representations  
59. Predicting the Present and Future States of Multi-agent Systems from Partially-observed Visual Data  
60. Neural Causal Discovery with Learnable Input Noise  
61. A Solution to China Competitive Poker Using Deep Learning  
62. GamePad: A Learning Environment for Theorem Proving  
63. Marginal Policy Gradients: A Unified Family of Estimators for Bounded Action Spaces with Applications  
64. REVERSED NEURAL NETWORK - AUTOMATICALLY FINDING NASH EQUILIBRIUM  
65. Accelerated Value Iteration via Anderson Mixing  
66. Finding Mixed Nash Equilibria of Generative Adversarial Networks  
67. Explaining AlphaGo: Interpreting Contextual Effects in Neural Networks  
68. Exploration by Uncertainty in Reward Space  
69. BEHAVIOR MODULE IN NEURAL NETWORKS  
70. Countering Language Drift via Grounding  
71. Sample-efficient policy learning in multi-agent Reinforcement Learning via meta-learning  
72. Deep reinforcement learning with relational inductive biases  
73. What Would pi\* Do?: Imitation Learning via Off-Policy Reinforcement Learning  
74. L-Shapley and C-Shapley: Efficient Model Interpretation for Structured Data  
75. Learning agents with prioritization and parameter noise in continuous state and action space  
76. Large-Scale Study of Curiosity-Driven Learning  
77. Playing the Game of Universal Adversarial Perturbations  
78. Generalization and Regularization in DQN  
79. GENERALIZED ADAPTIVE MOMENT ESTIMATION  
80. Backplay: 'Man muss immer umkehren'  
81. Encoder Discriminator Networks for Unsupervised Representation Learning  
82. Sample Efficient Deep Neuroevolution in Low Dimensional Latent Space  
83. Count-Based Exploration with the Successor Representation  
84. Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on Challenging Deep Reinforcement Learning Problems  
85. Hybrid Policies Using Inverse Rewards for Reinforcement Learning  
86. SUPERVISED POLICY UPDATE  
87. Stochastic Learning of Additive Second-Order Penalties with Applications to Fairness 
88. Improving On-policy Learning with Statistical Reward Accumulation  ",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9jjniz/iclr_19_submitted_papers_with_keyword/
"""R2D2: Recurrent Experience Replay in Distributed Reinforcement Learning"", Anonymous 2018 [new ALE/DMLab-30 SOTA: ""exceeds human-level in 52/57 ALE""; large improvement over Ape-X using a RNN]",1538104372,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9jjfgx/r2d2_recurrent_experience_replay_in_distributed/
Asymptotic Analysis of Pure-Exploration MABs,1538067432,"Hey everyone,

&amp;#x200B;

recently, I got in touch with a project using pure-exploration multi armed bandits.

My task was mostly realizing and implementing stuff. Everything works so far and I've understood the algorithms, however, I didn't get all the mathematics behind them.

As I would like to give an asymptotic analysis of my algorithm (which comprises moret than just MABs), I need to somehow obtain the complexity of the KL-LUCB algorithm (paper found [here](http://proceedings.mlr.press/v30/Kaufmann13.pdf)). The authors appended many calculations, in which I cannot find any e.g. big O notation.

Do you know of any such resource stating an optimal or worst-case bound for pure-exploration MABs? Doesn't need to match the KL-LUCB specifically, however, just a coarse classification.

&amp;#x200B;

Any help or hint is greatly appreciated! Thanks!",reinforcementlearning,HenryHux,False,/r/reinforcementlearning/comments/9jelbd/asymptotic_analysis_of_pureexploration_mabs/
"""Building safe artificial intelligence: specification, robustness, and assurance"" --Pedro A. Ortega, Vishal Maini, &amp; the DeepMind safety team",1538065264,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9jeaci/building_safe_artificial_intelligence/
[D] Has DeepMind released anything about Starcraft 2 yet?,1537996093,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9j6kbk/d_has_deepmind_released_anything_about_starcraft/
Do I need AWS/a GPU for this simple RL task or am I doing something wrong?,1537993385,"Hi, I've been learning and just watched the David Silver video on policy gradients, so I thought I'd try doing Puckworld with the actor-critic model. If you're unfamiliar, Karpathy has a nice writeup here: https://cs.stanford.edu/people/karpathy/reinforcejs/puckworld.html

I implemented it, but I'm running into some trouble. I've actually backtracked to just do Q-learning with a Q-network and experience replay, to simplify things, but it's still failing. I'm using pytorch, just running it on my dinky laptop. Right now, to simplify it even more, I'm not even moving the target, it's stationary, and there's no ""enemy"" dot chasing you (like in his example).

When I run it for 10^5 steps, it really doesn't converge, and as far as I can tell no dominant strategy happens. I only need to worry about a few parameters for this: gamma (1.0), alpha (I scanned it from 10^-1 to 10^-6, all equally bad), epsilon (tried greedy, and 0.2 and decreasing). 

My Q network has 1 hidden layer of 100 units, so it goes (input state vector)-&gt;(weights layer 1)-&gt;ReLU-&gt;(weights layer 2)-&gt;(output action vector). The state vector is 6 numbers (the puck position, puck velocity, and target position), and the output vector is 4 (up, down, left, right).

What I'm worried about is that I may need to run it for many more iterations that I can on this laptop, because the DS slide with it shows it improving after like, 10^7 iterations... On the other hand, mine seems to be doing stuff that just seems like it must be wrong, like going and sitting against a wall, far from the target. This is a pretty simple task where it gets a reward for just *being* in better locations, with no real planning, so it seems like if stuff is working right, it shouldn't do that, right?

",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9j66ag/do_i_need_awsa_gpu_for_this_simple_rl_task_or_am/
Monte carlo and its first visit,1537991047,"Hi,

My question is why we increment the counter in first visit Monte Carlo after a state is visited for the first time??

We then increment total return 

 S(s) &lt;--- S(s) + Gt
Is Gt in the above eqn is the total return after visiting all states in the current root?",reinforcementlearning,KarthikMgk,False,/r/reinforcementlearning/comments/9j5tnh/monte_carlo_and_its_first_visit/
DQN algorithms in simple colab notebooks,1537986034,,reinforcementlearning,crush-name,False,/r/reinforcementlearning/comments/9j52oh/dqn_algorithms_in_simple_colab_notebooks/
"S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning",1537969600,[https://github.com/araffin/robotics-rl-srl](https://github.com/araffin/robotics-rl-srl) ,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9j2n49/srl_toolbox_environments_datasets_and_evaluation/
Any RL technique incorporating safety?,1537943081,,reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/9j00t1/any_rl_technique_incorporating_safety/
Why does DRL performance collapse after convergence?,1537932688,"I trained a DRL agent and it converged for around 10k episode, but after that given more episode to train, performance collapsed. How can I figure it out? Is it over learned?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9iyz6v/why_does_drl_performance_collapse_after/
code for `The mechanics of n-player differentiable games' from DeepMind,1537896186,"github: [https://github.com/deepmind/symplectic-gradient-adjustment](https://github.com/deepmind/symplectic-gradient-adjustment)

paper: [http://proceedings.mlr.press/v80/balduzzi18a/balduzzi18a.pdf](http://proceedings.mlr.press/v80/balduzzi18a/balduzzi18a.pdf)",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9iu552/code_for_the_mechanics_of_nplayer_differentiable/
I started learning ML by starting with RL. Is it okay to do that way? And...,1537895641,"Hi,

by starting ML with RL. will it make learning general ML in future make it any easier? Coz it is said RL is math heavy part of ML. By getting to know the tough stuff first, Make the other forms of ML look easier??

&amp;#x200B;

Thanks in advance",reinforcementlearning,KarthikMgk,False,/r/reinforcementlearning/comments/9iu26i/i_started_learning_ml_by_starting_with_rl_is_it/
"RL in very large (3k) action spaces, A2C?",1537892996," 

I'm trying to achieve an optimal policy in a given environment (too complex for DP). Its a fairly simple environment:

\- Every day (from 0 to 300) the agent selects an action (a percentage essentially).  
\- Based on that percentage there is a probability of an occurrence to be recorded. At the same time, there is always a probability (which too is tied to the action value) of early termination with extremely negative rewards.  
\- On day 300 based on the number of occurrences a final reward is attributed, whose size is proportional to the number of occurrences (more occurrences, more negative the reward is).  
Note: Rewards are always negative, the idea is to minimize the number of occurrences without achieving early termination.

Actions go from 0 to 3 with a 0.001 increment (3.000 actions).

As I'm not very proficient in TF I've been using some prebuilt models, namely, A2C, should it be capable of handling said environment? It is by itself simple, the only problem I see is the large action number combined with the probability of early termination.

Additionally, will be greatly appreciated if a more experienced user doesn't mind giving me a hand, as its quite a hard process to learn and tune.",reinforcementlearning,Jkl_mp,False,/r/reinforcementlearning/comments/9itnsu/rl_in_very_large_3k_action_spaces_a2c/
Is dynamic programming very necessary to become a good at RL?,1537892731,"Hi,

Since dynamic programming is a part of rl, is it necessary ti be very good at dp to become a good in rl ?

In real world, we mostly donot face problems with a perfect environment or fully known mdp for the problems.
So is DP very necessary to become good RL engineer?

Thanks in advance

",reinforcementlearning,KarthikMgk,False,/r/reinforcementlearning/comments/9itmdm/is_dynamic_programming_very_necessary_to_become_a/
Texar is easy to use and powerful.,1537883314,"[https://medium.com/@texar/introducing-texar-a-modularized-versatile-and-extensible-toolkit-for-text-generation-and-beyond-589a5832b023](https://medium.com/@texar/introducing-texar-a-modularized-versatile-and-extensible-toolkit-for-text-generation-and-beyond-589a5832b023)

Website: [https://texar.io](https://texar.io/)

GitHub: [https://github.com/asyml/texar](https://github.com/asyml/texar)

&amp;#x200B;

https://i.redd.it/43vxts343eo11.png",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9is9te/texar_is_easy_to_use_and_powerful/
[R] [1809.02869] Modelling User's Theory of AI's Mind in Interactive Intelligent Systems,1537879641,,reinforcementlearning,terasnuoli,False,/r/reinforcementlearning/comments/9irt2q/r_180902869_modelling_users_theory_of_ais_mind_in/
Can someone explain calculatePolicy function.. Im having a lot of doubts,1537845748,,reinforcementlearning,KarthikMgk,False,/r/reinforcementlearning/comments/9iooli/can_someone_explain_calculatepolicy_function_im/
"""Neural Arithmetic Logic Units"", Trask et al 2018 {DM}",1537836948,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9inmyr/neural_arithmetic_logic_units_trask_et_al_2018_dm/
"""Adversarial Imitation via Variational Inverse Reinforcement Learning"", Qureshi &amp; Yip 2018",1537825634,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9im75l/adversarial_imitation_via_variational_inverse/
"I used RL to solve the ""egg drop puzzle""",1537815278,,reinforcementlearning,diddilydiddilyhey,False,/r/reinforcementlearning/comments/9ikphj/i_used_rl_to_solve_the_egg_drop_puzzle/
Reward shaping in Evolutionary Strategy,1537743482,Is there any paper on evolutionary strategy's reward shaping? ,reinforcementlearning,whikwon,False,/r/reinforcementlearning/comments/9icsfm/reward_shaping_in_evolutionary_strategy/
How can I implement continuous action for this game?,1537728643,"The game is called Hop you can see it in this video [https://www.youtube.com/watch?v=ezpl1dnF4vs](https://www.youtube.com/watch?v=ezpl1dnF4vs). I have successfully done a discrete action a3c for this game.

&amp;#x200B;

The action in this is just the x pos of the mouse. For the discrete I had 3 actions moving the mouse 30 pixels, moving mouse -30 pixel and not moving. My state was the image of the game resized 87\*87 grayscaled. My model was 2 convultion layers and one dense and one recurrent layer.

&amp;#x200B;

This worked really good but I want to implement continuous action for this game. I haven't done this before and I'm really stuck for days now. So I what I'm trying to do is to turn the image of the game into an exact position of the mouse. I 'm using this code structure [https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py](https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py). The output of tanh is then multiplied my 240(the game width is 480) then add to it 240. For example, the output is 0 , 240\*0+240=240, the mouse will be in the middle of the game. The rewards when you hit the green plate will give you 1 reward when you die you get -1 and anything else is 0.

&amp;#x200B;

It so slow. After 15000 episodes it couldn't get 2 rewards in a row. It stays in the action range of 0.2 to 0.1(green plates are more common in the right). It need to jump between negative and positive to not terminate. I have tried lowering learning rate, using beta distribution, increase batch size, increase convulation layers, adding recurrent neural network, changed gamma with no success. I would really appreciate it if someone can help. What activation function should I use, Optimiser,entropy and gamma.

&amp;#x200B;

This is example of mean and sigma of one episode, it never goes negative only in the at first, the mean can be as low as 0.05 but never negative.

&amp;#x200B;

mean        sigma

&amp;#x200B;

0.20931076   0.18241276

&amp;#x200B;

0.18813142     0.27827954

&amp;#x200B;

0.17932524      0.15135024

&amp;#x200B;

0.17869654      0.01679147

&amp;#x200B;

0.18108326     0.01578329

&amp;#x200B;

0.16879782      0.01404662

&amp;#x200B;

0.1827652       0.0492083

&amp;#x200B;

0.18620484      0.17246781

&amp;#x200B;

0.20845698        0.45260274

&amp;#x200B;

0.14595826        0.24136347

&amp;#x200B;

0.14366359        0.2080477",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/9iati3/how_can_i_implement_continuous_action_for_this/
How can I implement continuousction for this game ?,1537727773,"The game is called Hop you can see it in this video [https://www.youtube.com/watch?v=ezpl1dnF4vs](https://www.youtube.com/watch?v=ezpl1dnF4vs). I have successfully done a discrete action a3c for this game.

&amp;#x200B;

The action in this is just the x pos of the mouse. For the discrete I had 3 actions moving the mouse 30 pixels, moving mouse -30 pixel and not moving. My state was the image of the game resized 87\*87 grayscaled. My model was 2 convultion layers and one dense and one recurrent layer.

&amp;#x200B;

This worked really good but I want to implement continuous action for this game. I haven't done this before and I'm really stuck for days now. So I what I'm trying to do is to turn the image of the game into an exact position of the mouse. I 'm using this code structure [https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py](https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py). The output of tanh is then multiplied my 240(the game width is 480) then add to it 240. For example, the output is 0 , 240\*0+240=240, the mouse will be in the middle of the game. The rewards when you hit the green plate will give you 1 reward when you die you get -1 and anything else is 0.

&amp;#x200B;

It so slow. After 15000 episodes it couldn't get 2 rewards in a row. It stays in the action range of 0.2 to 0.1(green plates are more common in the right). It need to jump between negative and positive to not terminate. I have tried lowering learning rate, using beta distribution, increase batch size, increase convulation layers, adding recurrent neural network, changed gamma with no success. I would really appreciate it if someone can help. What activation function should I use, Optimiser,entropy and gamma.

&amp;#x200B;

This is example of mean and sigma of one episode, it never goes negative only in the at first, the mean can be as low as 0.05 but never negative.

&amp;#x200B;

mean           sigma

&amp;#x200B;

&amp;#x200B;

0.20931076       0.18241276

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.18813142        0.27827954

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.17932524        0.15135024

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.17869654        0.01679147

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.18108326     0.01578329

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.16879782     0.01404662

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.1827652      0.0492083

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.18620484      0.17246781

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.20845698     0.45260274

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.14595826     0.24136347

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

0.14366359      0.2080477",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/9iap2d/how_can_i_implement_continuousction_for_this_game/
How can I implimt cocontinuous action for this game ?,1537727171,"The game is called Hop you can see it in this video [https://www.youtube.com/watch?v=ezpl1dnF4vs](https://www.youtube.com/watch?v=ezpl1dnF4vs). I have successfully done a discrete action a3c for this game.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

The action in this is just the x pos of the mouse. For the discrete I had 3 actions moving the mouse 30 pixels, moving mouse -30 pixel and not moving. My state was the image of the game resized 87\*87 grayscaled. My model was 2 convultion layers and one dense and one recurrent layer.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

This worked really good but I want to implement continuous action for this game. I haven't done this before and I'm really stuck for days now. So I what I'm trying to do is to turn the image of the game into an exact position of the mouse. I 'm using this code structure [https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py](https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py). The output of tanh is then multiplied my 240(the game width is 480) then add to it 240. For example, the output is 0 , 240\*0+240=240, the mouse will be in the middle of the game. The rewards when you hit the green plate will give you 1 reward when you die you get -1 and anything else is 0.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

It so slow. After 15000 episodes it couldn't get 2 rewards in a row. It stays in the action range of 0.2 to 0.1(green plates are more common in the right). It need to jump between negative and positive to not terminate. I have tried lowering learning rate, using beta distribution, increase batch size, increase convulation layers, adding recurrent neural network, changed gamma with no success. I would really appreciate it if someone can help. What activation function should I use, Optimiser,entropy and gamma.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

This is example of mean and sigma of one episode, it never goes negative only in the at first, the mean can be as low as 0.05 but never negative.

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

mean              sigma

&amp;#x200B;

0.20931076    0.18241276

&amp;#x200B;

0.18813142     0.27827954

&amp;#x200B;

0.17932524     0.15135024

&amp;#x200B;

0.17869654     0.01679147

&amp;#x200B;

0.18108326      0.01578329

&amp;#x200B;

0.16879782     0.01404662

&amp;#x200B;

0.1827652     0.0492083

&amp;#x200B;

0.18620484     0.17246781

&amp;#x200B;

0.20845698     0.45260274

&amp;#x200B;

0.14595826     0.24136347

&amp;#x200B;

0.14366359     0.2080477",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/9ialwj/how_can_i_implimt_cocontinuous_action_for_this/
[P] Counterfactual Regret Minimization – the core of Poker AI beating professional players,1537720865,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9i9r7k/p_counterfactual_regret_minimization_the_core_of/
A3C continouse action for an android game,1537712221,"The game is called Hop you can see it in this video [https://www.youtube.com/watch?v=ezpl1dnF4vs](https://www.youtube.com/watch?v=ezpl1dnF4vs). I have successfully done a discrete action a3c for this game. 

&amp;#x200B;

The action in this is just the x pos of the mouse. For the discrete I have 3 actions moving the mouse 30 pixels, moving mouse -30 pixel and not moving. My state was the image of the game resized 87\*87 grayscaled. My model was 2 convultion layers and one dense and one recurrent layer. 

&amp;#x200B;

This worked really good but I want to implement continuous action for this game. I haven't done this before and I'm really stuck for days now. So I what I'm trying to do is to turn the image of the game into an exact position of the mouse. I 'm using this code structure [https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py](https://github.com/stefanbo92/A3C-Continuous/blob/master/a3c.py). The output of tanh is then multiplied my 240(the game width is 480) then add to it 240. For example, the output is 0 , 240\*0+240=240, the mouse will be in the middle of the game. The rewards when you hit the green plate will give you 1 reward when you die you get -1 and anything else is 0.

&amp;#x200B;

&amp;#x200B;

It so slow. After 15000 episodes it couldn't get 2 rewards in a row. It stays in the action range of 0.2 to 0.1. It need to jump between negative and positive to not terminate. I have tried lowering learning rate, using beta distribution, increase batch size, increase convulation layers, adding recurrent neural network, changed gamma with no success.  I would really appreciate it if someone can help. What activation function should I use, Optimiser,entropy and gamma. 

&amp;#x200B;

&amp;#x200B;

This is example of mean and sigma of one episode, it never goes negative only in the at first, the mean can as low as 0.05 but never negative. 

&amp;#x200B;

mean             sigma

0.20931076    0.18241276

0.18813142       0.27827954

0.17932524      0.15135024

0.17869654     0.01679147

0.18108326      0.01578329

0.16879782     0.01404662

0.1827652      0.0492083

0.18620484    0.17246781

0.20845698     0.45260274

0.14595826    0.24136347

0.14366359     0.2080477

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,dark16sider,False,/r/reinforcementlearning/comments/9i8o0o/a3c_continouse_action_for_an_android_game/
"Seeing a strange problem with the learning rate in the Mountain Car problem, any ideas?",1537656717,"So I'm learning some RL and naively implemented the mountain car problem. I did it just using space discretization, discretizing the x and v dimensions into 40 sections each. I also implemented TD(0), TD(lambda), and Experience Replay with stochastic gradient descent. It seems to be able to solve it and work.

What I'm doing is running many episodes for a given set of parameters, and plotting the number of steps each episode takes, so hopefully I can see that number decrease as in does more episodes and learns a better Q(S,A).

However, I was messing around with the parameters, seeing how they affected things, and I found something I can't explain. I had it working when I had the learning rate alpha = 0.05, but when I varied it, up to 1.0, I found that the steps per episode would decrease for all alpha values, but for the large ones, at some point just... skyrocket to the max (10,000) steps per episode, and stay there.

So for those alpha values, it would solve it, be improving, but then... something breaks. Here's an example:

https://imgur.com/DqJw5aP

alpha = 0.05 never has that weird jump (up to 300 episodes, anyway). The others, for increasing alpha, get it earlier and earlier. What could be going on? Why would it be converging and suddenly get ruined? thanks for any advice.

",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9i3ti9/seeing_a_strange_problem_with_the_learning_rate/
"""Zero-shot Sim-to-Real Transfer with Modular Priors"", Lee et al 2018",1537649906,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9i30nq/zeroshot_simtoreal_transfer_with_modular_priors/
Number of Stochastic Policies,1537636217,I understand that that the number of deterministic policies in a finite MDP (finite states/actions) is |A|\^^(|S|). But i don't know how many stochastic policies there are  in a finite MDP? Would there be infinite stochastic policies since there are infinite ways to make a distribution over actions.,reinforcementlearning,thboy8,False,/r/reinforcementlearning/comments/9i1ac9/number_of_stochastic_policies/
What is your review of David Silver's RL course?,1537624108,"Good enough for beginners? Any prerequisites / books, blogs that would help me understand the lectures better?",reinforcementlearning,l0gicbomb,False,/r/reinforcementlearning/comments/9hzsjf/what_is_your_review_of_david_silvers_rl_course/
No optimal policy,1537590126,When is it the case when there exists no optimal policy in an MDP (which may have infinite states/actions)? Do these MDPs exists with no optimal policy? Is there an example of an MDP that has no optimal policy?,reinforcementlearning,thboy8,False,/r/reinforcementlearning/comments/9hwzhg/no_optimal_policy/
eccv18-vlease Invited Talk slides: Learning dexterity by Peter Welinder (OpenAI),1537585187,[https://eccv18-vlease.github.io/static/slides/peter.pdf](https://eccv18-vlease.github.io/static/slides/peter.pdf),reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9hwht0/eccv18vlease_invited_talk_slides_learning/
Q-Learning Double DQN Prioritized Experience CNN to play game 2048,1537583224,"Hello guys, 

&amp;#x200B;

I am making a project of reinforcement learning that will play game 2048. I am using some techniques., like prioritized experience, Double DQN, calculating advantage to stay in state or take a action. My inspirations of reinforcement learning is this notebook: [Notebook](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Dueling%20Double%20DQN%20with%20PER%20and%20fixed-q%20targets/Dueling%20Deep%20Q%20Learning%20with%20Doom%20(%2B%20double%20DQNs%20and%20Prioritized%20Experience%20Replay).ipynb) . And my inspiratio for network model is this: [model](https://github.com/navjindervirdee/2048-deep-reinforcement-learning/blob/master/Code/Train-2048.ipynb). 

However, my agent and model don't seems getting better.  All the same executions seems to stuck in the same performance. Never  exceeds   \~6k scores. This is pics about reward, loss, scores and episodes durations: [results](https://imgur.com/a/cmeaf6u)

&amp;#x200B;

And this is my respository in github: [repository](https://github.com/FelipeMarcelino/2048-DDQN-PER-Reinforcement-Learning) 

&amp;#x200B;

I transform matrix of numbers into two power matrix and then pass this power matrix to cnn and then to linear layer and output. My rewards is number of merges added to  new highest cell value in log\_2. If the highest is the same then reward is only numbers of merge. 

&amp;#x200B;

Someone has some insights which can help me get better results in this algorithm?  

&amp;#x200B;

Thanks!!

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,FelipwMarcelino,False,/r/reinforcementlearning/comments/9hwady/qlearning_double_dqn_prioritized_experience_cnn/
"""[R]"" Emergence of Scenario-Appropriate Collaborative Behaviors for Teams of Robotic Bodyguards",1537578893,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/9hvu74/r_emergence_of_scenarioappropriate_collaborative/
[R] Emergence of Scenario-Appropriate Collaborative Behaviors for Teams of Robotic Bodyguards,1537578822,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/9hvtwy/r_emergence_of_scenarioappropriate_collaborative/
Illustrated Guide to Deep Q-Network Architectures,1537548405,,reinforcementlearning,user180921,False,/r/reinforcementlearning/comments/9hrrua/illustrated_guide_to_deep_qnetwork_architectures/
[D][DL] Possibility and Existing work on Multiple Deep RL Models in one model ?,1537530049,"I was pondering over the possibility of using a single neural net model to represent different policies that might depend upon a certain subset of the state. What I mean to say is I want a model which on the basis of a certain subset of the state vector can augments its policy to work on the remaining states. (which can be trained on different reward functions for that state).

I know we can use multiple models for each type of state and put a finite state machine controller based on that subset of state , but I don't want to train multiple models and then switch models according to the state change. I want this knowledge or power to be inbuilt in the model.

To make more clear.
Suppose my state vector is 
```
S : [s1,s2,s3,s4,s5,s6,s7,s8]
```
And this state vector my subset of S lets call is `S' =  [s1,s2,s3]` can possibly represent a policy that is to be followed which should act on the remaining state slots.
i.e.say for
`S' = [1,0,0]` the policy should only try to act or change state slots `[s4,s6,s8]`
`S' = [0,1,0]` the policy should only try to act or change state slots `[s5,s6]`
`S' = [0,0,1]` the policy should only try to act or change state slots `[s5,s6,s7]`

I hope I am able to convey my problem and doubt correctly.

",reinforcementlearning,intergalactic_robot,False,/r/reinforcementlearning/comments/9hovjl/ddl_possibility_and_existing_work_on_multiple/
High-quality Python implementation for Bayesian Inverse Reinforcement Learning papers?,1537528936,"Are there Python implementations for the various Bayesian IRL papers, starting with the first Ramachandran and Amir paper itself?
If not, what would be a fast and efficient roadmap to create a repo on top of other libraries? (I am not familiar with general Bayesian inference modules in Python.)",reinforcementlearning,banksyb00mb00m,False,/r/reinforcementlearning/comments/9hoq6e/highquality_python_implementation_for_bayesian/
Keep it stupid simple.,1537514235,"[https://arxiv.org/pdf/1809.03406.pdf](https://arxiv.org/pdf/1809.03406.pdf)

hierarchical RL work from CMU

**# Abstract**

Deep reinforcement learning can match and exceed human performance, but if even **minor** changes are introduced to the environment artificial networks often **can’t** **adapt**. 

Humans meanwhile are quite **adaptable**. 

We hypothesize that this is partly because of **how** humans use **heuristics**, and partly because humans can **imagine** new and more challenging **environments** to **learn** **from**.

 We’ve developed a model of **hierarchical** reinforcement learning that **combines** both these elements into a **stumbler**\-**strategist** **network**. 

We test transfer performance of this network using Wythoff’s game, a gridworld environment with a known optimal strategy. 

We show that combining imagined play with a heuristic–labeling each position as “good” or “bad”–both accelerates learning and promotes transfer to novel games, while also improving model interpretability.",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9hn2d4/keep_it_stupid_simple/
"""Benchmarking Reinforcement Learning Algorithms on Real-World Robots"", Mahmood et al 2018",1537495518,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9hl7pq/benchmarking_reinforcement_learning_algorithms_on/
TStarBots: Defeating the Cheating Level Builtin AI in StarCraft II in the Full Game,1537428842,[https://arxiv.org/abs/1809.05214](https://arxiv.org/abs/1809.05214),reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9hda6w/tstarbots_defeating_the_cheating_level_builtin_ai/
"""Sampled Policy Gradient (SPG) for Learning to Play the Game Agar.io"" - SPG outperforms DPG in this environment",1537386621,,reinforcementlearning,Antonenanenas,False,/r/reinforcementlearning/comments/9h8euy/sampled_policy_gradient_spg_for_learning_to_play/
"""[R]"" "" Sampled Policy Gradient for Learning to Play the Game Agar.io""",1537386024,,reinforcementlearning,Antonenanenas,False,/r/reinforcementlearning/comments/9h8bml/r_sampled_policy_gradient_for_learning_to_play/
An article I wrote about how deep reinforcement learning isn't a silver bullet for trading,1537373589,,reinforcementlearning,llens2017,False,/r/reinforcementlearning/comments/9h6iu0/an_article_i_wrote_about_how_deep_reinforcement/
Principles of Deep RL by David Silver,1537370055,"[http://www.deeplearningindaba.com/uploads/1/0/2/6/102657286/principles\_of\_deep\_rl.pdf](http://www.deeplearningindaba.com/uploads/1/0/2/6/102657286/principles_of_deep_rl.pdf)

informative. ",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9h60h2/principles_of_deep_rl_by_david_silver/
"""Deterministic Implementations for Reproducibility in Deep Reinforcement Learning"", Nagarajan et al 2018 [nondeterminism/high performance variance caused by all of: GPU nondeterminism, minibatch sampling, NN initialization, and exploration]",1537293997,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9gx7zm/deterministic_implementations_for_reproducibility/
"""MB-MPO: Model-Based Reinforcement Learning via Meta-Policy Optimization"", Clavera et al 2018 [fixing exploitation of environment models with bootstrapped ensembling during imaginary training and MAML meta-learning at real runtime]",1537286843,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9gw5w1/mbmpo_modelbased_reinforcement_learning_via/
"Handling noise in GP-based hyperparameter optimization: ""Constrained Bayesian Optimization with Noisy Experiments"", Letham et al 2018 [tuning HHVM compiler settings, ranking systems] {FB}",1537285927,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9gw0tx/handling_noise_in_gpbased_hyperparameter/
Restoring Environment Issue.,1537257762,"How do you restore environment during training RL agent?   


As same states are generated by the random seed, I think it will affect the learning process.   
How do you think so? ",reinforcementlearning,whikwon,False,/r/reinforcementlearning/comments/9gsv7f/restoring_environment_issue/
"after a long time, the bone just wants to fall back٩😩ི۶",1537249780,,reinforcementlearning,wuweijia1994,False,/r/reinforcementlearning/comments/9gs5lo/after_a_long_time_the_bone_just_wants_to_fall/
Developing a stock trading setup using best combination of indicators,1537237663,"I have an idea for a project. 

I want to develop a system that uses the historical price of a particular stock and uses the best combination of various indicators (like moving averages, Stochastic, RSI,etc.).

This final combination will be decided by the system itself after having tried every possible combination of all the indicators available. The system will choose the best combination of indicators out of all the possible combinations. This will eliminate manual tuning of parameters and enable the user to directly have the best setup instead of trying multiple combinations himself/herself.

Can anyone suggest me a step-by-step procedure to follow in order to successfully implement this project. Thank you. ",reinforcementlearning,ronith_sinha,False,/r/reinforcementlearning/comments/9gqtrg/developing_a_stock_trading_setup_using_best/
What environment do you use for RL applications?,1537236323,"Hello all, 

First to explain my use case: I am not planning on writing new RL algorithms but rather look into doing research in novel applications of RL. Thus, I need a pretty extendable but also reliable framework.

I have been using [Keras-RL](https://github.com/keras-rl/keras-rl) and pretty much enjoyed the experience however I wanted to have PPO implementation.

I switched to [tensorforce](https://github.com/reinforceio/tensorforce), while pretty impressive in scope and detailed implementation their documentation is not yet clean and you have to create your architecture using their GIN, dictionary like specification, which is limited. (It is difficult to pass in arbitrary networks which I am very much interested in.)  
[Google Dopamine](https://github.com/google/dopamine) looks really cool, however it is a bit too new and doesn't have PPO.

I thought about working with straight [GYM](https://gym.openai.com/) but honestly, not confident that I can implement all the algorithms I want, bug free and performantly, and to be honest, don't want to spend the time.

While researching for this post I came across [stable baselines based on openai](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html). Anyone used this one?

&amp;#x200B;

Thank you!",reinforcementlearning,LupusPrudens,False,/r/reinforcementlearning/comments/9gqnst/what_environment_do_you_use_for_rl_applications/
Looking for Explanation of Figure 2 in PPO,1537234725,"I understand that as you move from left to right on the plot we are interpolating between the old and the new policy. However, beyond that I'm not sure what it's saying. 

&amp;#x200B;

My best guess is that it's arguing that the surrogate minimum clipped objective is easier to optimize.",reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9gqgln/looking_for_explanation_of_figure_2_in_ppo/
How are Contextual Bandits different from TD(1)?,1537211662,Title,reinforcementlearning,IliliIIliIillliI,False,/r/reinforcementlearning/comments/9gnbyq/how_are_contextual_bandits_different_from_td1/
Why is Q-learning considered an off-policy algorithm?,1537191127,"Can someone explain why Q-learning is considered to be an off policy algorithm?

From what I've read on the internet, I understand that since we always select the action with the highest Q-value for bootstrapping, it is considered to be off-policy, because the policy could be anything. But if our policy was to select the action with the highest Q-value (greedy), then wouldn't Q-learning become on-policy?",reinforcementlearning,shubhamjha97,False,/r/reinforcementlearning/comments/9gkc48/why_is_qlearning_considered_an_offpolicy_algorithm/
Testing RL algorithms on more complex environments,1537182677,"We're all familiar with OpenAI Gym but it seems to be more common place for single agent RL games, but is there a similar kind of collection for more complex environments or Multi-Agent ones? 

There are papers which publish results from Starcraft, Minecraft, etc. but I have no way of verifying their results or testing my own theories either. 

",reinforcementlearning,hobbesfanclub,False,/r/reinforcementlearning/comments/9gje1a/testing_rl_algorithms_on_more_complex_environments/
"""Searching Toward Pareto-Optimal Device-Aware Neural Architectures"", Cheng et al 2018 [MONAS/DPP-Net]",1537139705,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9gf5c4/searching_toward_paretooptimal_deviceaware_neural/
"Nvidia NTECH September 2018 talk, by Ilya Sutskever [on DRL, OA5, and safety]",1537117839,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9gc84e/nvidia_ntech_september_2018_talk_by_ilya/
What professor/PhD student/researchers to follow for cutting edge Reinforcement learning research papers?,1537077408,There are way too many papers getting added in arxiv and top conferences these days. I would like to put google scholar follow notifications for people whom I should absolutely follow to keep myself of the ongoing RL research. But who are the people I should follow? ,reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/9g8e4j/what_professorphd_studentresearchers_to_follow/
TreeQN &amp; ATreeC Summary,1536978125,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9fy0f7/treeqn_atreec_summary/
Code review: REINFORCE with baseline (Keras),1536918067,"I am trying to train an agent for the [OpenAI Gym LunarLander-v2 environment](https://gym.openai.com/envs/LunarLander-v2/) using Python and Keras. I attempted to implement the REINFORCE algorithm with baseline from Sutton &amp; Barto:

https://i.redd.it/yh4tf2wtb6m11.png

My main confusion is with the gradients. Does training the neural network deal with the gradients automatically, or does an explicit gradient have to be calculated as part of the target data? See the targets used for training the state-value and main network.

The agent that I implemented does pretty well, I get much better scores than with my Deep Q-learning implementations.

I would appreciate some feedback on the algorithm. Have I implemented it correctly? Am I messing up by not calculating gradients when updating my neural networks?

[CODE](https://pastebin.com/zi8Tmm85)",reinforcementlearning,SuspiciousMariachi,False,/r/reinforcementlearning/comments/9fqm8b/code_review_reinforce_with_baseline_keras/
StarCraft 2 AI Tournamnet. Titan X up for grabs,1536901268,,reinforcementlearning,Turings_Ego,False,/r/reinforcementlearning/comments/9fp8nc/starcraft_2_ai_tournamnet_titan_x_up_for_grabs/
Custom OpenAI gym environment,1536889231,"I am beginner to Open AI gym.I tried it a few examples of some predefined environments present like Cartpole.

I wanted to try and write my own environment with the help of the this post:
https://stackoverflow.com/questions/45068568/is-it-possible-to-create-a-new-gym-environment-in-openai

I was following Martin Thoma’s answer. I tried running his environment.I cloned the banana-gym repo on my system and have gym installed..when I try doing gym.make(‘Banana-v0’) , I get no registered env with id: ‘Banana-v0’.

I would really like to have more detailed steps so a novice like me could follow it too.If anyone has any experience with this please let me know!
Thanks!",reinforcementlearning,ps2046,False,/r/reinforcementlearning/comments/9fnxkw/custom_openai_gym_environment/
"""Searching for Efficient Multi-Scale Architectures for Dense Image Prediction"", Chen et al 2018 {Google} [random NAS for semantic segmentation architectures]",1536854231,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9fj7gu/searching_for_efficient_multiscale_architectures/
Which are some good RL papers in finance?,1536843951,"Ex. This is one on Portfolio management

[https://arxiv.org/abs/1706.10059](https://arxiv.org/abs/1706.10059)",reinforcementlearning,pratikbhavsar,False,/r/reinforcementlearning/comments/9fht1e/which_are_some_good_rl_papers_in_finance/
"""Multi-task Deep Reinforcement Learning with PopArt [reward normalization+Impala]"", Hessel et al 2018 [median human performance on 57 ALE tasks w/1 NN]",1536801923,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9fdp1w/multitask_deep_reinforcement_learning_with_popart/
"""Solving Imperfect-Information Games via Discounted Regret Minimization"", Brown &amp; Sandholm 2018 [CFR]",1536770126,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f9cgs/solving_imperfectinformation_games_via_discounted/
"""PRIMAL: Pathfinding via Reinforcement and Imitation Multi-Agent Learning"", Sartoretti et al 2018 [large-scale decentralized robot warehouse navigation by learning to imitate model-based planners]",1536769061,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f96x4/primal_pathfinding_via_reinforcement_and/
"""VPE: Variational Policy Embedding for Transfer Reinforcement Learning"", Arnekvist et al 2018",1536766343,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f8sep/vpe_variational_policy_embedding_for_transfer/
"""Massively Parallel Dynamic Programming on Trees"", Bateni et al 2018 {GB}",1536765537,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f8nyy/massively_parallel_dynamic_programming_on_trees/
GitHub - lezwon/DeepGamingAI_FIFARL: Using Reinforcement Learning to play FIFA,1536758716,,reinforcementlearning,lezwon,False,/r/reinforcementlearning/comments/9f7q49/github_lezwondeepgamingai_fifarl_using/
Do policy gradient methods follow the Generalized Policy Iteration?,1536750215,"Do policy gradient methods (eg. REINFORCE) follow the Generalized Policy Iteration?

As per my understanding, there's no Policy Evaluation step in REINFORCE, we directly optimize the policy.",reinforcementlearning,shubhamjha97,False,/r/reinforcementlearning/comments/9f6py9/do_policy_gradient_methods_follow_the_generalized/
Is HER can be implemented in any problems?,1536747335,"As I understand, HER replace the goal to virtual goal S_T which stored in Experience Replay that the last state is S_T, so agent could learn how to achieve the state S_T.
But what if S_T can’t be replaced to virtual goal? For instance, in puck-pushing environment, if the robot failed to push the puck on the right position, HER can replace the goal to the position of the puck where the robot accidentally pushed, so the robot could learn how to push the puck to that position. But what if the robot even failed to push the puck? We don’t want robots to learn how not to push the puck. This is the part I was wonder. Is HER only can be implemented when the last state(which could be gained by exploration) can be replaced to a goal state?",reinforcementlearning,jinPrelude,False,/r/reinforcementlearning/comments/9f6g4t/is_her_can_be_implemented_in_any_problems/
Actor Critic DRL for continuous action space,1536737586,"I am trying to implement an a3c algorithm for a problem having a continuous actionspace in tensorflow. As network I am trying to build a CNN  inputting the gray scale pixel matrix of my ennvironment. 
I am having a hard time implementing it and I am getting stuck. I can only find references for 2d environments. I am working on a 3d environment. 
Would help a lot if anyone has a reference I could learn from :)) 

This is my first post. Sorry if I missed something regarding the rules :)",reinforcementlearning,dml_21,False,/r/reinforcementlearning/comments/9f5n7d/actor_critic_drl_for_continuous_action_space/
How to deal with a Q-learning scenario where one wrong move spells defeat?,1536732870,"My Q-learning basically tries to survive 1000 steps without going into the danger zone, but the issue is that the random exploration keeps putting it there, and I'm having a hard time seeing when the agent has successfully learned.

i set epsilon = current_iteration/max_iterations so it performs better as it reaches the end of the simulation, but that's true if i have 100 iterations or 100,000,000",reinforcementlearning,notaninja4375,False,/r/reinforcementlearning/comments/9f5896/how_to_deal_with_a_qlearning_scenario_where_one/
Train Donkey Car in Unity Simulator with Reinforcement Learning,1536730655,,reinforcementlearning,baylearn,False,/r/reinforcementlearning/comments/9f51bb/train_donkey_car_in_unity_simulator_with/
"""The use of embeddings in OpenAI Five for DoTA2"" --Tambet Matiisen",1536708543,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f2hoa/the_use_of_embeddings_in_openai_five_for_dota2/
"""Model-Based Stabilisation of Deep Reinforcement Learning"", Liebfried et al 2018 {PROWLER.io}",1536692144,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0aqi/modelbased_stabilisation_of_deep_reinforcement/
"""Addressing Sample Inefficiency and Reward Bias in Inverse Reinforcement Learning"", Kostrikov et al 2018 {GB} [GAIL]",1536692124,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0amw/addressing_sample_inefficiency_and_reward_bias_in/
"""Addressing Function Approximation Error in Actor-Critic Methods"", Fujimoto et al 2018 [min-DDQN/Twin Delayed Deep Deterministic policy gradient algorithm (TD3)]",1536692105,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0aj7/addressing_function_approximation_error_in/
"""Efficient Counterfactual Learning from Bandit Feedback"", Narita et al 2018 {CyberAgent/Cygames}",1536692085,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0agb/efficient_counterfactual_learning_from_bandit/
"""Challenges of Context and Time in Reinforcement Learning: Introducing Space Fortress [Gym] as a Benchmark"", Agarwal et al 2018",1536692052,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0aav/challenges_of_context_and_time_in_reinforcement/
"""Accelerated Reinforcement Learning for Sentence Generation by Vocabulary Prediction"", Hashimoto &amp; Tsuruoka 2018",1536692027,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0a6w/accelerated_reinforcement_learning_for_sentence/
"""Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning"", Zahavy et al 2018",1536692005,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9f0a3g/learn_what_not_to_learn_action_elimination_with/
Does td error anneal to zero when converges?,1536683247,"In actor-critic, I get td errors to update the actor, does td error anneal to zero when the model converges?
",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9ez2aj/does_td_error_anneal_to_zero_when_converges/
The Bottleneck Simulator: A Model-based Deep Reinforcement Learning Approach,1536675236,,reinforcementlearning,SaveUser,False,/r/reinforcementlearning/comments/9exwvt/the_bottleneck_simulator_a_modelbased_deep/
What algorithm do we use when using a neural network as our function approximator?,1536647263,"I just started with the Reinforcement lecture series by David Silver.

[https://youtu.be/UoPei5o4fps?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;t=4259](https://youtu.be/UoPei5o4fps?list=PLzuuYNsE1EZAXYR4FJ75jcJseBmo4KQ9-&amp;t=4259)

At this frame we can see that for non linear function approximator we cannot use any of the listed methods. How do neural network function approximaotrs and deep q learning work then?",reinforcementlearning,kj_venom11,False,/r/reinforcementlearning/comments/9ev3oo/what_algorithm_do_we_use_when_using_a_neural/
Notes from the ai.x 2018 Conference: Faster Reinforcement Learning via Transfer (John Schulman),1536643625,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/9eurtq/notes_from_the_aix_2018_conference_faster/
[R][P] Unity: A General Platform for Intelligent Agents,1536640390,,reinforcementlearning,hardmaru,False,/r/reinforcementlearning/comments/9eufzn/rp_unity_a_general_platform_for_intelligent_agents/
"""Improving On-policy Learning with Statistical Reward Accumulation"", Deng et al 2018",1536611634,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9eqtl9/improving_onpolicy_learning_with_statistical/
"""Dense Object Nets (DON)"" for self-supervised learning of coordinate-mapping onto objects for general robot arm gripping",1536610983,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9eqq61/dense_object_nets_don_for_selfsupervised_learning/
When are activation functions used in Neural Networks?,1536561045,"So I've been reading up as many tutorials and papers as I can on NNs the last few days trying to get a better grasp on them, but I'm a little confused on when activation functions come into play.

My current understanding of how the process works is:

input x1,x2,x3 get multiplied by w1, w2, and w3 respectively as they fire out to a node in the next layer and if the sum of all three of these is greater than or equal to the bias of the next node, then that node will fire as well, else it's treated as a 0.

Does the activation function apply as soon as it leaves the node or directly after summing all the inputs multiplied by their respective weights.",reinforcementlearning,justking14,False,/r/reinforcementlearning/comments/9ekf0n/when_are_activation_functions_used_in_neural/
Teaching a bot to play tic tac toe in pure numpy.,1536560781,,reinforcementlearning,ltbringer,False,/r/reinforcementlearning/comments/9eke10/teaching_a_bot_to_play_tic_tac_toe_in_pure_numpy/
Recurrent World Models Facilitate Policy Evolution,1536553300,,reinforcementlearning,CartPole,False,/r/reinforcementlearning/comments/9ejlv1/recurrent_world_models_facilitate_policy_evolution/
"""Computational mechanisms of curiosity and goal-directed exploration"", Schwartenbeck et al 2018 [Friston free-energy formulation of exploration]",1536354093,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9dyitg/computational_mechanisms_of_curiosity_and/
"""ARCHER: Aggressive Rewards to Counter bias in Hindsight Experience Replay"", Lanka &amp; Wu 2018",1536343597,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9dwwng/archer_aggressive_rewards_to_counter_bias_in/
"""Dreaming about Driving"" --Wayve [world models]",1536343509,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9dww38/dreaming_about_driving_wayve_world_models/
Is it mandatory to have several parallel environments when using PPO ?,1536320370,"Hello, 

&amp;#x200B;

I'm wondering whether having several environments is mandatory to train a successful policy when using PPO ? Couldn't one generate as much experience with a single environment, providing longer sequences ? 

&amp;#x200B;

Thanks ! ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/9dtiwt/is_it_mandatory_to_have_several_parallel/
"Is there a ""good time"" to start learning RL?",1536291416,"Just about 3 months ago I completed the DL specialization on Coursera. I've completed reading and implementing the 'Hand on ML with scikit-learn and tensorflow' book by Aurelien Geron. Currently I am exploring Convnet architectures and reading papers about the same. 

For some reason, I don't like Deep Learning. I don't like how we train layers and layers of \[linear\_function + activation\] to fit any dataset we want. I think I'm gonna switch to RL and explore that now. 

Is this the correct time to do RL? or should I learn Convnets, other DL methods before hopping on to RL?",reinforcementlearning,l0gicbomb,False,/r/reinforcementlearning/comments/9dqgba/is_there_a_good_time_to_start_learning_rl/
What is the requirements for doing master's under University of Alberta's RLAI lab(sutton etc),1536288027,I'm from non CS background so I thought I could better equip myself before applying for masters. Like do I need to publish any paper or present at a conference or something like that.,reinforcementlearning,tameddemon,False,/r/reinforcementlearning/comments/9dq16m/what_is_the_requirements_for_doing_masters_under/
"""Visual Reinforcement Learning with Imagined Goals"", Nair &amp; Pong 2018 {BAIR}",1536287006,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9dpwa9/visual_reinforcement_learning_with_imagined_goals/
"""A (Long) Peek into Reinforcement Learning""",1536283301,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9dpdwc/a_long_peek_into_reinforcement_learning/
"Having trouble with using SARSA for a puzzle game, any advice? Conceptual question",1536268857,"Hi, I'm trying to solve the egg drop puzzle, [which is described here](https://www.geeksforgeeks.org/egg-dropping-puzzle-dp-11/). I've watched the David Silver RL lessons up to lecture 5, and I'm trying to solve this puzzle using  SARSA with TD(lambda) and model free control, the way he (and Sutton and Barto) show the algorithm. I think I've almost got it but I must be missing some detail and I was hoping someone could point me in the right direction. 

I'm doing on-policy learning, where I generate an episode by choosing a number between 1 and N (# of total floors) for the ""break floor"" to be on, and then have the agent use its current policy to get the next action (using epsilon-greedy), and then update Q and E (my eligibility trace matrices) after each time step.

Here's what I'm doing (I'll try to describe without any code). For states, I always have 2 terminal states, the ""solved"" state, and the ""broke both eggs"" state. In addition, I'm using N = 10 for now, so I say there are 2*N = 20 non-terminal states, because at any point during the puzzle you can have up to N floors left to search, and you can have either 1 (1e) or 2 eggs (2e) left. So for N = 10, I have 22 states.

For actions, I say you can drop the egg from any floor, 1 to N, so for N = 10, Q and E are 22 x 10 matrices.

To define the rewards and next state, I have a performAction(state, action) function. The gist of it is this. The break floor (bf) is randomly chosen at the beginning of the episode. I'll call the action d, where it's the floor I'm dropping from for that action. There are two possibilities for a given action, the egg breaks (d &gt;= bf), or it doesn't (d &lt; bf). If it does, then if it was a 1e state, it returns a reward of -1000 and a next state of the ""both broken"" state. If it broke in a 2e state, then it gets a reward of -1 and the next state is the state with 1e and d floors left (because you know it breaks on floor d now, so it only has to search up to floor d).

If doesn't break, the reward is -1 in either case (1e or 2e), and it goes to the state with the same number of eggs, but now only (f - d) floors left to search (if it didn't break dropping from floor 7 out of 10, you know there are only 10-7 floors left to search).

The rest is pretty much what is described [here](https://www.youtube.com/watch?v=_dLeD-_h3e4&amp;t=47m0s) in David Silver's lecture. I take the action, get the next reward and state, choose the next e-greedy action, and then calculate the TD error, and update E and Q.

Here's where I'm stuck. When I run it on lots of episodes, it correctly learns what to do for 1e states (drop from floor 1 for each state, otherwise it risks breaking the only remaining egg and getting the huge penalty). However, the 2e states never learn, even with ~10^5 episodes. They seem to just pick a random action for each 2e state and that stays the best one.

I think I might even know what's happening. If Q is correct for the 1e states, then if, initially, when it's still trying more randomness from the e-greedy, it might try some non-optimal action for a 2e state like (2e, 10 floors) (the initial state), dropping it on floor 8. If it gets lucky and actually solves the puzzle after that initial move, then the state action pair ((2e, 10f), drop 8) will end up with a relatively good Q value (just -1 for each drop until the terminal solved state). On the other hand, better actions from that same state, like drop 3, might get unlucky after that move, and then break both eggs and get punished badly with that -1000. Then, it will basically never get chosen again by the e-greedy because it will almost always choose the drop 8 that initially got lucky. The fact that the drop 8 is slightly inferior to the drop 3 won't matter because even if the drop 3 gets chosen randomly again, it won't change its Q value very much.

So I think that's what's wrong, but I'm not sure how to change it. I tried decreasing that -1000 to -100 and -20, but it doesn't seem to change much, and besides, it really seems like you should be able to give some state-action pair a really bad consequence. My other parameters are lambda = 0.9, alpha = 0.5, gamma = 0.9, and epsilon starts as 0.5 and then decreases by 1% each episode (though I've tried lots of combos of them and nothing helps.

Any tips or help would be greatly appreciated, if you have any ideas. Thank you!",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/9dn8m6/having_trouble_with_using_sarsa_for_a_puzzle_game/
Introduction to Reinforcement Learning using MXNet 🤖 • r/MachineLearning,1536264005,,reinforcementlearning,tomtx0,False,/r/reinforcementlearning/comments/9dmfep/introduction_to_reinforcement_learning_using/
"Did someone check Siraj Raval's Move37 curriculum, what do you think of it?",1536244892,"Newbie to RL. I am gonna start with that course, is it any good?

Link: [https://www.theschool.ai/courses/move-37](https://www.theschool.ai/courses/move-37)

Also, any other supplementary material like books, blogs you'd recommend? 

Thanks!",reinforcementlearning,l0gicbomb,False,/r/reinforcementlearning/comments/9dj9mx/did_someone_check_siraj_ravals_move37_curriculum/
Doubt: Why is DDPG off-policy? Is it because they add a normal noise to the actions chosen by the current policy unlike the DPG algorithm that uses importance sampling?,1536232402,,reinforcementlearning,utsavsing,False,/r/reinforcementlearning/comments/9dhga2/doubt_why_is_ddpg_offpolicy_is_it_because_they/
Best algorithm today for multi-agent hierarchical RL with continuous state and action spaces?,1536231091,"It's a pretty specific domain but I know of algorithms which are very good in only one of those things, 

Multi-agent - MADDPG 
Hierarchical - Hierarchical Actor Critic 
Continuous State and Action Spaces - Proximal Policy Optimization 

There are many others but yeah, only for those specific domains but not combined. 

I can't find anything that's solid for the case I mentioned. Do you guys know an existing paper which addresses everything?


",reinforcementlearning,shura04,False,/r/reinforcementlearning/comments/9dhape/best_algorithm_today_for_multiagent_hierarchical/
Why are Gradient TD methods not used in Deep RL?,1536228901,"In 2009, Sutton's group published ""Convergent temporal-difference learning with arbitrary smooth function approximation"", which described ""true"" gradient descent variants of TD learning (normally, you don't backpropagate through the next-state value estimate, making conventional TD(0) a semi-gradient method).

Those variants are GTD (Gradient Temporal Differences), GTD2 (v2 of GTD), and TDC (TD with gradient Corrections), and the paper proved convergence even in the off-policy case with neural networks.

But I'm unable to find any studies that apply gradient TD methods to neural networks in modern Deep RL. Are there issues with convergence speed? Unscalable computation? Why are we still stabilizing off-policy TD with target networks?",reinforcementlearning,TD-lambda,False,/r/reinforcementlearning/comments/9dh1th/why_are_gradient_td_methods_not_used_in_deep_rl/
DQN: can I reuse previous data,1536218875,"Hi everyone !

I am playing with Deep Q Network and I am not very good and as time and experiments pass, I wonder:
Is it possible to reuse my previously generated episodes to train my new networks ?

It seem that there is no information out there about this. Did you tried this ? Did it work ? Can it work with other algorithms ? ",reinforcementlearning,frenchytrendy,False,/r/reinforcementlearning/comments/9dfzs3/dqn_can_i_reuse_previous_data/
World model talk by David Ha,1536213843,[https://www.facebook.com/SKTBrain/videos/723182251361194/](https://www.facebook.com/SKTBrain/videos/723182251361194/),reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9dffoi/world_model_talk_by_david_ha/
"Short history of OpenAI's DoTA2 research: hand-written rules -&gt; domain randomization -&gt; 1x1 -&gt; 5x5; OA5 ""will compete in a full Dota 2 match with all Heroes either later this year or in 2019""",1536202503,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9de1ye/short_history_of_openais_dota2_research/
Gotta Learn Fast: A New Benchmark for Generalization in RL,1536201766,"[https://arxiv.org/pdf/1804.03720.pdf](https://arxiv.org/pdf/1804.03720.pdf)

from OpenAI",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9ddybt/gotta_learn_fast_a_new_benchmark_for/
Oscillating total reward when using Policy Gradients,1536171067,"So, I've been trying to train an agent to play the LunarLander environment present in OpenAI Gym using the REINFORCE algorithm (MC Policy Gradients), but I'm facing the issue that the total reward oscillates. Here's an example- 

https://i.redd.it/s0pz3toqngk11.png

Can someone help me figure out the cause of such behaviour? ",reinforcementlearning,shubhamjha97,False,/r/reinforcementlearning/comments/9d98f2/oscillating_total_reward_when_using_policy/
"[D] Interactive Connect Four game, powered by a TensorFlow AlphaZero server-side",1536162497,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9d7ssn/d_interactive_connect_four_game_powered_by_a/
When SARSA is better than Expected SARSA?,1536156602,"Hi! As I know that the main reason for using Expected SARSA instead of SARSA when it is possible is to reduce the double stochastity of SARSA. Moreover the variance of traditional SARSA is larger than expected SARSA
But when do we need to use use traditional SARSA? Maybe it is related to the parameter w or to the state/action space?",reinforcementlearning,zkid18,False,/r/reinforcementlearning/comments/9d6wct/when_sarsa_is_better_than_expected_sarsa/
"Looking for implementations for this RL Paper - FFNet: Video Fast-Forwarding via Reinforcement Learning. I just started reading RL literature and almost complete with the basics (MDP, Q-Learning, Policy Gradient). Also implemented some OpenAI Gym ATARI games.",1536137016," Now, I am looking for some implementations from other research papers so that I could explore some areas which are not games. 

For this particular reason, I chose this paper which has is related to videos and more on the Q-Learning. ",reinforcementlearning,phoenixlads,False,/r/reinforcementlearning/comments/9d4jrt/looking_for_implementations_for_this_rl_paper/
Require a suggestion for passing state for robotics environment in gym environments,1536135043,"When using the FetchPickAndPlace robotics environment in openai gym, I want to work with more than one block present where the target is still to pick one of the blocks and take it to the target position. What is the state that I should pass to the algorithm for training? Currently the state that is passed(in case of a single block present) is: gripper location, object position, relative position of the object wrt the gripper and linear and angular velocities of the objects and the joints of the robot. How do I modify the state so that I pass the extra block location information too along with the original information. Any suggestions are welcome. Thank you.",reinforcementlearning,utsavsing,False,/r/reinforcementlearning/comments/9d4d0g/require_a_suggestion_for_passing_state_for/
"""Single reach plans in dorsal premotor cortex during a two-target task"", Dekleva et al 2018 [""Our results suggest that premotor cortex plans only one option at a time""]",1536089610,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9cyx39/single_reach_plans_in_dorsal_premotor_cortex/
"""Gibson Env: Real-World Perception for Embodied Agents"", Xia et al 2018 [creating large-scale real-world visual environment simulations from panorama photographs of ""1400 floor spaces from 572 full buildings"" for sim2real training]",1536089332,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9cyvo9/gibson_env_realworld_perception_for_embodied/
Grad schools with good programs for someone interested in RL,1536078806,"I know that RL is popular at Ualberta and Umass Armhest. 

I'd be grateful for other recommendations. (Does not necessarily have to be in U.S or Canada)

thanks!",reinforcementlearning,Karmaflax,False,/r/reinforcementlearning/comments/9cx9xw/grad_schools_with_good_programs_for_someone/
"""Aurora’s Approach to Self-Driving Car Development"": 'Testing is the first step of reinforcement learning'",1536075224,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9cwp60/auroras_approach_to_selfdriving_car_development/
[D] crypto market and reinforcement learning,1536059134,"Hi all. As a pet project I'm trying to find out if it is possible to create some kind of reinforcement learning algorithm to trade on btc-eur pair (I wanted to try and why not crypto, good as any game solving). 
The main idea was to generate data which would be used to learn some AI agent. And then repeat the process and pitch data generator vs classifier to gain ""better"" data and train a better AI agent.
My approach is something like this:
- have some artificial agent which trades with btc-eur
- he has 3 possible actions: Buy(2), Sell(1), Hold(0)
- have a GA(genetic alg) to generate actions data. Population are artificial agents. Initial actions are randomly generated. Each iteration then does crossover and mutation over this action array.
- Fitness is computed return of agent (return(t)/investment -1) vs return of buy and hold (trying to find a better ""strategy"" than b&amp;h)
- have state as rolling pct, lower boilinger pct,  mean pct, median pct, upper boilinger, where pct is percentual price change (price(t)/price(t-1) -1)
- use this state as input to random forrest classifier (tried others from sklearn, but i kinda like random forrest - cheap and ""good"" results), with labels as actions array - which I got from the GA
- repeat GA but fitness is calculated against return of classifier
- repeat classifier training in loop
- after some iterations choose best classifier based on test data interval (which is different from train)

I used pct as it is ""relative"" data and could be considered somehow more general then absolute price data. To be honest, I didn't believe the classifier would be able to classify these inputs to actions. But it did. I don't have a repeatable way and 
provable way to generate always ""good"" agents, which would perform pretty good on test data. I don't know if it even can be done(probably not, but why not try:D). What I have found is, when GA data generator was set on some difficult interval of price data, e.g. 01-10-2017 till 01-05-2018, it would generate agent which would trade good at test interval, e.g. 1-05-2018 till 24.08.2018. And also they would be transferable to other ""securities"", as ETFs. As I said it is a pet project for fun. The results are not ""good"" in matter of usefulness. I don't recon using my aproach for anything serious. I'm curious if anyone else is playing with this and what is his approach. 

I'm just suprised that in some (very) limited fashion it can work (in simulation environment:D). 

",reinforcementlearning,tekedozai,False,/r/reinforcementlearning/comments/9cultv/d_crypto_market_and_reinforcement_learning/
http://deepbayes.ru/,1535991023,"[https://www.youtube.com/playlist?list=PLe5rNUydzV9Q01vWCP9BV7NhJG3j7mz62 …](https://t.co/FRJR0Z93ni) super powerful Bayesian lectures from [~~#~~**deepbayes**](https://twitter.com/hashtag/deepbayes?src=hash) 

github: [https://github.com/bayesgroup/deepbayes-2018 …](https://t.co/DQpxSBFjXM) and 

homepage [http://deepbayes.ru](https://t.co/VTHcTd8Rpn) 

&amp;#x200B;",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9cmprr/httpdeepbayesru/
RL research topics that don't need large scale-computing,1535989211,"Many papers related to RL are focusing on using DL to train agents, which can be very time-consuming and take a lot of computational resources. Not everyone can offer the time and the CPU/GPUS, are there any research topics on RL that don't need large scale computing?
",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/9cmglm/rl_research_topics_that_dont_need_large/
SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning,1535989095,[https://sites.google.com/view/solar-iclips](https://sites.google.com/view/solar-iclips) probabilistic graphical model (PGM) structure based model-based rl,reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9cmfyw/solar_deep_structured_latent_representations_for/
Final Year Project,1535911802,"Hi everyone,

Could you guys float some cool ideas (application based if possible) for my FYP specifically involving multi-agent reinforcement leaning.

I have been studying game theory, finished David silvers reinforcement leaning course, and now reading upon some MARL and am struggling to think of a topic to implement. Some help would really be appreciated. ",reinforcementlearning,knifelyf,False,/r/reinforcementlearning/comments/9ce60w/final_year_project/
"BlueWhale: Facebook RL implementations in Pytorch/Caffe of DQN, DDPG, &amp; SARSA with export &amp; Gym support [deployed in FB production for ""Growth, Marketing, Network Optimization, and Story Ranking services""]",1535907991,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9cdnf4/bluewhale_facebook_rl_implementations_in/
"AlphaGo ""rollout policy network"" 3us evaluation time - how ?",1535898235,"This is a question about the alphaGo ""rollout policy network"". It is the light weight NN used during rollouts - and supposedly evaluated in 3micro-s (vs 4 milli-s for the full policy network).

&amp;#x200B;

Does anyone know if this is evaluated on the GPU or the CPU ? 

Seems a small NN which maybe is not worth the GP-CPU overhead ?

&amp;#x200B;

Also, does AlphaZero Go also use a light-weight rollout policy ?

&amp;#x200B;

Many Thanks

&amp;#x200B;",reinforcementlearning,temptempyahoo,False,/r/reinforcementlearning/comments/9cce03/alphago_rollout_policy_network_3us_evaluation/
Learn Deep Reinforcement Learning in 60 days,1535895079,,reinforcementlearning,andri27-ts,False,/r/reinforcementlearning/comments/9cc1v4/learn_deep_reinforcement_learning_in_60_days/
RL/ML for robotics?,1535884533,"I'm interested in research groups working on machine learning and robotics (with a focus on reinforcement learning) for postdoc/cooperation. Ideally with focus on industrial applications. Any leads?

I'm applied math PhD, working on ML for the past 4 years in industry, currently split between industry and academia. ",reinforcementlearning,moravak,False,/r/reinforcementlearning/comments/9cb65t/rlml_for_robotics/
"""Applying optimal control theory to complex epidemiological models to inform real-world disease management"", Bussell et al 2018 [uses BOCOP]",1535846449,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9c7u7d/applying_optimal_control_theory_to_complex/
CNN and DDPG,1535834409,"hey,

&amp;#x200B;

i wanna add a CNN network to my DDPG network, any idea how that should be done? also do you have some good example code from github that i can learn from?

&amp;#x200B;

thanks",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/9c6dd4/cnn_and_ddpg/
LOLA-DiCE and higher order gradients,1535816376,"The DiCE paper  ([https://arxiv.org/pdf/1802.05098.pdf](https://arxiv.org/pdf/1802.05098.pdf)) provides a nice way to extend stochastic computational graphs to higher-order gradients. However, then applied to LOLA-DiCE it does not seem to be used and the algorithm is limited to single order gradients, something that could have been done without DiCE.

Am I missing something here?",reinforcementlearning,lepton99,False,/r/reinforcementlearning/comments/9c3zgw/loladice_and_higher_order_gradients/
"""Approximate Exploration through State Abstraction"", Taïga et al 2018 {MILA/DM}",1535769388,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9bzopn/approximate_exploration_through_state_abstraction/
"""Learning End-to-end Autonomous Driving using Guided Auxiliary Supervision"", Mehta et al 2018 {Intel}",1535768283,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9bzkqf/learning_endtoend_autonomous_driving_using_guided/
"Dexterous Manipulation with Reinforcement Learning: Efficient, General, and Low-Cost {BAIR}",1535748172,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9bx8lu/dexterous_manipulation_with_reinforcement/
[P] AIToolbox: Tabular implementations of 30+ MDP and POMDP papers in C++,1535736500,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9bvlfj/p_aitoolbox_tabular_implementations_of_30_mdp_and/
Intel AI Lab releases a new version of Reinforcement Learning Coach,1535704957,,reinforcementlearning,itaicaspi,False,/r/reinforcementlearning/comments/9brxdh/intel_ai_lab_releases_a_new_version_of/
"""ExpIt-OOS: Towards Learning from Planning in Imperfect Information Games"", Kitchen &amp; Benedetti 2018 [expert iteration]",1535677869,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9bp9ei/expitoos_towards_learning_from_planning_in/
[D] Introducing a modification of an algorithm as extension or a new method?,1535677317,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/9bp72d/d_introducing_a_modification_of_an_algorithm_as/
"Ape-X implementation in Python Tensorflow by Felipe Such {Uber} ('This repo replicates the results Horgan et al obtained in ""Distributed Prioritized Experience Replay""')",1535662836,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9bnhrt/apex_implementation_in_python_tensorflow_by/
GAIL implementations?,1535659651,"I tried running the OpenAI baselines code for GAIL recently and was not able to reproduce the results mentioned for some base mujoco tasks. The discriminator seems to comfortably learn to classify the real and fake samples with 100% accuracy. Do we need to tweak the hyperparameters for this repository? Has anyone else had a similar experience?

I did try out the theano code for a long time. However, it is not suitable for some of the experiments I intend to do for my projects. Hence, the requirement for a PyTorch/Tensorflow version.

Anyone else has an implementation that I can use off the shelf for benchmarking standard mujoco environments? (I’m a bit time constrained, hence the requirement of an off-the-shelf implementation)

Thanks in advance!

",reinforcementlearning,rockermaxx,False,/r/reinforcementlearning/comments/9bn25u/gail_implementations/
Safe Reinforcement Learning - Approaches,1535630875,,reinforcementlearning,harshitsikchi,False,/r/reinforcementlearning/comments/9bj1np/safe_reinforcement_learning_approaches/
Research internship??,1535573236,"So I am a masters student in Germany working on reinforcement learning and was wondering how to get a research internship in any of the research groups. It's really hard to work on reinforcement learning in the industry.
Any pointers or sources would be great. Thanks!

https://github.com/navneet-nmk/pytorch-rl

",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/9bcr0a/research_internship/
Merge or Not? Learning to Group Faces via Imitation Learning,1535508282,,reinforcementlearning,ewanlee,False,/r/reinforcementlearning/comments/9b54o3/merge_or_not_learning_to_group_faces_via/
"""A Study of Reinforcement Learning for Neural Machine Translation"", Wu et al 2018 {MSR/SYU}",1535484536,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9b1y9i/a_study_of_reinforcement_learning_for_neural/
"""Improving Abstraction in Text Summarization"", Kryściński et al 2018",1535480748,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9b1eh8/improving_abstraction_in_text_summarization/
Google AI members released Dopamine - a research framework for fast prototyping of reinforcement learning algorithms,1535396048,"Google AI blog:

[https://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html](https://ai.googleblog.com/2018/08/introducing-new-framework-for-flexible.html)

git repo:

[https://github.com/google/dopamine](https://github.com/google/dopamine)

useful materials:

[https://github.com/google/dopamine/tree/master/docs#downloads](https://github.com/google/dopamine/tree/master/docs#downloads)

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,tigerneil,False,/r/reinforcementlearning/comments/9arosr/google_ai_members_released_dopamine_a_research/
"""LIFT: Reinforcement Learning in Computer Systems by Learning From Demonstrations"", Schaarschmidt et al 2018",1535332873,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9akssm/lift_reinforcement_learning_in_computer_systems/
Tuning parameters in Reinforcement Learning,1535210787,"Hello, I am a relative newbie in RL (as I started working on a RL project a month ago or so). I am using Keras to build my prototype and I need to do some serious parameter tuning because the results are really bad. However, I am unsure as to which framework/library to use. In a classical model, I look at the loss and try to minimize that. In my RL scenario, the loss is actually quite good, it even hits 0 often, but the accumulated reward is really bad, so I need some method that also keeps track of my reward and tries to maximize that in some way. Also, yes, running a normal gridsearch python script takes way too long to have an exhaustive parameter search.

​

I would appreciate some help, suggestions for any libraries I can use, some implementation methodology, or any constructive resources as to how to tune a RL model.",reinforcementlearning,steamingironkettle,False,/r/reinforcementlearning/comments/9a7k9c/tuning_parameters_in_reinforcement_learning/
Is it sensible to apply deep reinforcement learning to biological problems with &lt; 1 million samples?,1535168529,"I recently heard some people talking about applying deep reinforcement learning to biology. This seemed odd to me because I don't think biology has sample sizes close what is needed to get reinforcement learning to work well (my understanding of the sample inefficiency of reinforcement learning comes from this article: [https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)).  


There's a Schmidt Science Fellow (Dr. Peyton Greenside) in Emma Brunskill's group at Stanford who intends to apply deep reinforcement learning to design synthetic DNA sequences. In her introductory video, she listed autonomous vehicles as a ""popular"" application of reinforcement learning [https://schmidtsciencefellows.org/fellow/peyton-greenside/](https://schmidtsciencefellows.org/fellow/peyton-greenside/) (at around the 40 second mark in the linked video). I thought this was an odd statement, since according to the aforementioned blog post, leading robotics groups like Boston Dynamics don't use Deep RL and the sample inefficiency means that Deep RL is currently not something you'd want to use except when you can get tens of millions of datapoints (not the case in biology, outside of perhaps protein folding simulations). I also find it strange that Dr. Greenside doesn't intend to use domain knowledge because, in her words, the domain knowledge is ""biased"". Dr. Greenside does not appear to have any background in reinforcement learning, though she has some background in deep learning. I am also very unclear how she plans to design the RL agent and the reward function.  


Does this seem like a practical research direction, or just another case of falling for hype?",reinforcementlearning,leonidaskamal,False,/r/reinforcementlearning/comments/9a3xci/is_it_sensible_to_apply_deep_reinforcement/
[D] A History of Reinforcement Learning - Prof. A.G. Barto,1535144921,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/9a17pg/d_a_history_of_reinforcement_learning_prof_ag/
Tensor strided slice ?,1535124925,"hey when i have,

&amp;#x200B;

**s\_dim = 3**

then when i do:

**self.S = tf.placeholder(tf.float32, \[None, s\_dim\], 's')\[0\]**

and i print self.S i get

**Tensor(""strided\_slice:0"", shape=(3,), dtype=float32)**

&amp;#x200B;

but when i try the example code:

**s\_dim = 7**

then do:

**self.S = tf.placeholder(tf.float32, \[None, s\_dim\], 's')\[0\]**

and then it prints:

**Tensor(""s:0"", shape=(?, 3), dtype=float32)**

&amp;#x200B;

*why does that print so completly diffrent outcomes?*

&amp;#x200B;

more detail:

&amp;#x200B;

*mine:*

&amp;#x200B;

**s\_dim = 3** 

**a\_dim = 3**

**a\_bound = 3**

&amp;#x200B;

*example:*

&amp;#x200B;

**self.action\_space =** [**spaces.Box**](https://spaces.Box)**(low=-self.max\_torque, high=self.max\_torque)**

**self.observation\_space =** [**spaces.Box**](https://spaces.Box)**(low=-high, high=high)**

**high = np.array(\[1., 1., self.max\_speed\])**

&amp;#x200B;

**s\_dim = env.observation\_space.shape\[0\]**

**a\_dim = env.action\_space.shape\[0\]**

**a\_bound = env.action\_space.high**

&amp;#x200B;

**print(s\_dim) = 7**

**print(a\_dim) = 2**

**print(a\_bound) = 2**

&amp;#x200B;

i am aware that i am clearly doing something wrong, but what?

&amp;#x200B;

*main error:*

&amp;#x200B;

    Exception in thread Thread-1:
    Traceback (most recent call last):
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\threading.py"", line 916, in _bootstrap_inner
        self.run()
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\threading.py"", line 864, in run
        self._target(*self._args, **self._kwargs)
      File ""ddpg.py"", line 158, in startpoint
        ddpg = DDPG(a_dim, s_dim, a_bound)
      File ""ddpg.py"", line 52, in __init__
        self.a = self._build_a(self.S,)
      File ""ddpg.py"", line 104, in _build_a
        net = tf.layers.dense(s, 30, activation=tf.nn.relu, name='l1', trainable=trainable)
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\layers\core.py"", line 190, in dense
        return layer.apply(inputs)
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 774, in apply
        return self.__call__(inputs, *args, **kwargs)
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\layers\base.py"", line 329, in __call__
        outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 688, in __call__
        self._assert_input_compatibility(inputs)
      File ""C:\Users\Gebruiker\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 1409, in _assert_input_compatibility
        str(x.get_shape().as_list()))
    ValueError: Input 0 of layer l1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [3]

&amp;#x200B;

if you know the solution, let me know...

&amp;#x200B;

Thanks for reading.",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/99yf7p/tensor_strided_slice/
"OpenAI's OA5 vs pro DoTA2 matches at The International (TI) 2018: Results - human victories, 0/2",1535076547,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/99thy9/openais_oa5_vs_pro_dota2_matches_at_the/
"in Counterfactual Multi-Agent Policy Gradients, what is $u'^a$",1535036256,"in https://arxiv.org/abs/1705.08926, in equation (4), what is $u'^a$, I suppose it is other actions apart from $u^a$. ",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/99o2wb/in_counterfactual_multiagent_policy_gradients/
How Do You Deploy An RL Model?,1534985166,"I'm familiar with deploying supervised models in Python and R, but I've never seen deployment discussed in a reinforcement learning context. ",reinforcementlearning,Turin_Martell,False,/r/reinforcementlearning/comments/99itwm/how_do_you_deploy_an_rl_model/
[N] First OpenAI OA5 DoTA2 match begins livestreaming at The International (TI) tournament,1534981671,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/99ieuw/n_first_openai_oa5_dota2_match_begins/
What's the best way to create a self-play agent for games like Contract Bridge?,1534958351,"Contract Bridge has large enough game tree, requires cooperation between partners, and is a game of imperfect information. What would be a good approach to train an RL agent to learn Bridge?",reinforcementlearning,banksyb00mb00m,False,/r/reinforcementlearning/comments/99f89n/whats_the_best_way_to_create_a_selfplay_agent_for/
[P] RLenv.directory : Your go to place for finding that perfect learning environment.,1534956012,"Hello, fellow AI tamer!  


In the past month, I have been working on a project to collect a list of all reinforcement learning environments and make them easy to explore through a web platform hosted on GitHub pages. If you are searching for a new reinforcement learning environment do check it out!    


This project has three main objectives:  
1. Encourage the creation of more and diverse learning environments through facilitating the discovery of new environments.  
2. Facilitate the exploration of existing environments.  
3. Unifying interfaces, through either, wrapper libraries or by starting a conversation about standards in reinforcement learning environment interfaces.  


For me it's important to get the feedback of the community, for this I would like to ask for a minute of your time to answer two questions:  


1. What kind of features do you look for when searching an RL env?  
2. Is there any feature that you find consistently lacking in the existing RL env implementations?   


This project is still under active development and open to suggestions! If there is an environment you created or know of that is not yet indexed feel free to open a pull request!   


Web platform:  
 [http://RLenv.directory](http://rlenv.directory/)  


Repository:  
[http://github.com/pschydlo/RLenv.directory](http://github.com/pschydlo/RLenv.directory)",reinforcementlearning,rewardsignal,False,/r/reinforcementlearning/comments/99ew8d/p_rlenvdirectory_your_go_to_place_for_finding/
Quick question about MDPs and value iteration,1534955533,"So I'm watching [this RL video](https://www.youtube.com/watch?v=Nd1-UUMVfz4&amp;index=3&amp;list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) and he's talking about how, if you're given the reward matrix R\_a,s and transition prob. matrix P\_a,s,s', you can iterate towards the optimal value function v and policy pi. He says there are two ways: 1) you iteratively calculate v(k+1) from v(k) for each state, using the Bellman equation, or 2) you do some combination of that value function improvement, and then reassign values to your policy by taking the argmax of the action-value q for each state (basically, with the latest v iteration, seeing which is the best move to take for a given state) and setting that action to 1 in your policy.

&amp;#x200B;

So he seemed to say (and show) that just doing the first part (not updating the policy at all) should still give you the optimal value function after some number of iterations, at which point you can grab the policy from the final one. But then why would you do the latter method? Does it converge faster or something? thanks for any advice.",reinforcementlearning,GrundleMoof,False,/r/reinforcementlearning/comments/99etwx/quick_question_about_mdps_and_value_iteration/
S_Dim inputs question,1534950909,"Hey,

&amp;#x200B;

so i use DDPG and i have my s\_dim, a\_dim and a\_bound 

&amp;#x200B;

my environment (quik) consists of mostly pictures and numbers returning and there are 3 actions that there can be taken. so with the info i gathered so far that would result my paramaters being:

&amp;#x200B;

s\_dim = ??

a\_dim = 3

a\_bound = 3

&amp;#x200B;

(Probaly Not...)

&amp;#x200B;

i also tried:

&amp;#x200B;

s\_dim = env.reset() (returns numbers and rgb data made like \[255, 255, 255 ...\]

a\_dim = env.action\_space (returns actions as list, so \[left,right,jump\]

a\_bound = 3 

&amp;#x200B;

after this would be done i'd need to transistion it:

&amp;#x200B;

        def store_transition(self, s, a, r, s_):
            transition = np.hstack((s, a, [r], s_))
            index = self.pointer % MEMORY_CAPACITY  # replace the old memory with new memory
            self.memory[index, :] = transition
            self.pointer += 1

but you may guessed it already, it didn't work...

&amp;#x200B;

**E*****rrors:***

&amp;#x200B;

as last shown:

&amp;#x200B;

*self.memory = np.zeros((MEMORY\_CAPACITY, s\_dim \* 2 + a\_dim + 1), dtype=np.float32)*

*TypeError: can only concatenate tuple (not ""list"") to tuple*

&amp;#x200B;

if i input  s\_dim = 3, a\_dim = 3, a\_bound = 3:

&amp;#x200B;

*ValueError: Input 0 of layer l1 is incompatible with the layer: : expected min\_ndim=2, found ndim=1. Full shape received: \[4\]*

&amp;#x200B;

&amp;#x200B;

if you have any idea, please let me know

&amp;#x200B;

(i would not like to be spammed with articles, thanks)

&amp;#x200B;

Jan

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;

&amp;#x200B;",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/99e6ig/s_dim_inputs_question/
Use of importance sampling term in TRPO/PPO,1534949412,"In the TRPO algorithm (and subsequently in PPO also), I do not understand the motivation behind replacing the log probability term from standard policy gradients 

https://i.redd.it/4fo8fh1hpnh11.png

 with the importance sampling term of the policy output probability over the old policy output probability 

https://i.redd.it/lfzkkdwnpnh11.png

Could someone please explain this step to me?

I understand once we have done this why we then need to constrain the updates within a 'trust region' (to avoid the old policy output in the denominator increasing the gradient updates outwith the bounds in which the approximations of the gradient direction are accurate), I'm just not sure of the reasons behind including this term in the first place.",reinforcementlearning,msinto93,False,/r/reinforcementlearning/comments/99dyyv/use_of_importance_sampling_term_in_trpoppo/
OpenAI Baselines updated: deduplicated code and benchmarks!,1534908775,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/999y86/openai_baselines_updated_deduplicated_code_and/
"OpenAI Five landing page: timeline, bibliography/video links, training/performance curve",1534790529,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/98w8ue/openai_five_landing_page_timeline/
Reinforcement Learning and Generative models in Pytorch,1534754925,"Hey everyone.  
So for the past 6-7 months, I have been working on maintaining a single library for all pytorch reinforcement learning algorithms (as well as generative models). Something similar to keras-rl. Do check it out.  
It is still under active development. Feel free to contribute to the repository if you have any particular algorithm in mind.

[https://github.com/navneet-nmk/pytorch-rl](https://github.com/navneet-nmk/pytorch-rl)

[Pytorch-RL](https://github.com/navneet-nmk/pytorch-rl)",reinforcementlearning,Teenvan1995,False,/r/reinforcementlearning/comments/98ry5a/reinforcement_learning_and_generative_models_in/
rlsl: Reinforcement Learning for Skip Lists,1534737904,,reinforcementlearning,corestar,False,/r/reinforcementlearning/comments/98qgk9/rlsl_reinforcement_learning_for_skip_lists/
Private talk about mutiple questions,1534710609,"Dear Reader,

\&amp;nbsp;

i would really like to get in contact with somebody that has ""alot"" of knownledge about Reinforcement Learning or DDPG in particularly. I have some ""Basic"" questions that i couldn't find on the internet so far, atleast not clear ""enough"" for me...

If you are in the mood to help me with some questions, please pm me on reddit or even better contact me on Discord.

\&amp;nbsp;

I would really appreciate the help and work.

I am NOT in the posistion to pay you in any way for your help, only volunatry help is what i want/need.

If you don't know anything, but you know somebody that does, let him contact me.

\&amp;nbsp;

Readings,

\&amp;nbsp;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/98nb52/private_talk_about_mutiple_questions/
"""Task complexity interacts with state-space uncertainty in the arbitration process between model-based and model-free reinforcement-learning at both behavioral and neural levels"", Kim et al 2018",1534602417,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/98c1ma/task_complexity_interacts_with_statespace/
"""Distributional Multivariate Policy Evaluation and Exploration with the Bellman GAN"", Freirich et al 2018",1534538442,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/985thx/distributional_multivariate_policy_evaluation_and/
"""MnasNet: Towards Automating the Design of Mobile Machine Learning Models""",1534533236,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9853mk/mnasnet_towards_automating_the_design_of_mobile/
[D] Parallelizing Pure-Exploration in Multi-Armed Bandit Settings?,1534529278,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/984jox/d_parallelizing_pureexploration_in_multiarmed/
"""Safety-first AI for autonomous data centre cooling and industrial control"", Kasparik/Gamble/Gao {DM} [+12% efficiency, increasing to 30%]",1534515487,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/982mxe/safetyfirst_ai_for_autonomous_data_centre_cooling/
[R] TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning,1534432435,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/97t51k/r_td_or_not_td_analyzing_the_role_of_temporal/
"""Skill Rating for Generative Models"", Olsson et al 2018 {GB} [ELO for GANs]",1534431265,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/97sz1n/skill_rating_for_generative_models_olsson_et_al/
"""Deep RTS: A Game Environment for Deep Reinforcement Learning in Real-Time Strategy Games"", Andersen et al 2018",1534429786,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/97srjo/deep_rts_a_game_environment_for_deep/
Large-Scale Study of Curiosity-Driven Learning,1534305843,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/97f8ha/largescale_study_of_curiositydriven_learning/
"On ""Delayed impact of fair machine learning"", Liu et al 2018 [reminder: use of predictive models for decision-making turns them into RL models]",1534293405,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/97dszj/on_delayed_impact_of_fair_machine_learning_liu_et/
[D] Best Framework for practical PPO?,1534256608,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/978rwn/d_best_framework_for_practical_ppo/
[R] Amanuensis: The Programmer's Apprentice,1534202231,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/9737t6/r_amanuensis_the_programmers_apprentice/
Problems with training actor-critic (huge negative loss),1534070812," I am implementing actor critic and trying to train it on some simple environment like CartPole but my loss goes towards -∞ and algorithm performs very poorly. I don't understand how to make it converge as this behaviour seems intuitively correct (if I have an action with some very low probability, taking log will result in large negative value, which then negated and multiplied by (possibly) negative advantage resulting again in large negative value). 

What current guesses are:

* This situation is possible if I sample a lot of actions with low probability, which sounds not very right.
* If I store a lot of previous transitions in history then actions which had high probability in the past can have low probability with the current policy, resulting in large negative loss. But reducing replay history will result in correlated updates which is also a problem. 

[source code](https://github.com/v-shmyhlo/reinforcement-learning/blob/master/ac.py)

https://i.redd.it/u9jeu3nm3nf11.png",reinforcementlearning,v_shmyhlo,False,/r/reinforcementlearning/comments/96odae/problems_with_training_actorcritic_huge_negative/
The AI Driving Olympics at NIPS 2018: will RL approaches be winners?,1534026106,"I am one of the organizers of [the AI Driving Olympics](https://www.duckietown.org/research/ai-driving-olympics) at NIPS 2018, in which 6 universities are involved (U. Montréal / MILA, ETH Zürich, Georgia Tech, Tsinghua, NCTU, TTIC), plus 2 industry partners (self-driving car company nuTonomy and Amazon Web Services).

We are excited because this is going to be the first robotic competition at a machine learning conference: you send your code - we run it on our robots. Or, you can get a robot yourself through [the Kickstarter](https://www.kickstarter.com/projects/163162211/duckietown-a-playful-road-to-learning-robotics-and?ref=ay75ep) run by our non-profit foundation.

We are really curious to see what the winning approach will be. There is no constraint on the techniques one can use.


AMA in the comments. I am here with students and collaborators /u/stratanis, /u/gzardini,  /u/manfred_diaz, /u/afdaniele, /u/duckietown-udem.
",reinforcementlearning,andrea,False,/r/reinforcementlearning/comments/96keqk/the_ai_driving_olympics_at_nips_2018_will_rl/
"A profile of Google Brain's Quoc Le: unsupervised NN, Seq2Seq, doc2vec/NTM, neural architecture search/AutoML",1534017479,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/96jdjk/a_profile_of_google_brains_quoc_le_unsupervised/
"""Learning to Optimize SQL Join Queries With Deep Reinforcement Learning"", Krishnan et al 2018",1534003794,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/96hnrd/learning_to_optimize_sql_join_queries_with_deep/
"""Policy Optimization as Wasserstein Gradient Flows"", Zhang et al 2018",1534002073,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/96hfrc/policy_optimization_as_wasserstein_gradient_flows/
Do you think that an RL agent has achieved some level of causality already?,1533976192,"Background: I think causality is absolutely an important aspect that an intelligent agent needs to have. I totally agree with Judea Pearl though I'm not a master in causality myself.

I think that there is no RL that implements the causality concept explicitly. However, does the RL kinda ""understand"" the causality ""implicitly"" already?

Because the way we teach the agent is by letting them learn from their experiences in an environment. It is conceiveable that the agent to be successful must understand some level of causality (it must know that its action causes something). How far do you agree with this? And how much more we need to do to encourage the agent to understand more about the causality?",reinforcementlearning,phizaz,False,/r/reinforcementlearning/comments/96f4ot/do_you_think_that_an_rl_agent_has_achieved_some/
Do you think that an RL agent has achived some level of causality already?,1533975372,[deleted],reinforcementlearning,[deleted],False,/r/reinforcementlearning/comments/96f2l6/do_you_think_that_an_rl_agent_has_achived_some/
Compete in the AI Driving Olympics at NIPS 2018,1533957096,[removed],reinforcementlearning,Debbie_Down,False,/r/reinforcementlearning/comments/96dgb5/compete_in_the_ai_driving_olympics_at_nips_2018/
A clean and modular PyTorch implementation of A2C and PPO,1533896771,,reinforcementlearning,lcswillems,False,/r/reinforcementlearning/comments/965rcs/a_clean_and_modular_pytorch_implementation_of_a2c/
How does RTDP have better convergence check than DP?,1533892993,"I was reading Section 8.7 of Sutton and Barto's book *Reinforcement Learning: an Introduction* , about Real Time Dynamic Programming (RTDP), another name for on-policy trajectory sampling value iteration algorithm. I understand that DP's value iteration updates until Δv is sufficiently small, and I also understand that the value iteration method could be updating even when the policy has become optimal. However, I don't see how RTDP's case is any different. 

\---

Another advantage of RTDP is that as the value function approaches the optimal

value function v\* , the policy used by the agent to generate trajectories approaches an

optimal policy because it is always greedy with respect to the current value function.

This is in contrast to the situation in conventional value iteration. In practice, value

iteration terminates when the value function changes by only a small amount in a sweep,

which is how we terminated it to obtain the results in the table above. At this point,

the value function closely approximates v\* , and a greedy policy is close to an optimal

policy. However, it is possible that policies that are greedy with respect to the latest

value function were optimal, or nearly so, long before value iteration terminates. (Recall

from Chapter 4 that optimal policies can be greedy with respect to many different

value functions, not just v\*.) Checking for the emergence of an optimal policy before

value iteration converges is not a part of the conventional DP algorithm and requires

considerable additional computation.

\---",reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/965elg/how_does_rtdp_have_better_convergence_check_than/
"""Backprop Evolution"", Alber et al 2018 {GB} [evolving new SGD update rules for faster training]",1533865287,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/962n6x/backprop_evolution_alber_et_al_2018_gb_evolving/
"""What Happens When Bots Teach Themselves to Cheat"" {Wired}",1533863512,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/962fg0/what_happens_when_bots_teach_themselves_to_cheat/
[D] Dijkstra's in Disguise: optimizing graph traversals appears in many fields,1533751168,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/95odi7/d_dijkstras_in_disguise_optimizing_graph/
How to use Beta distribution policy?,1533744795,"I implemented beta policy 
 http://proceedings.mlr.press/v70/chou17a/chou17a.pdf and since in beta distribution, x is within the range of [0, 1], but in many scenarios, actions have different ranges, for example [0, 30]. How can I make it?

As the paper demonstrated, I implement Beta policy actor-critic on MountainCarContinuous-v0. Since the action space of MountainCarContinuous-v0 is [-1, 1] and the sampling output of Beta distribution is always within [0, 1], therefore the car can only move forward, not able to move backwards in order to climb the peak with a flag on it.",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/95nesh/how_to_use_beta_distribution_policy/
"[N] ""OpenAI Five Benchmark: Results"" (OA writeup for DoTA victory)",1533695819,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/95i5g0/n_openai_five_benchmark_results_oa_writeup_for/
[D] Are OpenAI codes difficult to read or is it just me,1533695472,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/95i3ys/d_are_openai_codes_difficult_to_read_or_is_it/
[D] Are OpenAI codes difficult to read or is it just me,1533679294,,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/95g3te/d_are_openai_codes_difficult_to_read_or_is_it/
best algorithms for multiple action types with large action spaces?,1533673215,"a problem I'm trying to work on involves a small amount of state spaces (about 15 to 20), but at each timestep can perform one action out of four (for example it can go up, down, left or right) at a chosen speed. if i discretize the speed small enough, this makes a large amount of action spaces, and I'm not sure the best algorithm to use, as most of the things I have read (Suttons book etc.) discuss large state spaces but not large action spaces.

If I wanted to make the speed continuous, that confuses me further because it can pick only one of multiple actions, so that would make a separate gauss curve for each, and I'm not sure how to handle this. Is there anything obvious I'm overlooking here? I'd really rather not go into deep reinforcement learning unless I really have to, as I'm still trying to learn the basics.

Sorry for any grammar/spelling mistakes, english isn't my first language",reinforcementlearning,ReasonablePapaya,False,/r/reinforcementlearning/comments/95f7vr/best_algorithms_for_multiple_action_types_with/
Building a Matrix with reinforcement learning and artificial imagination,1533638358,,reinforcementlearning,AnnaKow,False,/r/reinforcementlearning/comments/95aicn/building_a_matrix_with_reinforcement_learning_and/
[D] Should gradients be propagated through the critic?,1533628633,"I am yet to try an actor-critic, but I have a theory that they should not.

In an attempt to justify my design decisions I did a little thought experiment that made me realize why deep Q learning is so unstable. It seems that the gradients for the non-top layers lie in the reward^2 space which is a very bad spot for them to be considering bootstrapped RL has feedback loops.

In Q learning, in the final linear layer the weights will necessarily have to reflect the scale of the rewards. If the rewards are big, then those big rewards will also be multiplied by large weights on the backward pass. On the other hand if the rewards are small, they will be multiplied by small weights on the backward pass.

As a way of a more concrete example, suppose a network has only one weight `W = 50` in the final layer and the rewards are in the set `{0,100}` and the input to it is `1`. In general, the gradient with respect to the weight will either be `-50` or `50`. The gradient with respect to the input will be `W * error = 50 * 50 = 2500` or `50 * -50 = -2500`. If the network uses tanh units, the weights of the final layer being in the reward space is a given. In practice, when I tried deep Q learning I found that relu units blow up even faster than tanh units.

In policy gradients, the situation is better because the weights in the final layer will not reflect the scale of the reward. This acts as a dampener.

PG's cost function will also dampen the policy moves (and the gradients) when the probabilities get tilted in one direction.

I had a bunch of questions that I wanted to answer and I think the thought experiment outlined about covers them.

1) Assuming Zap Q stabilizes deep Q learning should I go for the AC or should I resurrect the plan from 3 months ago?

The scheme Zap Q uses will not change the reality that the weights in the top layer will have to reflect the scale of the rewards. Zap Q would do nothing for to stabilize the gradients in the non-top layers. The assumption cannot possibly hold. 

2) In AC should I really propagate the gradients through just the actor? (Maybe it would be better to propagate gradients through both the critic and the actor or maybe even just the critic. That would have the benefit of making AC into an off policy algorithm.)

The thought experiment says that I should go with my original intuition and block the critic from interfering with the rest of the net. The actor has two levels of dampening (weights in distribution space, scaling by probabilities) which Q learning lacks.

Now since most of my RL information comes from online courses and I am inferring the way it should be done, I want to check here whether the above reasoning is sound.

Are critics in practice just linear layers sharing the top with the actor and have their gradients blocked? David Silver and Sergey Levine do not actually go into much detail of how they should be done.",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/959ok6/d_should_gradients_be_propagated_through_the/
How to add Q Learning also to my robot (firebird v)?,1533560131,"I am having line follower firebird V(ref#1). I can able to connect it with raspberry pi. Here I want to test q learning algorithms for auto driving car.  I have implementation of q learning algorithms in python. 


-Is there any references/pointer/blog how am I supposed to do it?
- we know algorithm but how to make implementation in reality?


Ref
(1)
http://www.nex-robotics.com/products/fire-bird-v-robots/fire-bird-v-atmega2560-robotic-research-platform.html
",reinforcementlearning,jig4physics,False,/r/reinforcementlearning/comments/9511kh/how_to_add_q_learning_also_to_my_robot_firebird_v/
Looking for help using Reinforcement Learning to teach an agent not to die,1533526534,"As the first step in what I hope to be a much larger project, I'm teaching an agent to survive a simple simulation, and as it keeps failing, I keep making it simpler, but it still doesn't seem to be working like I'd hoped

I started with 50 different levels of hunger and thirst and two different actions: eat or drink (also play, but that didn't last long).  The current level of hunger and thirst made up the state, and the simulation ended when thirst or hunger reached 0 with the reward being given at the end (though this seemed to cause it die right away to get the reward so now its rewarded for surviving and punished when the simulation ends).  The results were fairly random so I started making changes.  Instead of 50 different levels each, there were only 5 each.  I changed the way rewards were given numerous times, and even took out thirst all together so the actions are eat or don't but the results are still fairly random. even after 5,000,000 simulations I'll still get a run that only last 30 steps when the 50th lasted 1000 (1000 being the max allowed)

Any advice moving forward?",reinforcementlearning,justking14,False,/r/reinforcementlearning/comments/94xx23/looking_for_help_using_reinforcement_learning_to/
RL cuts Google datacenter cooling costs by an additional 15% {DM},1533518895,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/94x1mj/rl_cuts_google_datacenter_cooling_costs_by_an/
OpenAI Five Benchmark: crushes audience team; stream of 3-game match against pros begins,1533501444,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/94uziv/openai_five_benchmark_crushes_audience_team/
Modular Multi-Objective Deep Reinforcement Learning with Decision Values,1533491587,,reinforcementlearning,ttajmajer,False,/r/reinforcementlearning/comments/94tpui/modular_multiobjective_deep_reinforcement/
[R] Modular Multi-Objective Deep Reinforcement Learning with Decision Values,1533490975,,reinforcementlearning,ttajmajer,False,/r/reinforcementlearning/comments/94tmzx/r_modular_multiobjective_deep_reinforcement/
"""What to pay attention to in the OpenAI Five DoTA Benchmark"" --Smerity",1533479331,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/94s86a/what_to_pay_attention_to_in_the_openai_five_dota/
Basic reinforcement learning in tic-tac-toe game implementation,1533462243,,reinforcementlearning,kidminks,False,/r/reinforcementlearning/comments/94qs21/basic_reinforcement_learning_in_tictactoe_game/
Reinforcement Learning in a Nutshell,1533320599,,reinforcementlearning,vector_machines,False,/r/reinforcementlearning/comments/94c2jb/reinforcement_learning_in_a_nutshell/
Useful Open Source RL environments and algorithm implementations,1533313357,,reinforcementlearning,kohjingyu,False,/r/reinforcementlearning/comments/94b043/useful_open_source_rl_environments_and_algorithm/
What is the right implementation of DPPO + LSTM?,1533194112,"I was impressed by the blog post https://blog.openai.com/learning-dexterity/ and looking to implement the model. 

However, I can not find the paper or code that has the exact architecture. Could you help me finding that?",reinforcementlearning,whikwon,False,/r/reinforcementlearning/comments/93x4t4/what_is_the_right_implementation_of_dppo_lstm/
"TensorFlow tutorial: ""Playing CartPole through Asynchronous Advantage Actor Critic (A3C)""",1533174647,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/93v6fv/tensorflow_tutorial_playing_cartpole_through/
Reinforcement Learning Summer School (RLSS 2017) notes,1533154249,,reinforcementlearning,y0b1byte,False,/r/reinforcementlearning/comments/93siqi/reinforcement_learning_summer_school_rlss_2017/
AlphaGo Zero implementation and discussion blog post,1533142663,,reinforcementlearning,_sulo,False,/r/reinforcementlearning/comments/93qr97/alphago_zero_implementation_and_discussion_blog/
"""Why testing self-driving cars in SF is challenging but necessary"", Kyle Vogt {Cruise} [optimal sample selection]",1533090167,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/93kwt4/why_testing_selfdriving_cars_in_sf_is_challenging/
"""VALOR: Variational Option Discovery Algorithms"", Achiam et al 2018 {OA}",1533002625,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/93ad19/valor_variational_option_discovery_algorithms/
"""Visual Analogies between Atari Games for Studying Transfer Learning in RL"", Sobol et al 2018 [uses UNIT GAN for cross-domain]",1533002536,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/93acnm/visual_analogies_between_atari_games_for_studying/
"Recent progress in DRL for robotics: ""How Robot Hands Are Evolving to Do What Ours Can"" {AutoLab/BAIR/OA/UWash}",1532983685,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/937vgf/recent_progress_in_drl_for_robotics_how_robot/
"PPO-LSTM+domain-randomization in MuJuCo/Unity for sim2real transfer in a robotic hand grasper: Dactyl, ""Learning Dexterity"" {OA}",1532983525,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/937ul7/ppolstmdomainrandomization_in_mujucounity_for/
Learning Dexterous In-Hand Manipulation,1532983483,,reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/937udy/learning_dexterous_inhand_manipulation/
Help with basic Monte Carlo control.,1532956920,"Hi there,

I'm working my way through David Silver's intro to RL course on [YouTube](https://youtu.be/0g4j2k_Ggc4?t=33m59s) and I'm a bit stuck on the concepts behind some of the equations being used for a basic form of Monte Carlo control. Any help or advice would be greatly appreciated.

## Current Understanding

Our agent follows some policy (π). We sample the states / actions / rewards for the kth episode. While we're running through this episode we keep a running total of two values: N and Q:

* N - number of times we've taken a given action from a given state
* Q - estimated value of taking a given action from a given state

When, at timestep t, the agent visits state St and takes action At we update N and Q using our cumulative reward Gt:

    N(St, At) &lt;-- N(St, At) + 1
    Q(St, At) &lt;-- Q(St, At) + (1 / N(St, At)) * (Gt - Q(St, At)) 

Then after this episode we update our policy using some policy improvement algorithm, which isn't really relevant to my question (we use epsilon-greedy in the lecture).

## Questions

## Q1

The lectures don't clarify whether we reset the value of N to 0 in-between episodes. It seems like we can't, because if we reset N, then we end up replacing Q(St, At) with Gt, which seems quite counter-productive.

If we don't ever reset N, aren't we hyperbolically discounting our error signal `Gt - Q`? Doesn't that mean that the signal from later episodes will count for less than the signal from earlier episodes?

## Q2

Once the policy changes when we do our policy improvement, doesn't that invalidate our current estimate of Q? Our current estimate of Q is the value of taking an action given some state, under some *policy*. Do we need to throw out our old values and re-estimate Q from scratch? How would we be able to use a Q from our old policy in estimating the Q for our new policy?

## Nomenclature:

    St - state visited at timestep t (in episode k)
    At - action taken at timestep t (in episode k)
    Gt - cumulative reward gained at timestep t (in episode k)
    N - number of visits
    Q - estimated value of taking an action given a state S
    t - timestep in episode k
    k - the episode we're in",reinforcementlearning,The_Amp_Walrus,False,/r/reinforcementlearning/comments/9340df/help_with_basic_monte_carlo_control/
Can i help my agent learn generic actions ?,1532950741,"As a very simple example, if the agent sees a red sign, then pick the red fruit, pick yellow fruit if the sign is yellow. Etc

(I don't need memory. Assume.sign is always visible)

Obviously it will learn all the distinct colors with enough samples. I want to speed it up.

I have seen some paper on relational RL. What is the SOTA ? Is that an ""accepted"" or experimental technique?",reinforcementlearning,yazriel0,False,/r/reinforcementlearning/comments/933btb/can_i_help_my_agent_learn_generic_actions/
A simple Gym Gridworld Environment for illustrating a 'treacherous turn' w/tabular Q-learning,1532882749,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/92w2zc/a_simple_gym_gridworld_environment_for/
"ToriLLE: Learning environment for hand-to-hand combat, based on Toribash",1532802005,,reinforcementlearning,Miffyli,False,/r/reinforcementlearning/comments/92o7qd/torille_learning_environment_for_handtohand/
[D]QN strange behaviou[R],1532794499,"I have tested an implementation of DQN which is a mix of the approaches suggested [here](https://arxiv.org/abs/1312.5602)
and [here](https://deepmind.com/research/dqn/).
I made 4 different versions, depending on having two networks or not and having memory or not. The versions with the experience replay memory are behaving somewhat expected: the one with the two networks is slightly better than the one without two networks. However, without the memory, the implementation which uses only one network is much better than the one with two networks. This seems counter-intuitive to me. Are there any suggestions as to what may have been done wrong?",reinforcementlearning,BeardlessOtter,False,/r/reinforcementlearning/comments/92n9le/dqn_strange_behaviour/
"""Google AI Chief Jeff Dean’s ML System Architecture Blueprint"": Training/Batch Size/Sparsity and Embeddings/Quantization and Distillation/Networks with Soft Memory/Learning to Learn (L2L)",1532701847,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/92d00m/google_ai_chief_jeff_deans_ml_system_architecture/
"[N] OpenAI files proposal for new larger campus near Golden Gate Bridge in SF, in Fort Winfield Scott",1532700226,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/92cs88/n_openai_files_proposal_for_new_larger_campus/
Reinforcement Learning with Q-Learning in Node.js,1532680369,,reinforcementlearning,Vincenius_,False,/r/reinforcementlearning/comments/92as73/reinforcement_learning_with_qlearning_in_nodejs/
Deep Reinforcement Learning Course new article: Advantage Actor Critic methods: let’s play Sonic the Hedgehog!,1532677062,[removed],reinforcementlearning,cranthir_,False,/r/reinforcementlearning/comments/92aiim/deep_reinforcement_learning_course_new_article/
"Model-Agnostic Meta-Learning (MAML) PyTorch re-implementation for bandits/tabular MDPs/MuJuCo/2D navigation, by Tristan Deleu",1532652280,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/927x03/modelagnostic_metalearning_maml_pytorch/
"""Backprop-Q: Generalized Backpropagation for Stochastic Computation Graphs"", Xu et al 2018 {Hulu}",1532631194,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9251nn/backpropq_generalized_backpropagation_for/
"""Learning Plannable Representations with Causal InfoGAN"", Kurutach et al 2018 {BAIR}",1532631153,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9251en/learning_plannable_representations_with_causal/
"""Variational Bayesian Reinforcement Learning with Regret Bounds"", O'Donoghue 2018 {DM}",1532631109,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/92516q/variational_bayesian_reinforcement_learning_with/
Implementing a small NN MPC for Half-Cheetah in Gym (Holly Grim),1532617525,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9231ga/implementing_a_small_nn_mpc_for_halfcheetah_in/
State Abstractions for Life-Long RL (David Abel),1532617402,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9230tp/state_abstractions_for_lifelong_rl_david_abel/
Evaluating the performance of an RL agent in self-play?,1532616823,"In single-player games, we can check if an agent is learning, by comparing the obtained *environment reward* at one point in time against what was obtained in the past.

I'm sure my understanding is slightly off, but I don't see how this extends to self-play. When considering a two-player (zero-sum) game and training by *self-play*, for every **+r** reward an agent receives, the other copy of itself receives a **-r** reward -- giving an overall performance/reward of 0. This would be the case regardless of the agent's playing capability.

If this is the case, then how can we quantifiably measure a trained agent's performance? Two ideas are:

* (a) - Simulate games against some external baseline, e.g. comparing a chess-playing agent against Stockfish
* (b) - Compare the learned policy against some theoretical *optimal policy* (i.e. from Nash Equilibrium)

But what if the Nash Equilibrium is unknown, and there are no external baselines?

I think that the point which I'm missing, is that an agent could be compared against earlier versions of itself. But this raises some more questions (I'll start with two):

* Should the agent play against the most recent version? Or play games against all past versions (in which case, how to decide the proportion of games against each version?)?
* Is the agent supposed to play against earlier versions during **training** as well as **testing**?

Thanks in advance for helping my clear up my misunderstanding!",reinforcementlearning,stochastic_neighbour,False,/r/reinforcementlearning/comments/922xw4/evaluating_the_performance_of_an_rl_agent_in/
Merge convolutional neural network and reinforcement learning for baxter simulator,1532533528,"This model is used for baxter simulation.

I have built a deep convolutional neural network that could take an object image and outputs the shape of the image.
Based on this shape, I want to build another network that could use this label(object shape) to output end joint position values(baxter simulation robot) to pick the object up.
Based on either it successfully or fails to pick the item up, the reward will be sent to RL network that uses Deep Q Learning to tune its network.
At the moment, I am not sure how to build right networks to fulfill each of these goals:

a) Neural network 1 - Read pixel values and recognize object shape/ read pixel values and outputs endjoint position values

b) Neural Network 2 - Deep Q - tune values based on successful or fails for the baxter simulator to pick the object based on the provided output from NN1",reinforcementlearning,SuccessfulMechanic2,False,/r/reinforcementlearning/comments/91syr4/merge_convolutional_neural_network_and/
Guidance on time-series RL project (previous data has variable time between action and reward),1532511994,"Hey all,

I haven't done much RL before and am starting work on a project with some colleagues to create an RL learner than can accurately dose medication to patients in a hospital setting (using a [large ICU database](https://mimic.physionet.org/) to train the learner). The goal for the learner will be to correctly dose the patient every 12 hours, with full reward being given when levels in the blood are between 30-50 units at the 12 hour after each infusion (the medication's half-life is 4-6 hours and needs to remain above 30 units to be effective. An infusion may start with a 150 unit dose and decay to 30 after 12 hours, in time for the next infusion to push it back up above the 30 units mark). The medication is given to patients every 12 hours and while we have 50k+ cases of an infusion with a follow-up blood test prior to the next infusion, the time between each infusion and the blood test varies between 1-18 hours.

So, in order to build this, am I correct in my assumption that we'll need to use our previous data to build a predictor of post-infusion medication concentrations first, use it to predict the blood concentrations at t+12 hours for each of our 50k infusions, and then train the reinforcement learner on the new data? Here's a visual version in case that helps: [https://imgur.com/a/fAYUy2I](https://imgur.com/a/fAYUy2I)

Hope this makes sense, thank you!",reinforcementlearning,globalminima,False,/r/reinforcementlearning/comments/91qfhe/guidance_on_timeseries_rl_project_previous_data/
Should I use MC tree search on my poker bot?,1532449707,I have gtx 1080 ti so can it calculate enough states to play better than human or is MC Tree Search just for supercomputers? Should I use something that approximate more?,reinforcementlearning,UserWithComputer,False,/r/reinforcementlearning/comments/91iwsu/should_i_use_mc_tree_search_on_my_poker_bot/
Mike Cook discusses OpenAI's DoTA research with RPS,1532367682,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/9196b0/mike_cook_discusses_openais_dota_research_with_rps/
"""Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning"", Depeweg et al 2017",1532296805,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/911g16/decomposition_of_uncertainty_in_bayesian_deep/
"""Generative Adversarial Imitation from Observation"", Torabi et al 2018 [GAIL vs GAIfO]",1532288928,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/910gl1/generative_adversarial_imitation_from_observation/
"""Adaptive Mechanism Design: Learning to Promote Cooperation"", Baumann et al 2018 [see also LOLA]",1532285068,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/90zyto/adaptive_mechanism_design_learning_to_promote/
"""Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search"", Zela et al 2018",1532282753,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/90zo7g/towards_automated_deep_learning_efficient_joint/
"""General Value Function Networks"", Schlegel et al 2018",1532281030,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/90zg9c/general_value_function_networks_schlegel_et_al/
[D] Question about calculating values of Advantage functions using eligibility traces,1532277883,I was looking at eligibility traces lecture and I was wondering how to calculate the advantage of last 3 states when I set my n=3,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/90z1x4/d_question_about_calculating_values_of_advantage/
"""Backplay: 'Man muss immer umkehren'"", Resnick et al 2018 [curriculum learning from demonstration; see OA's similar MR work]",1532274076,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/90yl8u/backplay_man_muss_immer_umkehren_resnick_et_al/
"""Robot Learning in Homes: Improving Generalization and Reducing Dataset Bias"", Gupta et al 2018",1532271863,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/90ybyv/robot_learning_in_homes_improving_generalization/
Will better algorithm be faster than working algorithm?,1532271094,So I was thinking if I have two different algorithms and I know that A algorithm is working but not converge to global optimal as fast as algorithm B. Is it plausible that A is faster to achieve something than B unless it's not faster to converge? So lets say Im making chess program and I want beat local chess club tournament. So that mean I dont have to solve whole chess it is enough for me if I beat best players in the club. It is better to use the best chess algorithm which finaly solves chess or use something simpler? Is the best algorithm often slower if you dont have to solve whole problem?,reinforcementlearning,UserWithComputer,False,/r/reinforcementlearning/comments/90y8vu/will_better_algorithm_be_faster_than_working/
"[R] ""Episodic Control as Meta-Reinforcement Learning"", Ritter et al 2018",1532242348,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/90w0as/r_episodic_control_as_metareinforcement_learning/
"""Automatically Composing Representation Transformations as a Means for Generalization"", Chang et al 2018",1532228066,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/90usu5/automatically_composing_representation/
[D] On “Solving” Montezuma’s Revenge: Looking beyond the hype of recent Deep RL successes,1532137143,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/90lpsk/d_on_solving_montezumas_revenge_looking_beyond/
Any good blogs/videos/tutorials for beginners in reinforcement learning,1532093452,I have experience with MEAN STACK and basic knowledge of ML,reinforcementlearning,Vinayak98,False,/r/reinforcementlearning/comments/90fzz0/any_good_blogsvideostutorials_for_beginners_in/
[D] ICML 2018 Reinforcement Learning talks,1532041748,"https://www.youtube.com/watch?v=SfdGU8HpMcc

https://www.youtube.com/watch?v=8rTLD_MQyog

https://www.youtube.com/watch?v=x98LGXidIYA

https://www.youtube.com/watch?v=sgZzfwTTh1M

https://www.youtube.com/watch?v=ZPnoRe_DXPw

https://www.youtube.com/watch?v=SCKoXka_G3I

I just found these by searching ICML 2018 on youtube and I thought I would share them here. I believe they are available on the ICML facebook page as well.

",reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/90aqcs/d_icml_2018_reinforcement_learning_talks/
"Question on ""10.4. Deprecating the Discounted Setting""",1531976043,"Hello! I had a question while reading about discounts in continuing tasks in Reinforcement Learning: an Introduction (Sutton and Barto). In 10.4, they say ""But in the approximate case,it is questionable whether one should ever use [the continuing, discounted problem formulation]."" Am I correct in thinking that they are only talking about the average reward setting?

Thank you in advance for your help!",reinforcementlearning,seungjaeryanlee,False,/r/reinforcementlearning/comments/902uch/question_on_104_deprecating_the_discounted_setting/
"OpenAI DotA update: several restrictions lifted from 5x5 agent games (+wards, +Roshan, fixed hero mirror match ~&gt; 18 heroes), human-equivalent reaction time, just w/more PPO training; pro match at 2PM PST 4 August 2018",1531932755,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8zx78c/openai_dota_update_several_restrictions_lifted/
Q value,1531882227,Is pagerank some kind of a Q-value?,reinforcementlearning,bobojjhh,False,/r/reinforcementlearning/comments/8zrora/q_value/
[D] Policy Gradient: Test-time action selection,1531875288,"During training, it's common to select the action to take by sampling from a Bernoulli or Normal distribution using the output probability of the agent.

This makes sense, as it allows the network to both explore and exploit in good measure during training time.

During test time, however, is it still desirable to sample actions randomly from the distribution? Or is it better to just use a greedy approach and choose the action with the maximum output from the agent?

It seems to me that during test-time when using random sampling if the less-optimal action happens to be picked at a critical moment, it could cause the agent to have a catastrophic failure.

I've tried looking around, but couldn't find any literature or discussions covering this, however I may have been using the wrong terminology! ",reinforcementlearning,FatChocobo,False,/r/reinforcementlearning/comments/8zqvbh/d_policy_gradient_testtime_action_selection/
"""Program Synthesis in 2017-18"", Alex Polozov",1531872346,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8zqibo/program_synthesis_in_201718_alex_polozov/
K-FAC for RL (ACKTR) Performance Figures,1531841081,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8zm24l/kfac_for_rl_acktr_performance_figures/
"""The Uncertainty Bellman Equation and Exploration"", O'Donoghue et al 2017 {DM}",1531760435,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8zcm8d/the_uncertainty_bellman_equation_and_exploration/
ICML Notes,1531733579,,reinforcementlearning,pdxdabel,False,/r/reinforcementlearning/comments/8z9il5/icml_notes/
Markov Decision Property,1531718906,,reinforcementlearning,vector_machines,False,/r/reinforcementlearning/comments/8z8b1b/markov_decision_property/
"Are you interested in Computer Science and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1531698024,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/8z63nd/are_you_interested_in_computer_science_and_want/
"""Visual Reinforcement Learning with Imagined Goals"", Nair et al 2018",1531689977,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8z56al/visual_reinforcement_learning_with_imagined_goals/
"""Conditional Neural Processes"", Garnelo et al 2018 {DM}",1531540942,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8yqk0j/conditional_neural_processes_garnelo_et_al_2018_dm/
Terminology for q-network evaluating state (multiple outputs) vs evaluating state and action (single output),1531537981,"What is the terminology for when you give a state to the neural network to get values for each action, vs. giving a state-action pair to the network to get a single value? I am adapting anyrl to work with my personal project, and need to learn about switching from the first method to the second. Besides changing the loss function and input/output to the network, dueling networks will also need to be applied differently (if they even apply at all). I'm having trouble finding info, however.",reinforcementlearning,AIIDreamNoDrive,False,/r/reinforcementlearning/comments/8yq9it/terminology_for_qnetwork_evaluating_state/
Terminology for a q-network evaluating a state and outputting values for each action vs evaluating a state-action pair,1531537122,"What is the terminology for when you give a state to the neural network and it outputs values for each action, vs. giving a state-action pair to the network and it outputs a single value? I am working on adapting anyrl to work with my personal project, and want to research/Google if there are complications when switching from the first method to the second, besides changing the transition function and inputs.",reinforcementlearning,AIIDreamNoDrive,False,/r/reinforcementlearning/comments/8yq6bd/terminology_for_a_qnetwork_evaluating_a_state_and/
OpenAI gym Baselines HER with demonstrations - Using demos to teach complex tasks to robotic manipulators with Reinforcement learning (DDPG),1531498808,,reinforcementlearning,rishabhJangir,False,/r/reinforcementlearning/comments/8ylcom/openai_gym_baselines_her_with_demonstrations/
"""How Many Random Seeds? Statistical Power Analysis in Deep Reinforcement Learning Experiments"", Colas et al 2018 [t-tests inappropriate for comparing DRL algorithms; &gt;20 runs required per condition given current instability of performance]",1531428052,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ydido/how_many_random_seeds_statistical_power_analysis/
[D] What is a good paper progression for learning to implement self-play in Reinforcement Learning?,1531424637,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8yd0kh/d_what_is_a_good_paper_progression_for_learning/
Handling entropy collapse in policy gradient methods,1531417038,How do you prevent a policy from converging to generating distributions over actions with 100% probability assigned to a single action? Current ways I get around this are by increasing entropy bonus term and/or adding a small value to the probability estimates used to parameterize the multinomial(discrete action space). Specifically I'm running into this problem on atari experiments after about 15 million steps.,reinforcementlearning,Data-Daddy,False,/r/reinforcementlearning/comments/8ybwe1/handling_entropy_collapse_in_policy_gradient/
Question about gaussian policy implementation,1531384665,"Hi! I'm trying to implement PPO and Soft Actor Critic for continuous control with action range \[-1,1\].

It seems like there are various implementations for gaussian policy.

1. Have separate log\_std parameter (independent to the state, like TRPO paper)
2. Linear mean and Softplus variance (like A3C paper)
3. Linear mean and clamped Linear log\_std, then tanh to the sample (from SAC implementation [https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/sac/sac.py](https://github.com/vitchyr/rlkit/blob/master/rlkit/torch/sac/sac.py))

I would like to make variance dependent to the state but in method 2, mean values often go outside reasonable range, having large variance. And method 3 is somewhat numerically unstable (NAN values probably from arctanh).

Does anyone knows stable way to implement the gaussian policy?",reinforcementlearning,gliese581gg2,False,/r/reinforcementlearning/comments/8y7z50/question_about_gaussian_policy_implementation/
Is there an RL algorithm that computes a maximum likelihood estimate incrementally?,1531348214,"After watching [this video](https://youtu.be/IwWN35TiABM) I have the impression that the maximum likelihood estimate they are talking about is \*the\* optimal solution given limited data. 

They also point out flaws of both TD(0) and TD(1), but the only advantage I see with the temporal difference methods is that they are incremental.

So my question is, is there an algorithm that can compute the MLE incrementally and efficiently? If not, why is that? If yes, where can I learn about it?",reinforcementlearning,konseptgrokker,False,/r/reinforcementlearning/comments/8y44qb/is_there_an_rl_algorithm_that_computes_a_maximum/
"""Is Q-learning Provably Efficient?"", Jin et al 2018 [tabular Q-learning with UCB exploration &amp; optimal learning rate has `sqrt(H)` more regret than model-based RL]",1531329950,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8y1koo/is_qlearning_provably_efficient_jin_et_al_2018/
How can I sample action from Gaussian distribution with action dimension space larger than one?,1531324879,"In TensorFlow, `tf.contrib.distributions.Normal(mu, sigma)`, How can I sample one action of which dimension value is larger than one? https://stackoverflow.com/questions/51260136/reinforcement-learning-how-can-i-sample-action-from-gaussian-distribution-with",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/8y0tji/how_can_i_sample_action_from_gaussian/
Can I use information in the reward function that cannot be extracted by the state?,1531309543,"I have a pretty basic question in RL. Assume that you have a robot that you want to learn to perform a task based on the tactile or force measurements for manipulating an object. So, I am thinking of using RL for learning the mapping between these force measurements (state) and motion (task positions or velocities, actions). The problem is that it is not trivial to shape the reward for this MDP using only this state (force measurements), but it would be easier to include on the reward the current position of the object in each timestep. If the position of the object is not in the state of the MDP can I use it in the reward function? Or this means that I would have a different MDP with the position of the object in the state and thus the learned policy will be a mapping between force measurements and object position to robot motion? I know that it doesn't make sense but imagine the case in which we can in the reward the position of the object in simulation and then try to evaluate the learned policy on a real robot which lacks visual measurements (assume now perfect simulation of the real environment and not domain shifting issues)? Or this is wrong in principal in terms of MDP definition?

Fundamentally the question is if I can learn a policy which maps states to actions by using additional information on the reward during training, and then evaluate the policy without this additional information.",reinforcementlearning,SuperiorCalifornium,False,/r/reinforcementlearning/comments/8xyv5k/can_i_use_information_in_the_reward_function_that/
"""An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution"", Liu et al 2018 {Uber} [adding an additional layer of absolute positions to convolution layers]",1531277759,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8xvxct/an_intriguing_failing_of_convolutional_neural/
[D] Reinforcement learning’s foundational flaw,1531264218,,reinforcementlearning,baylearn,False,/r/reinforcementlearning/comments/8xuags/d_reinforcement_learnings_foundational_flaw/
Proximal Policy Optimization vs Soft Actor Critic,1531236714,"Hello, 

I've recently seen a lot of mentions to PPO agents, notably in OpenAI researches. However, there exist a recent ""new"" agent, SAC, that seems to perform just as well, if not better. Has anyone implemented it ? Tried it ? What are your thoughts ? 

SAC paper: https://arxiv.org/abs/1801.01290

Thanks !  ",reinforcementlearning,UpstairsCurrency,False,/r/reinforcementlearning/comments/8xq8rl/proximal_policy_optimization_vs_soft_actor_critic/
What is reinforcement learning? The complete guide,1531233038,,reinforcementlearning,AnnaKow,False,/r/reinforcementlearning/comments/8xpnqm/what_is_reinforcement_learning_the_complete_guide/
"Are you interested in Reinforcement Learning and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1531198658,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/8xlppv/are_you_interested_in_reinforcement_learning_and/
"""Feature-wise transformations: A simple and surprisingly effective family of conditioning mechanisms""",1531161424,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8xen5f/featurewise_transformations_a_simple_and/
"""The Pursuit of (Robotic) Happiness: How TRPO and PPO Stabilize Policy Gradient Methods""",1531159602,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8xebq8/the_pursuit_of_robotic_happiness_how_trpo_and_ppo/
How to fix reinforcement learning,1531148305,,reinforcementlearning,SaveUser,False,/r/reinforcementlearning/comments/8xce1x/how_to_fix_reinforcement_learning/
Udacity: PyTorch Python notebooks for Deep Reinforcement Learning nanodegree courses,1531076232,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8x3rcc/udacity_pytorch_python_notebooks_for_deep/
Is it possible to use Gaussian distribution as the policy distribution in DDPG?,1531065228,"Since DDPG is a deterministic algorithm, is it possible to use Gaussian distribution as the policy distribution in DDPG?",reinforcementlearning,AlexanderYau,False,/r/reinforcementlearning/comments/8x2d1m/is_it_possible_to_use_gaussian_distribution_as/
Good reward functions,1530951255,"Im working on a thesis where a robotic arm is learning to complete the wire loop game by taking a 7cm x 7cm picture every n centimeters. This picture is then transformed to a parametrization of a curve. 
Interestingly if I give the robot a sparse reward at every x &gt; n centimeters, it converges but if I include the length of each curve or one of the actions which have the most effect on the length, it never converges to a working policy. 
How is that possible? 

What are your guidelines for good reward functions in RL? 
Are there mistakes one should avoid when choosing a reward function? ",reinforcementlearning,gimme-rewards,False,/r/reinforcementlearning/comments/8wrpz7/good_reward_functions/
[D] A Deep Dive into Reinforcement Learning,1530907021,,reinforcementlearning,qwert7890-,False,/r/reinforcementlearning/comments/8wn62h/d_a_deep_dive_into_reinforcement_learning/
StarCraft: Broodwars gym environment with MvN combat and explore modes,1530894308,,reinforcementlearning,apsdehal,False,/r/reinforcementlearning/comments/8wlfx9/starcraft_broodwars_gym_environment_with_mvn/
"""Variance Networks: When Expectation Does Not Meet Your Expectations"", Neklyudov et al 2018",1530892506,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8wl6zh/variance_networks_when_expectation_does_not_meet/
Recommendations for beginning with RL,1530890991,"I began RL with david silver's lecture videos on youtube. I'm currently at the 3rd video and realized I'm having confusions about the differences between value iteration, policy iteration, differences between notations q and v for value functions, etc. Where can I read about and implement things practically to get a better grip on the subject ?   


Thanks. ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/8wkzm4/recommendations_for_beginning_with_rl/
What are disadvantages and advantages of different policy gradient algorithms?,1530886871,I have been trying to create ai that play poker but I'm not sure what algorithm is the best. I know that algorithms that calculate values won't work because environment is stochastic. I was thinking policy gradient algorithms but I'm not sure which I should choose. Currently I'm going with reinforce algorithm but I hear that there is pretty big chance that it converge to local minimum. Can someone suggest me algorithms or write advantages and disadvantages of algorithms so I could choose best one.,reinforcementlearning,UserWithComputer,False,/r/reinforcementlearning/comments/8wkg86/what_are_disadvantages_and_advantages_of/
Comparison &amp; Selection of RL Algorithms in Continuous Action Spaces,1530764344,"To date we have a variety of methods and algorithms which can handle continuous control tasks. But I'm finding it incredibly difficult to know which one to use when &amp; why. Of course, it's possible that one may work better on one domain than another, but it doesn't seem like the papers compare against all previous methods on the usual Atari/MuJoCo domains. So my question is, how should one select an algorithm to use? Or better yet, is there a reason why some papers only compare against A3C for example, rather than also against DDPG?A list of papers, and their comparisons in first publish date order for clarity (from TRPO):

1. [TRPO](https://arxiv.org/abs/1502.05477)
   1. NB: Policy network architecture can significantly impact results
   2. Significantly outperforms DDPG/ACKTR/PPO in Swimmer-v1
2. [DDPG](https://arxiv.org/abs/1509.02971)
   1. Performs better than DPG
   2. More sample efficient than TRPO (possibly)
   3. Model-free setting: large number of episodes required
   4. But is it **better** than TRPO?
   5. NB: Policy network architecture can significantly impact results
   6. Seems to outperform TRPO/ACKTR/PPO in stable learning environments (HalfCheetah-v1) but not unstable (Hopper-v1)
3. [A3C](https://arxiv.org/abs/1602.01783)
   1. Quick to learn
   2. No comparison with other continuous control methods given
4. [NAF](https://arxiv.org/abs/1603.00748)
   1. Compares to DDPG
      1. NAF learns a smooth, stable policy
      2. DDPG learns an unstable policy
      3. Therefore NAF is more suitable for domains where precision is required (robot arm manip e.g.)
   2. NAF performs better than DDPG on 80&amp;#37; or so of tested tasks
5. [ACER](https://arxiv.org/abs/1611.01224)
   1. Compared to A3C (with and without trust region updating)
   2. ACER outperforms A3C by a significant margin
6. [PPO](https://arxiv.org/abs/1707.06347)
   1. Outperforms A2C and A3C on most continuous control environments
   2. Performs better than ACER sometimes and sometimes not
   3. Performs better than TRPO
7. [ACKTR](https://arxiv.org/abs/1708.05144)
   1. Outperforms A2C
   2. Outperforms TRPO on most, but not all, tasks
   3. 25&amp;#37; slower than A2C (or more computation)
   4. Scalable - no performance loss with larger batch sizes (A2C sample efficiency worsens)

Some results taken from this very useful paper: [https://arxiv.org/abs/1709.06560](https://arxiv.org/abs/1709.06560) (compares DDPG/TRPO/PPO/ACKTR)

So it seems I would always use PPO over TRPO. But none of the other methods have been compared to PPO, so that leaves me at a loss. Especially ACKTR which performs better than TRPO, but no comments relating to it's superior method, PPO, which I find to be strange. It seems there's also no need for A2C or A3C now we have ACER. But again, ACKTR compares to only A2C, not ACER!? There is no comment on whether DDPG or TRPO is better, and anyway NAF seems to perform better than DDPG on most tasks. 

My conclusion (correct me if you think differently), is that the only methods which we can blindly turn away from are TRPO (between by PPO and ACKTR - although not all the time, but a significant portion of the time), and A2C/A3C (beaten by PPO). But we still need to test:

1. DDPG
   1. good in stable learning environments
   2. sometimes better than NAF in general
2. NAF
   1. usually better than DDPG, especially in unstable environments
3. ACER
4. PPO
5. ACKTR

Honestly, I'm finding it difficult to understand why the comparisons of methods in newer papers are so vague and incomplete. But this was all the information I could extract. If you have any suggestions, improvements, fixes, let me know! It would be great to shorten that last list to make projects quicker!",reinforcementlearning,ViralRiver,False,/r/reinforcementlearning/comments/8w7mn2/comparison_selection_of_rl_algorithms_in/
"[P] Complete ""World Models"" (Ha &amp; Schmidhuber 2018) implementation in Python Chainer for CarRacing-v0 &amp; ViZDoom, by Adeel Mufti",1530750549,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8w6bbz/p_complete_world_models_ha_schmidhuber_2018/
Help needed for gym environment (policy gradient),1530739134,"I am stuck with this problem and about to go crazy. Could you please take a look at it? 

[https://stackoverflow.com/questions/51180689/tf-multinomial-outputs-number-other-numbers-than-range](https://stackoverflow.com/questions/51180689/tf-multinomial-outputs-number-other-numbers-than-range)",reinforcementlearning,hamzaemra,False,/r/reinforcementlearning/comments/8w542y/help_needed_for_gym_environment_policy_gradient/
"""Diffusion-Based Approximate Value Functions"", Klissarov &amp; Precup 2018",1530733040,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8w4czl/diffusionbased_approximate_value_functions/
"""Learning Montezuma's Revenge from a Single Demonstration"", Salimans &amp; Chen {OA} [PPO with backward chaining from reward state for curriculum learning]",1530725406,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8w3eui/learning_montezumas_revenge_from_a_single/
"""Learning Montezuma's Revenge from a Single Demonstration"", Salimans &amp; Chen [PPO with backward chaining from reward state for curriculum learning]",1530725339,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8w3ekm/learning_montezumas_revenge_from_a_single/
"""Accurate Uncertainties for Deep Learning Using Calibrated Regression"", Kuleshov et al 2018",1530724807,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8w3c5b/accurate_uncertainties_for_deep_learning_using/
Testing policy learned by Tensorflow PPO agent,1530687523,,reinforcementlearning,abhik_singla,False,/r/reinforcementlearning/comments/8vzma6/testing_policy_learned_by_tensorflow_ppo_agent/
"Google Waymo's Arizona self-driving car program after 1 year: &gt;400 daily riders, &gt;24k miles daily; no fatalities or major injuries",1530674206,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8vycrr/google_waymos_arizona_selfdriving_car_program/
"""Adversarial Exploration Strategy for Self-Supervised Imitation Learning"", Hong et al 2018",1530653097,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8vvz96/adversarial_exploration_strategy_for/
"""Human-level performance in first-person multiplayer games with population-based deep reinforcement learning"", Jaderberg et al 2018 {DM} [multi-agent DRL with two-level RNNs for simple procedurally-generated Quake Capture-The-Flag (CTF) game]",1530639070,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8vu3tp/humanlevel_performance_in_firstperson_multiplayer/
[D] Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update,1530624600,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8vs7c9/d_sampleefficient_deep_reinforcement_learning_via/
[D] Why does Rudder take a difference of states?,1530615222,"&gt; Since P˜ is an MDP, the reward can be predicted solely from (aT , sT ). To avoid this Markov property in the input sequence, we replace s by a difference ∆(s, s0) between state s and its successor s0.

I saw Zap Q doing this as well and I am still not sure why this is done. I feel like I need a more in depth explanation in order to understand this properly.

This is on page 4 of the [Rudder paper](https://arxiv.org/abs/1806.07857), a bit below the midpoint.",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8vr9c1/d_why_does_rudder_take_a_difference_of_states/
Why do traditional control theorists hate AI ?,1530593873,"I was talking to my professor who works on control theory (robust control, optimal control). I mentioned the words 'neutral networks' and 'reinforcement learning' in my sentence and he immediately got pissed. He asked me not to waste my time with such methods and focus on what's on my plate. I was genuinely curious about how the computer science community is trying to solve similar problems in robotics. Why do you think he said that? ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/8vphwo/why_do_traditional_control_theorists_hate_ai/
Switching from Control theory to Reinforcement Learning,1530562929,"Hi all, I'm planning to make a switch in my research topic from traditional control theory (Model based control) to Reinforcement learning based control in robotics. This involves switching advisors and schools for my PhD. I'm genuinely interested in the kind of problems that RL and deep RL is trying to solve. Do you think this switch is going to be hard ? What are your comments ?   


Thanks in advance. ",reinforcementlearning,ReinforcementBoi,False,/r/reinforcementlearning/comments/8vlx38/switching_from_control_theory_to_reinforcement/
"""Bots &amp; Thoughts from ICRA2018"" (35th International Conference on Robotics and Automation): robotics meets DRL (Eric Jang)",1530457450,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8vagxa/bots_thoughts_from_icra2018_35th_international/
[D] What are the following terms in the Fastest Convergence for Q-Learning paper?,1530451041,"I am referring to [the paper](https://arxiv.org/abs/1707.03770) by Devray and Meyn.

Could anyone explain what the basis function **psi**, the eligibility vectors **zeta** and linearization in stochastic approximation **A** and the vector sequence **b** are? There is a table of terms on the last page.

One would expect that figuring what the basic terms are would be simple, but Google is turning up nothing. Neither is doing a search for these terms through the stochastic approximation book by Borkar.",reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8v9tpj/d_what_are_the_following_terms_in_the_fastest/
"""Toward an Integration of Deep Learning and Neuroscience"", Marblestone et al 2016",1530407991,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8v6ira/toward_an_integration_of_deep_learning_and/
AlphaZero tweaks: averaging both MCTS value and final win-loss result for improved training?,1530385178,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8v437o/alphazero_tweaks_averaging_both_mcts_value_and/
"[R] ""Scalable Deep RL for Robot Grasping Task"": continuous Q-learning for off-policy robot DRL; 96% grasping success",1530377489,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8v34e8/r_scalable_deep_rl_for_robot_grasping_task/
"""One-Shot Imitation from Watching Videos"": imitation learning + MAML, Yu &amp; Finn {BAIR}",1530377189,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8v334d/oneshot_imitation_from_watching_videos_imitation/
[R] Procedural Level Generation Improves Generality of Deep Reinforcement Learning,1530322946,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/8uyi0r/r_procedural_level_generation_improves_generality/
"""Automatic Exploration of Machine Learning Experiments on OpenML"", Kühn et al 2018 [2.5 million hyperparameter settings]",1530301228,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8uw0i8/automatic_exploration_of_machine_learning/
"""The power of ensembles for active learning in image classification"", Beluch et al 2018",1530294043,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8uv2ia/the_power_of_ensembles_for_active_learning_in/
Does anybody have experience with training Multi-Agent RL algorithms on Multiwalker/Pursuit environments?,1530270087,"I'm trying to reproduce results from this [paper](http://ala2017.it.nuigalway.ie/papers/ALA2017_Gupta.pdf), in particular those ones concerning DDPG.  
I'm able to get convergence for the Waterworld environment, but I can't do the same on the others (Pursuit and Multiwalker). I already tried to use different parameters (learning rates, noise level, etc.), but it just doesn't work.

Does anybody have experience with these environments? It would be nice to know if there are some critical differences which are not so obvious that are not described in the paper.",reinforcementlearning,emapesce,False,/r/reinforcementlearning/comments/8us90b/does_anybody_have_experience_with_training/
"""Implicit Quantile Networks for Distributional Reinforcement Learning"", Dabney et al 2018 {DM}",1530202634,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ukwjj/implicit_quantile_networks_for_distributional/
Reinforcement learning: Self-driving cars in the browser (DDPG),1530108681,,reinforcementlearning,thibo73800,False,/r/reinforcementlearning/comments/8u9x81/reinforcement_learning_selfdriving_cars_in_the/
IMPALA is now open sourced (Deepmind),1530030775,,reinforcementlearning,vector_machines,False,/r/reinforcementlearning/comments/8u1g63/impala_is_now_open_sourced_deepmind/
[1806.08990] Stroke-based Character Recognition with Deep Reinforcement Learning (draft),1529985285,,reinforcementlearning,hzwer,False,/r/reinforcementlearning/comments/8twx2f/180608990_strokebased_character_recognition_with/
"""GEP-PG: Decoupling Exploration and Exploitation in Deep Reinforcement Learning Algorithms"", Colas et al 2018 [novelty search for populating replay buffers]",1529947970,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8tsfl5/geppg_decoupling_exploration_and_exploitation_in/
"OpenAI DoTA update: PPO LSTM reaches amateur-level 5x5 DoTA via self-play (256 GPUs/128k CPU, 65000x realtime), highly parallelized framework called 'Rapid'; no hierarchical or abstraction for long-term planning",1529937368,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8tqzvq/openai_dota_update_ppo_lstm_reaches_amateurlevel/
Learning Q-learning and similar modeling techniques,1529708688,"Hi everyone,

I am hoping to get some advice from you guys - I'm a behavioral neuroscientist and interested in how reward processing is disrupted in psychiatric disorders. To this end, I mostly use probabilistic learning tasks and effort-valuation tasks, but in the near future I will hopefully begin using the 2-step reinforcement learning task as well.

I have recently had some data analyzed with a Q-learning model and in the future, if I begin using the 2-step task, I am going to have to learn a model that combines both reinforcement learning and model-free State Action Reward State Action.

As computational psychiatry is somewhat of an emerging field, I would like to get a more in depth feel for these models. I have a (very) basic understanding of python and was able to follow the Q-learning code to analyze data myself, but honestly I am a little lost.

As someone who is just beginning to dip their toes in the water of reinforcement learning computational analysis, do you have any recommendations of where I can start to (1) gain a fundamental understanding of these types of models and (2) begin to learn the necessary python programming skills to be able to perform the analyses myself.

Thanks in advance.",reinforcementlearning,bigfuds,False,/r/reinforcementlearning/comments/8t5n74/learning_qlearning_and_similar_modeling_techniques/
"""Teaching Uncalibrated Robots to Visually Self-Adapt"" {GB}",1529694856,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8t3zj1/teaching_uncalibrated_robots_to_visually/
"""Model-Ensemble Trust-Region Policy Optimization"", Kurutach et al 2018",1529690163,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8t3dir/modelensemble_trustregion_policy_optimization/
"OpenAI Retro Contest (Sonic meta-RL) results: AliBaba team wins 1st place, 4,692/10,000; 229 submissions; winners use PPO/DQN w/hyperparameter tuning; next contest launches in a few months",1529686514,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8t2w0v/openai_retro_contest_sonic_metarl_results_alibaba/
"""RENA: Resource-Efficient Neural Architect"", Zhou et al 2018",1529682916,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8t2f9r/rena_resourceefficient_neural_architect_zhou_et/
"""Meta Learning by the Baldwin Effect"", Fernando et al 2018 {DM}",1529682865,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8t2f16/meta_learning_by_the_baldwin_effect_fernando_et/
"""Gradient Adversarial Training of Neural Networks"", Sinha et al 2018 {Magic Leap}",1529682755,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8t2eji/gradient_adversarial_training_of_neural_networks/
"[Q] Combining NAF and Rainbow DQN, possible?",1529653043,"I'm working on a problem with a continuous action space, and a relatively small observation space (around 100 features max). I want to try various algorithms/agents and compare their results. I've split this into 3 different classes of methods:

Value function (discrete)

\-&gt; Rainbow DQN

Policy/AC (continuous, gradient)

\-&gt; A3C

\-&gt; DDPG

\-&gt; TRPO

\-&gt; PPO

\-&gt; ACER

Policy/AC (continuous, gradient-free)

\-&gt; CMAES

\-&gt; CEM

I've read most papers, but still unsure of whether it's a case of trying each on my task and seeing which performs best, or if there's some which are clearly worse than others and I should avoid.

But in addition to this, I've seen some information on using NAF to extend Q-based value-function algorithms to handle continuous action spaces. Is NAF an agent itself which should  therefore be added to the second list, or can it be used to extend existing Q-based algorithms (such as rainbow-DQN)? Thank you!",reinforcementlearning,ViralRiver,False,/r/reinforcementlearning/comments/8szhjs/q_combining_naf_and_rainbow_dqn_possible/
Is there research on methods that don't always try to win?,1529614967,"From the research I've seen, the goal of reinforcement learning is generally to create an agent that can win very well (i.e. reward +1 for winning, -1 for losing). But if I wanted to create a chess bot, how could I frame the reward such that difficulty scales to the player, making a fun/even match? Is there research out there on this topic?",reinforcementlearning,Wootbears,False,/r/reinforcementlearning/comments/8svj3i/is_there_research_on_methods_that_dont_always_try/
"""GQN: Neural scene representation and rendering"", Eslami et al 2018 {DM} [deep environment models]",1529607729,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8sukfh/gqn_neural_scene_representation_and_rendering/
"""Meta-Learning Transferable Active Learning Policies by Deep Reinforcement Learning"", Pang et al 2018",1529603737,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8su0ir/metalearning_transferable_active_learning/
Does RL do a forward pass to calculate actions twice for each state?,1529602641,"I am implementing a RL algorithm in Tensorflow and just wanted to make sure I wasn't going crazy. The first pass through the network is to determine the action that then gets passed to the environment to move to the next step and receive the current reward. Then once the rewards have been collected the state, action selected, and rewards get feed back into the network where it calculates the action again to determine the loss and then the backward pass goes to determine the gradient and finally update the parameters. Is it an issue if the first and second forward passes are not identical? Wouldn't this be the case in A3C or if you have some random element in your network like dropout? Thank you for any help!",reinforcementlearning,DayMan116,False,/r/reinforcementlearning/comments/8stv2u/does_rl_do_a_forward_pass_to_calculate_actions/
Need for a pure-Python VizDoom-like environment for Gym?,1529591051,"I'm at the [Mila](https://mila.quebec/) and I've building/maintaining two OpenAI Gym environments. One is a 2D [gridworld](https://github.com/maximecb/gym-minigrid), the other one is a [3D lane-following](https://github.com/duckietown/gym-duckietown) environment.  I see that there are many RL/DL papers using the VizDoom environments for Gym, but the students here at the Mila are having all kinds of issues getting these environments to work, in part because they have a lot of dependencies.

I'm thinking of creating an alternative to VizDoom that would be pure Python (using Pyglet/OpenGL for graphics). I would likely ship this with a docker image for easy deployment. The environment, like gym-duckietown, would have very few dependencies (numpy, pyglet, that's it). There would also be some focus on making the environment easy to script with Python. Maybe support for procedurally creating 2D mazes and placing objects around easily.

This post is my attempt at a mini market study before I dive into this.  I think the case can be made that this would reduce the barrier to entry both in terms of easing the pain of installing the environment, and in terms of making it easy for students who are only familiar with Python to script their own experiments. No need to know C, or to mess with Doom file formats.

I would like to know what the people on this subreddit think. Do you think it could be useful? Would you consider using it? What kind of features would you like to see? I'm thinking I would likely have a YAML map format with some easy to edit top-down grid description, and support for OBJ models. May go farther and try to use constructive solid geometry (CSG) for procedural generation if that makes sense. Feedback welcome.

Also curious to hear from the naysayers. Please link to other competing options and tell me why these are already good enough, what kind of scripting support they have, etc.",reinforcementlearning,maximecb,False,/r/reinforcementlearning/comments/8ss9qk/need_for_a_purepython_vizdoomlike_environment_for/
"RUDDER -- Reinforcement Learning algorithm that is ""exponentially faster than TD, MC, and MC Tree Search (MCTS)""",1529568394,,reinforcementlearning,AdversarialDomain,False,/r/reinforcementlearning/comments/8sq3n1/rudder_reinforcement_learning_algorithm_that_is/
TRPO (Trust Region Policy Optimization) tutorial [Depth First Learning],1529548834,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8soaxb/trpo_trust_region_policy_optimization_tutorial/
[P] A Collection of Paper Notes on RL,1529537205,,reinforcementlearning,y0b1byte,False,/r/reinforcementlearning/comments/8sn132/p_a_collection_of_paper_notes_on_rl/
"""Gated Path Planning Networks"", Lee et al 2018 [VINs as RNNs]",1529527167,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8slrwx/gated_path_planning_networks_lee_et_al_2018_vins/
"""Auto-Meta: Automated Gradient Based Meta Learner Search"", Kim et al 2018 [PNAS for Reptile]",1529527093,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8slrkj/autometa_automated_gradient_based_meta_learner/
[P] Reinforcement Learning with Pytorch - Free course,1529519550,"Hello All,

We've created course ""Reinforcement Learning with Pytorch"" in Udemy... and we want to share with you some free coupons. 

This is not self promotion - but if this post will be treated like that, I will remove it asap - don't want to break the rules here... more details below...

Short version:

use AI-PROMO-REDDIT promo code in Udemy and you will get this course for free. Full link with agenda:

[https://www.udemy.com/reinforcement-learning-with-pytorch/?couponCode=AI-PROMO-REDDIT](https://www.udemy.com/reinforcement-learning-with-pytorch/?couponCode=AI-PROMO-REDDIT)

What we would like in return - is honest review in Udemy! That's it. If you like it or not - just leave review.

Long(er) version:

We've spent quite significant time to create this course - we wanted to go from very basics up to DQN and some of its improvements. It's more like introduction to RL for total beginners. Everything in Pytorch.

We created full HD quality - but seems one has to ask Udemy for enabling full 1080p. And then problem started - our video settings didn't fully match Udemy expected settings - and so called Udemy's Adaptive Streaming got crazy in few cases and was showing videos in 144p. Due to that we received 2 bad reviews pointing out to video and sound quality ... and as we still don't have many ""students"" - we also don't have many reviews (5 total) - so those 2 bad reviews made average review really bad (below 4).

We recreated the videos completely and now quality is not an issue any more.

So now this thread here... We would like to get honest reviews - not affected by technical problems but real reviews about content. We know the course is not perfect, but we just want to know if it makes sense for us to invest more time in it (and maybe make more courses in the future) or not... 

So again - we're just searching for honest reviews. As loyal reddit user I find this place the best for it.

So here it goes... I hope you enjoy it :) ",reinforcementlearning,aiatamai,False,/r/reinforcementlearning/comments/8skqqs/p_reinforcement_learning_with_pytorch_free_course/
Turning to an RL career,1529448057,"Hello everyone,

I need an advice from the experts.  

**Current state:** I am doing the last three months of my EngD on aerospace components material production and process simulation.  

**Objective:** Work with reinforcement learning, be part of a team from where I can learn more. At the moments I am spending my nights studying it but the idea that I potentially make a living out of it really excites me.  

**Background information (optional):** A couple of year ago I stumble upon ""Automated antenna design with evolutionary algorithms.""  by Hornby, Gregory, et al. and I started reading about genetic algorithms. Then I took classic *Andrew NG machine learning course* on Cursera which was also my first coding experience. Since that, I tried to apply machine learning where I could, examples are:  

\- Anomalies detection in quality control in material production where I am doing my thesis

\- Dimensional reduction to drastically reduce simulation attempts to achieve certain conditions

\- Clustering results of some measurement to determine the minimum experimental resolution to still being able to distinguish certain zones of the sample but saving weeks of testing

In the meantime I found  the *lectures videos from UCL by David Silver* on reinforcement learning. I started using python and I did the assignments. To practice my python I also wrote an openAI environment to simulate a market exchange, able to use the data saved by a logger that I run for a few months. I am aware that there is no way I can implement a better agent that the ones you are already doing so… why not to try to see them in action? Plus I needed something compelling enough to keep me awake at nights, rather than the conventional “make a vector of...” kind of exercises. I wrote all the functions, I thought how to define a state which included both my “wallet”, all the market prices, volumes, sell orders, buy orders etc. at a defined timestep and an almost continuous actions space where the agent can decide what to buy or sell and how much, and even cancel a previous order. Being my first python project longer than a few decades of lines it was a bit overkilled and it needs so much refactoring…

At the moment I am studying the very good *lectures by  Sergey Levine at Berkeley University*. I have been experimenting a bit more real machine learning on kaggle, struggled with tensorflow and I can’t wait to experiment with tensorforce.  

Last month I even reached the last stage of selection for a graduate program in autonomous driving, but at the end they went for someone else due to better understanding of the economy behind it, useful for the specific position available. I was actually contacted back from the company as apparently they liked me from a technical and attitude point of view (I showed the projects mentions above). Although it was a failure this event has shaken me because I got very close to what I thought it was outside my reach. Even though it was almost 10k£/y less than the jobs I am being offered in my degree field I would have taken it with no doubts.

**Other interests:** Linux, design optimisation and data driven processing.

**Question:** What do you suggest me in order to find a job that can form me further? Do you have any experience I can relate to? Advice? Additionally, feel free also to point out what do you think I should study next or maybe enough exploration? Honestly, I think need to make my hands dirty with neural nets.

Thanks for the help!",reinforcementlearning,staffire,False,/r/reinforcementlearning/comments/8sd7od/turning_to_an_rl_career/
Question about AlphaGo MCTS backup.,1529438362,"Reference - Figure 3d. in the nature paper (https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf).

As per the text below the image, when an MCTS simulation reaches the leaf, v(s_leaf) is retrieved from the value network and backed up to all the edges encountered in that Monte-Carlo run. I'm confused if the v(s_leaf) is accumulated for the player edges and the opponent edges alike. That is, when updating the average Q(s,a) for the player and opponent edges, is v(s_leaf) included with a positive sign always? If yes, why don't we have a negative sign for the opponent edges? Since actions in the following MC runs are chosen according to max Q (with an exploration term), wouldn't using positive sign for opponent edge updates play suboptimal opponent actions?",reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/8sby51/question_about_alphago_mcts_backup/
[P] Playing Atari with deep reinforcement learning - our approach,1529405054,,reinforcementlearning,AnnaKow,False,/r/reinforcementlearning/comments/8s7vbp/p_playing_atari_with_deep_reinforcement_learning/
"""Improving width-based planning with compact policies"", Junyent et al 2018 [IW expert iteration]",1529350121,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8s2a4k/improving_widthbased_planning_with_compact/
"""Motion Planning Networks"", Qureshi et al 2018",1529348000,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8s20fy/motion_planning_networks_qureshi_et_al_2018/
"Are you interested in Machine Learning and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1529343545,,reinforcementlearning,DiscoverAI,False,/r/reinforcementlearning/comments/8s1f9e/are_you_interested_in_machine_learning_and_want/
"[D] Reinforcement Learning: Novelty, Uncertainty, Exploration, Unsupervised Categorization, and Long-term Memory",1529341383,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8s14sa/d_reinforcement_learning_novelty_uncertainty/
"[P] OpenAI Retro Contest Report, by Oleg Mürk (15th place): Rainbow DQN with NoisyNet exploration and expert replay",1529332590,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8rzzfd/p_openai_retro_contest_report_by_oleg_mürk_15th/
simple game,1529323148,"any suggestions for a simple game I could apply reinforcement learning to? I'm fairly new to this having just covered q-learning.

tried to have a go at doom but after 2 weeks messing around with the gym environment and not being able to successfully install it, I gave up.

Any other ideas?

thanks",reinforcementlearning,errminator,False,/r/reinforcementlearning/comments/8ryxtu/simple_game/
[R] Sample-Efficient Deep RL with Generative Adversarial Tree Search,1529310353,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8rxwby/r_sampleefficient_deep_rl_with_generative/
[D] gradient normalization in RL,1529207112,Should we use gradient normalisation in RL? If so how should we set the values?,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/8rojew/d_gradient_normalization_in_rl/
"""Learning Real-World Robot Policies by Dreaming"", Piergiovanni et al 2018 [deep environment models]",1529183924,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8rmcxn/learning_realworld_robot_policies_by_dreaming/
"""Solving the Rubik's Cube Without Human Knowledge"", McAleer et al 2018 [expert iteration]",1529166505,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8rkgp3/solving_the_rubiks_cube_without_human_knowledge/
TensorFlow implementation of Deep Q Learning,1529128559,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/8rhhgn/tensorflow_implementation_of_deep_q_learning/
Action Probability with Thompson Sampling in Deep Reinforcement Learning,1529054318,"In some implementations of off-policy Q learning we need to know the action probabilities given by the behavior policy mu(a) (e.g., if we want to use importance sampling).

In my case, I am using Deep Q-Learning and selecting actions using Thompson Sampling. I implemented this following the approach in [1]: I added dropout to my Q-network and select actions by performing a single stochastic forward pass through the Q-network (i.e., with dropout enabled) and choosing the action with the highest Q-value.

Could anyone help me calculating mu(a) when using Thompson Sampling based on dropout?

[1] http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html",reinforcementlearning,Nicocasma,False,/r/reinforcementlearning/comments/8r9lbf/action_probability_with_thompson_sampling_in_deep/
"DeepRL-Tutorials: Pytorch implementation of DQNs, Multi-step Returns, Double DQN, Dueling DQN, Prioritized Replay, Noisy Networks for Exploration, Categorical DQN (C51), Rainbow, and Distributional DQN with Quantile Regression",1529019872,"[https://github.com/qfettes/DeepRL-Tutorials](https://github.com/qfettes/DeepRL-Tutorials)

Disclaimer: I am the author of this repository. I was inspired by a similar repository which seemed abandoned with several bugs still intact. I'll be adding markup now that I have a good base of algorithms. I hope it's useful!",reinforcementlearning,q2_electricboogaloo,False,/r/reinforcementlearning/comments/8r6j3t/deeprltutorials_pytorch_implementation_of_dqns/
David Meyer's Notes on Likelihood Ratio Policy Gradients for Reinforcement Learning,1529009434,,reinforcementlearning,satsatsat,False,/r/reinforcementlearning/comments/8r5a40/david_meyers_notes_on_likelihood_ratio_policy/
"Work Building on Deep Recurrent Q Learning (Hausknecht and Stone, 2015)",1529007933,I was hoping you all could share work building on DRQNs. Links to publications will be greatly appreciated!,reinforcementlearning,q2_electricboogaloo,False,/r/reinforcementlearning/comments/8r534p/work_building_on_deep_recurrent_q_learning/
[D] Study Group and Maximum Entropy RL,1528989831,"It's been 11 months since I created my [RL forum](https://discord.gg/KBDDpmW) on discord. We started from the very basics and went through most of [Sutton and Barto's book](http://incompleteideas.net/book/the-book-2nd.html). Then we explored DDPG, TRPO, PPO, Alpha Go and other (Deep) RL methods.

The forum has served as a (low\-traffic) QA site for the last few months because, as topics got more advanced and specific, interests began to diverge. Nonetheless, I was asked by some members to share my ""learning path"", so I decided to resume the ""study group"" functionality of the forum.

I'm about to dive deep into *Maximum Entropy RL (MERL)* \[[1](https://arxiv.org/abs/1805.00909),[2](https://arxiv.org/abs/1702.08165),[3](https://arxiv.org/abs/1801.01290)\]. I'm writing this post hoping to reach some of the people who were interested in the ""study group"" idea but were looking for more advanced stuff.

I'll assume some mathematical maturity, because we'll need some measure theory, optimal transport theory, convex optimization, PGMs, etc... BTW, I intend to read the [PGM bible](https://www.amazon.com/Probabilistic-Graphical-Models-Principles-Computation/dp/0262013193) by Koller and Friedman sooner or later. I already know a thing or two about PGMs and variational inference, but not enough to *fully* understand Levine's paper \[[1](https://arxiv.org/abs/1805.00909)\].

Keep in mind that I'm not a teacher but a learner so I welcome suggestions and any kind of help you can give me (especially if you already know the topics we're about to learn).

***If you're interested,*** [***join*** ](https://discord.gg/KBDDpmW)***the*** ***forum******.***

The relevant channels are **#tasks\-and\-announcements** and **#maximum\-entropy\-rl**. The latter is also the place where to make suggestions and have meta\-discussions about MERL (e.g. what should we read next?).  
I'll create additional channels as we go. I usually announce them on **#tasks\-and\-announcements**.

You can already find the first task on **#tasks\-and\-announcements**:

    Optimal Transport
    
    video: https://vimeo.com/248504509
    
    Optimal Transport (OT) deals with the problem of ""transporting"" a probability distribution into another with minimum effort. This is useful because this minimum effort can be seen and used as a distance between the two distributions (the Wasserstein metric is one of such distances).
    
    Some knowledge of OT is useful to properly understand the Stein Variational GD which is an important tool for Maximum Entropy RL.
    
    You might also find my explanation of the Wasserstein distance useful (4.1, 4.2, 4.3): https://mtomassoli.github.io/2017/12/08/distributional_rl/#wasserstein-distance
    
    Discussion in:  #optimal-transport",reinforcementlearning,Kiuhnm,False,/r/reinforcementlearning/comments/8r2peq/d_study_group_and_maximum_entropy_rl/
Resources to get started with Multi-Agent RL,1528966046,Is there some introductory survey paper or article I could refer to once I know about RL to get started with multi-agent RL (where agents can be co-operative or competitive). Thanks.,reinforcementlearning,lifeadvicesponge,False,/r/reinforcementlearning/comments/8r0aky/resources_to_get_started_with_multiagent_rl/
"""Unsupervised Meta-Learning for Reinforcement Learning"", Gupta et al 2018",1528932614,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8qx47d/unsupervised_metalearning_for_reinforcement/
"OpenAI Retro contest writeup: 5th place, Felix Yu: PPO with ImageNet initialization, expert agents for specific groups of levels and runtime selection of expert to use",1528909820,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8qu4wb/openai_retro_contest_writeup_5th_place_felix_yu/
[R][1806.04594] Exponential Weights on the Hypercube in Polynomial Time,1528904540,,reinforcementlearning,sudeepraja,False,/r/reinforcementlearning/comments/8qtf3v/r180604594_exponential_weights_on_the_hypercube/
[P] Racetrack environment for tabular RL • r/MachineLearning,1528890759,,reinforcementlearning,nondifferentiable,False,/r/reinforcementlearning/comments/8qrt5e/p_racetrack_environment_for_tabular_rl/
What are ways to compare convergence times?,1528876136,Say a RL algorithm converges to a threshold in different time steps. And taking the best value is obviously not the best thing to do. What are standard ways to compare the performance of how fast an algorithm converges if the convergence times differ?,reinforcementlearning,futureroboticist,False,/r/reinforcementlearning/comments/8qqmr9/what_are_ways_to_compare_convergence_times/
[1806.04242v1] The Potential of the Return Distribution for Exploration in RL,1528874404,,reinforcementlearning,gergi,False,/r/reinforcementlearning/comments/8qqhun/180604242v1_the_potential_of_the_return/
"""Optimus: an efficient dynamic resource scheduler for deep learning clusters"", Peng et al 2018",1528818701,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8qk15o/optimus_an_efficient_dynamic_resource_scheduler/
Intuitively what differentiates GTD2 and TDC algorithm?,1528771674,I am reading about Gradient Temporal Difference algorithm and Temporal difference correction algorithm. Both minimizes Mean Squared Projected Bellman Error. How just treating the derivative of same objective function MSPBE creates so much difference in terms of performance and convergence speed? ,reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/8qfh2h/intuitively_what_differentiates_gtd2_and_tdc/
"""Randomized Prior Functions for Deep Reinforcement Learning"", Osband et al 2018 {DM}",1528768351,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8qf3mk/randomized_prior_functions_for_deep_reinforcement/
"""Deep Curiosity Search (DeepCS): Intra-Life Exploration Improves Performance on Challenging Deep Reinforcement Learning Problems"", Stanton &amp; Clune 2018",1528739752,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8qbggw/deep_curiosity_search_deepcs_intralife/
"""Program Synthesis Through Reinforcement Learning Guided Tree Search"", Simmons-Edler et al 2018 [RSIC assembly micro-optimization]",1528736891,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8qb2eq/program_synthesis_through_reinforcement_learning/
[D]Explanation of the gradient of the DDPG,1528717585,Can someone please explain the intuitive meaning of the gradient of the DDPG.  I read both the papers from Silver and Lillicrap but I am still blank about how the update rule works. ,reinforcementlearning,schrodingershit,False,/r/reinforcementlearning/comments/8q8w07/dexplanation_of_the_gradient_of_the_ddpg/
"""Test-driven"" development approach to RL. Disclaimer: I'm the author and would love any feedback.",1528660896,,reinforcementlearning,uglyfrog123,False,/r/reinforcementlearning/comments/8q3f63/testdriven_development_approach_to_rl_disclaimer/
[R] Reinforcement Learning: Hidden Theory and New Super-Fast Algorithms (part 2),1528620069,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8pzl15/r_reinforcement_learning_hidden_theory_and_new/
"[D] ""Thoughts On ICLR 2018 and ICRA 2018"" --Alex Irpan",1528588149,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8pwxtk/d_thoughts_on_iclr_2018_and_icra_2018_alex_irpan/
"""Deep Variational Reinforcement Learning for POMDPs"", Igl et al 2018",1528565456,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8pudhe/deep_variational_reinforcement_learning_for/
"""Re-evaluating evaluation: Nash averaging"", Balduzzi et al 2018 {DM}",1528565272,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8pucob/reevaluating_evaluation_nash_averaging_balduzzi/
[R] Blog post on World Models for Sonic,1528524453,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/8pqvhe/r_blog_post_on_world_models_for_sonic/
RL-Adventure-2: PyTorch implementations of A2C/GAE/PPO/ACER/DDPG/TDDDPG/GAIL/HER with Cartpole demos [Dulat Yerzat],1528508215,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ppfxy/rladventure2_pytorch_implementations_of/
"""PLATIPUS: Probabilistic Model-Agnostic Meta-Learning"", Finn et al 2018 [Bayesian MAML via noise posterior sampling]",1528506684,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ppan4/platipus_probabilistic_modelagnostic_metalearning/
Reinforcement learning startup ideas,1528474167,I have been thinking to start company and I know something about Reinforcement Learning so I think it could be great product. I started looking ideas by finding companies on the internet. It supraise me that maybe 90% of them are just selling to other companies basicly consultation. I want to sell directly to consumers and I don't want to compete with Google or other big company right away. I think I need to find good nich where I sell. I have been thinking something fitness or health product but then I just don't have good ideas. Do you know any new startups with great inovative ideas? Or do you have some ideas you might want to share? I would love to hear and discuss more. If there is someone like me (started learning 6 months ago) and want to co-operate with me I would love to talk and maybe we can come up somethink great together.,reinforcementlearning,UserWithComputer,False,/r/reinforcementlearning/comments/8pl5kv/reinforcement_learning_startup_ideas/
ELI5: Different approaches to Reinforcement Learning,1528451333,"I've been trying to learn Reinforcement Learning recently, and most resources on the internet (Youtube and Blog posts) say that there are broadly three kinds of RL algorithms:

1. Value-based
2. Policy-based
3. Model-based

While those resources try their best to explain the differences between them, I somehow get lost due to a lot of mathematical/technical jargon. It would be pretty helpful if someone can explain in a simplified manner the differences between those approaches.

Also, I often come across other terms like ""Model free RL"", ""Off policy methods"", etc which confuse me further. Can you please share an explanation/resource that can clarify and distinguish all of them at once.",reinforcementlearning,satwik_,False,/r/reinforcementlearning/comments/8piltu/eli5_different_approaches_to_reinforcement/
"[R] Been There, Done That: Meta-Learning with Episodic Recall",1528437087,,reinforcementlearning,wassname,False,/r/reinforcementlearning/comments/8phhh4/r_been_there_done_that_metalearning_with_episodic/
[D] Actor Critic (DDPG) Diverging after Finding Solution • r/MachineLearning,1528402801,,reinforcementlearning,MrDoOO,False,/r/reinforcementlearning/comments/8pdmyb/d_actor_critic_ddpg_diverging_after_finding/
"""Learning to Follow Language Instructions with Adversarial Reward Induction"", Bahdanau et al 2018 {DM} [RL-GAN]",1528396644,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8pcsqt/learning_to_follow_language_instructions_with/
"""Model-free, Model-based, and General Intelligence"", Geffner 2018",1528393017,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8pcagt/modelfree_modelbased_and_general_intelligence/
"[R] OpenAI Retro Contest - Guide on ""How to Score 6k points on Leaderboard in AI Challange"" - Noob Programmer",1528381603,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/8paqay/r_openai_retro_contest_guide_on_how_to_score_6k/
Proximal Policy Optimization (PPO) implementation with documentation for Atari Breakout,1528295390,,reinforcementlearning,mlvpj,False,/r/reinforcementlearning/comments/8p1282/proximal_policy_optimization_ppo_implementation/
"[R] Any materials or source codes where I can look for episode less RL, Specifically with continous action space.",1528286640,,reinforcementlearning,Hardik_Meisheri,False,/r/reinforcementlearning/comments/8p01lw/r_any_materials_or_source_codes_where_i_can_look/
"14th European Workshop on Reinforcement Learning (EWRL'18) in Lille, France",1528285226,"SequeL (Sequential Learning Team in Lille, France) is organizing the 14th European Workshop on Reinforcement Learning, October 1st to 3rd(European means it takes place in Europe, but people from all over the world are more than welcome)There will be around 10 invited speakers \+ 3 tutorials, spanning over 3 days in Lille, France :[https://www.google.com/maps/place/Lille/@50.6270063,3.0290634,12.51z/data=!4m5!3m4!1s0x47c2d579b3256e11:0x40af13e81646360!8m2!3d50.62925!4d3.057256](https://www.google.com/maps/place/Lille/@50.6270063,3.0290634,12.51z/data=!4m5!3m4!1s0x47c2d579b3256e11:0x40af13e81646360!8m2!3d50.62925!4d3.057256%22%3Ehttps://www.google.com/maps/place/Lille/@50.6270063,3.0290634,12.51z/data=!4m5!3m4!1s0x47c2d579b3256e11:0x40af13e81646360!8m2!3d50.62925!4d3.057256)Feel free to send a paper and join ! (Registration will be announced soon)Website : [https://ewrl.wordpress.com](https://ewrl.wordpress.com/ewrl14-2018/%22%3Ehttps://ewrl.wordpress.com/ewrl14-2018/)[/ewrl14\-2018/""\&gt;https://ewrl.wordpress.com/ewrl14\-2018/](https://ewrl.wordpress.com/ewrl14-2018/%22%3Ehttps://ewrl.wordpress.com/ewrl14-2018/)Invited Speakers :

* Richard Sutton
* Martin Riedmiller
* Remi Munos
* Joelle Pineau
* Nicolo Cesa\-Bianchi
* Tze Leung Lai
* Andreas Krause
* Gergely Neu
* TBA

Tutorials :

* Advanced Topics in Bandit: Csaba Szepesvári and Tor Lattimore
* TBA
* TBA

Key dates :

* **Paper submissions due: 15 June 2018, 12am CET**
* Notification of acceptance: Mid\-July 2018
* Camera ready due: September 2018
* Workshop begins: 1 October 2018
* Workshop ends: 3 October 2018",reinforcementlearning,mseurin,False,/r/reinforcementlearning/comments/8ozwlq/14th_european_workshop_on_reinforcement_learning/
Relational Deep Reinforcement Learning [R],1528275136,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/8oz2hi/relational_deep_reinforcement_learning_r/
[R] NIPS 2017 Learning to Run: Descriptions for 8 of the top solutions,1528240183,,reinforcementlearning,PresentCompanyExcl,False,/r/reinforcementlearning/comments/8ovjmf/r_nips_2017_learning_to_run_descriptions_for_8_of/
"""More Than a Feeling: Learning to Grasp and Regrasp using Vision and Touch"", Calandra et al 2018",1528232148,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ouh7z/more_than_a_feeling_learning_to_grasp_and_regrasp/
When would you consider the rewards too sparse to be solved by DQNs,1528153737,"For context, I have a problem in which a reward is received approximately once every thousand timesteps.

In Atari pong, rewards are received approximately once every 50 timesteps, and it can be solved in less than a million frames by Rainbow. 

Do you think the rewards in this problem would likely be too sparse for an algorithm like rainbow?

More generally, when do you consider a problem to have ""sparse"" rewards?

I know the simplest solution would be to test and see, but I'd like to see what insight the community has so that I can improve my thought process before diving in.",reinforcementlearning,qfettes,False,/r/reinforcementlearning/comments/8olv8z/when_would_you_consider_the_rewards_too_sparse_to/
"Any suggestions for good ""options discovery"" papers?",1528122115,I want to explore the options discovery area. It would be great if you have some good papers in mind. Thank you in advance! ,reinforcementlearning,shivaang12,False,/r/reinforcementlearning/comments/8ohmuu/any_suggestions_for_good_options_discovery_papers/
Reinforcement Learning research groups outside the US,1528110392,"Hi r/reinforcementlearning  

I was looking up for research groups outside US working in reinforcement learning. I have come across the following :
* [Reasoning and Learning Lab, McGill University, Canada](http://rl.cs.mcgill.ca/index.html) (Doina Precup and Joelle Pineau are with DeepMind and FAIR Montreal respectively)
* [Reinforcement Learning and Artificial Intelligence, U Alberta, Canada](http://spaces.facsci.ualberta.ca/rlai/people/) (Rich Sutton, Michael Bowling, Patrick Pilarski are with DeepMind Edmonton;  Csaba Szepesvári is with DeepMind London)
* [Reinforcement learning and online learning group, Imperial College London, UK](http://www.imperial.ac.uk/machine-learning/research/theory/reinforcement-online-learning/) (Marcus Deisenroth is at Prowler.IO)
* [Whiteson Research Lab, U Oxford, UK](https://whirl.cs.ox.ac.uk/)
* [Inria SequeL, Lille, France](https://team.inria.fr/sequel/team-members/) (Mohammad Ghavamzadeh, Rémi Munos, Bilal Piot are at DeepMind, Alessandro Lazaric is at FAIR Paris, Olivier Pietquin is at Google Brain)
* [Juergen Schmidhuber's group, IDSIA, Switzerland](http://people.idsia.ch/~juergen/rl.html) (now at NNAISENSE)
* [Shie Mannor's group at Technion, Israel](http://shie.webee.eedev.technion.ac.il/research/)
* [Gergely Neu at UPF Barcelona, Spain](http://cs.bme.hu/~gergo/)
* [Balaraman Ravindran's group at IIT-Madras, India](http://rbc-dsai.iitm.ac.in/)    

It seems to me at most places the professors are on sabbatical at research companies or hold cross appointments. I was wondering if they still guide Master and PhD students because I plan to apply for Fall 19 and am looking to write a thesis in RL. I will be applying to US universities also but most US universities working in RL (Berkeley, Stanford, Brown, CMU, UMass etc) are very highly rated and I would like to apply to other universities as well to hedge my bets. Any kind of insight into this as well as other research groups in academia would be most welcome.",reinforcementlearning,lifeadvicesponge,False,/r/reinforcementlearning/comments/8ogepl/reinforcement_learning_research_groups_outside/
"""Toward machine-guided design of proteins"", Biswas et al 2018 [biological experimentation: massive directed evolution in petri dish for local exploitation plus supervised learning for predicting new possible distant global optima]",1528046671,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8oa3g0/toward_machineguided_design_of_proteins_biswas_et/
"""DELIP: Variational Inference for Data-Efficient Model Learning in POMDPs"", Tschiatschek et al 2018",1527988376,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8o54zu/delip_variational_inference_for_dataefficient/
"""AutoAugment: Learning Data Augmentation Policies from Data"", Kubuk et al 2018 {GB} [CIFAR-10, CIFAR-100, SVHN, ImageNet, Stanford Cars SOTAs]",1527984223,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8o4q8i/autoaugment_learning_data_augmentation_policies/
"""Visceral Machines: Reinforcement Learning with Intrinsic Rewards that Mimic the Human Nervous System"", McDuff &amp; Kapoor 2018 {MS} [bootstrapping auxiliary loss using human blood pressure spikes in self-driving car task]",1527977189,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8o3zml/visceral_machines_reinforcement_learning_with/
"""Learning a Prior over Intent via Meta-Inverse Reinforcement Learning"", Xu et al 2018",1527965764,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8o2qdb/learning_a_prior_over_intent_via_metainverse/
"""MolGAN: An implicit generative model for small molecular graphs"", De Cao &amp; Kipf 2018",1527965476,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8o2p7o/molgan_an_implicit_generative_model_for_small/
"""SOORL: Strategic Object Oriented Reinforcement Learning"", Keramati et al 2018",1527960045,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8o22bz/soorl_strategic_object_oriented_reinforcement/
Agents and Devices: A Relative Definition of Agency [R],1527886133,,reinforcementlearning,goolulusaurs,False,/r/reinforcementlearning/comments/8nvc9o/agents_and_devices_a_relative_definition_of/
Are there any implementations of Deep Q Networks for robotics environments in Open AI gym?,1527868987,"Hi, I'm trying to learn Deep Reinforcement learning, and gym has been pretty useful for me in trying different algorithms.

Till now I have read through lot of articles covering atari and other game environments, but I'm unable to find a resource which could help me in creating an agent for the Robotics environments (Here's the link to them https://gym.openai.com/envs/#robotics).

I know that the concepts for implementing an agent are almost the same across game environments and robotics environments, but since I'm in initial stage of my learning, it is very helpful to have a reference implementation for the same with which I can evaluate my approach.
",reinforcementlearning,satwik_,False,/r/reinforcementlearning/comments/8nt2ml/are_there_any_implementations_of_deep_q_networks/
Upcoming Webinar — Deep Reinforcement Learning in Robotics with NVIDIA Jetson,1527868862,,reinforcementlearning,nanobot_1000,False,/r/reinforcementlearning/comments/8nt1x9/upcoming_webinar_deep_reinforcement_learning_in/
[P] Comparing simple Reinforcement Learning bandits • r/MachineLearning,1527857895,,reinforcementlearning,nondifferentiable,False,/r/reinforcementlearning/comments/8nro4a/p_comparing_simple_reinforcement_learning_bandits/
Learning the Reward Function for a Misspecified Model AMA,1527841186,"Erik Talvitie, author of paper ""[Learning the Reward Function for a Misspecified Model](https://nurture.ai/p/f780e95a-5db3-4bbb-87e4-b9a5b13400f2)"" has agreed to do an AMA!

The AMA is not live. You can post a question or comment on the papers anytime before 15 June \(the earlier you post, the more likely your question will be answered\). 

Here is how you post a question:

1. Click on the paper's link above
2. Highlight any text on the paper to form a public discussion thread 
3. Post your questions/comments in the comment box. It could be on:

* Difficulties in replicating experimental results
* Terms and concepts that are not sufficiently elaborated
* How ideas in the paper can be extended further

https://i.redd.it/ahnkvkkfmc111.png",reinforcementlearning,leenz2,False,/r/reinforcementlearning/comments/8nq9p3/learning_the_reward_function_for_a_misspecified/
Contextual bandit - LinUCB,1527839424,"Hi,
I've been learning contextual bandit algorithms and there is any a few examples. I know it is relatively new area of research. If there is someone who understand the concept, can he/she help me with this code? I found it internet as an example and I'm not sure how I can implement it to my own purpose. Let say I have 5x3 ([1,2,3][4,5,6][7,8,9][10,11,12][13,14,15]) how I can add that to code and get answer?

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

data = pd.read_csv('news.csv')

n=data.shape[0]  # number of data points
k=3   # number of features - away_team, draw, home_team
n_a=101  # number of actions

D=data                        # our data
headers = D.columns
th=np.zeros((n_a,k))+0.1      # our real theta, what we will try to guess/


choices=np.zeros(n)
rewards=np.zeros(n)
explore=np.zeros(n)
norms  =np.zeros(n)
b      =np.zeros_like(th)
A      =np.zeros((n_a, k,k))
for a in range (0,n_a):
    A[a]=np.identity(k)
th_hat =np.zeros_like(th) # our temporary feature vectors, our best current guesses
p      =np.zeros(n_a)
alph   =0.2

# LinUCB, using a disjoint model
# This is all from Algorithm 1, p 664, ""A contextual bandit approach..."" Li, Langford
for i in range(0,n):
    x_i = D.iloc[i]   # the current context vector
    
    for a in range (0,n_a):
        A_inv      = np.linalg.inv(A[a])        # we use it twice so cache it
        th_hat[a]  = A_inv.dot(b[a])            # Line 5
        ta         = x_i.dot(A_inv).dot(x_i)    # how informative is this ?
        a_upper_ci = alph * np.sqrt(ta)         # upper part of variance interval
        a_mean     = th_hat[a].dot(x_i)         # current estimate of mean
        p[a]       = a_mean + a_upper_ci        # top CI
      
    norms[i]       = np.linalg.norm(th_hat - th,'fro')    # diagnostic, are we converging ?
    
    # Let's not be biased with tiebreaks, but add in some random noise
    p= p + ( np.random.random(len(p)) * 0.000001)
    choices[i] = p.argmax()   # choose the highest, line 11
    
    # see what kind of result we get
    rewards[i] = th[int(choices[i])].dot(x_i)  # using actual theta to figure out reward
   
    # update the input vector
    A[int(choices[i])]      += np.outer(x_i,x_i)
    b[int(choices[i])]      += rewards[i] * x_i
    
    
print(""done"")




PSEUDO CODE FROM PAPER
0: Inputs: α ∈ R+
1: for t = 1, 2, 3,...,T do
2: Observe features of all arms a ∈ At: xt,a ∈ Rd
3: for all a ∈ At do
4: if a is new then
5: Aa ← Id (d-dimensional identity matrix)
6: ba ← 0d×1 (d-dimensional zero vector)
7: end if
8: ˆθa ← A−1 a ba
9: pt,a ← ˆθ

a xt,a + α
q
x
t,aA−1 a xt,a
10: end for
11: Choose arm at = arg maxa∈At pt,a with ties broken arbitrarily,
and observe a real-valued payoff rt
12: Aat ← Aat + xt,atx
t,at
13: bat ← bat + rtxt,at
14: end for

(http://rob.schapire.net/papers/www10.pdf)",reinforcementlearning,TapirCurrency,False,/r/reinforcementlearning/comments/8nq50e/contextual_bandit_linucb/
[D] Comprehensive Introduction to Monte Carlo Methods,1527808799,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8nn2ha/d_comprehensive_introduction_to_monte_carlo/
Open Philanthropy Project awards $5m in PhD fellowships to 7 DL/RL researchers,1527791125,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8nks6k/open_philanthropy_project_awards_5m_in_phd/
"""BDD100K: A Large-scale Diverse Driving Video Database"" {BAIR} [100k 40s videos: tagging/bounding boxes/segmentation/object detection; multiple cities/days/weather/time; 3 CVPR 2018 Workshop competitions]",1527787722,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8nkb1z/bdd100k_a_largescale_diverse_driving_video/
Looking for Members to join the team on Q-Learning StockTrading,1527778626,"Hey reader,

&amp;nbsp;

I am currently looking for more programmers/members to join the Team! We are going to implement Q\-Learning on StockTrading, will be using a new method that may be working.... Are you experienced with Python Algorithme Coding or something else that you think can stimulate the team, send me a PM with your email and ALL your information about what your coding skills are and yourself. Please note that you won't get money for this project! if it works we will be doing split profits, but don't set money on first priority!

&amp;nbsp;

PM me if you are intrested, leave Telegram, Email and some information about you in the PM. if you have any questions, ask me....

&amp;nbsp;

See you maybe soon!

&amp;nbsp;

Jan",reinforcementlearning,Jandevries101,False,/r/reinforcementlearning/comments/8nj2ab/looking_for_members_to_join_the_team_on_qlearning/
"""Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models"", Chua et al 2018 {BAIR} [deep environmental models w/bootstrap for MPC]",1527732794,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8neazm/deep_reinforcement_learning_in_a_handful_of/
"""Dual Policy Iteration"", Sun et al 2018 [on expert iteration/approximate policy iteration/AlphaZero]",1527726536,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ndoj2/dual_policy_iteration_sun_et_al_2018_on_expert/
"""The Actor Search Tree Critic (ASTC) for Off-Policy POMDP Learning in Medical Decision Making"", Li et al 2018",1527726295,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ndnkd/the_actor_search_tree_critic_astc_for_offpolicy/
Can I inject uncertainty into my observation space for reinforcement learning problems?,1527713869,"I am currently using reinforcement learning to control energy storage systems in smart homes. For this problem, my observation space incorporates the weather forecast and energy demand. The RL agents learns what control strategy to use now based on its observation of what the weather and demand will be in the next 5 hours. Crucially, these observations are all assumed to be known with certainty (Markov). However, in reality, such forecasts will never be certain. So my question is, are there any approaches/papers/ideas out there for incorporating this uncertainty into the learning process?

In addition, based on my description above, can I classify my environment as a partially observable markov decision process? Thanks!",reinforcementlearning,redictator,False,/r/reinforcementlearning/comments/8nc6jm/can_i_inject_uncertainty_into_my_observation/
OpenAI Fellows Fall 2018 applications open: internships for non-academics [deadline: 8 July 2018],1527697225,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8n9wmt/openai_fellows_fall_2018_applications_open/
What are the recent good papers on temporal difference learning? (Ideally with open source code),1527668330,,reinforcementlearning,hmi2015,False,/r/reinforcementlearning/comments/8n6vuq/what_are_the_recent_good_papers_on_temporal/
I have football data and I try to make rl bet profitablely,1527665424,"I have team_home, team_away and I try to make rl which maximize reward so it is using real betting odds to calculate reward. How is that plausible. I already made prototype with deeplearning and it print chance of home_team win, draw and away_team win. I tested to predict always the biggest one but it made a huge loss. Is there way to make rl test different ways to bet? I have some knowledge about rl. In my oppinion problem is that basicly there is no next state. Other problem is that should I give percents that my deep learning program make or raw data which I put into deep learning program? ",reinforcementlearning,UserWithComputer,False,/r/reinforcementlearning/comments/8n6o2i/i_have_football_data_and_i_try_to_make_rl_bet/
Is there have to be a next state in reinforcement learning?,1527664596,I'm making program that predicts football results. I already made Ai that gives percent of each team to win. I made spread sheet where is my Ai's predictions of winners and the real result. I tested how often it predicts right and how much it produce money then. Result was that it lose money. But I started wonder might there be hidden structure which make it profitable to bet in a long run. And how I can find this. Well I started to think reinforcement learning. Problem is thought that I don't have next state because one state is one game and second game is whole different game. Basicly my states go in line and there is just one next state and action dont have any affect on that. So how I can make this kind of rl? Is monte carlo good choise? But how I made it without second state.,reinforcementlearning,UserWithComputer,False,/r/reinforcementlearning/comments/8n6lsl/is_there_have_to_be_a_next_state_in_reinforcement/
"""Playing hard exploration games by watching YouTube"", Aytar et al 2018 {DM} [new _Montezuma's Revenge_ records]",1527652184,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8n5i2z/playing_hard_exploration_games_by_watching/
"""Observe and Look Further: Achieving Consistent Performance on Atari"", Pohlen et al 2018 {DM} [new _Montezuma's Revenge_ records]",1527652181,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8n5i2n/observe_and_look_further_achieving_consistent/
"""Zero-Shot Dual Machine Translation"", Sestorain et al 2018",1527634224,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8n3i7r/zeroshot_dual_machine_translation_sestorain_et_al/
"""Contextual Policy Optimisation"", Paul et al 2018 [curriculum learning via hyperparameter optimization on simulator settings to find informative settings]",1527633505,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8n3f53/contextual_policy_optimisation_paul_et_al_2018/
"""Control Bootcamp"": 37 video lectures on optimal control theory (Steve Brunton, based on _Machine Learning Control_, Duriez et al 2017)",1527632644,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8n3bcg/control_bootcamp_37_video_lectures_on_optimal/
Asynchronous vs Synchronous Reinforcement Learning,1527600078,"When is Asynchronous RL better(and in what sense) than synchronous RL? From what I've gathered it seems to only be better in terms of speed when you have access to a GPU cluster.

My thoughts are with respect to A3C and A2C but I imagine this generalizes ",reinforcementlearning,Data-Daddy,False,/r/reinforcementlearning/comments/8mz1fs/asynchronous_vs_synchronous_reinforcement_learning/
[R] Learning Self-Imitating Diverse Policies,1527596331,,reinforcementlearning,hardfork48,False,/r/reinforcementlearning/comments/8mymzd/r_learning_selfimitating_diverse_policies/
"""LOKI: Fast Policy Learning through Imitation and Reinforcement"", Cheng et al 2018",1527564886,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8mvz3q/loki_fast_policy_learning_through_imitation_and/
[D] Generic Python MCTS library with parallelization?,1527534750,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8mskaz/d_generic_python_mcts_library_with_parallelization/
Reinforcement Learning of Theorem Proving: solves within the same number of inferences over 40% more problems than a baseline prover,1527529640,,reinforcementlearning,AlexCoventry,False,/r/reinforcementlearning/comments/8mrxaw/reinforcement_learning_of_theorem_proving_solves/
Eli5:. The importance of convergence.,1527519675,"I'm just getting started in reinforcement learning, and was recommended this paper [Learning to Predict by the Methods of Temporal Differences](https://link.springer.com/article/10.1023%2FA%3A1022633531479).  So far it has been a really easy read, however there's one line that caught me up and im hoping to get some more clarification on given how important it seems:

""The first step toward a formal understanding of any learning procedure is to prove that it converges asymptotically to the correct behavior with experience.""

Also I was wondering if anyone can put the content in context for me being from 1988, are these TD methods still used or does it go by a new name?

Thanks for any clarification!",reinforcementlearning,avm24,False,/r/reinforcementlearning/comments/8mqoln/eli5_the_importance_of_convergence/
VizDoom maze generator,1527452368,,reinforcementlearning,agiantwhale,False,/r/reinforcementlearning/comments/8mkfmy/vizdoom_maze_generator/
Deep Reinforcement Learning For Sequence to Sequence Models (Source Code),1527354867,,reinforcementlearning,yaserkl,False,/r/reinforcementlearning/comments/8mbjuk/deep_reinforcement_learning_for_sequence_to/
Can I use reinforcement learning for product recommendation in a e-commerce website? Are there any resources available on this topic?,1527348401,,reinforcementlearning,meftaul,False,/r/reinforcementlearning/comments/8mauur/can_i_use_reinforcement_learning_for_product/
"Full ""Gym Retro"" Python library released by OpenAI: &gt;1000 games supported on Atari/TurboGrafx/GB/GBC/GBA/NES/SNES/Master/Genesis/GameGear",1527295444,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8m6j8l/full_gym_retro_python_library_released_by_openai/
"""BLOSSOM: Optimization, fast and slow: optimally switching between local and Bayesian optimization"", McLeod et al 2018",1527213531,,reinforcementlearning,gwern,False,/r/reinforcementlearning/comments/8ly5k4/blossom_optimization_fast_and_slow_optimally/
[D] Can second order methods solve Baird's counter example?,1527097780,[Baird's counter example](https://www.youtube.com/watch?v=Ijyfo_LneEg) is a very specific setup that causes linear nets to diverge when doing bootstrapped updates with SGD. Could second order methods push past it?,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8lldwi/d_can_second_order_methods_solve_bairds_counter/
[R] Small steps and giant leaps: Minimal Newton solvers for Deep Learning [large scale 2nd order optimizer],1527072203,,reinforcementlearning,abstractcontrol,False,/r/reinforcementlearning/comments/8licte/r_small_steps_and_giant_leaps_minimal_newton/
DQN for continuous and discrete action simultaneously,1527066373,"Hello! I am currently implementing a DQN algorithm for a stockmarket portfolio management problem. The actions are discrete - buy, hold or sell and also the action amount which is continuous. Any ideas on how to do this with DQN. I am specifically looking for DQN algorithm.",reinforcementlearning,aditya1702,False,/r/reinforcementlearning/comments/8lhwq8/dqn_for_continuous_and_discrete_action/
Simple but complete explanation of Actor Critic,1527018981,"Can anyone point me to something?
I would like to also understand the math behind it but I’m not comfortable with just looking at complex formulas.

Thank you :)",reinforcementlearning,JunkyByte,False,/r/reinforcementlearning/comments/8ld224/simple_but_complete_explanation_of_actor_critic/
"If RL is used to learn solutions for problems when the reward/transition functions are unknown, how are the simulated learning environments constructed?",1527001051,"My understanding of RL is that it is used to solve problems when the reward and transition functions are unknown \(i.e. for solving model\-free problems\). Otherwise, if these functions are known, model\-based dynamic programming techniques could be used instead.

I see that simulations are often used when finding RL solutions, since a physical implementation is often impractical. My question, though, is how can you build a simulator if the transition and reward functions are unknown?

When building the simulator environment, the RL engineer surely must know these functions \(or close approximations\), otherwise what good would the simulation be? Then, if the AI engineer *does* know these functions so to build the simulator, why then use RL to learn a solution? Why not use some kind of model\-based algorithm, if the model is in fact known to the engineers?

Thanks in advance.",reinforcementlearning,learning_RL,,/r/reinforcementlearning/comments/8lalv2/if_rl_is_used_to_learn_solutions_for_problems/
[D] What is the actual cost function for PPO?,1526991011,"I find the formalism with regards to the PPO confusing. It is confusing even in the PG case.

By that I mean that `grad log policy(p | s) * A` is not an actual cost function. `cross_entropy(policy,target) * A` on the other hand is.

For `min(r * A, clip (r * A, 1 - e, 1 + e))` where `r = policy_new / policy_old` where am I supposed to put in `cross_entropy`? I want to put it into the ratio, but that would make it confusing as it would no longer be a probability which I am guessing the algorithm is expecting.",reinforcementlearning,abstractcontrol,,/r/reinforcementlearning/comments/8l9hkk/d_what_is_the_actual_cost_function_for_ppo/
