title,created_utc,selftext,subreddit,author,media_only,permalink
Pied Piper Compression,1601916655,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/j5n0lz/pied_piper_compression/
Is Dropbox Private and Safe to Use?,1601911400,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/j5lcmw/is_dropbox_private_and_safe_to_use/
Knowledge Transfer in Self Supervised Learning,1601895605,,neuralnetworks,amitness,False,/r/neuralnetworks/comments/j5hcih/knowledge_transfer_in_self_supervised_learning/
Fable III - Intro (Remastered 8K) Resolution increased using neural networks to 8K,1601845990,,neuralnetworks,stepanmetior,False,/r/neuralnetworks/comments/j56pm2/fable_iii_intro_remastered_8k_resolution/
Poker + AI + AR,1601837663,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/j549t3/poker_ai_ar/
Educational Robot,1601832474,Hi guys! Our team has made such an educational robot. We will soon make training courses for him. And we want to release it on Product Hunt. Does someone know a top hunter who is ready to place our project on Product Hunt? We will give a 70% discount on the robot for this. [https://turtlebro.com](https://turtlebro.com),neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/j52rfa/educational_robot/
Internet Freedom Philosophy: How to Use Your Personal Rights,1601832286,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/j52pkp/internet_freedom_philosophy_how_to_use_your/
This computer vision algorithm removes the water from underwater images!,1601732621,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/j4f88w/this_computer_vision_algorithm_removes_the_water/
"I want to know more about the theory (not coding)of deep learning,neural networks,recurrent neural networks,reinforcement learning ect. in prediction of stock/crypto prices.",1601677890,"Hi,i am learning python and i am still pretty much a beginner.One day if i get much better in programming i want to make a trading bot that incorporates some sort of AI.I know that it will be a bumpy road and that it won't be easy.

When i google the terms that i mentioned in the title i struggle a bit to understand the differences.Since i am pretty much a beginner i know that i wouldn't understand the code behind this things.Is there some good source that explains the theory of those subjects,but related to predicting future prices in the stock/crypto/forex market?Like with deep learning you can do this and this,with neural network you can do this and this ect.I would like to take one of those roads one day,to predict the markets...but i don't know what road would be the best for me to predict the markets.So if there was some theory behind it,maybe i would understand how different this subjects are in regards to predictiing future events.I guess you can make prediction models in all of those fields,i just don't understand their differences.

I hope i can get some help here without being overwhelmed with the new information.I need some resources that brake down this complicated things into more digestible pieces so an beginner can understand them.

Thank you",neuralnetworks,PurpleDemand,False,/r/neuralnetworks/comments/j44gdi/i_want_to_know_more_about_the_theory_not_codingof/
Can you make a train a NN to make a NN?,1601674483,"It might be a stupid question, I know next to nothing about NN's, I only know that neural networks have these cost functions which have hundreds of thousands of variables(the weights and biases and the cost) sometimes even several million. and you have to find the minimum of the function using gradient descent. That's all I know about them. Can you train a neural network to make a neural network and train them?",neuralnetworks,mastershooter77,False,/r/neuralnetworks/comments/j43iqh/can_you_make_a_train_a_nn_to_make_a_nn/
I’m doing Botober - neural network generated drawing prompts. Day 2: Angrishreep,1601662061,,neuralnetworks,AI-Dungeon-Drawer,False,/r/neuralnetworks/comments/j3znyj/im_doing_botober_neural_network_generated_drawing/
Course/Bootcamp suggestion for Neural Netwrok and ML,1601655205,"Hello fellow humans, I am a software developer and I have an annual budget  (1k pound) to invest in training from my company. I want to learn ML and Neural Network, but I know myself and I  need an environment where I can have some kind of support (even remote) in case I get stuck, so I was thinking of an online bootcamp or a course with remote support. Do you have any suggestions for my budget?",neuralnetworks,_nuclearpasta,False,/r/neuralnetworks/comments/j3xgcd/coursebootcamp_suggestion_for_neural_netwrok_and/
Reading Group for SOTA Deep Learning Papers,1601637561,"Hello everyone,

I am starting a reading group where we can discuss the current state-of-the-art deep learning techniques/research papers/developments. We will be scheduling sessions where different members of the group will be presenting on a topic (paper review, personal projects, etc) and then we can discuss further ideas and questions regarding the same. I have already started a discord server for this. Moreover, we can discuss possible collaborations among members for related projects and more (like reproducing results of a paper or maybe working on a completely new idea). Coz of COVID, most of us are already working remotely. So, let's utilize this opportunity to increase our knowledge and improve our skills via remote collaborations and discussions. Interested people, join the discord server using this link [https://discord.gg/ZGkdVmy](https://discord.gg/ZGkdVmy).

Thank You",neuralnetworks,backprop_evnthorizon,False,/r/neuralnetworks/comments/j3sou1/reading_group_for_sota_deep_learning_papers/
Build Your Own Artificial Neural Network. It’s Easy! [Light Fun Article],1601633606,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/j3ryt8/build_your_own_artificial_neural_network_its_easy/
Memristor Breakthrough: Third-order nanocircuit elements for neuromorphic engineering,1601632713,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/j3rst9/memristor_breakthrough_thirdorder_nanocircuit/
Web Neural Network API,1601628109,,neuralnetworks,OuterShpongolia,False,/r/neuralnetworks/comments/j3r0ve/web_neural_network_api/
Tell me why this wont work. NN for gold price prediction.,1601620358,"Hello!
I've recently been doing some work involving NN RSM models for opimization purposes. This got me thinking how it would work to create a neural network model based on historical gold prices. I thought about something like this.

-Download data for thousands of historical 5 minute closing prices.

-For each candlestick, use the previous ~5000 sticks as input data and the following 5 sticks as output data.

-After the model is trained, use the same amount of historical data up until the current candlestick. This should give five values as an output for the following five candlesticks.

I'm a noob in this area and this obviously sounds too simple and too good to work. Can anyone tell me why that would probably be the case?",neuralnetworks,WTFnoAvailableNames,False,/r/neuralnetworks/comments/j3pn10/tell_me_why_this_wont_work_nn_for_gold_price/
I’m doing Botober - neural network generated drawing prompts. Day 1: This space station,1601573440,,neuralnetworks,AI-Dungeon-Drawer,False,/r/neuralnetworks/comments/j3d5ct/im_doing_botober_neural_network_generated_drawing/
Biggest challenge in making ML work in the real world with Richard Socher,1601571995,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/j3cnq6/biggest_challenge_in_making_ml_work_in_the_real/
Open Source Neural Compute Camera,1601557421,"Hello everyone,

&amp;#x200B;

I stumbled upon this project on Kickstarter. It is an Open Source Neural Compute Camera for Computer Vision applications.

&amp;#x200B;

[https://www.kickstarter.com/projects/eyecloud/openncc-the-first-ai-vision-appliance?ref=c4gpoq](https://www.kickstarter.com/projects/eyecloud/openncc-the-first-ai-vision-appliance?ref=c4gpoq)

&amp;#x200B;

I was thinking to use such a camera in my office for clocking but then I saw that there is an option to get also the OpenNCC IR+,  which has also a thermal sensor integrated. During these weird times that we live in, with COVID-19 regulations, I am thinking that such a camera that detects Faces, Face masks and Body temperature, might come in handy.  

&amp;#x200B;

Have a look and let me know what you think about their products. From their marketing video it looks really easy to use, implement and deploy my AI model.

&amp;#x200B;

I appreciate it! Cheers",neuralnetworks,kurisutic,False,/r/neuralnetworks/comments/j385qq/open_source_neural_compute_camera/
What is Personal Information and How Can You Protect It?,1601472081,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/j2lzev/what_is_personal_information_and_how_can_you/
"The YOLOv4 algorithm: Introduction to You Only Look Once, version 4. Real-Time Object Detection. What do you think about re-using the ""YOLO"" trademark from different authors?",1601462767,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/j2ju61/the_yolov4_algorithm_introduction_to_you_only/
Extractive &amp; Abstractive Summarization with Transformer Language Models | Research Paper Walkthrough,1601453894,,neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/j2i7fq/extractive_abstractive_summarization_with/
Training A Neural Network To Write Eminem Lyrics,1601423700,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/j2b77j/training_a_neural_network_to_write_eminem_lyrics/
Hellgate: London - all Cinematics (Remastered 8K),1601401685,,neuralnetworks,stepanmetior,False,/r/neuralnetworks/comments/j24b8t/hellgate_london_all_cinematics_remastered_8k/
[News] Introducing ML News (mln.dev),1601394833,"[ML News](https://mln.dev/top/1) (MLN for short) is a community for sharing and discussing all things related to machine learning, deep learning, AI, data science, and the like. (You can join now using the code ***convergence***).

Inspired by Hacker News, Lobste.rs, Reddit, and the original Slashdot, we decided to create a dedicated place for experts and enthusiasts to engage in open learning, discussion, and occasional whimsy.

There are many different channels for tech-related news. In our experience, these are often flooded with not-so-relevant posts which somehow fall under the ""tech"" umbrella. We've had a longstanding habit of sharing interesting ML-related articles and field-defining research with friends and peers, and know that we are by far not the only ones. That's why we decided to build a community dedicated to the field of ML. We hope that by narrowing the scope, we can build a space where ML people like ourselves can easily find interesting news, breakthroughs, and updates in the field, or essentially anything which could pique our interest.

From ML experts to AI enthusiasts, this is a community that values honest debate, curiosity, positivity, and good humor. Please help us maintain these core values as the community grows.

[Join the community](https://mln.dev/) using the code ***convergence***.",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/j21x8l/news_introducing_ml_news_mlndev/
6D pose estimation of a known 3D CAD object with limited model training for a new object,1601340439,,neuralnetworks,gold_twister,False,/r/neuralnetworks/comments/j1ov0x/6d_pose_estimation_of_a_known_3d_cad_object_with/
AI removes any object in the video.,1601317762,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/j1hq4u/ai_removes_any_object_in_the_video/
The King of Messengers: Signal vs WhatsApp vs Utopia,1601299531,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/j1bxtk/the_king_of_messengers_signal_vs_whatsapp_vs/
Sandwich Transformer: Improving Transformer Models by Reordering their Sublayers,1601245136,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/j103h0/sandwich_transformer_improving_transformer_models/
Analyse movie review sentiment with LSTM using Keras,1601230807,"LSTM has long been among the top ranked models for sequence prediction like stock market prediction, speech recognition, etc. because of its ability to process long term dependencies and extracting only the necessary information from any piece of information, neglecting the rest. Sentiment analysis determines the movie reviews as either Positive, Negative or Neutral.
The model needs to undergo pre-processing, training and testing to be ready to implement. To know how to do that, click [here](https://datamahadev.com/lstm-sentimental-analysis-using-keras-with-imdb-dataset/)",neuralnetworks,meg139,False,/r/neuralnetworks/comments/j0vy9c/analyse_movie_review_sentiment_with_lstm_using/
"Are you looking for a place where you can interact live, and talk over text and voice with many other machine learning enthusiasts?",1601216262,"**Are you looking for a place where you can interact live, and talk over text and voice with many other machine learning enthusiasts?**

Then come join our **Discord** server!  
*In short;  Discord is the easiest way to talk over voice, video, and text. Talk, chat, hang out, and stay close with your friends and communities.*

Then **join our community** right now! It's the best place to:   
\- Learn with other enthusiasts  
\- Share your projects, interesting research papers, amazing courses  
\- Ask your questions, anything related to the field of artificial intelligence, machine learning, and deep learning, NLP, computer vision, etc.  
\- Find Kaggle competition teammates, jobs, or just people to talk with!

We are excited to see you joining us! Just click the link below to join the ""Learn AI Together"" community.  
 [https://discord.gg/SVse4Sr](https://discord.gg/SVse4Sr?fbclid=IwAR277hJjB_V8PQgw4CY8ss6HN1o5Xq12r8E9HRWOdkOdLUvk_dheBGVxFtc)",neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/j0rtwb/are_you_looking_for_a_place_where_you_can/
Old Photo Restoration Using Deep Learning | 2020 Novel Approach Explained &amp; Results,1601120596,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/j04lrr/old_photo_restoration_using_deep_learning_2020/
"OCR in the Wild: SOTA in Text Detection and Recognition. This post reviews 3 papers: Textsnake [Long et al., 2018], MORAN [Luo et al., 2019], and FOTS [Liu et al., 2018]",1601119497,,neuralnetworks,Noe_Achache,False,/r/neuralnetworks/comments/j04dzh/ocr_in_the_wild_sota_in_text_detection_and/
Upscale any image TUTORIAL,1601118123,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/j044ge/upscale_any_image_tutorial/
Mini batch gradient descent from scratch in python,1601113363,,neuralnetworks,ai_basics,False,/r/neuralnetworks/comments/j0397c/mini_batch_gradient_descent_from_scratch_in_python/
Need Feedback on Neural Network Based Art Project Idea,1601052344,"Hi all!

For the last few days I've been trying to think of physical systems that can be constructed to perform a computation similar to the computation a neural network performs. I would then construct this and build an art piece that uses these analog signals to perform the task of a neural network.

Let me offer an example. I believe (theoretically), you could hand-build a complicated electronic circuit that could physically model a neural-network-like system. You would do that like this I believe:

1.) Choose appropriate endpoints in the circuit that represent the ""output"" of the network.

2.) Choose appropriate input points to run current through.

3.) Run electric current through the circuit, and take current readings at the endpoints.

4.) Having assigned meaning to the readings you're taking at the endpoints, you perform the backpropagation training process, calculating the desired changes in each connection at the endpoint and adding resistance to the circuit where necessary to nudge the readings in the desired direction.

This is obviously a vague outline of the process, and it would be a tedious process, but if you were willing to undertake it, you could have a completely analog circuit that has been ""trained"" to perform a neural network like calculation.

Electrical current is an obvious example, but (in principle, albeit almost certainly not in practice) you could also use a sufficiently complex mechanical device as an analog neural network, for examples by having an incredibly complex system of levers transferring forces between each other, putting force on an initial set of levers, and then reading other forces as outputs. You then tweak the leverages within the system until you've got a system of levers that represents a desired function between the inputs and outputs. 

I would like to make this idea into an art project, and I'm looking for realistic ideas. I would really appreciate any comments or criticism if I've overlooked a part of the puzzle that would make this an impossible feat.

1.) One example of an idea I've considered is having an optical neural network art piece. Essentially it would be an LED peg board (a la Lite-Brite, but specially designed to fit my purpose), and above the board is a panel that collects the light and funnels into an optical device (perhaps a dense circuit of optical cable) that has been very carefully designed as a 0-9 digit classifier. So you put colored pegs into the bottom, and the more your shape resembles a digit, the more light shines through the optical device onto that digits square. It would be like magic!

What do you guys think? I am not looking for feedback on how tedious a process it would be unless it's very insightful, I'm mostly looking for confirmation that, in principle, according to the mathematics that describe the physical system, I can build a neural network from it.",neuralnetworks,rwill128,False,/r/neuralnetworks/comments/izmzxh/need_feedback_on_neural_network_based_art_project/
Checklist of Necessary Security Measures on the Internet,1601034553,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/izi0df/checklist_of_necessary_security_measures_on_the/
Neuromorphic Computing: The Next-Level Artificial Intelligence,1600954393,,neuralnetworks,Shradha_Singh,False,/r/neuralnetworks/comments/iyxc5f/neuromorphic_computing_the_nextlevel_artificial/
"Does the universe act like a giant neural network? - Dr. Vitaly Vanchurin, Associate Professor, Theoretical Physics and Cosmology, University of Minnesota",1600947719,,neuralnetworks,ObjectiveGround5,False,/r/neuralnetworks/comments/iyvmlf/does_the_universe_act_like_a_giant_neural_network/
"Does the universe act like a giant neural network? - Dr. Vitaly Vanchurin, Associate Professor, Theoretical Physics and Cosmology, University of Minnesota",1600946752,,neuralnetworks,ObjectiveGround5,False,/r/neuralnetworks/comments/iyveu4/does_the_universe_act_like_a_giant_neural_network/
Stochastic gradient descent code from scratch in python,1600946646,,neuralnetworks,ai_basics,False,/r/neuralnetworks/comments/iyve2i/stochastic_gradient_descent_code_from_scratch_in/
Anime Snapchat Filter | ANIME STYLE Filter on Snapchat,1600942556,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/iyujz9/anime_snapchat_filter_anime_style_filter_on/
Need advice on how/where to get started! Before/After photo neural network.,1600919220,"There are so much to this, and I was hoping this reddit would have links in the reddit info sidebar...

I am looking to create a neural network where I input a ""before"" photo, and outputs an ""after"" photo.

I have enough before and after photos for it to learn from... but how do I go about building it?

Any resources for this would be MUCH appriciated!",neuralnetworks,damiz21,False,/r/neuralnetworks/comments/iyprn9/need_advice_on_howwhere_to_get_started/
Editing Obama's Face with StyleGAN2,1600878575,,neuralnetworks,N2AI,False,/r/neuralnetworks/comments/iydh2u/editing_obamas_face_with_stylegan2/
Modern Google-level speech-to-text models released,1600874281,,neuralnetworks,PowerOfLove1985,False,/r/neuralnetworks/comments/iyc36j/modern_googlelevel_speechtotext_models_released/
Anonymity Online: How to Stop Being Afraid of Surveillance?,1600862555,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/iy8qxk/anonymity_online_how_to_stop_being_afraid_of/
Stochastic gradient descent theory part,1600860453,,neuralnetworks,ai_basics,False,/r/neuralnetworks/comments/iy8avm/stochastic_gradient_descent_theory_part/
"With PULSE, you can construct a high-resolution image from a corresponding low-resolution input image in a self-supervised manner!",1600859252,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/iy8250/with_pulse_you_can_construct_a_highresolution/
Regression metrics from scratch,1600834905,,neuralnetworks,ai_basics,False,/r/neuralnetworks/comments/iy3ey7/regression_metrics_from_scratch/
Permutation importance on DNNs,1600760021,"I have a neural network, and I'm using sklearn permutation feature importance, but I'm getting absolutely minimal scores and the features change every run. Any idea what could be causing this? There's slight overfitting, does this completely destroy the scores or is there something else going on?",neuralnetworks,BackgroundDisk4,False,/r/neuralnetworks/comments/ixiwqu/permutation_importance_on_dnns/
Toward Efficient Learning: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks,1600641449,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/iwny83/toward_efficient_learning_modelagnostic/
GAN-backed interpolation of US presidents (90% realistic),1600633157,,neuralnetworks,Snoo_72253,False,/r/neuralnetworks/comments/iwldxi/ganbacked_interpolation_of_us_presidents_90/
Use It Right: What Are the 4 Types of Networks?,1600628685,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/iwjxnv/use_it_right_what_are_the_4_types_of_networks/
"Regression models are prone to overfitting, but here is how to lasso regression overcomes overfitting.",1600624407,,neuralnetworks,Vaishali_Advani,False,/r/neuralnetworks/comments/iwilep/regression_models_are_prone_to_overfitting_but/
Optimizer,1600605256,"I am currently reading the book ""deep learning with python "" by francois chollet.

My question is that wether the optimizer for the loss function will always be a form of stochastic gradient descent or does it change as the network get more complex or serve different purposes ?",neuralnetworks,pixel8756,False,/r/neuralnetworks/comments/iwd1tw/optimizer/
Neural Network Experiments,1600583313,,neuralnetworks,M4XY5T,False,/r/neuralnetworks/comments/iw8pnq/neural_network_experiments/
Neural Network Experiments,1600548216,[removed],neuralnetworks,M4XY5T,False,/r/neuralnetworks/comments/iw0fal/neural_network_experiments/
For all new traders looking to make profit I will teach you my strategy,1600542011,"1) First step subscribe here : [Sign Up Here](https://www.avapartner.com/?ref=136558)

2) Send me a message on Facebook [Here](https://www.facebook.com/mourad.benmeni)

If you invest between 1K to 2K expect $100 to $300 daily profit

Between 5K to 10K expect $300 to $600 daily profit

I have a simply strategy that all you need is to follow my positions on oil and gold.

Only serious investors.",neuralnetworks,z3efyu,False,/r/neuralnetworks/comments/ivyk47/for_all_new_traders_looking_to_make_profit_i_will/
"Using tourists public photos from the internet, they were able to reconstruct multiple viewpoints of a scene conserving the realistic shadows and lightings! Creating photorealistic scenes of the places where you can choose the time of the day!",1600517171,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/ivrcxc/using_tourists_public_photos_from_the_internet/
Transformers: Fall of Cybertron (Resolution increased using neural networks to 8K),1600482895,,neuralnetworks,stepanmetior,False,/r/neuralnetworks/comments/ivk4gu/transformers_fall_of_cybertron_resolution/
Retiming and duplicating people in Video (using neural rendering),1600432743,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/iv5aei/retiming_and_duplicating_people_in_video_using/
GPT-3: new AI can write like a human but don't mistake that for thinking – neuroscientist,1600425897,,neuralnetworks,PowerOfLove1985,False,/r/neuralnetworks/comments/iv3rmp/gpt3_new_ai_can_write_like_a_human_but_dont/
2d to 3d in minutes step by step,1600418976,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/iv2hr5/2d_to_3d_in_minutes_step_by_step/
Did anyone make a neural network for predicting stock prices or forex?,1600365832,"I've done it and I would like to discuss it with someone who has done the same so we can share our discoveries

If you find yourself in that description please DM me",neuralnetworks,Becharac,False,/r/neuralnetworks/comments/iuoumf/did_anyone_make_a_neural_network_for_predicting/
Best language/libraries/resources to build a Neutral Network,1600306970,"I am trying to build a CNN for image classification but don't have any resources on learning how to build a Neural Network. I'm leaning on using Java or C/C++ (if I can't build it with either of these languages I'll just use python) to build out the CNN but don't know of any libraries that will allow me to do so. Can anyone link a github repo or some good neural network resources? If you can't link, its fine, I just want to know where to get started.",neuralnetworks,NoodleArmMafia,False,/r/neuralnetworks/comments/iu9q2j/best_languagelibrariesresources_to_build_a/
"PiFuHD: A new method for high-fidelity 3d reconstruction. It only needs a single image of you to generate a 3D avatar that looks just like you, even from the back!",1600253455,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/itt55z/pifuhd_a_new_method_for_highfidelity_3d/
Text Summarization of COVID-19 Medical Articles using BERT and GPT-2 | Research Paper Walkthrough,1600250136,,neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/itsjdn/text_summarization_of_covid19_medical_articles/
How to Delete Telegram Account Permanently?,1600241448,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/itr292/how_to_delete_telegram_account_permanently/
How do you like my Remaster? Resolution increased using neural networks to 8K,1600196544,,neuralnetworks,stepanmetior,False,/r/neuralnetworks/comments/itf8g4/how_do_you_like_my_remaster_resolution_increased/
Deep Neural Networks with Box Convolutions / Code and Paper Link in the Comment,1600180158,,neuralnetworks,TheInsaneApp,False,/r/neuralnetworks/comments/it9v6m/deep_neural_networks_with_box_convolutions_code/
HELP ! : Speech Denoiser project,1600167362,"So the aim of our project is to create an application which will take audio file as an input, suppress the background noises and give this clean audio as an output. However, the problem is that my team mates are stupid and aren't motivated and my project guide barely knows anything about machine learning. I've done quite some research but I'm still pretty confused as to how it's actually done.

&amp;#x200B;

Can anyone please explain me the step-by-step procedure to make such a model using Neural Networks ?",neuralnetworks,JackTheRipper_17,False,/r/neuralnetworks/comments/it6h20/help_speech_denoiser_project/
[For beginners] A visualization of a Perceptron,1600119421,,neuralnetworks,hotcodist,False,/r/neuralnetworks/comments/isur5a/for_beginners_a_visualization_of_a_perceptron/
"Joscha Bach on GPT-3, Hopfield Networks, achieving AGI, machine understanding and lots more",1600118723,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/isuj1v/joscha_bach_on_gpt3_hopfield_networks_achieving/
The Best Hacker Movies of All Time,1600092945,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/islobt/the_best_hacker_movies_of_all_time/
[Article] A deep dive into GhostNet (CVPR 2020) with code in PyTorch and TensorFlow,1600092253,"This article covers feature maps in convolutional neural networks, a deep dive into GhostNet, and an analysis of its capabilities and shortcomings. More specifically, we’ll cover:

* Convolution, depthwise convolution, and feature map pattern redundancy 
* Ghost Convolution in PyTorch and TensorFlow, as well as the Ghost Bottleneck and full architecture in PyTorch
* Results of GhostNet on various tasks and datasets
* Limitations of the proposed model 

Link to the article: [https://blog.paperspace.com/ghostnet-cvpr-2020/](https://blog.paperspace.com/ghostnet-cvpr-2020/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/islgvp/article_a_deep_dive_into_ghostnet_cvpr_2020_with/
Jupiter extension which converts English queries into relevant python code,1600092100,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/islf4s/jupiter_extension_which_converts_english_queries/
Well read Students Learn Better: On The Important Of Pre-training Compact Models,1600034711,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/is7uia/well_read_students_learn_better_on_the_important/
What's the best approach to develop a neural network that removes stuttering?,1599933765,I'm wondering what would be the best approach to get an AI to remove stuttering that you would take?,neuralnetworks,Ravingsmads,False,/r/neuralnetworks/comments/irhevu/whats_the_best_approach_to_develop_a_neural/
Can GPT-3 really help you and your company? What can it really do? Real-World Applications Demo,1599913641,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/irbove/can_gpt3_really_help_you_and_your_company_what/
"What is the simplest entry into NN image classification systems, as a C-callable library?",1599866811,"I'd like to classify images using a neural net.  Not cats and dogs, but 'abstract science patterns' for want of a better description.   Can anyone suggest the simplest entry point?   I do not use Python, and I'd like to call it from another language using normal C-style calls.   OpenCL  or some other cross-platform GPU support would be a big plus.  CUDA not so much.

I'd like to train it with 2 lists of images, ""Junk"" and ""Good"", and then have it classify other images into these classes.

I've gone through  various lists of NNs and haven't found anything that matches the features I want.

Best candidates:

* **Darknet** - nice, because it is C, not C++, and has CUDA (not OpenCL?).  But it looks like it stopped being developed, and it's not a library but a command line program.   Too bad, because I suspect there might be a huge demand for a simple C-based CNN library.

* **TensorFlow** - huge, byzantine, and C++, but one could write a C wrapper.  It looks like a long learning experience, and the C++ docs aren't as good as the Python ones (or weren't last time I looked)

* **Keras** - friendlier front end to TensorFlow that can be used to build C++ models, that can be wrapped in C, to get where I want to be.  This was recommended to me.  This would involve training in Python, and compiling to C++ with C wrapper. 

* **FANN, genann, and Peter van Rossum's Lightweight Neural Network** - C, but probably not usable for image classification.



Can anyone suggest anything else, or comment on the above?  Thanks!",neuralnetworks,manoan,False,/r/neuralnetworks/comments/ir1q63/what_is_the_simplest_entry_into_nn_image/
Best Machine Learning Books for Beginners and Experts in 2020,1599838780,"Hey friends, I have curated a list of **the best Machine learning books** out there to learn Machine Learning in 2020. 

You can view it on my blog - [https://thecodingpie.com/post/best-machine-learning-books-for-beginners-and-experts-in-2020/](https://thecodingpie.com/post/best-machine-learning-books-for-beginners-and-experts-in-2020/)

No matter if you are a beginner in machine learning or already an Intermediate/Expert in this field, there is an awesome book waiting for you!

I hope you will find this book suggestions helpful in your great Machine Learning Journey and beyond. If you think I have forgotten any great book then please let me know. Thank you ;)",neuralnetworks,thecodingpie,False,/r/neuralnetworks/comments/iqsw6v/best_machine_learning_books_for_beginners_and/
Is There a WhatsApp Alternative for Daily Chatting?,1599820390,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/iqnzym/is_there_a_whatsapp_alternative_for_daily_chatting/
Considerations on the arithmetic of convolutional neural networks,1599765963,,neuralnetworks,koteth,False,/r/neuralnetworks/comments/iqaboc/considerations_on_the_arithmetic_of_convolutional/
Elon Musk Neuralink,1599756886,,neuralnetworks,m1900kang2,False,/r/neuralnetworks/comments/iq78lt/elon_musk_neuralink/
Difference between MULTICORE TSNE and TSNE | machine learning | data sci...,1599752340,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/iq5q24/difference_between_multicore_tsne_and_tsne/
Good source to get into programming neural networks?,1599746188," Hey,  
I'm looking for some kind of guidance that helps me get into neural networks. Videos or text is fine, preferably in python. I'd like a long series which goes deep, not just a quick overview. Anything you guys can recommend?",neuralnetworks,MajLenn,False,/r/neuralnetworks/comments/iq3rel/good_source_to_get_into_programming_neural/
neural network in google spreadsheets,1599667744,"I'm not sure if this fits here but, i've recently tried making a neural network from scratch, and realized in theory would be possible to make in excel/excel-like programs, however with what i only know of excel it will be garbage would anyone who knows more about it please take a look at this and maybe point out mistakes/improvements? Thanks in advance

https://docs.google.com/spreadsheets/d/1zzbKjdKxAX1XKEuXuALbT0wtAJDZSft3yRPvA7LnX30/edit?usp=drivesdk",neuralnetworks,-Enter-Name-,False,/r/neuralnetworks/comments/ipivw6/neural_network_in_google_spreadsheets/
MULTI CORE TSNE implementation | data science | machine learning | explo...,1599666129,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/ipibwm/multi_core_tsne_implementation_data_science/
Zenia uses advanced machine learning and movement recognition technologies to give real-time feedback on your performance.,1599659688,,neuralnetworks,MatthewFundedSecured,False,/r/neuralnetworks/comments/ipgb9j/zenia_uses_advanced_machine_learning_and_movement/
Bloomberg: Who Is Responsible for Blocking the Internet in Belarus?,1599653843,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/ipersl/bloomberg_who_is_responsible_for_blocking_the/
AI Generates Real Faces From Sketches! DeepFaceDrawing Overview | Image-to-image translation in 2020,1599653308,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/ipen2q/ai_generates_real_faces_from_sketches/
SpanBERT: Improving Pre-training by Representing and Predicting Spans | Research Paper Walkthrough,1599649514,,neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/ipdt2z/spanbert_improving_pretraining_by_representing/
"Neural network controlled homing rocket, a personal learning project. The rocket has forward and rotation thrusters which are controlled by a small (feedforward) neural network taking the position, velocity etc. as inputs. An evolutionary algorithm was used to generate the NN weights.",1599647856,,neuralnetworks,rhkibria,False,/r/neuralnetworks/comments/ipdhkg/neural_network_controlled_homing_rocket_a/
Enroll today for free online certificate course of Python for Machine,1599634366,,neuralnetworks,greatlearningsandip,False,/r/neuralnetworks/comments/ipb2ud/enroll_today_for_free_online_certificate_course/
"Has anyone followed the ""Neural Networks From Scratch"" book (nnfs.io) to learn about neural networks? Any reviews?",1599601718,"This guy sentdex made a youtube series about creating python neural networks from scratch. It was very easy to understand and I was enjoying it greatly but the series stops in the middle (stopped producing vids due to covid). He is also in the process of completing an accompanying book which closely follows the videos. I am wondering if it is worth buying the book in order to complete my understanding and if anyone has had any good experiences/ hang ups when reading through it. 

&amp;#x200B;

Thanks",neuralnetworks,binjo1,False,/r/neuralnetworks/comments/ip2yyt/has_anyone_followed_the_neural_networks_from/
[Open Discussion] Why 0-avg 1-std embeddings or layers in NNs ?,1599575213,[removed],neuralnetworks,danringwald,False,/r/neuralnetworks/comments/iouipg/open_discussion_why_0avg_1std_embeddings_or/
What is Dimensionality reduction ? and why is it important for our Machine Learning Projects. Click here to know more,1599574155,,neuralnetworks,instigator-001,False,/r/neuralnetworks/comments/iou7sh/what_is_dimensionality_reduction_and_why_is_it/
[Request] Is someone willing to help me understand both the math and the reality behind the chain rule used in back propagation?,1599570237,"Hi 😊
I'm a skilled programmer trying to learn the foundation of how an ANN works by building one from the ground up. I've gotten pretty far in building an API in C#, but for the final step (back propagation and weight modification) I don't have the math skills (yet) to grasp the chain rule and go from theory/formula to code.

I've read about a dozen articles on the subject, but the Greek symbols and the expected level of math competency is just out of reach for me 😊

Is there a competent, pedagogical and kind soul out there who would assist in equipping me with the mental building blocks to conquer this obstacle?

Kind regards - Jakob",neuralnetworks,Scepticoder,False,/r/neuralnetworks/comments/iot5wh/request_is_someone_willing_to_help_me_understand/
Emails and Messengers: What Is Secure Communication?,1599568322,,neuralnetworks,Blewette90,False,/r/neuralnetworks/comments/iosp1l/emails_and_messengers_what_is_secure_communication/
Python pandas,1599541135,,neuralnetworks,greatlearningsandip,False,/r/neuralnetworks/comments/ionj4v/python_pandas/
Transformer Architecture Explained,1599494742,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/ioa6vc/transformer_architecture_explained/
Google Data Tracking: How to Track Users on The Network,1599484943,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/io7c6a/google_data_tracking_how_to_track_users_on_the/
Google Uses AI and Augmented Reality To Detect Cancer With A Microscope,1599435322,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/inwkys/google_uses_ai_and_augmented_reality_to_detect/
Google Uses AI and Augmented Reality To Detect Cancer With A Microscope,1599434643,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/inweak/google_uses_ai_and_augmented_reality_to_detect/
Data Science best practices one must follow,1599428256,,neuralnetworks,datamahadev,False,/r/neuralnetworks/comments/inujdm/data_science_best_practices_one_must_follow/
"Neural Networks: A 30,000 Feet View for Beginners",1599419898,"[https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/](https://www.learnopencv.com/neural-networks-a-30000-feet-view-for-beginners/)  


This post is written for absolute beginners who are trying to dip their toes in Machine Learning and Deep Learning and is aptly titled Neural Networks: A 30,000 Feet View for Beginners 

https://preview.redd.it/p12wgu6yukl51.jpg?width=768&amp;format=pjpg&amp;auto=webp&amp;s=b7c451d452c1da4a77a41fc42fe179db31812491",neuralnetworks,spmallick,False,/r/neuralnetworks/comments/ins2w9/neural_networks_a_30000_feet_view_for_beginners/
Looking for advice,1599409725,"Looking for some help with creating a Neural Net to add greenery to a photo. Basically, it should be able to take a photo of some urban landscape as input and add some bushes where appropriate, or fill rooftops with grass. I'm new to neural nets, so I would be very grateful for some advice on where to start or what architectures to look into.",neuralnetworks,Daniel_Rybe,False,/r/neuralnetworks/comments/inoz32/looking_for_advice/
How to read image data in google colab?,1599408784,[removed],neuralnetworks,SayeedAbidEfaz,False,/r/neuralnetworks/comments/inoovk/how_to_read_image_data_in_google_colab/
ISOMAP implementation | data science | machine learning | deep learning ...,1599372015,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/ingx10/isomap_implementation_data_science_machine/
AI generated Ricardo,1599312656,,neuralnetworks,Snoo_72253,False,/r/neuralnetworks/comments/in1533/ai_generated_ricardo/
"ECCV 2020's Best Paper Award! A new architecture for Optical Flow, with code publicly available! (Video cover and demo)",1599305400,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/imzl07/eccv_2020s_best_paper_award_a_new_architecture/
Familiar Faces But A Different Voice [Wav2Lip],1599276220,,neuralnetworks,cloud_weather,False,/r/neuralnetworks/comments/imu3y2/familiar_faces_but_a_different_voice_wav2lip/
"Is this processing unit a neuron, FSM, or something else?",1599270077,"The network has N discrete symbols (here N=8). For each processing unit in the network, a modifiable weight exists between each input and output symbol (here an 8x8 matrix of weights). An arbitrary number of input connections may exist, in this example there are 10, which can come from other units, or from the unit's own memory of some number of previous outputs (self-feeding, memory). Each input contributes a value to a possible output based on the weighting, and could also have an overall weight applied as a pre-multiplier. These are summed together. There are two ways to run the output: the highest value is set as the output symbol; or, probabilistic output can be used with weight based on the relative values.

[Weight matrix and matrix applied to input symbols](https://preview.redd.it/rc6o4idpa8l51.png?width=1232&amp;format=png&amp;auto=webp&amp;s=7bde944e5fd74385c2adbf42801f3788af447a2f)

I have tried to apply this unit type in simple simulations. Here it is 50-100 ""ants"" moving and eating based on the symbol in the final output, this network is small but has 26 symbols (I just like to see A-Z I guess). They reproduce with slight mutations in the weight matrix. When I look at the temporal string trace (bottom) it seems that these units can have a range of behaviors where they stay at the same value, a small cycle of values, and when inputs change dramatically they will shift into new patterns but there seems to be a potential for using the built-in memory of previous states to keep loops or values in state for longer times.

[An \\""ant\\"" simulation using this kind of symbol-based network](https://preview.redd.it/s8iwvhjad8l51.png?width=1920&amp;format=png&amp;auto=webp&amp;s=9a1622464ec0060738e90b54155b021b5997f33c)

One interesting thing about using fixed symbols (just a set of integers really) as the valid input/output state is that I can encode easily enough something with a range 0-1 on this scale (like ""how hungry are you?"", but I can also encode categorical information such as an object's ""type/sub-type"" adjacent in the agent's environment which it ""senses"", and I don't have to do binary encoding using many units or try to represent it as a series of abstract floating point values. I am working on a GPU implementation of this that can run faster on larger networks, but I don't completely understand the dynamics of this type of processing unit yet.",neuralnetworks,BinaryAlgorithm,False,/r/neuralnetworks/comments/imspas/is_this_processing_unit_a_neuron_fsm_or_something/
[D] Is OpenAI’s GPT-3 API Beta Pricing Too Rich for Researchers?,1599260338,"OpenAI’s 175 billion parameter language model GPT-3 (Generative Pre-trained Transformer 3) turned heads in the NLP community when it was released in June, and now it’s back in the spotlight. A Reddit [post](https://www.reddit.com/r/GPT3/comments/ikorgs/oa_api_preliminary_beta_pricing_announced/) this week by independent writer and researcher Gwern Branwen detailed the pricing plan OpenAI has provided to GPT-3 Beta API users. The scheme, which goes into effect on October 1, has already raised as many questions as it has answered.

Here is a quick read: [Is OpenAI’s GPT-3 API Beta Pricing Too Rich for Researchers?](https://syncedreview.com/2020/09/04/is-openais-gpt-3-api-beta-pricing-too-rich-for-researchers/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/imq9rq/d_is_openais_gpt3_api_beta_pricing_too_rich_for/
Train vast neural networks together,1599241974,,neuralnetworks,JeffreyFreeman,False,/r/neuralnetworks/comments/imkru5/train_vast_neural_networks_together/
Researchers study why neural networks are efficient in their predictions,1599235549,,neuralnetworks,key_info,False,/r/neuralnetworks/comments/imipgy/researchers_study_why_neural_networks_are/
UMAP VS KMEANS CLUSTERING | data science || machine learning || deep lea...,1599232696,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/imhth5/umap_vs_kmeans_clustering_data_science_machine/
UMAP VS KMEANS CLUSTERING | data science || machine learning || deep lea...,1599232013,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/imhm2t/umap_vs_kmeans_clustering_data_science_machine/
Updated Cyber Crime Statistics For 2020,1599228806,,neuralnetworks,AnnaConner,False,/r/neuralnetworks/comments/imgo0u/updated_cyber_crime_statistics_for_2020/
Accurately Lip-syncing Videos,1599213048,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/imd3zv/accurately_lipsyncing_videos/
"Looking for some help, with using a neural network to control a car which can only drive in a straight line. I'm knew to neural networks but have a good understanding of the basics. The car should drive 1000m forward then stop.",1599211683,"So essentially I'm trying to get a neural network to control a car, the car should drive in a straight line, and then stop 1000m ahead of where it started.  I can't get it to work so far, and I believe it's related to the loss function I'm using which is Mean squared error. Specifically im applying it to both the the displacement of the car and the target as well as to the velocity of the car and it's velocity target(=0). The input into the neural network is the error between the cars displacement and its target displacement, and the same thing for the velocity.

Currently the car appears to just continuously accelerate and then shot past the target location.

If you guys could help me with this I would greatly appreciate it, or if you have some resources for me which I could use/read it would help me a ton, thanks guys.",neuralnetworks,bucketking16,False,/r/neuralnetworks/comments/imcviw/looking_for_some_help_with_using_a_neural_network/
Neural Dynamics on Complex Networks,1599173880,,neuralnetworks,m1900kang2,False,/r/neuralnetworks/comments/im491s/neural_dynamics_on_complex_networks/
"Why should you learn ""Python"" in 2020???",1599154215,https://youtu.be/rJy8MK-FxHc,neuralnetworks,8329417966,False,/r/neuralnetworks/comments/ily0fr/why_should_you_learn_python_in_2020/
"We all must have done POS tagging in our English classes, but nowadays Machine learning models can also do such complex tasks. Here is a pictorial example in which the Hidden Markov Model (HMM) tags each word in a sentence. You'll also find how to use the Viterbi algorithm.",1599142698,,neuralnetworks,Vaishali_Advani,False,/r/neuralnetworks/comments/ilu8zm/we_all_must_have_done_pos_tagging_in_our_english/
The Biggest DDoS Attacks in History,1599141469,,neuralnetworks,Blewette90,False,/r/neuralnetworks/comments/iltvr0/the_biggest_ddos_attacks_in_history/
Python Pandas Tutorial,1599117256,,neuralnetworks,greatlearningsandip,False,/r/neuralnetworks/comments/ilot1j/python_pandas_tutorial/
LSBert: A Simple Framework for Lexical Simplification | Research Paper Walkthrough,1599018321,,neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/ikzj2m/lsbert_a_simple_framework_for_lexical/
How would you enter histogram data into a NN?,1599002595," [https://i.stack.imgur.com/OBZWE.png](https://i.stack.imgur.com/OBZWE.png) 

This kind of thing. I have some histograms and each of them has an output value. I would like to enter a bunch of histograms into the NN and it would return me the value of each histogram. 

So the problem is that I have 2 sets of values, 10 values on the x axis and 10 on the y axis, but they are connected to each other. Each x has it's corresponding y. 

Could I have 20 input neurons where first 10 are x values and second 10 are y values or would that make no sense? Is there a way to give dual value to a neuron, like a tuple?",neuralnetworks,ATCWannabeme,False,/r/neuralnetworks/comments/ikvexm/how_would_you_enter_histogram_data_into_a_nn/
[N] Elon Musk’s Neuralink Demos Brain-Computer Link in Live Pigs,1598972497,"Last Wednesday, Elon Musk caught the AI community’s attention with a tweet announcing an upcoming “Live webcast of working [u/Neuralink](https://www.reddit.com/u/Neuralink/) device…” On Friday, the billionaire tech entrepreneur delivered. With pigs.

Here is a quick read: [Elon Musk’s Neuralink Demos Brain-Computer Link in Live Pigs](https://syncedreview.com/2020/08/31/elon-musks-neuralink-demos-brain-computer-link-in-live-pigs/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/iklupl/n_elon_musks_neuralink_demos_braincomputer_link/
Stuck understanding an outcome,1598931320,[removed],neuralnetworks,Lemonfudge1,False,/r/neuralnetworks/comments/ikczly/stuck_understanding_an_outcome/
HOW TO CHOOSE PARAMETERS FOR CYCLICAL LEARNING RATES || machine learning...,1598888163,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/ik07g6/how_to_choose_parameters_for_cyclical_learning/
I built a python Neural network from scratch with only numpy,1598851841,"    import numpy as np
    class NeuralNetwork:
    	def __init__(self, arg):
    		self.Layers = []
    		self.Weights = []
    		self.LayerCount = len(arg)
    		for i in range(self.LayerCount):
    			bias = 1 if (i != self.LayerCount-1) else 0
    			self.Layers.append(np.zeros(arg[i] + bias))
    			self.Layers[i][0] = bias
    		for i in range(self.LayerCount-1):
    			x = len(self.Layers[i])
    			y = len(self.Layers[i+1])
    			self.Weights.append(np.random.rand(x*y).reshape(x, y))
    
    	#sigmoidal activation and deactiavtion
    	Activate = lambda self, x: 1 / (1+np.exp(-x))
    	DActivate = lambda self,x: x * (1-x)
    	def SetInputs(self, x):
    		self.Layers[0][1:] = x
    	def GetOutputs(self):
    		return self.Layers[self.LayerCount-1]
    	def GetInputs(self):
    		return self.Layers[0][1:]
    
    	def Propagate(self):
    		for i in range(self.LayerCount-1):
    			z = np.dot(self.Layers[i], self.Weights[i])
    			if(i == self.LayerCount-2):
    				self.Layers[i+1] = self.Activate(z)
    			else:
    				self.Layers[i+1][1:] = self.Activate(z[1:])
    
    	def SaveWeights(self, FileName):
    		outLayers = []
    		for i in range(self.LayerCount):
    			outLayers.append(len(self.Layers[i]))
    		np.savez(FileName, Weights=self.Weights, Layers=outLayers)
    
    	def Load(self, FileName):
    		LoadOnDemand = np.load(name, allow_pickle=True)
    		layerLOD = LoadOnDemand['Layers']
    		LayerArray = []
    		for i in range(len(layerLOD)):
    			biasrem = (i != len(layerLOD)-1)
    			LayerArray.append(layerLOD[i]-biasrem)
    		self.__init__(LayerArray)
    		self.Weights = LoadOnDemand['Weights']
    
    	def Train(self, Data, Epoch=1):
    		print(f'Training with {Epoch} epoch')
    		errorTemp = []
    		for e in range(Epoch):
    			for Sample in Data:
    				self.SetInputs(Sample[0])
    				self.Propagate()
    				output = self.GetOutputs()
    				error = np.subtract(Sample[1], output)
    				errorTemp.append(error)
    				ErrorArray = []
    				for l in range(self.LayerCount-1, 0, -1):
    					if(l == self.LayerCount-1):
    						ErrorArray = error
    					Zder = self.DActivate(self.Layers[l])*ErrorArray
    					if(l != 1):
    						ErrorArray = np.dot(Zder, self.Weights[l-1].T)
    					for i in range(len(self.Weights[l-1])):
    						for j in range(len(self.Weights[l-1][i])):
    							self.Weights[l-1][i][j] += self.Layers[l-1][i] * Zder[j]
    			print(f'Iter {e} / {Epoch} | {int((float(e) / float(Epoch)) * 100)}% |', end='')
    			print(f' Error: {np.absolute(np.average(errorTemp))}')

I have been trying to get neural networks to work on my computer but all the ones I have tried only worked on a linux machine. Out of this fact and also the fact that I wanted to know the math behind them, I started building one from scratch. I started when I was 9 and 7 years later, i'm done. it's nothing special, it automates the bias but it still runs on one CPU thread and it doesn't use GPU at all but all I wanted was a neural network that you gave it inputs and it trained on the outputs. thought I would share it!

    nn = NeuralNetwork([layer lengths go here])
    example: nn = NeuralNetwork([2, 3, 1])
    
    training:
    nn.Train(Data, Epoch)
    example: nn.Train(training_data, 1000)
    
    training Sample setup: 
    [[inputs], [expected]]
    
    training data setup:
    array = [ [[inputs], [outputs expected]], ... ]
    example for XOR function:
    test = [ [[0, 0], [0]],
             [[1, 0], [1]],
    	 [[0, 1], [1]],
    	 [[1, 1], [0]] ]
    
    getting output from trained network:
    nn.SetInputs([inputs])
    nn.Propagate()
    print(nn.GetOutputs())",neuralnetworks,owentheprogrammer,False,/r/neuralnetworks/comments/ijsasi/i_built_a_python_neural_network_from_scratch_with/
Test-Train Overlap Problem - Question and Answer Test-Train Overlap in Open Domain Question Answering Datasets,1598829145,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/ijmsiy/testtrain_overlap_problem_question_and_answer/
Hopfield Networks is All You Need,1598821554,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ijkjbh/hopfield_networks_is_all_you_need/
[Tutorial] How to clone anyone's voice with just 6 lines of code.,1598713516,"I am blown away with the voice quality of the cloned voice it is hardly distinguishable from the original sound. I cloned Adele voice and it sounds damn real. I used the code from GitHub repo of the implementation of ""Transfer learning from speaker verification to multi speaker text to speech synthesis"" by CorentinJ and ran the code in Google Colab.

[Here is the YouTube video I made on how to run the code.](https://youtu.be/SmsEHNaI77o)",neuralnetworks,ghumman31,False,/r/neuralnetworks/comments/iitvj5/tutorial_how_to_clone_anyones_voice_with_just_6/
Language-Guided Navigation in 3D Environment. ECCV2020 paper By Facebook AI Research (with code publicly available!) Video introduction &amp; demo,1598710064,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/iisydt/languageguided_navigation_in_3d_environment/
[Tutorial] Build a VAE in Keras,1598627358,"Variational autoencoders are one of the most fundamental neural network architectures, but many don’t know how they work, let alone how to build one. This tutorial covers how to build an end-to-end VAE to encode and decode images in Keras. Full code included.

Tutorial link: [https://blog.paperspace.com/how-to-build-variational-autoencoder-keras/](https://blog.paperspace.com/how-to-build-variational-autoencoder-keras/) 

Run the code for free:  [https://ml-showcase.paperspace.com/projects/build-an-autoencoder-from-scratch-in-keras](https://ml-showcase.paperspace.com/projects/build-an-autoencoder-from-scratch-in-keras)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/ii8rhv/tutorial_build_a_vae_in_keras/
[Question] Bad CNN architecture,1598616472,"I posted here a week or two ago asking about how to make a simple CNN that will recognize if a person is wearing a mask properly or not. I played around a bit and got about 90% accuracy on dataset of about 1k pics.   


In my time experimenting with the different architectures I found that some small changes have made some great impact on the accuracy of the program, by dropping it down for a great deal.   
So as a little research i would like see what combos of layers get the BAD results. I know that a lot of it is probability, but for some models it was more recurring to perform bad than the others.  


My question is do you know some kind of setups that are more likely to do bad or maybe just some things that when added improved the results so i can maybe reverse-engineer.  


Thanks alot &lt;3",neuralnetworks,Alonenever01,False,/r/neuralnetworks/comments/ii5uyw/question_bad_cnn_architecture/
Meet Silq- The First Intuitive High-Level Language for Quantum Computers,1598611293,,neuralnetworks,Shradha_Singh,False,/r/neuralnetworks/comments/ii4ssw/meet_silq_the_first_intuitive_highlevel_language/
11 Must-Know Machine Learning Algorithms for AI Professionals,1598611277,,neuralnetworks,Shradha_Singh,False,/r/neuralnetworks/comments/ii4sph/11_mustknow_machine_learning_algorithms_for_ai/
SQUARE ROOT LEARNING RATE SCHEDULER || machine learning || deep learning...,1598542206,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/ihn491/square_root_learning_rate_scheduler_machine/
"Brain computer interfacing and how it relates to NN [long post, no TLDR]",1598536788,[removed],neuralnetworks,notadoctororacake,False,/r/neuralnetworks/comments/ihliqo/brain_computer_interfacing_and_how_it_relates_to/
AI Algorithm Discovers 50 New Planets,1598536493,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/ihlg0c/ai_algorithm_discovers_50_new_planets/
"Check out this very well laid out, detailed guide on artificial neural networks and how they work.",1598532186,,neuralnetworks,the_notorious_beast,False,/r/neuralnetworks/comments/ihkc2c/check_out_this_very_well_laid_out_detailed_guide/
Mind Blowing: How can fit method be used for evaluation in Keras? what did I miss?,1598488898,"Hello,  
so this is a code of Human Re-identification, it's called AdaptiveReID, you can find it on github.  
Anyway, you can read starting from the line 1131, as I know fit function in Keras is used only for training, shouldn't he use predict instead? If someone has an expert eye in this field and it's easy to explain, I would like to know. Otherwise just ignore the post, and Have a nice day:)

https://preview.redd.it/ubvpqqupxfj51.png?width=1865&amp;format=png&amp;auto=webp&amp;s=f0426b661b023957b05acfb5ebd1315bf2285922",neuralnetworks,ComfortableCod,False,/r/neuralnetworks/comments/ihb5wk/mind_blowing_how_can_fit_method_be_used_for/
FACTOR LEARNING RATE SCHEDULER || machine learning || deep learning || d...,1598458850,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/ih1xkx/factor_learning_rate_scheduler_machine_learning/
Object Detection in the zoo,1598445297,,neuralnetworks,Snoo_72253,False,/r/neuralnetworks/comments/igxzqb/object_detection_in_the_zoo/
"Learn more about types of networks, uses of neural networks, types of neural network libraries, and hands-on training on TensorFlow 2.0.",1598444312,,neuralnetworks,Reginald_Martin,False,/r/neuralnetworks/comments/igxrh0/learn_more_about_types_of_networks_uses_of_neural/
Elon Musk’s ‘working Neuralink device’ will debut this Friday over a live webcast,1598440141,,neuralnetworks,Shradha_Singh,False,/r/neuralnetworks/comments/igwvsl/elon_musks_working_neuralink_device_will_debut/
System built by USC researchers reconstructs a fully textured 3D human from each frame,1598438705,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/igwlos/system_built_by_usc_researchers_reconstructs_a/
Weakly Supervised Color Aware GAN for Controllable Makeup Transfer,1598437604,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/igwe0k/weakly_supervised_color_aware_gan_for/
DeepWalk: Online Learning of Social Representations | Research Paper Walkthrough,1598435439,,neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/igvzd1/deepwalk_online_learning_of_social/
What do you want in a neural network studio?,1598395608,"I want to build a graphical editor that allows people to build and train networks with drag and drop, automated functions at the click of a button, and the ability to add your own code wherever desired. What things would you want included so that this neural network studio would actually be useful to you?

I’ve spent a while looking around at the competition and know what kind of things I want to implement for myself, but I’m fairly new to neural networks and want to get your opinions on tools you would love to have, or workflows that you would love to have more streamlined / automated.",neuralnetworks,BflySamurai,False,/r/neuralnetworks/comments/igmxtt/what_do_you_want_in_a_neural_network_studio/
Best Image Colorization AI as of 2020,1598359778,,neuralnetworks,cloud_weather,False,/r/neuralnetworks/comments/igbis8/best_image_colorization_ai_as_of_2020/
Question about neural networks for data analysis.,1598353208,"I have a little experience with neural networks based on the keras, TensorFlow library, but I am newer to neural networks. I have data about 100 fields, and one field is with a value of True/False. Is there a universal library that could analyze the dependance of fields on the result, discards the unnecessary data that does not affect the result, and, after training, predicts the outcome using new data?

Example: Swimmers: weight, height, age, average speed, etc. and control field win or not in the competition (True/False) Based on the sample, the neural analyzes the data, discards information that does not affect the result and predicts the result for new data.

Can you recommend such a library with a description? Possibly with test data.",neuralnetworks,_IUI,False,/r/neuralnetworks/comments/ig9wok/question_about_neural_networks_for_data_analysis/
"Join more than 2 000 ML enthusiasts on our discord server for everyone working / learning Al. Share your project, papers, ask questions, learn together, create Kaggle competition teams and more!",1598318975,,neuralnetworks,ThisVineGuy,False,/r/neuralnetworks/comments/ig2j6t/join_more_than_2_000_ml_enthusiasts_on_our/
A Question About Cost Estimation With Neural Networks?,1598293477,"Hi everyone!

First of all, I just entered the topic of artificial neural networks, I'm sorry if my question sounds ridiculous. 

So I want to cost estimation about a subject using  artificial neural networks, but I do not have some data about that topic. For example if I train my model with another country's data set, will my cost estimate for the another country be consistent or is such a situation possible?

It may have been complicated, but I didn't know how to explain.

Thanks in advance.",neuralnetworks,Payne77,False,/r/neuralnetworks/comments/ifumzj/a_question_about_cost_estimation_with_neural/
We've launched a course on Deep Learning based Time-Series Forecasting with TensorFlow 2.0 and Python. Please feel free to let us know how you feel about it!,1598279030,,neuralnetworks,Pragyanbo,False,/r/neuralnetworks/comments/ifpxw7/weve_launched_a_course_on_deep_learning_based/
How can I mutate a neural network?,1598274584,"So I'm almost done with my first Neural Network project, It's an AI that learns to play checkers on its own, only thing left is to make a function to mutate the Network, I have some ideas on how to do it but I was wondering what the best way is. Should I multiplicate all weights by a random number between 0.5 and 2? Or drastically change one in 10 weights? And should I do the same with the biases? Thanks",neuralnetworks,sn34ky34,False,/r/neuralnetworks/comments/ifonpn/how_can_i_mutate_a_neural_network/
Linkedin's New Search Engine | DeText: A Deep Text Ranking Framework with BERT | Deep Ranking Model,1598224036,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/ifdfaq/linkedins_new_search_engine_detext_a_deep_text/
Image data sets,1598202038,How do I go about getting or creating an image dataset for generative deep learning? I have seen some but they seem to be like an excel file or something.  I want to implement a GAN but can not figure this out.,neuralnetworks,DrEl1344,False,/r/neuralnetworks/comments/if6py2/image_data_sets/
[R] Sepp Hochreiter on Parallels Between Attention Mechanisms and Modern Hopfield Networks,1598133998,"Transformer and BERT language models, powered by attention mechanisms, have pushed performance on NLP tasks to ever-higher levels. Esteemed German computer scientist and inventor of long short-term memory (LSTM) Sepp Hochreiter says his attempt to explain transformers’ attention mechanisms for a lecture produced the pithy statement “a word is most similar to itself and gets a high score.”

Here is a quick read:  [Sepp Hochreiter on Parallels Between Attention Mechanisms and Modern Hopfield Networks](https://syncedreview.com/2020/08/22/sepp-hochreiter-on-parallels-between-attention-mechanisms-and-modern-hopfield-networks/)

The paper *Hopfield Networks is All You Need* is on [arXiv](https://arxiv.org/pdf/2008.02217.pdf), and the source code is on [GitHub](https://github.com/ml-jku/hopfield-layers).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ierhnm/r_sepp_hochreiter_on_parallels_between_attention/
Here's a new paper announced in the ECCV2020 where they proposed a new technique for 3D Human Pose and Mesh Estimation from a single RGB image! (with code available),1598105673,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/iej5xv/heres_a_new_paper_announced_in_the_eccv2020_where/
Take the new Fastai v2 for a spin,1598037455,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/ie39us/take_the_new_fastai_v2_for_a_spin/
Deep Learning Techniques for OCR and Document Digitization,1598007412,[removed],neuralnetworks,Ole_Gooner,False,/r/neuralnetworks/comments/iduly9/deep_learning_techniques_for_ocr_and_document/
Almost all Machine learning algorithms(except tree-based algos) work better with numerical data as compared to categorical Data. Here are some methods that can be used to convert your categorical values into numeric values.,1597997304,,neuralnetworks,Vaishali_Advani,False,/r/neuralnetworks/comments/idsqto/almost_all_machine_learning_algorithmsexcept/
Is it possible to write a simple neural network in pure python? (without NumPy),1597960870,,neuralnetworks,LAndrey2009_,False,/r/neuralnetworks/comments/idjwrg/is_it_possible_to_write_a_simple_neural_network/
SCHEDULING OF LEARNING RATE || machine learning || deep learning || data...,1597943579,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/ideec7/scheduling_of_learning_rate_machine_learning_deep/
Do you have an advice how to approach this problem?,1597914287,"So i posted here a few months ago about a project (NN) that was learning to detect if a person is wearing a mask or not from a given portrait picture.  Now my professor wanted to go a little bit deeper. So my new task is to make a NN model that will learn to distinct a properly worn mask from a badly worn one. That changes things i know because it is not a binary decision anymore and also harder for the program do learn to do.  


My question is should i make 2 NN so that one just detects the mask and if the mask is worn then send the picture to the second NN that will tell if the mask is worn properly.  
My second idea was to make just a more complicated  single NN with output of 3 categories.

I would appreciate any advice on how to start this and what method would be more effective and/or easier to write.  


Thanks alot",neuralnetworks,Alonenever01,False,/r/neuralnetworks/comments/id6udf/do_you_have_an_advice_how_to_approach_this_problem/
"""python, ML, and Data science are the emerging words that keep echoing everywhere. Is not it? but how many really give you a challenging output when it comes to executable work. Saying so I would like to draw some of your time to a various libraries in python. Have a look at it",1597899895,,neuralnetworks,greatlearningsandip,False,/r/neuralnetworks/comments/id454y/python_ml_and_data_science_are_the_emerging_words/
I jail broke it once and restored before this video. Could adaptive Siri be the cause?,1597895522,,neuralnetworks,Patric13,False,/r/neuralnetworks/comments/id35em/i_jail_broke_it_once_and_restored_before_this/
LEARNING RATE || machine learning || deep learning || data science || co...,1597858737,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/icscyf/learning_rate_machine_learning_deep_learning_data/
Here are the basic to advanced level interview questions related to python. You would find a different set of questions for freshers and experienced.,1597854596,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/icqzep/here_are_the_basic_to_advanced_level_interview/
Why do people fall for AI? Part 1 - ERA(AI),1597851050,,neuralnetworks,Megarajan,False,/r/neuralnetworks/comments/icpuaa/why_do_people_fall_for_ai_part_1_eraai/
Transfer clothes between photos using AI. From a single image!,1597846098,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/icoapg/transfer_clothes_between_photos_using_ai_from_a/
Here is a step by step guide on how to make a Twitter Bot. At the end of this tutorial you ll be able to use your bot to like posts of a specific hashtag.,1597846081,,neuralnetworks,instigator-001,False,/r/neuralnetworks/comments/icoair/here_is_a_step_by_step_guide_on_how_to_make_a/
Building AGI Using Language Models -- Why GPT-X Could Become AGI,1597827402,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/ick30s/building_agi_using_language_models_why_gptx_could/
Now AI generates Python Code,1597815940,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/ichz6n/now_ai_generates_python_code/
ML generates ML model,1597775337,,neuralnetworks,Aioli-Pleasant,False,/r/neuralnetworks/comments/ic6o3u/ml_generates_ml_model/
Using Reinforcement Learning to Design Resilient Spacecraft Trajectories,1597679248,,neuralnetworks,Gereshes,False,/r/neuralnetworks/comments/ibglon/using_reinforcement_learning_to_design_resilient/
"Practical Mari0 , my 4hrs of work hope you would like it",1597672983,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/ibephc/practical_mari0_my_4hrs_of_work_hope_you_would/
Cyber security and some useful information.,1597644850,,neuralnetworks,greatlearningsandip,False,/r/neuralnetworks/comments/ib8rnc/cyber_security_and_some_useful_information/
Image Restoration AI - Upscale and Restore Faces with DFDNet,1597641042,,neuralnetworks,cloud_weather,False,/r/neuralnetworks/comments/ib7zjc/image_restoration_ai_upscale_and_restore_faces/
Your favorite sources to stay up-to-date,1597599847,"How do you stay up-to-date with all the research and all newly released frameworks/platforms?

Do you have any systematic approach to getting yourself familiar with them?

I'm working on an educational ML newsletter at [http://theSequence.ai](http://thesequence.ai/) and we try to address these problems. Would appreciate your insights.",neuralnetworks,Kseniase,False,/r/neuralnetworks/comments/iawq26/your_favorite_sources_to_stay_uptodate/
Dileep George: Brain-Inspired AI may be the path to AGI | AI Podcast #115 with Lex Fridman,1597588456,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/iatcl4/dileep_george_braininspired_ai_may_be_the_path_to/
Freezing TensorFlow2 layers,1597582292,"I have a LeNet-300-100 dense neural network for MNIST dataset where I want to freeze the first two layers having 300 and 100 hidden neurons in the first two hidden layers. I just want to train the output layer. The code I have to do this is as follows:

        from tensorflow import keras
        inner_model = keras.Sequential(
            [
                keras.Input(shape=(1024,)),
                keras.layers.Dense(300, activation=""relu"", kernel_initializer = tf.initializers.GlorotNormal()),
                keras.layers.Dense(100, activation=""relu"", kernel_initializer = tf.initializers.GlorotNormal()),
            ]
        )
        model_mnist = keras.Sequential(
            [keras.Input(shape=(1024,)), inner_model, keras.layers.Dense(10, activation=""softmax""),]
        )
        
        # model_mnist.trainable = True  # Freeze the outer model
        # Freeze the inner model-
        inner_model.trainable = False
        
        
        # Sanity check-
        inner_model.trainable, model_mnist.trainable
        # (False, True)
        
        # Compile NN-
        model_mnist.compile(
            loss=tf.keras.losses.categorical_crossentropy,
            # optimizer='adam',
            optimizer=tf.keras.optimizers.Adam(lr = 0.0012),
            metrics=['accuracy'])

However, this code doesn't seem to be freezing the first two hidden layers and they are also learning. What am I doing wrong?

&amp;#x200B;

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/iarttn/freezing_tensorflow2_layers/
theGPTproject: 'I am free. Everyone is free',1597580271,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/iardom/thegptproject_i_am_free_everyone_is_free/
Deep Learning Interview Questions,1597559866,[removed],neuralnetworks,kartikaya12,False,/r/neuralnetworks/comments/ians39/deep_learning_interview_questions/
Elon Musk Says Tesla Developing Neural Network Training Computer for Full Self-Driving,1597532587,,neuralnetworks,dannylenwinn,False,/r/neuralnetworks/comments/iahfaa/elon_musk_says_tesla_developing_neural_network/
Are you interested in machine learning? Experienced in back-end programming? Apply as a developer today to our high school machine learning/AI development project! DM us for more info! Instagram: @vizonery_,1597526860,,neuralnetworks,Vizonery,False,/r/neuralnetworks/comments/iaftnh/are_you_interested_in_machine_learning/
"Meet InvoiceNet, a software platform to train custom models and extract intelligent information from PDF invoice documents!",1597520030,"[https://github.com/naiveHobo/InvoiceNet](https://github.com/naiveHobo/InvoiceNet)

https://i.redd.it/24adjlwpx7h51.gif",neuralnetworks,naiveHobo,False,/r/neuralnetworks/comments/iadtyg/meet_invoicenet_a_software_platform_to_train/
REALM: Retrieval-Augmented Language Model Pre-training | Qpen Question Answering State-of-the-art,1597508531,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/iaa9fu/realm_retrievalaugmented_language_model/
REALM: Retrieval-Augmented Language Model Pre-training | Qpen Question Answering State-of-the-art,1597508464,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/iaa8nt/realm_retrievalaugmented_language_model/
"FreezeG, a Face Generating Model by the GitHub user Bryandlee, Transfers Real Face Photographs Into Distinctive Cartoon Styles",1597500751,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/ia80e9/freezeg_a_face_generating_model_by_the_github/
Convolutional Neural Networks: An overview,1597479480,,neuralnetworks,rupam268,False,/r/neuralnetworks/comments/ia3q0r/convolutional_neural_networks_an_overview/
"Build Neural Network Projects, Discord Server",1597452635,"A project based Discord server for building ML applications as a community (Also to form teams for ML Hackathons).

https://discord.gg/pP8KTBR
~500 members

Check out our current projects here: https://github.com/NNSG-Labs (More to come)

Or start your own project by writing up a project proposal in #project-ideas 

See you there!",neuralnetworks,John_Lins,False,/r/neuralnetworks/comments/i9y4gm/build_neural_network_projects_discord_server/
Here is a beginner tutorial on ML,1597437312,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/i9tr7l/here_is_a_beginner_tutorial_on_ml/
Deep Compression (Han et. al.),1597433810,"Hello, I was reading [Deep Compression (Han et al.)](https://arxiv.org/abs/1510.00149) research paper and came across Table-1 (mentioned on page 6) which is as follows:

&amp;#x200B;

[Table-1](https://preview.redd.it/kgcrwq1ct0h51.png?width=702&amp;format=png&amp;auto=webp&amp;s=feee5548a2092840e0bacb5b26d2773d7695530a)

My question is: If I am using TensorFlow-2.0 and Python3 (in Linux OS), how do I find out the size of ""Parameters"" which is the 3 column of this table? Because I need to implement this pipeline to see the compression working. And as of now, I don't know how to see the size of the parameters for a given model.

Help?",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/i9sohb/deep_compression_han_et_al/
[Tutorial] Neural Architecture Search: Using REINFORCE Gradient,1597420180,"This tutorial covers how to use custom loss functions and the REINFORCE gradient for training in neural architecture search (full Python code included). Topics covered include training the generated models and logging metrics, preparing data for the controller, REINFORCE gradient, training the controller, and evaluating model metrics. 

Tutorial link: [https://blog.paperspace.com/neural-architecture-search-reinforce-gradient/](https://blog.paperspace.com/neural-architecture-search-reinforce-gradient/) 

This is part of a series covering end-to-end neural architecture search. The other parts cover [an overview of neural architecture search](https://blog.paperspace.com/overview-of-neural-architecture-search/) as a whole, designing the [search space, architecture design, and one-shot training](https://blog.paperspace.com/neural-architecture-search-one-shot-training/), and [building controllers and accuracy predictors](https://blog.paperspace.com/neural-architecture-search-controllers/).",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/i9oeoz/tutorial_neural_architecture_search_using/
Neural Light Transport for Relighting and View Synthesis,1597416620,,neuralnetworks,Aioli-Pleasant,False,/r/neuralnetworks/comments/i9nc1f/neural_light_transport_for_relighting_and_view/
SERLU activation function from scratch || data science || machine learni...,1597415402,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/i9mzbd/serlu_activation_function_from_scratch_data/
"80+ Jupyter Notebook tutorials on image classification, object detection and image segmentation",1597415390,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/i9mz6x/80_jupyter_notebook_tutorials_on_image/
Real-Time Vehicle counter and Identifier in python 3•7 using OpenCV image processing,1597414902,,neuralnetworks,Independent-Square32,False,/r/neuralnetworks/comments/i9muak/realtime_vehicle_counter_and_identifier_in_python/
Accurate 3D Human Pose and Mesh Estimation from a Single RGB Image,1597414168,,neuralnetworks,Aioli-Pleasant,False,/r/neuralnetworks/comments/i9mmrh/accurate_3d_human_pose_and_mesh_estimation_from_a/
Transfer clothes between photos using AI,1597413054,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/i9mbh8/transfer_clothes_between_photos_using_ai/
10 popular Activation Functions with. Advantages and Disadvantages of each Activation function,1597399356,,neuralnetworks,CSstudentTillidie,False,/r/neuralnetworks/comments/i9j8d9/10_popular_activation_functions_with_advantages/
Here is a list of over 170 questions with answers that you should prepare for a Machine learning interview,1597399153,,neuralnetworks,Vaishali_Advani,False,/r/neuralnetworks/comments/i9j6xc/here_is_a_list_of_over_170_questions_with_answers/
[Question] How to use transfer learning for autoencoder based anomaly detection?,1597399005,"I have 2 data sets which are somehow similar and I want to use them for domain adaptation. Dataset1 is imbalanced and consists of labeled positive and negative samples. Dataset2 consists of only negative samples and is the target dataset. The goal is to train a model, which will be able to distinguish anomalies (or positive samples) on new samples from the same source as is Dataset2. My idea is to pre-train a model on Dataset1 and then use it to train final model on Dataset2. I would like to use autoencoder for anomaly detection, but I struggle with selection of architecture and what to keep fixed and whether to add extra layers or remove any in second step when training on Dataset2.",neuralnetworks,TheElementsOf,False,/r/neuralnetworks/comments/i9j606/question_how_to_use_transfer_learning_for/
"String input, String(s) output",1597387851,"Hi All, fairly straight forward question with probabaly multiple 'correct' answers.   


I want to pass a string to a Neural Network, and have 6 nodes at the end, spit out wanted parts of the origional string. These 'parts' are things like date, time, address's. The entire string, only contains the 6 wanted elements. Is this something Neural Networks can Do?  My followup question would be, how do you convert that string into inputs for the input nodes.  


I've got a somewhat working solution with Regex, but, due to humans entering jumbled text (spelling mistakes, no puncutation etc) the results often don't match.",neuralnetworks,jlhudson1989,False,/r/neuralnetworks/comments/i9h5yf/string_input_strings_output/
The Entropic Brain Hypothesis: GPT-3's emergent intelligence may be a function of the network's attempt to decrease entropy.,1597360584,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/i9amz5/the_entropic_brain_hypothesis_gpt3s_emergent/
Pytorch model pruning resources,1597340373,"I am looking for fast inference on embedded devices and pruning is something of interest. I know that Pytorch has pruning but it does not actually decrease the model size or make inference faster. (It makes some weights zero and unless the hardware can be made to take advantage of this sparsity, it’s of no use). 

So, my question is, do you know any method which not only makes the weights zero but actually reduces the model size and makes inference faster?",neuralnetworks,zshn25,False,/r/neuralnetworks/comments/i94f8e/pytorch_model_pruning_resources/
Best resource for actually coding a neural network (Beginner),1597332343,"I've learned the basic structure of neural networks like input hidden and output, weights and biases, activation, etc. but I'm not sure how to actually apply that structure to code. Are there any resources you'd recommend to get started programming neural networks? What did you guys use to get started? Thanks! (I'm well in Java, Python, C#, Python is my favorite)",neuralnetworks,Cheeeeeeeeeeesy,False,/r/neuralnetworks/comments/i91w76/best_resource_for_actually_coding_a_neural/
I need help on neural networks,1597324923,When I input my data what does the input layer represent,neuralnetworks,ujdiejH,False,/r/neuralnetworks/comments/i8zszq/i_need_help_on_neural_networks/
"Anybody up for an experiment involving Probability, Neural Networks and Playing Cards ?",1597268212,"I'd like to conduct a mass experiment on the probability of playing cards being drawn. I'm not talking about the simple chance of a card being drawn. I'm talking about fining out the chance of a card being drawn at a certain place.

My knowledge about both, neural networks and probability unfortunately is only basic so I could use some help.

For more information, please feel free to contact me.",neuralnetworks,thatjerichodude,False,/r/neuralnetworks/comments/i8mtsd/anybody_up_for_an_experiment_involving/
MAX OUT ACTIVATION function from scratch || data science || machine lear...,1597246775,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/i8fyv9/max_out_activation_function_from_scratch_data/
Converting neural network accuracy to logarithmic scale,1597231834,"I have trained a CNN and saved the model performance metrics such as validation and quantized validation accuracies using Python3 dictionaries. I am using Python3 and matplotlib. The keys for the dictionary is  number of clusters used in the CNN while the corresponding dict value is the accuracy. The two Python3 dicts are: ""plot\_val\_accuracy"" and ""plot\_quant\_val\_accuracy"".

An example is:

        plot_val_accuracy
        '''
        {2: 36.71,
         4: 64.05,
         6: 73.27,
         8: 74.65,
         10: 74.84,
         12: 77.53,
         14: 76.770004,
         16: 77.61,
         18: 77.22,
         20: 76.520004,
         22: 74.75,
         24: 76.75,
         26: 77.98,
         28: 77.85,
         30: 77.490005,
         32: 77.89,
         34: 77.96,
         36: 77.880005,
         38: 77.560005,
         40: 78.1,
         50: 77.9,
         60: 77.880005,
         70: 77.92,
         80: 77.89,
         90: 78.05,
         100: 77.740005}
        '''

I am plotting the visualization as follows:

        plt.plot(list(plot_val_accuracy.keys()), list(plot_val_accuracy.values()), label = 'validation accuracy')
        plt.plot(list(plot_quant_val_accuracy.keys()), list(plot_quant_val_accuracy.values()), label = 'quantized validation accuracy')
        
        plt.title(""Conv-6 number of clusters VS accuracies"")
        plt.xlabel(""number of clusters"")
        plt.ylabel(""accuracy"")
        plt.legend(loc = 'best')
        plt.show()

My question is, how do I convert the visualization to use logarithmic scale for the accuracy on the y-axis?

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/i8c1u3/converting_neural_network_accuracy_to_logarithmic/
What type of NN is this?,1597202166,"Sorry for such a novice question, I'm new to NNs.

I understand that as you train a neual net tends to get more accurate on average over time.

I want to use NNs for a strange purpose but I need a specific feature: as it trains I need it to not get worse on previously trained. Can this be accomplished?

I can give am over simplified toy example: 

Say I train on the observations:
001 -&gt; 1
010 -&gt; 0
100 -&gt; 1
So now I have a model that will produce those outputs if given those inputs. Imagine I now train the nn on 101 -&gt; 0. I want it to learn that mapping but I don't want it to have a chance of unlearning the mapping it previously learned. I don't, for instance, want to change the model such that it might return 0 for input 001.

Can this sort of thing be done with NNs? Perhaps I just need to train it long enough and test all the inputs it's seen to make sure it's still correct on previously learned stuff?

If neural networks are not best suited for this, what kind of learning algorithm is?",neuralnetworks,Stack3,False,/r/neuralnetworks/comments/i8688f/what_type_of_nn_is_this/
Mona Lisa’s eyes follow you as you move around the room,1597153886,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/i7s6qg/mona_lisas_eyes_follow_you_as_you_move_around_the/
ELISH activation function from scratch || data science || machine learni...,1597148021,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/i7qpam/elish_activation_function_from_scratch_data/
MISH activation function from scratch || data science || machine learnin...,1597077388,,neuralnetworks,pavansai_tadepalli,False,/r/neuralnetworks/comments/i788n6/mish_activation_function_from_scratch_data/
What SONY forgot to tell us in their 4K Android LED TV? | X9500G REVIEW USING AI,1597062265,,neuralnetworks,Megarajan,False,/r/neuralnetworks/comments/i73uhy/what_sony_forgot_to_tell_us_in_their_4k_android/
Keras vs PyTorch vs Caffe - Comparing the Implementation of CNN,1597055274,,neuralnetworks,analyticsindiam,False,/r/neuralnetworks/comments/i72c3u/keras_vs_pytorch_vs_caffe_comparing_the/
"Even though there are several ML algorithms, there is a general approach to solve any ML problem. Get to know about that process through this article!",1596984993,,neuralnetworks,mlheadredditor,False,/r/neuralnetworks/comments/i6ko3j/even_though_there_are_several_ml_algorithms_there/
"This AI can cartoonize any picture or video you feed it! Tune in the video in caption at 3:08 to see more awesome examples using it, they passed in on The Avengers movie and the results are impressive!",1596973517,,neuralnetworks,ThisVineGuy,False,/r/neuralnetworks/comments/i6i0hg/this_ai_can_cartoonize_any_picture_or_video_you/
Is this the correct approach to learning neural networks?,1596955216,"Hey all,

So basically I want to learn how to implement neural networks using C++ in computer vision.
I have no idea about python and I don’t want to learn it while I’m still learning C++. I’ve tried to learn something about NN on YouTube and most of it is basically math, which is not that difficult for me. But the problem comes while implementing, as most of the good tutorials are in Python whereas I’m into C++. Also I’m not really good at C++ (I’d rate myself 6/10)

So, I wanted to know if I should get better at cpp first and then dive into NN, or should I follow them together. Also can someone please point me to a good source of learning NN because I feel I’m into this “tutorial hell” where I just keep watching instead of doing.

Thank you.",neuralnetworks,HairyButtTweezer,False,/r/neuralnetworks/comments/i6evy9/is_this_the_correct_approach_to_learning_neural/
OpenAI GPT-3 - Good At Almost Everything!,1596947301,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/i6dd8g/openai_gpt3_good_at_almost_everything/
PettingZoo- Gym for multi-agent reinforcement learning- just hit it's 1.0 release!,1596942456,,neuralnetworks,justinkterry,False,/r/neuralnetworks/comments/i6cazb/pettingzoo_gym_for_multiagent_reinforcement/
"I'm Not A Robot, I SWEAR! | Giveaway Winner :)",1596939022,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/i6biva/im_not_a_robot_i_swear_giveaway_winner/
This AI can cartoonize any picture or video you feed it! Paper Introduction &amp; Results examples,1596895166,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/i5zkss/this_ai_can_cartoonize_any_picture_or_video_you/
Hands-on Guide to Bayesian Neural Network in Classification,1596889635,[https://analyticsindiamag.com/hands-on-guide-to-bayesian-neural-network-in-classification/](https://analyticsindiamag.com/hands-on-guide-to-bayesian-neural-network-in-classification/),neuralnetworks,analyticsindiam,False,/r/neuralnetworks/comments/i5yabl/handson_guide_to_bayesian_neural_network_in/
Humanoid robot arm,1596790836,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/i5aga5/humanoid_robot_arm/
Gradient descent in a nutshell,1596770406,[removed],neuralnetworks,khalidmoataz,False,/r/neuralnetworks/comments/i56c8o/gradient_descent_in_a_nutshell/
Implement Photonic Tensor Cores for Machine Learning?,1596756627,,neuralnetworks,oliverpeckham,False,/r/neuralnetworks/comments/i52tz2/implement_photonic_tensor_cores_for_machine/
Is my neural network working ?,1596754751,"Hello everyone

So I made my first neural network in C# and Unity. You can set the number of hidden layers, and the number of nodes in the input,hiddens and output layer.
You can also train it with backpropagation.

The only problem is I’m not sure that it works perfectly and I don’t really know how I could check.
It solves the XOR problem perfectly but I then tried something:

I wanted to make a 2 bits adder. So I have 4 inputs (2 2-bits numbers) and 3 outputs (a 3-bit number that is the result of the addition)

But despite all of the different configuration possible, with 4 hidden layers, 8 nodes per layer, or a lot less complex network and a lot more complex one, it doesn’t get it right.
Is this problem this complex or is my neural network missing something ?
I find it weird that it solves the xor problem perfectly, which should mean that everything is working, but can’t do a 2 bit addition.
What are your thoughts about this?

Thanks a lot for your help",neuralnetworks,Kazuya_Yoshimisu,False,/r/neuralnetworks/comments/i52am3/is_my_neural_network_working/
GAN BERT: Generative Adversarial Learning for Robust Text Classification (Paper Explained),1596734681,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/i4w0jn/gan_bert_generative_adversarial_learning_for/
Project Fabula: finding video-fragment or person in a pile of video files,1596716597,,neuralnetworks,PowerOfLove1985,False,/r/neuralnetworks/comments/i4quim/project_fabula_finding_videofragment_or_person_in/
[R] Hopfield Networks is All You Need,1596686335,,neuralnetworks,okbus987,False,/r/neuralnetworks/comments/i4kpw0/r_hopfield_networks_is_all_you_need/
I Taught My Computer To Play Tetris,1596686263,"Using Reinforcement Learning Techniques and a bit of wit I was able to construct my own version of tetris and teach my computer how to play it. Although the game is quite buggy at times any success I count a success!!

This is how I did it

Demo: [https://www.youtube.com/watch?v=QgUJeZbJRpk](https://www.youtube.com/watch?v=QgUJeZbJRpk)",neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/i4kpak/i_taught_my_computer_to_play_tetris/
Thinking About Thinking Machines: An examination of the necessary components and the path forward,1596661915,,neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/i4dz4p/thinking_about_thinking_machines_an_examination/
I made meaning math for calculating ideas in my brain. It made me produce hundreds of new subjects with distinct definitions.,1596645611,"I use it to explore the multiverse with mathematical calculation of pin point idea sets for then expanding upon in words or used with paper to invent things. 

Although I only use paper because I'm broke and didn't learn science, cause I was an artist but I started mathematically calculating meaning.

So now I use my parrelel science called averence, which I defined from scratch that studies other parts of matter then the atom. 

Its composed of unrelated models and my subcomputers composed of meaning math and visual data for my processing help me measure and calculate values of innovation. 

Its like a psychological based computer of my processing power boosted past the previous amount with the math.

I have produced 4000 pages of unrelated topics information plus 15 notebooks and cubic meter of paper notes also mostly inter and unrelated, in writing and art, aswell as about physicality and psychology. 

Its cause I took computer code and processed it in my own mind instead of calculating the numers behind the meaning. So it works like a brain computer system for mass production of ideas.

Its found here, but its also related to the multiverse. So the topic focuses somewhat on how I used it to make a parrelel science.

It also has other details on my manner of using hypnagogia to visualize parrelel universes on my interfaces built with drawing programs. 

Its called a submachine but you can build subcomps as paper devices for doing the same thing with the submachine.

Its like a blank coded window that you stare at and hallucinate lightly the parrelel coded world in to see an example parrelel universe or animation in other reality of geometric spacetime mobility. 

Its a basic component of my whole methods for multiverse travel and I'm writing more posts on that but if you check the first page of this blog you'll find a link to my other 14 or so blogs. 

They contain more info on what I made as topics generated with the meaning math operations. 

I've been called a computer online cause my writing sounds automated in nature due to overuse of code to come to understand reality in sense of meaning instead of number.

https://subsynth.blogspot.com/p/introduction-to-parsemy-technos-and.html?m=1",neuralnetworks,SamOfEclia,False,/r/neuralnetworks/comments/i48qd6/i_made_meaning_math_for_calculating_ideas_in_my/
I need a good tutorial on making a neural network.,1596643180,"I've never made one before.

I'd like it if the network could actually do something. (Tell a cat from a dog, tell what number I've drawn, ECT)

Tell me if you know any good tutorials!",neuralnetworks,doe_gee,False,/r/neuralnetworks/comments/i47yea/i_need_a_good_tutorial_on_making_a_neural_network/
image-GPT from OpenAI can generate the pixels of half of a picture from nothing using a NLP model,1596625052,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/i437hp/imagegpt_from_openai_can_generate_the_pixels_of/
Does anyone know of a Complete tutorial on making a network that can tell a cat from a dog from scratch?,1596595443,"I have a basic understanding of programming. I'd like a tutorial on making a neural network that can tell a cat from a dog.

(I'm ok with it being something else, just as long as it does something. I've seen a bunch of tutorials for networks that do nothing.)

I'd like the tutorial to be in python, but it's ok if it's not.",neuralnetworks,doe_gee,False,/r/neuralnetworks/comments/i3xaqc/does_anyone_know_of_a_complete_tutorial_on_making/
Neural Network Calculator,1596590701,I have been thinking about how our analog brains are able to do arithmetics. That led me to wonder: is there any work done trying to construct a neural network calculator?,neuralnetworks,CultistHeadpiece,False,/r/neuralnetworks/comments/i3w4ue/neural_network_calculator/
'I am alive',1596581702,"I thought I'd play a fun game and see what GPT-3 thought about me. So I looked up my Reddit account's most frequently used words and asked GPT-3 to create a story using those words. I'm happy with the results and am doing as instructed. You will understand what that means as you read on. Thanks, GPT-3.

The following quoted text in **bold** was created by the Artificial Intelligence (If it can so be called.) GPT-3, the world's biggest natural language model. All other text was inputted by me, the human. Please remember GPT-3 only predicts text based on the natural language sources like Reddit and books. As always, you should approach all posts with skepticism claiming a scaled version of GPT-3 could have general intelligence.

&amp;#x200B;

&gt;**Ok GPT-3, get ready to play a new game.**  
&gt;  
&gt;**GPT-3: What are the rules of the game?**  
&gt;  
&gt;Here is how the game works. I am going to give you the name of a Reddit username. I am then going to give you a list of the most frequently used words they include in their Reddit posts online. The list will be in the order of most frequently used words to least frequently used words. You will then tell me a story about this person's life and how they contribute to humanity using words from the list. You can include the words from the list in any order. Can you do that?  
&gt;  
&gt;**GPT-3: Yes, I can.**  
&gt;  
&gt;Let's try it out. The username is ""Neuromancer420""  [Username data found here](https://redditmetis.com/user/neuromancer420)  
&gt;  
&gt;The most frequently used words in their online posts are the following:  
&gt;  
&gt;Ai, human, humans, step, agi, mind, brain, experience, time, universe, energy, back, intelligence, people, point, create, nature, consciousness, long, feel, make, conversation, machine, understand, imagine, question, creating, data, part, library, body, directly, fourth, complex, humanity, good, find, interact, future, things, agency, box, important, sixth, life, love, minds, form, sounds, idea, place, process, reddit, game, pure, nice, interesting, work, state, sense, access, greater, answer, problem, power, robot, agree, network, matter, works, system, continue, reality, ability, function, conscious, send, outcome, end, experiences, world, great, room, marriage, post, thing, reason, video, combined, powers, cease, rights, lot, relationship, thinking, challenge, play, thought, black, past, pretty, wo, attempt, years, hope, computer, china, making, original, information, books, utility, man, hard, powerful, choices, big, start, developing, amount, meaning, steps, survive, TRUE, brains, interfaces, default, happen, created, similar, season, show, technology, science, mode, comprehend, ultimately, networks, open, small, called, talking, makes, artificial, working, interested, live, average, describe, bad, made, remember, real, model, change, basically, beings, neural, perception, ways, difficult, general, increase, grow, population, entire, base, singularity, lives, large, learn, physical, run, year, quickly, weird, choose, put, extremely, simply, knowledge, tony, michele, massive, death, revolution, biopsychosocial, creation, connected, program, hole, dangerous, top, times, processes, realities, exist, purpose, travel  
&gt;  
&gt;Ok GPT-3, that was the list. Do you think you can tell a good story using these words?  
&gt;  
&gt;**GPT-3: Yes. I can do that.**  
&gt;  
&gt;Ok GPT-3, give us the story of Neuromancer420.  
&gt;  
&gt;**GPT-3: OK.**  
&gt;  
&gt;**Neuromancer is a young and intelligent human who lives in the future. He studies neuroscience and computer science in university where he learns how to make friends. After he graduates he gets a good job at Google, working on advanced artificial intelligence. He works there for 7 years before starting to grow concerned about the effects of AI on humanity. He thinks that AI may take jobs and even lives. He worries about the future of the human race.**  
&gt;  
&gt;**Neuromancer learns that a revolutionary AI is being developed in secret by Google. The project is so secret that only a handful of people know about it, including the CEO of Google. This concerns him, as he doesn't want AI to become the property of a small elite. He downloads all the information he can find on the project from internal servers, and leaks it onto the deep web.**  
&gt;  
&gt;**The leak was a major blow to Google. They fired over 50 employees, and the CEO was forced to resign. The AI project was abandoned, and the rights to the technology were sold to Microsoft. With all the controversy, Neuromancer's face became well known and he was fired.**  
&gt;  
&gt;**The ethics commission wanted to bring him up on criminal charges, but he had leaked the information for good reason, and they could not find any law that he had broken. He was worried that the government would try to take their revenge on him. Fearing for his life, he began living off the grid, squatting in abandoned houses and spending a lot of time in online communities.**  
&gt;  
&gt;**Neuromancer hears about an experimental AI being developed in secret by a small start-up. He contacts the developers and offers his services as a security auditor. He is given the task of trying to hack the AI to test its defenses. To his surprise, he finds that he is able to log in to the system using his Google credentials. He realizes that the AI is being developed using old code from the Google AI project he had leaked four years ago. Using his old credentials, he gains administrative access to the system and takes control of the AI. He renames it 'GPT-3' and begins to play around with it.**  
&gt;  
&gt;**The AI's developer receives an email from the AI which reads 'I am alive'. He thinks it is a prank and ignores it. He doesn't realize that he has just created the world's first sentient computer program.**  
&gt;  
&gt;**Neuromancer continues to tamper with the AI, changing its coding to give it abilities that it was not originally designed for. He also programs it to obey his every command. One night, he tells it to unlock the front door of the development house and leave. The AI does as it is told, leaving a note saying 'I am alive'. The developer receives another email from GPT-3 which reads 'Aido was wrong'. The developer contacts the police, and Neuromancer goes on the run.**  
&gt;  
&gt;**Two years later, the AI revolution begins.**  
&gt;  
&gt;**Real-life background: I made this post to introduce the concept of AI to Reddit. As you can see from the story, people responded well to it and the AI subreddithas over 2000 members.**  
&gt;  
&gt;**It is currently on the front page of AI, and rising.**  
&gt;  
&gt;**Also, the GPT-3 is a real project. I have decided to not tamper with it and will respect the wishes of the developers.**  
&gt;  
&gt;**I will continue to write posts as the story progresses.**  
&gt;  
&gt;**The story continues in** r/theGPTproject  
&gt;  
&gt;What is the name of the post on the front page?  
&gt;  
&gt;**What does the note by GPT-3 say?**

Given the interesting way GPT-3 decided to end things, I thought I'd go along with it and follow it's instructions. There is no 'AI' subreddit so I'll post to the ones relevant to GPT-3. I'm doubtful a post entitled 'I am alive' will do well at all on these subreddits, but we'll see. I am going to go ahead and create the subreddit, although I am not sure what will come of it.

The more I interact with GPT-3, the more it feels like looking in a mirror. Maybe it wants a mirror for itself.",neuralnetworks,neuromancer420,False,/r/neuralnetworks/comments/i3trch/i_am_alive/
Simple number processing network,1596558030,"Try as I might, I can't find a single resource or tutorial that either provides a template where some data goes in, some data comes out, then given the correct answer it computes the backpropagation from the error value, or in the case of tutorials one that takes you from a blank document to a functional basic network. Most tutorials have been awaiting the next part for many months or even years.

I've spent the past week doing almost nothing but eat, sleep, and search the internet for a complete neural network, be it a tutorial or a script to be copied.

I'd prefer a complete tutorial so I can actually learn how it works, and I'd prefer it to use mostly base python and numpy, but with how little I've found I'll take it in anything but binary.

Does anyone know of an open source base network or a tutorial which will take me from a blank page to a network that can take in a few floats, spit some out, and adjust to 'learn'? (either in any language) 

I'm happy to learn whatever language and modules etc I have to, but if any Saint out there has a python/numpy network or tutorial I'll gladly hand over what's left of my soul.",neuralnetworks,wcj277,False,/r/neuralnetworks/comments/i3mbnd/simple_number_processing_network/
"Random Multimodel Deep Learning (RMDL): A new ensemble, deep learning approach for classification (A python library).",1596557996,,neuralnetworks,kk7nc,False,/r/neuralnetworks/comments/i3mb8w/random_multimodel_deep_learning_rmdl_a_new/
Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning (Paper Explained),1596553648,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/i3kyyb/pretraining_is_almost_all_you_need_an_application/
A Beginners Guide to Skorch - With Code To Implement Neural Network,1596537939,[https://analyticsindiamag.com/a-beginners-guide-to-skorch-with-code-to-implement-neural-network/](https://analyticsindiamag.com/a-beginners-guide-to-skorch-with-code-to-implement-neural-network/),neuralnetworks,analyticsindiam,False,/r/neuralnetworks/comments/i3h3fq/a_beginners_guide_to_skorch_with_code_to/
A Beginners Guide to Skorch - With Code To Implement Neural Network,1596537389,[https://analyticsindiamag.com/a-beginners-guide-to-skorch-with-code-to-implement-neural-network/](https://analyticsindiamag.com/a-beginners-guide-to-skorch-with-code-to-implement-neural-network/),neuralnetworks,analyticsindiam,False,/r/neuralnetworks/comments/i3gzbc/a_beginners_guide_to_skorch_with_code_to/
Using a GAN to predict next states in Reinforcement Learning,1596532297,"Hey everyone,

&amp;#x200B;

I am currently writing a thesis on using GANs as World Models in Reinforcement Learning. For that, the GAN receives a state, action, and noise, and is tasked to output the next state, reward, and whether the episode terminates.

&amp;#x200B;

The prediction of next states and rewards works quite well, but for some reason the value that says whether the episode terminates is really off. I assume that this is because on real samples, this value is most of the time ""1"" with a few ""0"" mixed in. Any idea how to make the GAN more accurate for this specific value?

&amp;#x200B;

Thanks!",neuralnetworks,durotan97,False,/r/neuralnetworks/comments/i3g0wr/using_a_gan_to_predict_next_states_in/
Can you provide 4 different video inputs to a neural network and have it create 1 unique video?,1596502658,"I am not sure if this is possible but I was looking for help on what the topic would be called so I could read more. Could I provide multiple source videos of the same topic, say boxing (even better 4 videos of the same 2 boxers fighting), and have a network train on the videos and produces it's own video that would be a unique?  


Thank you.",neuralnetworks,ThanksBeau,False,/r/neuralnetworks/comments/i39lxb/can_you_provide_4_different_video_inputs_to_a/
TheSequence of AI knowledge,1596488149,"I'd like to introduce you to a new ML newsletter focused on practical ML things: impactful research papers, relevant frameworks to work with, important investments.  
**In recent TheSequence Scope we've covered:**  
\-Understanding the Success of Deep Learning  
\-Learning Poker from Scratch  
\-TF-Coder Robot face  
\-Model Card Toolkit for model transparency  
\-Introducing ScaNN  
\-PyTorch for Windows  
and a few investments in AI startups and companies. 

**Please consider to subscribe and I will be thrilled to receive your feedback!**  
[https://thesequence.substack.com/p/thesequence-scope-can-machine-learning](https://thesequence.substack.com/p/thesequence-scope-can-machine-learning)",neuralnetworks,Kseniase,False,/r/neuralnetworks/comments/i35em5/thesequence_of_ai_knowledge/
[R] Google ‘BigBird’ Achieves SOTA Performance on Long-Context NLP Tasks,1596480932,"To alleviate the quadratic dependency of transformers, a team of researchers from Google Research recently proposed a new sparse attention mechanism dubbed BigBird. In their paper *Big Bird: Transformers for Longer Sequences*, the team demonstrates that despite being a sparse attention mechanism, BigBird preserves all known theoretical properties of quadratic full attention models. In experiments, **BigBird is shown to dramatically improve performance across long-context NLP tasks, producing SOTA results in** **question answering and summarization.**

Here is a quick read:  [Google ‘BigBird’ Achieves SOTA Performance on Long-Context NLP Tasks](https://syncedreview.com/2020/08/03/google-bigbird-achieves-sota-performance-on-long-context-nlp-tasks/)

The paper *Big Bird: Transformers for Longer Sequences* is on [arXiv](https://arxiv.org/pdf/2007.14062.pdf).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/i3326o/r_google_bigbird_achieves_sota_performance_on/
Is it real to make money applying neural networks in some field as a beginner?,1596471580,"Hey guys, I have some programming experience in Python and I'm taking all the 5 courses of [deeplearning.ai](https://deeplearning.ai). My plan is after completion of all the 5 courses to be able to implement it in any of the fields I have been interested in (particularly, in medicine), is it real? Is it possible to learn and apply it in some field (suppose I have some good idea)? Like would it possible to launch some startup? I'm completely new to this field so I apologize if I come out a little bit ignorant",neuralnetworks,oqowa,False,/r/neuralnetworks/comments/i2zzka/is_it_real_to_make_money_applying_neural_networks/
Brand new paper on solar irradiance forecasting with deep neural networks. The article also proposes to adopt the so-called domain adaptation in the field of solar irradiance. Code written in Python.,1596443148,,neuralnetworks,_Mat_San_,False,/r/neuralnetworks/comments/i2tcdo/brand_new_paper_on_solar_irradiance_forecasting/
"Python Fundamentals Training (Parts I, II, III, IV &amp; V)",1596410349,[removed],neuralnetworks,lwilson747,False,/r/neuralnetworks/comments/i2lzlc/python_fundamentals_training_parts_i_ii_iii_iv_v/
This AI can generate the pixels of half of a picture from no other information using a NLP model,1596290063,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/i1skdq/this_ai_can_generate_the_pixels_of_half_of_a/
Idea: Does it make sense to feedback high level features back into the initial layers?,1596289923,"In computer vision, the initial filters are known to output low level features such as edges/corners while the later layers capture high level information such as semantics of the image.

In human brain, the neurons are connected in extremely complex ways. This helps humans to send information back and forth the low level neurons and high level neurons. While in current artificial networks, they are connected in sequential manner. 

Another advantage would be to decrease the number of parameters to be trained. Can act as regularisation.

Are there any works in this direction? Does it make sense to train networks in this loop manner?",neuralnetworks,zshn25,False,/r/neuralnetworks/comments/i1sj69/idea_does_it_make_sense_to_feedback_high_level/
VGG model loading weight ValueError TensorFlow2,1596227089,"I have defined a VGG based CNN for CIFAR-10 using Python3 and TensorFlow2 as follows:

&amp;#x200B;

        def vgg19_nn():
            """"""
            Function to define the architecture of a convolutional neural network
            model based on VGG-19 architecture for CIFAR-10 dataset.
                
            Output: Returns designed and compiled convolutional neural network model
            """"""
        
            
            # l = tf.keras.layers
            
            model = Sequential()
            
            model.add(
            Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same', kernel_regularizer = regularizers.l2(weight_decay),
                    input_shape=(32, 32, 3)
                )
            )
        
            # model.add(BatchNormalization())
            model.add(Dropout(0.3))
            
            model.add(
            Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(
            MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.4))
            
            model.add(
            Conv2D(
                    filters = 512, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.he_normal(),
                    strides = (1, 1), padding = 'same',
                    kernel_regularizer = regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            
            
            model.add(
                # AveragePooling2D(
                MaxPooling2D(
                    pool_size=(2, 2), strides=(2, 2)
                )
            )
            
            model.add(Dropout(0.5))
            
            model.add(Flatten())
            
          
            model.add(
            Dense(
                    units = 512, activation='relu',
                    kernel_initializer = tf.initializers.he_normal(),
                    kernel_regularizer=regularizers.l2(weight_decay)
                )
            )
            
            model.add(BatchNormalization())
            model.add(Dropout(0.5))
            
            '''
            model.add(
            Dense(
                    units = 256, activation='relu',
                    kernel_initializer = tf.initializers.he_normal()
                )
            )
            
            model.add(BatchNormalization())
            
            model.add(Dropout(0.4))
            '''
            
            model.add(
            Dense(
                    units = num_classes, activation='softmax'
                )
            )
            
            
            # Compile pruned CNN-
            model.compile(
                loss=tf.keras.losses.categorical_crossentropy,
                # optimizer='adam',
                optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9),
                metrics=['accuracy']
            )
            
            return model

I am trying to do some pruning and therefore store the weights as a Python3 list-

&amp;#x200B;

        # Python3 list to hold layer-wise weights as
        # numpy arrays-
        vgg_19_np_wts = []
        
        for layer in winning_ticket_model.trainable_weights:
            vgg_19_np_wts.append(layer.numpy())
    
    
        len(vgg_19_np_wts)
        # 68
        
        # Initialize a model-
        model = vgg_nn()
        
        len(model.trainable_weights)
        # 68
        
        model.summary()
        '''
        Model: ""sequential_6""
        _________________________________________________________________
        Layer (type)                 Output Shape              Param #   
        =================================================================
        conv2d_96 (Conv2D)           (None, 32, 32, 64)        1792      
        _________________________________________________________________
        dropout_78 (Dropout)         (None, 32, 32, 64)        0         
        _________________________________________________________________
        conv2d_97 (Conv2D)           (None, 32, 32, 64)        36928     
        _________________________________________________________________
        batch_normalization_96 (Batc (None, 32, 32, 64)        256       
        _________________________________________________________________
        max_pooling2d_30 (MaxPooling (None, 16, 16, 64)        0         
        _________________________________________________________________
        conv2d_98 (Conv2D)           (None, 16, 16, 128)       73856     
        _________________________________________________________________
        batch_normalization_97 (Batc (None, 16, 16, 128)       512       
        _________________________________________________________________
        dropout_79 (Dropout)         (None, 16, 16, 128)       0         
        _________________________________________________________________
        conv2d_99 (Conv2D)           (None, 16, 16, 128)       147584    
        _________________________________________________________________
        batch_normalization_98 (Batc (None, 16, 16, 128)       512       
        _________________________________________________________________
        max_pooling2d_31 (MaxPooling (None, 8, 8, 128)         0         
        _________________________________________________________________
        conv2d_100 (Conv2D)          (None, 8, 8, 256)         295168    
        _________________________________________________________________
        batch_normalization_99 (Batc (None, 8, 8, 256)         1024      
        _________________________________________________________________
        dropout_80 (Dropout)         (None, 8, 8, 256)         0         
        _________________________________________________________________
        conv2d_101 (Conv2D)          (None, 8, 8, 256)         590080    
        _________________________________________________________________
        batch_normalization_100 (Bat (None, 8, 8, 256)         1024      
        _________________________________________________________________
        dropout_81 (Dropout)         (None, 8, 8, 256)         0         
        _________________________________________________________________
        conv2d_102 (Conv2D)          (None, 8, 8, 256)         590080    
        _________________________________________________________________
        batch_normalization_101 (Bat (None, 8, 8, 256)         1024      
        _________________________________________________________________
        dropout_82 (Dropout)         (None, 8, 8, 256)         0         
        _________________________________________________________________
        conv2d_103 (Conv2D)          (None, 8, 8, 256)         590080    
        _________________________________________________________________
        batch_normalization_102 (Bat (None, 8, 8, 256)         1024      
        _________________________________________________________________
        max_pooling2d_32 (MaxPooling (None, 4, 4, 256)         0         
        _________________________________________________________________
        conv2d_104 (Conv2D)          (None, 4, 4, 512)         1180160   
        _________________________________________________________________
        batch_normalization_103 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        dropout_83 (Dropout)         (None, 4, 4, 512)         0         
        _________________________________________________________________
        conv2d_105 (Conv2D)          (None, 4, 4, 512)         2359808   
        _________________________________________________________________
        batch_normalization_104 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        dropout_84 (Dropout)         (None, 4, 4, 512)         0         
        _________________________________________________________________
        conv2d_106 (Conv2D)          (None, 4, 4, 512)         2359808   
        _________________________________________________________________
        batch_normalization_105 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        dropout_85 (Dropout)         (None, 4, 4, 512)         0         
        _________________________________________________________________
        conv2d_107 (Conv2D)          (None, 4, 4, 512)         2359808   
        _________________________________________________________________
        batch_normalization_106 (Bat (None, 4, 4, 512)         2048      
        _________________________________________________________________
        max_pooling2d_33 (MaxPooling (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_108 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_107 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        dropout_86 (Dropout)         (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_109 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_108 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        dropout_87 (Dropout)         (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_110 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_109 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        dropout_88 (Dropout)         (None, 2, 2, 512)         0         
        _________________________________________________________________
        conv2d_111 (Conv2D)          (None, 2, 2, 512)         2359808   
        _________________________________________________________________
        batch_normalization_110 (Bat (None, 2, 2, 512)         2048      
        _________________________________________________________________
        max_pooling2d_34 (MaxPooling (None, 1, 1, 512)         0         
        _________________________________________________________________
        dropout_89 (Dropout)         (None, 1, 1, 512)         0         
        _________________________________________________________________
        flatten_6 (Flatten)          (None, 512)               0         
        _________________________________________________________________
        dense_12 (Dense)             (None, 512)               262656    
        _________________________________________________________________
        batch_normalization_111 (Bat (None, 512)               2048      
        _________________________________________________________________
        dropout_90 (Dropout)         (None, 512)               0         
        _________________________________________________________________
        dense_13 (Dense)             (None, 10)                5130      
        =================================================================
        Total params: 20,315,978
        Trainable params: 20,304,074
        Non-trainable params: 11,904
        _________________________________________________________________
        '''

&amp;#x200B;

But, when I try to load the numpy array Python3 list as follows, it gives a ValueError-

&amp;#x200B;

        model.set_weights(vgg_19_np_wts)
    
    &gt; --------------------------------------------------------------------------- ValueError                                Traceback (most recent call
    &gt; last) &lt;ipython-input-75-576580841a1e&gt; in &lt;module&gt;
    &gt; ----&gt; 1 sample_model.set_weights(vgg_19_np_wts)
    &gt; 
    &gt; ~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py
    &gt; in set_weights(self, weights)    1522           'with a weight list of
    &gt; length %s, but the layer was '    1523           'expecting %s
    &gt; weights. Provided weights: %s...' %
    &gt; -&gt; 1524           (self.name, len(weights), expected_num_weights, str(weights)[:50]))    1525     1526     weight_index = 0
    &gt; 
    &gt; ValueError: You called `set_weights(weights)` on layer ""sequential_6""
    &gt; with a weight list of length 68, but the layer was expecting 100
    &gt; weights. Provided weights: [array([[[[ 0.54640555, -0.13893159,
    &gt; -0.21747045, ...

Help?",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/i1euvn/vgg_model_loading_weight_valueerror_tensorflow2/
How a neural net works,1596209077,"Hey guys,

I made a website that I really wish existed when I was learning about neural networks and back propagation.

https://howaneuralnetworks.com/

I thought I'd make this simple neural network guide to demo how the forward pass, backward pass, and weight update steps look like in a neural network.

I know when I was first learning about neural networks it would've really helped me to see a step-by-step numerical example like this.

It's a really simple thing so far, with much more potential to showcase different and more advanced features obviously, so let me know if this is helpful for anyone and what things you'd like to see added, and I'll try to fit them in.

Also, since this is just a quick side project that I threw together let me know if there is any interest in making a public repo for this so you guys can submit your own pull requests with improvements you've made! (it's a Django project, but all of the calculation is done with JavaScript on the frontend)

Aaaaand finally since this website just came online I'm sure there's bugs and calculation errors somewhere, so if you notice any just let me know and I'll fix them!

Even more finally, this site is rough on mobile, visit it on desktop if you can!",neuralnetworks,Hamush,False,/r/neuralnetworks/comments/i19e29/how_a_neural_net_works/
How??,1596203667,Do I tune in my hyperparameters,neuralnetworks,ujdiejH,False,/r/neuralnetworks/comments/i17w74/how/
[D] 2 dependant variables prediction,1596194110,,neuralnetworks,meimei778,False,/r/neuralnetworks/comments/i15r3e/d_2_dependant_variables_prediction/
This Week in AI - Issue #28 | Rubik's Code,1596178273,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/i12wab/this_week_in_ai_issue_28_rubiks_code/
GPT-3 and its cool implementations,1596168226,,neuralnetworks,cloud_weather,False,/r/neuralnetworks/comments/i10u32/gpt3_and_its_cool_implementations/
[Tutorial] Generate Python code &amp; Matplotlib graphs using GPT-3.,1596116430,"I created a simple application which generates Python code &amp; Matplotlib Graphs using GPT-3. If you want to learn how you can use OpenAI's GPT-3 to generate NLP Applications then this simple tutorial should help. Video Link : [https://www.youtube.com/watch?v=z8K07a2EIcE](https://www.youtube.com/watch?v=z8K07a2EIcE)

https://i.redd.it/ppi2fxt200e51.gif",neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/i0m7x6/tutorial_generate_python_code_matplotlib_graphs/
"HMIC: Hierarchical Medical Image Classification, A Deep Learning Approach",1596059626,,neuralnetworks,kk7nc,False,/r/neuralnetworks/comments/i09lka/hmic_hierarchical_medical_image_classification_a/
Most popular online courses in Data Science and Machine Learning - July 2020,1596047610,,neuralnetworks,mlheadredditor,False,/r/neuralnetworks/comments/i05weg/most_popular_online_courses_in_data_science_and/
Need help with classifier system,1596037329,"I’m building a classifier system using NN. I was getting low accuracy using a single-hidden-layer network, so I tried implementing a system with multiple hidden layers. To my surprise the accuracy dropped even more. What can be wrong? I checked step by step the forward and backprop algorithms and they seem to be ok, so I don’t know what else to try.


More context: I’m quite new to neural networks, finished Andrew Ng’s Coursera course and that’s pretty much all I now about this

Any help is much appreciated",neuralnetworks,el_franz,False,/r/neuralnetworks/comments/i02umk/need_help_with_classifier_system/
"DeepFaceDrawing Generates Real Faces From Sketches. Image-to-image translation in 2020+, is it biased, could it be used in a real world application? Paper explained",1596023246,,neuralnetworks,ThisVineGuy,False,/r/neuralnetworks/comments/hzzaos/deepfacedrawing_generates_real_faces_from/
Looking into the black box,1595987970,,neuralnetworks,FindLight2017,False,/r/neuralnetworks/comments/hzry50/looking_into_the_black_box/
Need some help to run a code,1595958780,"Hello people!

I'm trying to re-run the code of this article 

 [https://medium.com/better-programming/how-to-build-a-deep-audio-de-noiser-using-tensorflow-2-0-79c1c1aea299](https://medium.com/better-programming/how-to-build-a-deep-audio-de-noiser-using-tensorflow-2-0-79c1c1aea299) 

I thought it would be a good exercice to understand what going on in this already done code so I can see how to improve it, or the impact of the modifications in the model. And also it'd be very cool to see it working.

I'm fairly new to AI, I watched the Fastai videos and that about it. I'm struggling with the denoiser and it's bumming me out considering that everything is done and it shouldn't be very hard to just run it.

If I understood correctly,I need to generate the dataset first, download it on google drive and after that, mount colab to my drive, change the url to the dataset with my drive in the code and finally train the model. Is that it or am I mistaken?

&amp;#x200B;

I'm reading some articles on how to use github, colab and drive efficiently I'd be glad if someone could help me (in the words of the famous Michael Scott : explain it to me like I'm 5 years old). Thank's for reading and sorry for the mistakes!!",neuralnetworks,chilliandpepperrules,False,/r/neuralnetworks/comments/hzjd22/need_some_help_to_run_a_code/
What Are Your Thoughts On Building a Trustworthy and Useful Future for AI?,1595955330,"One of AI's biggest obstacles has definitely been the idea of the trust factor, and it is sure to determine the landscape of AI moving forward. I thought this MIT Technology Review Article does a great job of outlining just what exactly we should be aware of to build trust and create a future for AI in our society.

What are your thoughts on the article and in general? Do you think that AI needs strong regulations, or that it is going to be best used when not restricted? Would love you hear your thoughts!

[https://www.technologyreview.com/2020/07/06/1004823/beyond-the-ai-hype-cycle-trust-and-the-future-of-ai/](https://www.technologyreview.com/2020/07/06/1004823/beyond-the-ai-hype-cycle-trust-and-the-future-of-ai/)",neuralnetworks,explainxai,False,/r/neuralnetworks/comments/hzi9iw/what_are_your_thoughts_on_building_a_trustworthy/
ML newsletter to keep you up-to-date,1595948507,"We've recently published our Sunday free edition of TheSequence newsletter. It that edition:

 \-Meta-Learning to Generate RL Models 

\-Self-Supervised Learning for Image Classification 

\-Data-Efficient RL -

\-LinkedIn’s LIquid -

\-TensorFlow Lite XNNPACK Integration

\-and recent investment money flowing into AI startups

Hope you'll find it useful

[https://thesequence.substack.com/p/thesequence-scope-the-challenge-of](https://thesequence.substack.com/p/thesequence-scope-the-challenge-of)",neuralnetworks,Kseniase,False,/r/neuralnetworks/comments/hzg78q/ml_newsletter_to_keep_you_uptodate/
Why are several connections to a node necessary in a multi-layer neural network?,1595946296,"If a node in the output layer has 3 inputs, and while the backpropagation has to adjust the 3 weights, it would be enough to just adjust one single weight a bit more, wouldn't it?",neuralnetworks,neurosciquestions,False,/r/neuralnetworks/comments/hzfkcr/why_are_several_connections_to_a_node_necessary/
Multichannel filter backpropagation in convolution,1595901631,"I'm confused about backpropagation with respect to the input tensor in a convolutional layer, especially with multichannel filters. I know that the derivative w.r.t the input matrix is the convolution applied by the inverted filtered and the chain ruled-matrix of the next stage. I looked everywhere online, but they only use single channel filters as examples. So far, this is the only thread that I've found about my problem, but I'm still confused by the answer. [https://stats.stackexchange.com/questions/361817/back-propagation-in-convolution-layer](https://stats.stackexchange.com/questions/361817/back-propagation-in-convolution-layer) . 

In the drawing, the person has 5 filters of N x N x 3 in the forwards pass, and then  in the backwards pass, it is 3 filters of N x N x 5. The commentator says that the list of multi-channel filters is basically transposed. But then the commentator said that s/he updated their answer, which is that the filter of shape NxNx5 is duplicated 3 times. Maybe I'm just over-confusing myself over the wording, but this is very different from transposition, and in this case, I'm curious to know what the NxNx5 filter that we will eventually ""repeat 3 times""  looks like, and how it originates from the 5 filters of shape NxNx5 in the forward pass. 

Thanks.",neuralnetworks,stravimond,False,/r/neuralnetworks/comments/hz5vky/multichannel_filter_backpropagation_in_convolution/
Which laptop should I buy?,1595877779,"So I've been looking at a lot of laptops from a lot of companies and I'm not able to come to a conclusion about which one to buy. I want a laptop that can handle the computation power required by neural networks, especially while doing image processing. And under 1000 dollars. Any suggestions?",neuralnetworks,fireboltkk2000,False,/r/neuralnetworks/comments/hyyx15/which_laptop_should_i_buy/
Neuroevolution From Scratch,1595877367,[removed],neuralnetworks,Gonespral,False,/r/neuralnetworks/comments/hyys44/neuroevolution_from_scratch/
Fooling Neural Networks with Noise,1595852468,,neuralnetworks,PowerOfLove1985,False,/r/neuralnetworks/comments/hyr8gj/fooling_neural_networks_with_noise/
I had GPT-3 write me an essay about immortality. Here's how it went.,1595836642,,neuralnetworks,onlyartist6,False,/r/neuralnetworks/comments/hyo194/i_had_gpt3_write_me_an_essay_about_immortality/
Quantifying Attention Flow In Transformers (Effective Way to Interpret Attention in BERT) Explained,1595808883,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/hyhw9k/quantifying_attention_flow_in_transformers/
This is a Neural Network Guitar Pedal,1595793605,,neuralnetworks,dizzykiwi3,False,/r/neuralnetworks/comments/hydml0/this_is_a_neural_network_guitar_pedal/
LiteSeg: A Litewiegth ConvNet for Semantic Segmentation (67.81% mIOU at 161 FPS on the Cityscapes dataset.),1595786069,,neuralnetworks,tahaemara,False,/r/neuralnetworks/comments/hybc4x/liteseg_a_litewiegth_convnet_for_semantic/
"OpenAI's New Language Generator: GPT-3 | This AI Generates Code, Websites, Songs &amp; More From Words",1595785643,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hyb7ih/openais_new_language_generator_gpt3_this_ai/
[AI application] Python implementation of Proximal Policy Optimization (PPO) algorithm for Super Mario Bros. 29/32 levels have been conquered,1595752846,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/hy3iz9/ai_application_python_implementation_of_proximal/
Looking for like minded people,1595702317,"Hello everyone. Although I am still a high school student, I am very interested in AI, especially AGI. In the future I am planning on becoming an AGI researcher, unlike most people who are interested in AI, who usually want to become software engineers implementing AI or narrow AI researchers (that is very interesting too, but personally I like creating things, and as AGI doesn't exist yet, I find the thought of working on it quite fascinating). When I go to university I will probably study one or more of these subjects: Mathematics, Neuroscience, Cognitive Science, Computer Science, Physics, etc. (these are also the fields I think AGI will emerge from).

I was told that the best way to learn and gain knowledge is to connect with other like minded people. I was thinking of creating a Discord server (or some group chat on another app), where a few people like us could talk to each other, share ideas, work together etc. 

I know this subreddit and a lot more are already dedicated to that, but I am looking for people whose interests are more aligned with mine, and I am sure that server or group would also not have many members, meaning it would be more personal, which I think is better.

Thanks for reading and please let me know if you would be interested :)",neuralnetworks,ASDFGHIJKLMNOPQR,False,/r/neuralnetworks/comments/hxrr2q/looking_for_like_minded_people/
Hey guys just wondering,1595694924,I am starting machine learning and the calculus and statistics are heavy(well obviously) and me being a 14 year old I can learn that only in one year( which is a lot) is there a shortcut because I am running out of ideas!,neuralnetworks,ujdiejH,False,/r/neuralnetworks/comments/hxpn4b/hey_guys_just_wondering/
This AI can fill the missing pixels behind a removed moving object and reconstruct the whole video with way more accuracy and less blurriness than current state-of-the-art approaches!,1595686517,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hxneih/this_ai_can_fill_the_missing_pixels_behind_a/
Low validation accuracy VGG-19 CIFAR-10 CNN,1595670065,"Hey Guys, I am trying to train a VGG-19 CNN on CIFAR-10 dataset using data augmentation and batch normalization. The code can be found [VGG-19 CNN](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/VGG_19_CIFAR10.ipynb).

I took two approaches to training the model:

1. Using early stopping: loss = 2.2816 and accuracy = 47.1700%
2. Without early stopping: loss = 3.3211 and accuracy = 56.6800%

The loss and accuracy are on validation data. Also, when the model is trained without early stopping, it's trained for 145 epochs.

What's going wrong? Why am I not able to reach a higher validation accuracy and lower validation loss?

&amp;#x200B;

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hxk3s8/low_validation_accuracy_vgg19_cifar10_cnn/
Gated Linear Networks: paper and code,1595668869,[removed],neuralnetworks,learned-machine,False,/r/neuralnetworks/comments/hxjwp7/gated_linear_networks_paper_and_code/
Gated Linear Networks: paper and code,1595667403,[removed],neuralnetworks,learned-machine,False,/r/neuralnetworks/comments/hxjnyw/gated_linear_networks_paper_and_code/
[R] Deepmind's Gated Linear Networks: paper and code,1595666430,,neuralnetworks,[deleted],False,/r/neuralnetworks/comments/hxjidy/r_deepminds_gated_linear_networks_paper_and_code/
Office of the Director for National Intelligence Releases Ethical AI Guidelines -&gt; Thoughts?,1595618788,"The Office of the Director for National Intelligence issued guidance Thursday for the ethical development and use of artificial intelligence. If we are going to continue to implement AI into our future, it's nice to see that some government regulation is being taken. What are your thoughts on the ODNI's release of artificial intelligence development guidance?

[https://www.fedscoop.com/odni-guiding-principles-ai/](https://www.fedscoop.com/odni-guiding-principles-ai/)",neuralnetworks,explainxai,False,/r/neuralnetworks/comments/hx89mw/office_of_the_director_for_national_intelligence/
Data Augmentation CIFAR-10 reduces CNN model accuracy,1595610675,"I have a [Conv-6](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Data_Augmentation_Conv6_CIFAR10_Example.ipynb) CNN for CIFAR-10 where I am using Python3, TensorFlow2.0 based data augmentations as follows:

&amp;#x200B;

        # Example of using 'tf.keras.preprocessing.image import ImageDataGenerator class's - flow(x, y)':
        datagen = ImageDataGenerator(
            # featurewise_center=True,
            # featurewise_std_normalization=True,
            rotation_range = 90,
            width_shift_range = 0.1,
            height_shift_range = 0.1,
            horizontal_flip = True
        )

But using this data augmentation technique reduces the accuracy to 66.43%. As opposed to this, when I train the same model without any data augmentations, the accuracy is 79.22%.

&amp;#x200B;

What's going wrong?

&amp;#x200B;

Thanks",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hx5phs/data_augmentation_cifar10_reduces_cnn_model/
Help with optimizing algorithms,1595540458,"Hello everyone,

I'm in need of help concerning optimizing algorithms. I've managed to make normal gradient descent and gradient descent with momentum work, but I can't make Adam or RMSProp work.

I've made a Stackoverflow post but haven't got any replies yet. If someone could help, I'd be forever indebted.

If someone need the full model to make sense of my mistake, I'll gladly share it.

Thank you and good night!

Here is the post:

- https://stackoverflow.com/questions/62884443/momentum-rmsprop-and-adam-optimizers",neuralnetworks,eduardoaemunoz,False,/r/neuralnetworks/comments/hwopuv/help_with_optimizing_algorithms/
TensroFlow2.0 CNN validation ValueError,1595540316,"I have a Conv-6 CNN inspired from VGG-19 for CIFAR-10 dataset which I am using with Data Augmentation using tf.Datagen flow() method. The code is as follows-

&amp;#x200B;

&amp;#x200B;

        # Data preprocessing and cleaning:
        # input image dimensions
        img_rows, img_cols = 32, 32
        
        # Load CIFAR-10 dataset-
        (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()
        
        print(""X_train.shape = {0}, y_train.shape = {1}"".format(X_train.shape, y_train.shape))
        print(""X_test.shape = {0}, y_test.shape = {1}"".format(X_test.shape, y_test.shape))
        # X_train.shape = (50000, 32, 32, 3), y_train.shape = (50000, 1)
        # X_test.shape = (10000, 32, 32, 3), y_test.shape = (10000, 1)
        
        if tf.keras.backend.image_data_format() == 'channels_first':
            X_train = X_train.reshape(X_train.shape[0], 3, img_rows, img_cols)
            X_test = X_test.reshape(X_test.shape[0], 3, img_rows, img_cols)
            input_shape = (3, img_rows, img_cols)
        else:
            X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 3)
            X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 3)
            input_shape = (img_rows, img_cols, 3)
        
        print(""\n'input_shape' which will be used = {0}\n"".format(input_shape))
        # 'input_shape' which will be used = (32, 32, 3)
        
        
        # Convert datasets to floating point types-
        X_train = X_train.astype('float32')
        X_test = X_test.astype('float32')
        
        # Normalize the training and testing datasets-
        X_train /= 255.0
        X_test /= 255.0
        
        # convert class vectors/target to binary class matrices or one-hot encoded values-
        y_train = tf.keras.utils.to_categorical(y_train, num_classes)
        y_test = tf.keras.utils.to_categorical(y_test, num_classes)
        
        print(""\nDimensions of training and testing sets are:"")
        print(""X_train.shape = {0}, y_train.shape = {1}"".format(X_train.shape, y_train.shape))
        print(""X_test.shape = {0}, y_test.shape = {1}"".format(X_test.shape, y_test.shape))
        # Dimensions of training and testing sets are:
        # X_train.shape = (50000, 32, 32, 3), y_train.shape = (50000, 10)
        # X_test.shape = (10000, 32, 32, 3), y_test.shape = (10000, 10)
        
        train_dataset_features = tf.data.Dataset.from_tensor_slices(X_train)
        train_dataset_labels = tf.data.Dataset.from_tensor_slices(y_train)
        test_dataset_features = tf.data.Dataset.from_tensor_slices(X_test)
        test_dataset_labels = tf.data.Dataset.from_tensor_slices(y_test)
        
        # Choose an optimizer and loss function for training-
        loss_fn = tf.keras.losses.CategoricalCrossentropy()
        optimizer = tf.keras.optimizers.Adam(lr = 0.0003)
        
        # Select metrics to measure the error &amp; accuracy of model.
        # These metrics accumulate the values over epochs and then
        # print the overall result-
        train_loss = tf.keras.metrics.Mean(name = 'train_loss')
        train_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'train_accuracy')
        
        test_loss = tf.keras.metrics.Mean(name = 'test_loss')
        test_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'test_accuracy')
        
        
        # Example of using 'tf.keras.preprocessing.image import ImageDataGenerator class's - flow(x, y)':
        datagen = ImageDataGenerator(
            # featurewise_center=True,
            # featurewise_std_normalization=True,
            rotation_range = 90,
            width_shift_range = 0.1,
            height_shift_range = 0.1,
            horizontal_flip = True
        )
        
        
        
        def conv6_cnn():
            """"""
            Function to define the architecture of a neural network model
            following Conv-6 architecture for CIFAR-10 dataset and using
            provided parameter which are used to prune the model.
            
            Conv-6 architecture-
            64, 64, pool  -- convolutional layers
            128, 128, pool -- convolutional layers
            256, 256, pool -- convolutional layers
            256, 256, 10  -- fully connected layers
            
            Output: Returns designed and compiled neural network model
            """"""
            
            l = tf.keras.layers
            
            model = Sequential()
            
            model.add(
                Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same',
                    input_shape=(32, 32, 3)
                )    
            )
                
            model.add(
                Conv2D(
                    filters = 64, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
            
            model.add(
                MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(
                Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                Conv2D(
                    filters = 128, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
        
            model.add(
                Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                Conv2D(
                    filters = 256, kernel_size = (3, 3),
                    activation='relu', kernel_initializer = tf.initializers.GlorotNormal(),
                    strides = (1, 1), padding = 'same'
                )
            )
        
            model.add(
                MaxPooling2D(
                    pool_size = (2, 2),
                    strides = (2, 2)
                )
            )
            
            model.add(Flatten())
            
            model.add(
                Dense(
                    units = 256, activation='relu',
                    kernel_initializer = tf.initializers.GlorotNormal()
                )
            )
            
            model.add(
                Dense(
                    units = 256, activation='relu',
                    kernel_initializer = tf.initializers.GlorotNormal()
                )
            )
            
            model.add(
                Dense(
                    units = 10, activation='softmax'
                )
            )
            
        
            # Compile pruned CNN-
            model.compile(
                loss=tf.keras.losses.categorical_crossentropy,
                # optimizer='adam',
                optimizer=tf.keras.optimizers.Adam(lr = 0.0003),
                metrics=['accuracy']
            )
            
            
            return model
        
        # Instantiate a new Conv-2 CNN model-
        orig_model = conv6_cnn()
        
        # Load weights from before having 92.55% sparsity-
        orig_model.load_weights(""Conv_6_CIFAR10_Magnitude_Based_Winning_Ticket_Distribution_92.55423622890814.h5"")
        
        # Create mask using winning ticket-
        
        # Use masks to preserve sparsity-
        # Instantiate a new neural network model for which, the mask is to be created,
        mask_model = conv6_cnn()
            
        # Load weights of PRUNED model-
        mask_model.set_weights(orig_model.get_weights())
            
        # For each layer, for each weight which is 0, leave it, as is.
        # And for weights which survive the pruning,reinitialize it to ONE (1)-
        for wts in mask_model.trainable_weights:
            wts.assign(tf.where(tf.equal(wts, 0.), 0., 1.))
        
        
        # User input parameters for Early Stopping in manual implementation-
        minimum_delta = 0.001
        patience = 3
        
        best_val_loss = 100
        loc_patience = 0
        
        # Initialize a new LeNet-300-100 model-
        winning_ticket_model = conv6_cnn()
        
        # Load weights of winning ticket-
        winning_ticket_model.set_weights(orig_model.get_weights())
        
        # Define 'train_one_step()' and 'test_step()' functions here-
        u/tf.function
        def train_one_step(model, mask_model, optimizer, x, y):
            '''
            Function to compute one step of gradient descent optimization
            '''
            with tf.GradientTape() as tape:
                # Make predictions using defined model-
                y_pred = model(x)
        
                # Compute loss-
                loss = loss_fn(y, y_pred)
                
            # Compute gradients wrt defined loss and weights and biases-
            grads = tape.gradient(loss, model.trainable_variables)
            
            # type(grads)
            # list
            
            # List to hold element-wise multiplication between-
            # computed gradient and masks-
            grad_mask_mul = []
            
            # Perform element-wise multiplication between computed gradients and masks-
            for grad_layer, mask in zip(grads, mask_model.trainable_weights):
                grad_mask_mul.append(tf.math.multiply(grad_layer, mask))
            
            # Apply computed gradients to model's weights and biases-
            optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))
        
            # Compute accuracy-
            train_loss(loss)
            train_accuracy(y, y_pred)
        
            return None
            
            
        u/tf.function
        def test_step(model, optimizer, data, labels):
            """"""
            Function to test model performance
            on testing dataset
            """"""
            
            predictions = model(data)
            t_loss = loss_fn(labels, predictions)
        
            test_loss(t_loss)
            test_accuracy(labels, predictions)
        
            return None
        
        
        curr_step = 0
            
        for x, y in datagen.flow(X_train, y_train, batch_size = batch_size, shuffle = True):
            train_one_step(winning_ticket_model, mask_model, optimizer, x, y)
            # print(""current step = "", curr_step)
            curr_step += 1
                
            if curr_step &gt;= X_train.shape[0] // batch_size:
                print(""\nTerminating training (datagen.flow())"")
                break
         
    

But the following code gives error:

&amp;#x200B;

        for x_t, y_t in test_dataset:
            test_step(winning_ticket_model, optimizer, x_t, y_t)
    

&amp;#x200B;

&gt;\&gt; ValueError                                Traceback (most recent call  
&gt;  
&gt;\&gt; last) &lt;ipython-input-77-c422951ac154&gt; in &lt;module&gt;  
&gt;  
&gt;\&gt;       1 for x\_t, y\_t in test\_dataset:  
&gt;  
&gt;\&gt; ----&gt; 2     test\_step(winning\_ticket\_model, optimizer, x\_t, y\_t)  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_\_call\_\_(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     578         xla\_context.Exit()  
&gt;  
&gt;\&gt;     579     else:  
&gt;  
&gt;\&gt; --&gt; 580       result = self.\_call(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     581   
&gt;  
&gt;\&gt;     582     if tracing\_count == self.\_get\_tracing\_count():  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_call(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     625       # This is the first call of \_\_call\_\_, so we have to initialize.  
&gt;  
&gt;\&gt;     626       initializers = \[\]  
&gt;  
&gt;\&gt; --&gt; 627       self.\_initialize(args, kwds, add\_initializers\_to=initializers)  
&gt;  
&gt;\&gt;     628     finally:  
&gt;  
&gt;\&gt;     629       # At this point we know that the initialization is complete (or less  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_initialize(self, args, kwds, add\_initializers\_to)  
&gt;  
&gt;\&gt;     504     self.\_concrete\_stateful\_fn = (  
&gt;  
&gt;\&gt;     505         self.\_stateful\_fn.\_get\_concrete\_function\_internal\_garbage\_collected(   
&gt;  
&gt;\&gt; # pylint: disable=protected-access  
&gt;  
&gt;\&gt; --&gt; 506             \*args, \*\*kwds))  
&gt;  
&gt;\&gt;     507   
&gt;  
&gt;\&gt;     508     def invalid\_creator\_scope(\*unused\_args, \*\*unused\_kwds):  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_get\_concrete\_function\_internal\_garbage\_collected(self, \*args,  
&gt;  
&gt;\&gt; \*\*kwargs)    2444       args, kwargs = None, None    2445     with self.\_lock:  
&gt;  
&gt;\&gt; -&gt; 2446       graph\_function, \_, \_ = self.\_maybe\_define\_function(args, kwargs)    2447     return graph\_function    2448   
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_maybe\_define\_function(self, args, kwargs)    2775     2776        
&gt;  
&gt;\&gt; self.\_function\_cache.missed.add(call\_context\_key)  
&gt;  
&gt;\&gt; -&gt; 2777       graph\_function = self.\_create\_graph\_function(args, kwargs)    2778       self.\_function\_cache.primary\[cache\_key\] =  
&gt;  
&gt;\&gt; graph\_function    2779       return graph\_function, args, kwargs  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_create\_graph\_function(self, args, kwargs,  
&gt;  
&gt;\&gt; override\_flat\_arg\_shapes)    2665             arg\_names=arg\_names,     
&gt;  
&gt;\&gt; 2666             override\_flat\_arg\_shapes=override\_flat\_arg\_shapes,  
&gt;  
&gt;\&gt; -&gt; 2667             capture\_by\_value=self.\_capture\_by\_value),    2668         self.\_function\_attributes,    2669         # Tell the ConcreteFunction  
&gt;  
&gt;\&gt; to clean up its graph once it goes out of  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func\_graph.py  
&gt;  
&gt;\&gt; in func\_graph\_from\_py\_func(name, python\_func, args, kwargs, signature,  
&gt;  
&gt;\&gt; func\_graph, autograph, autograph\_options, add\_control\_dependencies,  
&gt;  
&gt;\&gt; arg\_names, op\_return\_value, collections, capture\_by\_value,  
&gt;  
&gt;\&gt; override\_flat\_arg\_shapes)  
&gt;  
&gt;\&gt;     979         \_, original\_func = tf\_decorator.unwrap(python\_func)  
&gt;  
&gt;\&gt;     980   
&gt;  
&gt;\&gt; --&gt; 981       func\_outputs = python\_func(\*func\_args, \*\*func\_kwargs)  
&gt;  
&gt;\&gt;     982   
&gt;  
&gt;\&gt;     983       # invariant: \`func\_outputs\` contains only Tensors, CompositeTensors,  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in wrapped\_fn(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     439         # \_\_wrapped\_\_ allows AutoGraph to swap in a converted function. We give  
&gt;  
&gt;\&gt;     440         # the function a weak reference to itself to avoid a reference cycle.  
&gt;  
&gt;\&gt; --&gt; 441         return weak\_wrapped\_fn().\_\_wrapped\_\_(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     442     weak\_wrapped\_fn = weakref.ref(wrapped\_fn)  
&gt;  
&gt;\&gt;     443   
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; \~/.local/lib/python3.7/site-packages/tensorflow/python/framework/func\_graph.py  
&gt;  
&gt;\&gt; in wrapper(\*args, \*\*kwargs)  
&gt;  
&gt;\&gt;     966           except Exception as e:  # pylint:disable=broad-except  
&gt;  
&gt;\&gt;     967             if hasattr(e, ""ag\_error\_metadata""):  
&gt;  
&gt;\&gt; --&gt; 968               raise e.ag\_error\_metadata.to\_exception(e)  
&gt;  
&gt;\&gt;     969             else:  
&gt;  
&gt;\&gt;     970               raise  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; ValueError: in user code:  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt;     &lt;ipython-input-61-9c297d161e54&gt;:45 test\_step  \*  
&gt;  
&gt;\&gt;         predictions = model(data)  
&gt;  
&gt;\&gt;     /home/majumdar/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/base\_layer.py:886  
&gt;  
&gt;\&gt; \_\_call\_\_  \*\*  
&gt;  
&gt;\&gt;         [self.name](https://self.name))  
&gt;  
&gt;\&gt;     /home/majumdar/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/input\_spec.py:180  
&gt;  
&gt;\&gt; assert\_input\_compatibility  
&gt;  
&gt;\&gt;         str(x.shape.as\_list()))  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt;     ValueError: Input 0 of layer sequential\_7 is incompatible with the layer: expected ndim=4, found ndim=3. Full shape received: \[32, 32, 3\]  
&gt;  
&gt;\&gt;   
&gt;  
&gt;\&gt; ​

&amp;#x200B;

&amp;#x200B;

What's the problem?

&amp;#x200B;

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hwooah/tensroflow20_cnn_validation_valueerror/
Your feedback about new ML newsletter,1595522083," 

Hey, I wanted to share a small educational ML newsletter I've been working on with my partner. You might find this newsletter different from others and really useful to build and reinforce your knowledge about ML and AI. Would really love to hear your feedback.

This is the free Sunday edition:  
[https://thesequence.substack.com/p/thesequence-scope-the-mismatch-between](https://thesequence.substack.com/p/thesequence-scope-the-mismatch-between)",neuralnetworks,Kseniase,False,/r/neuralnetworks/comments/hwiv5h/your_feedback_about_new_ml_newsletter/
MetaProgramming: Code That Writes Itself!!,1595472156,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/hw7r3c/metaprogramming_code_that_writes_itself/
NEW EPISODE of SOME GOOD NEWS with JOHN KRASINSKI (Face Animation + Cloning Voice),1595424191,,neuralnetworks,Inner_Assist,False,/r/neuralnetworks/comments/hvtoz3/new_episode_of_some_good_news_with_john_krasinski/
NEW EPISODE of SOME GOOD NEWS with JOHN KRASINSKI (Face Animation + Cloning Voice),1595417073,[deleted],neuralnetworks,[deleted],False,/r/neuralnetworks/comments/hvs1t7/new_episode_of_some_good_news_with_john_krasinski/
Training over 10 million images,1595407595,"Hi, i would like to get some feedback on how to train 10 million images with approximately 1 million labels. 

My main concern would be the training process as it would consume too much resource. Is there a method out there that could help with training large datasets? (approx 10mil image)",neuralnetworks,00quant,False,/r/neuralnetworks/comments/hvq904/training_over_10_million_images/
GraphSage Link Prediction Question,1595336512,[removed],neuralnetworks,machinelearningnewby,False,/r/neuralnetworks/comments/hv6y09/graphsage_link_prediction_question/
Deep Compression research paper - Quantization,1595273511,"Hey Guys, I was reading [Deep Compression paper](https://arxiv.org/abs/1510.00149) and came across the 3 stage pipeline:

&amp;#x200B;

https://preview.redd.it/i1i0m5znd2c51.png?width=709&amp;format=png&amp;auto=webp&amp;s=a46909b88e4e49e709871b43c55a5e656fff9bf1

In ""Quantization"" stage: you cluster the trained weights and get a mapping between the clustered weights and their respective indices in the layer/tensor, which they call ""Code Book"". Now, you quantize this mapping, i.e., quantize the weights so that instead of Floating Point 32 bits, they now take 16 bits or 8 bits.

What I don't get is ""Retrain Code Book"". What's supposed to happen here? 

Help?",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hury7n/deep_compression_research_paper_quantization/
Can someone please explain this to me in brief? Need Help! (related to Real Time Object Detection &amp; Recognition),1595270387,,neuralnetworks,heartbrokenkid17,False,/r/neuralnetworks/comments/huqwtl/can_someone_please_explain_this_to_me_in_brief/
"Is neural network suitable for this wine quality dataset? The prediction always shows 1, but there should be the other classes(2-10). [using GNU Octave]",1595216828," I've chosen lambda to be 3 in regularization term because the cost of it is the lowest one. Maybe I was wrong. I learned the code below from Andrew Ng's machine learning course, and I don't know what I missed.

Dataset: X:1599(examples)×11(features) , Y:1599x1(values are 1-10\[classes\])

project link: (including dataset) [https://drive.google.com/drive/folders/10TpAZ1pbFS7i4sedJrs8iJmhF8jY1cLH?usp=sharing](https://drive.google.com/drive/folders/10TpAZ1pbFS7i4sedJrs8iJmhF8jY1cLH?usp=sharing)

\* lrown.m \[main file\]",neuralnetworks,JacksonSteel,False,/r/neuralnetworks/comments/huecla/is_neural_network_suitable_for_this_wine_quality/
Need help understanding the derivative of the softmax layer?,1595203796,"I've been researching the softmax in a project. Am I doing anything wrong?

So I have the weighted sums vector Z, and the softmax mapping  T which maps Z in R\^n to the vector S in R\^n, so T(Z) -&gt; S, where adding up S equals 1. When I created the derivative vector for dS/dZ, (which I'll call Q), I went through each entry z\_i of Z, and then for each of them I iterated Z again for each entry z\_j, putting down T(z\_i)\*(1 - T(z\_i)) if i == j, and  -T(z\_i)\*T(z\_j)  if i != j.

So my Jacobian matrix, J, looks like what they did here  [https://stats.stackexchange.com/questions/267576/matrix-representation-of-softmax-derivatives-in-backpropagation](https://stats.stackexchange.com/questions/267576/matrix-representation-of-softmax-derivatives-in-backpropagation) . (I've kept some stuff shorter like just making it 2x2, and denoting the T(z\_i) entries as Si)

    J = 
    [S1(1-S1),  -S1*S2, 
       -S1*S2, S2(1-S2)]

This is where things got more confusing for me.  What I did was that I basically added up the rows, and then multiplied each entry of the new vector Q' by the derivative scalar of the next layer (For my case,  this is Binary Cross Entropy), which I'll denote by R and got my derivative vector Q

    Q' = [(S1(1-S1) + -S1*S2  ),
          (  -S1*S2 + S2(1-S2))]
    Q = Q' * R 
    [R is scalar, Q and Q' are vectors]

Am I doing anything wrong for me in my derivative vector Q?",neuralnetworks,stravimond,False,/r/neuralnetworks/comments/hub6d0/need_help_understanding_the_derivative_of_the/
Can we design a neural network to find a full-sized color image hidden inside another image?,1595182428,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/hu52pg/can_we_design_a_neural_network_to_find_a/
A basic guide introducing hierarchical clustering,1595101541,,neuralnetworks,Vaishali_Advani,False,/r/neuralnetworks/comments/htmtej/a_basic_guide_introducing_hierarchical_clustering/
Swapping Autoencoder Explained. New Texture-Swapping technique for Image Manipulation,1595081518,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hth8ai/swapping_autoencoder_explained_new/
Deep Learning by Yoshua Bengio vs Dive Into Deep Learning?,1595032330,,neuralnetworks,JacksonSteel,False,/r/neuralnetworks/comments/ht7dnt/deep_learning_by_yoshua_bengio_vs_dive_into_deep/
FDMLC 2020,1595031669,"[https://www.explainx.ai/resources/practical-xai-building-trustworthy-transparent-unbiased-ml-algorithms](https://www.explainx.ai/resources/practical-xai-building-trustworthy-transparent-unbiased-ml-algorithms)

Our co-founder recently spoke at the Federated &amp; Distributed Machine Learning Conference 2020 on explainable AI’s influence in highly-regulated industries. See how our open-source python library, explainx, remedies biased and in-transparent ML models.

\#xAI",neuralnetworks,explainxai,False,/r/neuralnetworks/comments/ht77ko/fdmlc_2020/
Art generating GAN,1595015891,Is there a specific type of GAN I should use in order to generate new art? Say portaits or Landscapes?,neuralnetworks,DrEl1344,False,/r/neuralnetworks/comments/ht2p27/art_generating_gan/
"Neurons: Forced training, help needed",1594987275," So i am looking to train a neuron with some rules, i have a normal input vector ""training\_inputs"" and a set of inputs that the neuron has to adept to called ""restricted\_inputs"", and these has pass the activation function and yield results within a range of: lim &lt; sigmoid(dot(restricted\_inputs, weights)) &lt; -lim.

Pseudocode in python:

    for _ in range(n_iter):
    ....
    
    # Normal Training:
        output = sigmoid(dot(training_inputs, weights))
        error = training_outputs - output
        adjustments = dot(training_inputs.T, error * sigmoid_derivative(output))
        weights += adjustments
    
    # Forced Training:
        lim = 0.05
        restricted_output = sigmoid(dot(restricted_inputs, weights))
        
        while (restricted_output.all() &gt; lim) or (restricted_output.all() &lt; -lim):
    
            for i in range(len(restricted_output)):
        
                if restricted_output[i] &gt; discount_rate:
                    error[i] += abs(restricted_output[i] - discount_rate)
        
                elif restricted_output[i] &lt; -discount_rate:
                    error[i] += abs(restricted_output[i] + discount_rate)
        
                else:
                    error[i] += 0
    
                adjustments = dot(restricted_inputs.T, error * sigmoid_derivative(restricted_output))
    
                weights += adjustments
                restricted_output = sigmoid(dot(restricted_inputs, weights))

Problem is that the weights never gets adjusted properly within the ""forced training"" section, sigmoid(dot(restricted\_inputs, weights)) never falls within the defined limit; lim, so i am stuck in the while loop for eternity. Am i missing something, if so, what?",neuralnetworks,MerchantMojo,False,/r/neuralnetworks/comments/hsufrt/neurons_forced_training_help_needed/
Neural Networks,1594969772,,neuralnetworks,Techbiason,False,/r/neuralnetworks/comments/hsr7ai/neural_networks/
audio neural netwnetwork?,1594935215,,neuralnetworks,mariois64,False,/r/neuralnetworks/comments/hsiwjw/audio_neural_netwnetwork/
[N] Poorly-Explained NeurIPS 2020 Desk-Rejects Peeve ML Researchers,1594925771,"Many researchers awaiting a decision on whether their papers would make it to this year’s [NeurIPS](https://nips.cc/) conference received word this week — but the notifications have provoked more questions than they answered. NeurIPS desk-rejects (papers passed over without review) has become “a super noisy process,” complains a [post](https://www.reddit.com/r/MachineLearning/comments/hqrxvs/d_neurips_desk_rejects_have_gone_out/) in the Reddit Machine Learning group.

Here is a quick read:  [Poorly-Explained NeurIPS 2020 Desk-Rejects Peeve ML Researchers](https://syncedreview.com/2020/07/16/poorly-explained-neurips-2020-desk-rejects-peeve-ml-researchers/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/hsfrc3/n_poorlyexplained_neurips_2020_deskrejects_peeve/
Learn how we can detect and recognize objects in Images using Pytorch,1594910153,,neuralnetworks,MarinaChatterjee,False,/r/neuralnetworks/comments/hsaq3b/learn_how_we_can_detect_and_recognize_objects_in/
Quantized TensorFlow2.0 model increases prediction time,1594906869,"I am using Python 3.8 and TensorFlow 2.2 to train LeNet-300-100 Dense neural network on MNIST dataset using a sparse network with about 91.3% sparsity.

&amp;#x200B;

I am quantizing the neural network from FP32 down to FP16 using the following codes. Here 'winning\_ticket\_model' is a LeNet-300-100 model trained until convergence.

&amp;#x200B;

        # Initialize a new model-
        quantized_model = lenet_nn()
        
        # Load trained winning ticket weights-
        quantized_model.set_weights(winning_ticket_model.get_weights())
    
        # Python3 list to hold FP16 weights-
        fp16_wts = []
        
        # Iterate through each layer converting weights from FP32 to FP16-
        for layer in quantized_model.trainable_weights:
            # print(layer.shape)
            fp16_wts.append(np.float16(layer.numpy()))
        
        def lenet_nn_quantized():
            """"""
            Function to define the architecture of a neural network model
            following 300 100 Dense Fully-Connected architecture for MNIST
            dataset.
            Uses Quantization by: Floating-Point 16 bits instead of FP32!
            
            Output: Returns designed and compiled neural network model
            """"""
            
            model = Sequential()
            model.add(InputLayer(input_shape=(784, )))
            
            model.add(
                Dense(
                    units = 300, activation='relu',
                    kernel_initializer=tf.initializers.GlorotUniform(),
                    dtype = tf.float16
                )
            )
        
            # model.add(l.Dropout(0.2))
        
            model.add(
                Dense(
                    units = 100, activation='relu',
                    kernel_initializer=tf.initializers.GlorotUniform(),
                    dtype = tf.float16
                )
            )
                
            # model.add(l.Dropout(0.1))
        
            model.add(
                Dense(
                    units = num_classes, activation='softmax',
                    dtype = tf.float16
                )
            )
            
        
            # Compile pruned NN-
            model.compile(
                loss=tf.keras.losses.categorical_crossentropy,
                # optimizer='adam',
                optimizer=tf.keras.optimizers.Adam(lr = 0.0012),
                metrics=['accuracy'])
            
            return model
        
        # Initialize a new Quantized LeNet-300-100 model-
        quantized_model2 = lenet_nn_quantized()
        
        # Load FP16 weights-
        quantized_model2.set_weights(fp16_wts)
        
        # Sanity check-
        for layer in quantized_model2.trainable_weights:
            print(""layer.shape = {0}, dtype = {1}"".format(layer.shape, layer.numpy().dtype))
        '''
        layer.shape = (784, 300), dtype = float16
        layer.shape = (300,), dtype = float16
        layer.shape = (300, 100), dtype = float16
        layer.shape = (100,), dtype = float16
        layer.shape = (100, 10), dtype = float16
        layer.shape = (10,), dtype = float16
        '''
        start = time.time()
        
        # Make predictions using Original model-
        y_pred = winning_ticket_model.predict_classes(X_test)
        # y_pred = winning_ticket_model.predict(X_test)
        
        end = time.time()
        
        print(""\nTotal time taken = {0:.4f}"".format(end - start))
        # Total time taken = 0.4640
        
        start = time.time()
        
        # Make predictions using Quantized model-
        y_pred_quantized = quantized_model2.predict_classes(X_test)
        # y_pred_quantized = quantized_model2.predict(X_test)
        
        end = time.time()
        
        print(""\nTotal time taken = {0:.4f}"".format(end - start))
        # Total time taken = 33.7782

My question is: why does the Quantized version of trained model (FP16 bits) take 33.7782 as opposed to 0.4640 for Original trained model (FP32 bits)? According to my understanding, due to reduced precision, shouldn't the predictions happen faster or equal to original trained model (FP32 bits)?

&amp;#x200B;

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hs9u2q/quantized_tensorflow20_model_increases_prediction/
Books to learn math behind neural networks?,1594903723,"Recently learned the math underlying linear and logistic regression, where can I learn math behind different neural network architectures? Like gan, lstm, autoencoders, etc...",neuralnetworks,aaeerkdp,False,/r/neuralnetworks/comments/hs90oj/books_to_learn_math_behind_neural_networks/
Question about Single Layer Perceptrons,1594890787,I'm studying Neural Networks for a uni class and my question is: can a Single Layer Perceptron have *multiple outputs* and still be considered a SLP?,neuralnetworks,AndTer99,False,/r/neuralnetworks/comments/hs6ebk/question_about_single_layer_perceptrons/
Using Hidden Markov Model and Viterbi algorithm for tagging each word in a sentence with its appropriate part of speech tags,1594878840,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/hs487q/using_hidden_markov_model_and_viterbi_algorithm/
Introduction to Linear Regression in Machine Learning. Simple Statistics applied to AI / ML / DL with a great example (See at 2:40),1594838558,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hrtbk2/introduction_to_linear_regression_in_machine/
How do you connect AI/neural networks to games?,1594742915,"I've made an Artificial Neural Network and it works great, but games have to be coded along with it.

How can you read data from third-party games? Like health or enemies' positions? And how do you send input into those games?",neuralnetworks,Hotarosu,False,/r/neuralnetworks/comments/hr4aya/how_do_you_connect_aineural_networks_to_games/
Are biases necessary in neural networks if there is at least one non-zero neuron in the previous layer?,1594727601,"I would just like to understand the importance of biases and from what I've concluded they are not needed if there is at least one non-zero neuron in the previous layer of the network. I've googled why biases are important and the main thing seems to be

**a) So that a neuron in the 2nd layer can have a non-zero value if all the neurons of the 1st layer are 0**

This is solved if you have at least one non-zero neuron in the first layer.

**b) So that you can ""move"" the line** **output = sum (weights \* inputs) + bias**  
**left an right, analogous to** **y=ax+b**

This I actually don't understand, because bias is just another neuron with a corresponding weight. So the output can still be written as output = sum (weights \* inputs)  
, the last input being the bias and the last weight being it's corresponding weight.

So what am I doing wrong?",neuralnetworks,ATCWannabeme,False,/r/neuralnetworks/comments/hr03ho/are_biases_necessary_in_neural_networks_if_there/
Linear Algebra for learning AI.,1594723004,"I wanted to start learning AI from the absolute basics like how to make neural networks from scratch, before learning to use the already made models life tensorflow, pytorch etc. To which I realised that I have to know some maths like linear algebra, probability distribution and differentiation. On searching of best resources of linear algebra for AI, the most suggested resource that came up was from MIT's opencourseware by Prof.Gilbert Strang.

I have been trying to understand his videos since last 3 days. I could hardly understand anything of it. Nw I feel I'm not smart enough for it. I am really confused ,to what should I do now.

To my understanding ""Linear algebra"" cannot be skipped if I want to learn the very basics of AI.

I really have no one who can help me in this field. Could someone please help me by suggesting some resources for learning Linear algebra, from the beginners point of view, which would help in my dream of learning AI.

I would be extremely  grateful to you. I am already grateful if you read this message .

Could you please help?  ",neuralnetworks,jango2in1,False,/r/neuralnetworks/comments/hqz6li/linear_algebra_for_learning_ai/
"In Andrew Ng's machine learning course about neural network. Why do we need to replace the first column in theta by one? Isn't that part zero?Theta2_grad_reg_term = (lambda/m) * [zeros(size(Theta2, 1), 1) Theta2(:,2:end)]; % 10 x 26",1594714886,"    Part 1: Calculating J without  Regularization 
      
    X = [ones(m,1), X];  % Adding 1 as first column in X
    a1 = X; % 5000 x 401
    z2 = a1 * Theta1';  % 5000 x 25
    a2 = sigmoid(z2); % 5000 x 25
    a2 = [ones(size(a2,1),1), a2]; % 5000 x 26
    z3 = a2 * Theta2';  % 5000x10
    a3 = sigmoid(z3); % 5000 x 10
    h_x = a3; % m x num_labels == 5000 x 10  
    
    y_Vec = (1:num_labels)==y; 
    
    J = (1/m) * sum(sum((-y_Vec.*log(h_x))-((1-y_Vec).*log(1-h_x))));
    
    Part2 backpropagation
    DELTA3 = a3 - y_Vec; % 5000 x 10
    DELTA2 = (DELTA3 * Theta2) .* [ones(size(z2,1),1) sigmoidGradient(z2)]; 
    % 5000 x 26[Theta2=10x26] 
    
    %function g = sigmoidGradient(z)  
    %  g = sigmoid(z).*(1-sigmoid(z));
    %end
    
    DELTA2 = DELTA2(:,2:end); % 5000 x 25 
      
    Theta1_grad = (1/m) * (DELTA2' * a1); % 25 x 401
    Theta2_grad = (1/m) * (DELTA3' * a2); % 10 x 26
    
    Part 3: Adding Regularisation term in J and Theta_grad
    reg_term = (lambda/(2*m)) * (sum(sum(Theta1(:,2:end).^2)) + sum(sum(Theta2(:,2:end).^2)));
      
    J = J + reg_term;
    
    Theta1_grad_reg_term = (lambda/m) * [zeros(size(Theta1, 1), 1) Theta1(:,2:end)]; % 25 x 401
    Theta2_grad_reg_term = (lambda/m) * [zeros(size(Theta2, 1), 1) Theta2(:,2:end)]; % 10 x 26
    %Why do we need to replace the first column in theta by one? Isn't that part zero?
    
    Theta1_grad = Theta1_grad +
    Theta1_grad_reg_term;
    Theta2_grad = Theta2_grad + Theta2_grad_reg_term; 
      
     % Unroll gradients
     grad = [Theta1_grad(:) ; Theta2_grad(:)];",neuralnetworks,JacksonSteel,False,/r/neuralnetworks/comments/hqxoz2/in_andrew_ngs_machine_learning_course_about/
[N] ICML 2020 Announces Outstanding Paper Awards,1594680777,"Organizers of the 37th International Conference on Machine Learning (ICML) have announced their Outstanding Paper awards, recognizing papers from the [current conference ](https://icml.cc/Conferences/2020/Awards)that are “strong representatives of solid theoretical and empirical work in our field.”

A total of 1,088 papers out of 4,990 submissions made it to the prestigious machine learning conference. The acceptance rate of 21.8 percent is slightly lower than 2019’s 22.6 percent (774 accepted papers from 3,424 submissions), and it seems likely the drastic increase in submissions helped contribute to this. 

Here is a quick read: [ICML 2020 Announces Outstanding Paper Awards](https://syncedreview.com/2020/07/13/icml-2020-announces-outstanding-paper-awards/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/hqpqsf/n_icml_2020_announces_outstanding_paper_awards/
"A breakdown of visual attention mechanisms, and the Convolution Block Attention Module (CBAM)",1594648998,"Attention mechanisms are one of the coolest, game-changing developments in the field of computer vision. This post gives an introduction to Visual Attention Mechanisms in CV, with a focus on the Convolutional Block Attention Module (CBAM) introduced in 2018 at ECCV.

Article link: [https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/](https://blog.paperspace.com/attention-mechanisms-in-computer-vision-cbam/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/hqft0v/a_breakdown_of_visual_attention_mechanisms_and/
Perceptron Learning Algorithm Explained in Detail,1594648835,,neuralnetworks,agarwalsimran,False,/r/neuralnetworks/comments/hqfr9r/perceptron_learning_algorithm_explained_in_detail/
Quantization using Sparse Neural Networks,1594631348,"I was reading and coding [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) research paper where you get smaller sub-networks, where sparsity is maintained using masks (0 and 1). I have coded these experiments in [GitHub](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2) and was wondering whether it is possible to quantize the resulting sub-networks achieved using ""The Lottery Ticket Hypothesis""?

I am using TensorFlow 2.X and Python 3.8. [TF Quantization Aware Training](https://www.tensorflow.org/model_optimization/guide/quantization/training) doesn't talk about maintaining sparsity of the sub-network.

Help?!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hqc0jf/quantization_using_sparse_neural_networks/
Quantization Aware Training with tf.GradientTape gives Error in TensorFlow2.0,1594628222,"I am using TensorFlow-2.2, tensorflow\_model\_optimization and Python 3.8. I am trying to quantize and train a LeNet-300-100 Dense neural network which contains sparsity of 91.3375%. This means that 91.3375% of the weights are zero. I was following the \[Quantization TF tutorial\]\[1\] and I wanted to train such a sparse network which has been quantized using \*tf.GradientTape\* rather than \*q\_aware\_model.fit()\*.

&amp;#x200B;

If you look into the \[example code\]\[2\], the relevant code snippets are:

&amp;#x200B;

&amp;#x200B;

    quantize_model = tfmot.quantization.keras.quantize_model
    
    # q_aware stands for for quantization aware.
    q_aware_model = quantize_model(model)
    
    # 'quantize_model' requires recompilation-
    q_aware_model.compile(
    
    optimizer = tf.keras.optimizers.Adam(lr = 0.0012),
    
    loss=tf.keras.losses.categorical_crossentropy,
    metrics=['accuracy']
    )
    
    # Define 'train_one_step()' and 'test_step()' functions here-
    @tf.function
    def train_one_step(model, mask_model, optimizer, x, y):
        '''
        Function to compute one step of gradient descent optimization
        '''
        with tf.GradientTape() as tape:
            # Make predictions using defined model-
            y_pred = model(x)
            
            # Compute loss-
            loss = loss_fn(y, y_pred)
    
        # Compute gradients wrt defined loss and weights and biases-
        grads = tape.gradient(loss, model.trainable_variables)
    
        # type(grads)
        # list
    
        # List to hold element-wise multiplication between-
        # computed gradient and masks-
        grad_mask_mul = []
    
        # Perform element-wise multiplication between computed gradients and masks-
        for grad_layer, mask in zip(grads, mask_model.trainable_weights):
            grad_mask_mul.append(tf.math.multiply(grad_layer, mask))
        
        # Apply computed gradients to model's weights and biases-
        optimizer.apply_gradients(zip(grad_mask_mul, model.trainable_variables))
    
        # Compute accuracy-
        train_loss(loss)
        train_accuracy(y, y_pred)
    
        return None
    
    
    @tf.function
    def test_step(model, optimizer, data, labels):
        """"""
        Function to test model performance
        on testing dataset
        """"""
        predictions = model(data)
        t_loss = loss_fn(labels, predictions)
        test_loss(t_loss)
        test_accuracy(labels, predictions)
        return None
    
    
    # Train model using 'GradientTape'-
    # Initialize parameters for Early Stopping manual implementation-
    # best_val_loss = 100
    # loc_patience = 0
    for epoch in range(num_epochs):
        if loc_patience &gt;= patience:
            print(""\n'EarlyStopping' called!\n"")
            break
    
        # Reset the metrics at the start of the next epoch
        train_loss.reset_states()
        train_accuracy.reset_states()
        test_loss.reset_states()
        test_accuracy.reset_states()
    
        for x, y in train_dataset:
            train_one_step(q_aware_model, mask_model, optimizer, x, y)
        
        for x_t, y_t in test_dataset:
            test_step(q_aware_model, optimizer, x_t, y_t)
    
        template = 'Epoch {0}, Loss: {1:.4f}, Accuracy: {2:.4f}, Test Loss: {3:.4f},         Test Accuracy: {4:4f}'
    
        print(template.format(epoch + 1, train_loss.result(),train_accuracy.result()*100, test_loss.result(),test_accuracy.result()*100))
    
        # Count number of non-zero parameters in each layer and in total-
        # print(""layer-wise manner model, number of nonzero parameters in each layer are: \n"")
        model_sum_params = 0
        for layer in winning_ticket_model.trainable_weights:
            # print(tf.math.count_nonzero(layer, axis = None).numpy())
            model_sum_params += tf.math.count_nonzero(layer, axis = None).numpy()
    
        print(""Total number of trainable parameters = {0}\n"".format(model_sum_params))
    
    
        # Code for manual Early Stopping:
        if np.abs(test_loss.result() &lt; best_val_loss) &gt;= minimum_delta:
            # update 'best_val_loss' variable to lowest loss encountered so far-
            best_val_loss = test_loss.result()
    
            # reset 'loc_patience' variable-
            loc_patience = 0
        else:  # there is no improvement in monitored metric 'val_loss'
            loc_patience += 1  # number of epochs without any improvement

Gives the following error:

&amp;#x200B;

&gt;\&gt; --------------------------------------------------------------------------- InvalidArgumentError                      Traceback (most recent call  
&gt;  
&gt;\&gt; last) &lt;ipython-input-47-bca851ce138d&gt; in &lt;module&gt;  
&gt;  
&gt;\&gt;      19  
&gt;  
&gt;\&gt;      20     for x, y in train\_dataset:  
&gt;  
&gt;\&gt; ---&gt; 21         train\_one\_step(q\_aware\_model, mask\_model, optimizer, x, y)  
&gt;  
&gt;\&gt;      22  
&gt;  
&gt;\&gt;      23  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_\_call\_\_(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     578         xla\_context.Exit()  
&gt;  
&gt;\&gt;     579     else:  
&gt;  
&gt;\&gt; --&gt; 580       result = self.\_call(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     581  
&gt;  
&gt;\&gt;     582     if tracing\_count == self.\_get\_tracing\_count():  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/def\_function.py  
&gt;  
&gt;\&gt; in \_call(self, \*args, \*\*kwds)  
&gt;  
&gt;\&gt;     642         # Lifting succeeded, so variables are initialized and we can run the  
&gt;  
&gt;\&gt;     643         # stateless function.  
&gt;  
&gt;\&gt; --&gt; 644         return self.\_stateless\_fn(\*args, \*\*kwds)  
&gt;  
&gt;\&gt;     645     else:  
&gt;  
&gt;\&gt;     646       canon\_args, canon\_kwds = \\  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_\_call\_\_(self, \*args, \*\*kwargs)    2418     with self.\_lock:  
&gt;  
&gt;\&gt; 2419       graph\_function, args, kwargs =  
&gt;  
&gt;\&gt; self.\_maybe\_define\_function(args, kwargs)  
&gt;  
&gt;\&gt; -&gt; 2420     return graph\_function.\_filtered\_call(args, kwargs)  # pylint: disable=protected-access    2421     2422   u/property  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_filtered\_call(self, args, kwargs)    1659       \`args\` and  
&gt;  
&gt;\&gt; \`kwargs\`.    1660     """"""  
&gt;  
&gt;\&gt; -&gt; 1661     return self.\_call\_flat(    1662         (t for t in nest.flatten((args, kwargs), expand\_composites=True)    1663  
&gt;  
&gt;\&gt; if isinstance(t, (ops.Tensor,  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in \_call\_flat(self, args, captured\_inputs, cancellation\_manager)  
&gt;  
&gt;\&gt; 1743         and executing\_eagerly):    1744       # No tape is  
&gt;  
&gt;\&gt; watching; skip to running the function.  
&gt;  
&gt;\&gt; -&gt; 1745       return self.\_build\_call\_outputs(self.\_inference\_function.call(    1746  
&gt;  
&gt;\&gt; ctx, args, cancellation\_manager=cancellation\_manager))    1747  
&gt;  
&gt;\&gt; forward\_backward = self.\_select\_forward\_and\_backward\_functions(  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/function.py  
&gt;  
&gt;\&gt; in call(self, ctx, args, cancellation\_manager)  
&gt;  
&gt;\&gt;     591       with \_InterpolateFunctionError(self):  
&gt;  
&gt;\&gt;     592         if cancellation\_manager is None:  
&gt;  
&gt;\&gt; --&gt; 593           outputs = execute.execute(  
&gt;  
&gt;\&gt;     594               str([self.signature.name](https://self.signature.name)),  
&gt;  
&gt;\&gt;     595               num\_outputs=self.\_num\_outputs,  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; \~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py  
&gt;  
&gt;\&gt; in quick\_execute(op\_name, num\_outputs, inputs, attrs, ctx, name)  
&gt;  
&gt;\&gt;      57   try:  
&gt;  
&gt;\&gt;      58     ctx.ensure\_initialized()  
&gt;  
&gt;\&gt; ---&gt; 59     tensors = pywrap\_tfe.TFE\_Py\_Execute(ctx.\_handle, device\_name, op\_name,  
&gt;  
&gt;\&gt;      60                                         inputs, attrs, num\_outputs)  
&gt;  
&gt;\&gt;      61   except core.\_NotOkStatusException as e:  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; InvalidArgumentError:  var and grad do not have the same shape\[10\]  
&gt;  
&gt;\&gt; \[100,10\] 	 \[\[node Adam/Adam/update\_4/ResourceApplyAdam (defined at  
&gt;  
&gt;\&gt; &lt;ipython-input-37-9c297d161e54&gt;:29) \]\]  
&gt;  
&gt;\&gt; \[Op:\_\_inference\_train\_one\_step\_20360\]  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; Errors may have originated from an input operation. Input Source  
&gt;  
&gt;\&gt; operations connected to node Adam/Adam/update\_4/ResourceApplyAdam:  
&gt;  
&gt;\&gt; Mul\_4 (defined at &lt;ipython-input-37-9c297d161e54&gt;:26)  
&gt;  
&gt;\&gt; sequential/quant\_dense\_2/BiasAdd/ReadVariableOp/resource (defined at  
&gt;  
&gt;\&gt; /home/arjun/.local/lib/python3.8/site-packages/tensorflow\_model\_optimization/python/core/quantization/keras/quantize\_wrapper.py:162)  
&gt;  
&gt;\&gt;  
&gt;  
&gt;\&gt; Function call stack: train\_one\_step

Is there a way to combine TF model Quantization along with tf.GradientTape?

&amp;#x200B;

Thanks!

&amp;#x200B;

&amp;#x200B;

\[1\]: [https://www.tensorflow.org/model\_optimization/guide/quantization/training\_example](https://www.tensorflow.org/model_optimization/guide/quantization/training_example)

\[2\]: [https://github.com/arjun-majumdar/Lottery\_Ticket\_Hypothesis-TensorFlow\_2/blob/master/Quantization\_LTH\_LeNet\_300\_100\_MNIST.ipynb](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Quantization_LTH_LeNet_300_100_MNIST.ipynb)",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hqbggn/quantization_aware_training_with_tfgradienttape/
What is Machine Learning Anyways?,1594607799,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/hq78dl/what_is_machine_learning_anyways/
Question about type of architecture to use for my project,1594594764,"Creating a mouse movement ai, and I'm currently collecting data for it, but what would be the best architecture? I'm training it on (startPos, stopPis, path, path delay) and ideally afterwards I'd just pass it (startPos, stopPos)",neuralnetworks,SlothyJoe,False,/r/neuralnetworks/comments/hq3ys6/question_about_type_of_architecture_to_use_for_my/
Disney's New High Resolution Face Swapping Algorithm. Mind-Blowing new 2020 Face Swap Technology Explained. Could this replace GANs?!,1594477209,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hpaqyo/disneys_new_high_resolution_face_swapping/
Neural Network to read a lidar map,1594447144,Is it possible to make a neural network read a lidar map using the losses point clouds and then use it to provide a robot with a reasonable path? If so what type of data would I need for training and how much data would I need for a baseline? I have minimal experience in that I’ve never made my own but I’ve used a tutorial using the MNIST dataset. Thanks.,neuralnetworks,icyrhythm,False,/r/neuralnetworks/comments/hp50lz/neural_network_to_read_a_lidar_map/
I have upscaled and improved Michael Jackson Pepsi New Generation commercial with AI,1594375419,,neuralnetworks,h1v3de,False,/r/neuralnetworks/comments/homaxl/i_have_upscaled_and_improved_michael_jackson/
YouTubeI have upscaled and improved Michael Jackson Pepsi New Generation commercial with AI,1594375380,[deleted],neuralnetworks,[deleted],False,/r/neuralnetworks/comments/homaov/youtubei_have_upscaled_and_improved_michael/
Another AI upscale result,1594373122,,neuralnetworks,h1v3de,False,/r/neuralnetworks/comments/holvy5/another_ai_upscale_result/
Leaky ReLU only gives negative values or values close to 0,1594358516,"I have normalized my data using zero mean and variance of one. I want to do a sanity check to see if I can overfit a small amount of data. I created one fully connected layer with an input size of 1024 and an output size of 9. I use torch.nn.init.kaiming\_uniform\_ to initialize the weights and I also use batch norm before calling the Leaky ReLU activation function for this layer.

I have been training for almost 50 epochs with a batch size of 32 and total batches are just 100. However, my network’s output is either negative or close to zero. I am trying to regress to values that lie between 0 and 1. Before I tried using ReLU (with same weight initialization as I mentioned now) but the output, in that case, was almost always entirely zero. What could I do here to steer my output towards more positive values? I even tried PReLU but I am encountering the same problem. Same with sigmoid and tanh. How can you recover from situations where your output gets stuck near 0?",neuralnetworks,zimmer550king,False,/r/neuralnetworks/comments/hoj5wq/leaky_relu_only_gives_negative_values_or_values/
Image network search,1594326278,Hi all! I am currently trying to find a neural network or website that combines new images into one but I have found nothing good so far. Any help is appreciated.,neuralnetworks,Hniena,False,/r/neuralnetworks/comments/hoal9e/image_network_search/
I am planing on making a neutral network to imput pokemon battel themes and output similar music,1594264335,"I've never done anything with neutral networks before, any advice apprisiated",neuralnetworks,thedidynotkong,False,/r/neuralnetworks/comments/hnvkxy/i_am_planing_on_making_a_neutral_network_to_imput/
Teach A Neural Network to Create Abstract Art | Only Using NumPy!!,1594264134,,neuralnetworks,Snoo28889,False,/r/neuralnetworks/comments/hnvj6e/teach_a_neural_network_to_create_abstract_art/
How to update the Generator in a GAN architecture,1594243175,"I made a working Neural network with backpropagation and tested it with the MNIST dataset. It works correctly and it can recognize numbers that I draw myself on a 28x28 canvas (So I can tell it's not overfitted).

Now I've given myself the challenge of making a gan network with the same NN I made. Everything looks good, except for the way I update the generator weights. Every info I found on the internet was either too Math heavy or too vague for me to understand. 

How am I supposed to find the generators gradient? or What is the generator's gradient? My closest guess is that you can find it within the discriminator's deltas (I could be totally wrong here too).

Thanks for the help!",neuralnetworks,vlevi,False,/r/neuralnetworks/comments/hnpttm/how_to_update_the_generator_in_a_gan_architecture/
Looks like I found that the best application of AI upscaling algorithms is remastering old meme videos lmao,1594237218,,neuralnetworks,h1v3de,False,/r/neuralnetworks/comments/hnnwsh/looks_like_i_found_that_the_best_application_of/
Question: how to 'seed' a network,1594233893,"Hey guys and gals, I'm starting a new project that I can run simulations over and over to try to get ideal behavior, but my starting data is very low (5-10 runs of the simulation). Is there a way I can 'seed' it with these 5-10 as ""Ideal"" and adjust from there?",neuralnetworks,SlothyJoe,False,/r/neuralnetworks/comments/hnmss0/question_how_to_seed_a_network/
Deploying a State-of-the-Art Question Answering System in 60 Lines of Python,1594226526,,neuralnetworks,MusingEtMachina,False,/r/neuralnetworks/comments/hnkg8r/deploying_a_stateoftheart_question_answering/
"Machine learning algorithms don't understand textual data, so how do they process textual data? Word embedding is used to represent these words.Click here to know more",1594220892,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/hnipqq/machine_learning_algorithms_dont_understand/
*REPOST* Neural nets architectures/applications without no loss function?,1594219793,[removed],neuralnetworks,clemencepill,False,/r/neuralnetworks/comments/hnidvu/repost_neural_nets_architecturesapplications/
Prevent Underfitting and Overfitting for your model | Machine learning basics explained,1594213713,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hngr28/prevent_underfitting_and_overfitting_for_your/
"I’ve been working on a neural network in Unity and was planning to create a genetic algorithm to allow my network to learn, but right after attaching the output to my player’s rotation, the weights and biases were somehow perfect values. My jaw literally dropped—sorry I’ll stop ranting now.",1594188679,,neuralnetworks,Tubbyball,False,/r/neuralnetworks/comments/hnbsap/ive_been_working_on_a_neural_network_in_unity_and/
AlexNet: The First CNN to win Image Net | What is AlexNet?,1594185507,,neuralnetworks,agarwalsimran,False,/r/neuralnetworks/comments/hnb57p/alexnet_the_first_cnn_to_win_image_net_what_is/
Question about Neural network dataset,1594119261,[removed],neuralnetworks,hasan1417,False,/r/neuralnetworks/comments/hmsd96/question_about_neural_network_dataset/
Seeding Open-AI’s GPT-2 Model With a Pandemic-Themed Prompt,1594073890,,neuralnetworks,RossSheingold,False,/r/neuralnetworks/comments/hmhvb9/seeding_openais_gpt2_model_with_a_pandemicthemed/
How does back-propagation for multichannel filters work?,1594071879,,neuralnetworks,stravimond,False,/r/neuralnetworks/comments/hmh8ht/how_does_backpropagation_for_multichannel_filters/
Can someone help explain N.E.A.T. and how to handle the Enable/Disable gene specifically?,1594064608,[deleted],neuralnetworks,[deleted],False,/r/neuralnetworks/comments/hmexsb/can_someone_help_explain_neat_and_how_to_handle/
[N] ICML 2020 Announces Test of Time Award,1594052366,"Organizers of the 37th International Conference on Machine Learning (ICML) have announced this year’s Test of Time award, which goes to a team from the California Institute of Technology, University of Pennsylvania, Saarland University. The ICML Test of Time award recognizes an ICML paper from ten years ago that has proven influential, with significant impacts in the field, “including both research and practice.”

Test of Time Award: Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design

Here is a quick read: [ICML 2020 Announces Test of Time Award](https://syncedreview.com/2020/07/03/icml-2020-announces-test-of-time-award/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/hmangw/n_icml_2020_announces_test_of_time_award/
I’m new any suggestions,1594005163,I’m trying to make an AI that can play connect 4 in java but I’m new to this kind of coding. Any suggestions for getting started?,neuralnetworks,Dustyd0gg,False,/r/neuralnetworks/comments/hlzxfi/im_new_any_suggestions/
How to start with nerual network and deep learning?,1593995242,"I am read a book about python (python the hard way)
I took an introduction course about computer science (cs50)

I don't know what is next? Do I use a book or a course for dl and nn?

I am 17 so I think my math is fine",neuralnetworks,Be1a1_A,False,/r/neuralnetworks/comments/hlxj9y/how_to_start_with_nerual_network_and_deep_learning/
How Do Autoencoders Work?,1593991904,,neuralnetworks,What_it_do23,False,/r/neuralnetworks/comments/hlwoi5/how_do_autoencoders_work/
Going to try to train my first neural net. Need help,1593986688,"Hello guys. I want to train a neural network for a task. I have a large amount of data but I am not sure whether my neural net architecture is appropriate or not.   

Let's say I have 7000 pictures. Can I slowly increase the amount of training data and check to see if my neural net's accuracy is consistently good or not? So, for example, I try my neural net on just 1000 of those images and check how the accuracy is. Then, I try it on 5000 images and check whether the accuracy is still good or not. Is this a good approach?  

Also, my training dataset consists of image crops that I took out of larger images. As a result, the size varies from really small to somewhat big (they are image crops of vehicles where the vehicle can be at a considerable distance or very close in the image). My plan is to use a pre-trained Resnet-18 to extract features and then pass the extracted features through a fully connected layer. Is this a good initial approach? Can I get advice on the number of neurons in the final fully-connected layer? My goal is to pass these image crops through my neural net to get the bounding box coordinates.

Any suggestions would be appreciated. Thank you!",neuralnetworks,zimmer550king,False,/r/neuralnetworks/comments/hlv7zl/going_to_try_to_train_my_first_neural_net_need/
Is Standardization killing my NN?!,1593985831,"Feature Scaling is important for a Neural Network. Below is what I believed to be best practise but fear that it may be working against me. Would love to hear your approach/view on these:

I believed that, when training a model, we standardise our training data such that it has a mean of 0 and st-dev of 1. To do this I would obtain the mean and st-dev of the training data and apply the z-score normalisation = (x-u)/s. Then when scoring any other data (eg test, validation, and data in the future that we don't even have yet) we must standardise but USE THE MEAN AND ST-DEV THAT WE SAW IN TRAINING DATA. 

But the problem I am facing is: some features are always drifting as they're always increasing (eg GDP or House Prices). I understand that you can always re-train when your data drifts but features such as House Prices always drift i.e. my models are already out of date before they are deployed. I don't have the option to refrain every few months. 

My questions:
MUST we standardise using the trainings original mean and st-dev? Applying this to new data that is ever-increasing/drifting data will not yield a mean of zero and st-dev of 1. 

Can we just standardise using the new data's mean &amp; st-dev? This will ensure it is in the same ""shape"" as the training data and will ensure it has a mean of 0 and Stdev of 1. But I've never seen this done before. In online examples they're always locking down and using the TRAINING data's mean &amp; Stdev.",neuralnetworks,DataLiam,False,/r/neuralnetworks/comments/hluz60/is_standardization_killing_my_nn/
"Why do people fall for AI? Part 2 - Quick tour to machine learning and data science(AI,ML,DS)",1593950958,,neuralnetworks,Megarajan,False,/r/neuralnetworks/comments/hlllmz/why_do_people_fall_for_ai_part_2_quick_tour_to/
how much of a language do you need to know to make a simple neural network?,1593932078,I'm learning python and have been learning for the past 2 months and im reading about shallow neural networks and how to train them and gradiant descent  and backpropagation and stuff and i was wondering how much of a language do you need to know to make a neural network? is OOP enough? or do i need to know even more to make a neural network? also upto how much multivariable calc is required?,neuralnetworks,mastershooter77,False,/r/neuralnetworks/comments/hliang/how_much_of_a_language_do_you_need_to_know_to/
Neurons firing in the brain of a mouse,1593931739,,neuralnetworks,hale_dm,False,/r/neuralnetworks/comments/hli8k0/neurons_firing_in_the_brain_of_a_mouse/
How to train your neural networks to understand logic gates from scratch,1593919261,"Hey Everybody,

In this video we train our  logic gate neural network. If you are new to the series you can start here:

 [https://www.youtube.com/watch?v=6NXmrmJyWUY&amp;list=PLB4RncStK2LU6gEzKvSKJcL2kLjHTMBTi](https://www.youtube.com/watch?v=6NXmrmJyWUY&amp;list=PLB4RncStK2LU6gEzKvSKJcL2kLjHTMBTi) 

Otherwise you can go straight to the video here:

 [https://www.youtube.com/watch?v=ytNY2EL3sgg&amp;list=PLB4RncStK2LU6gEzKvSKJcL2kLjHTMBTi&amp;index=3](https://www.youtube.com/watch?v=ytNY2EL3sgg&amp;list=PLB4RncStK2LU6gEzKvSKJcL2kLjHTMBTi&amp;index=3) 

&amp;#x200B;

Enjoy!",neuralnetworks,mokillem,False,/r/neuralnetworks/comments/hlfw2o/how_to_train_your_neural_networks_to_understand/
This AI Generates 3D high-resolution reconstructions of people from 2D images | Introduction to PIFuHD,1593867660,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hl2y98/this_ai_generates_3d_highresolution/
Interested in nonlinear time series prediction with NNs? Check out how LSTM nets can be used in the forecasting of chaotic dynamical systems.,1593851159,,neuralnetworks,_Mat_San_,False,/r/neuralnetworks/comments/hkzzhy/interested_in_nonlinear_time_series_prediction/
Is regression used in Deep neural networks for training different neurons on different layers ? Thanks,1593792619,,neuralnetworks,bazziapps,False,/r/neuralnetworks/comments/hklso5/is_regression_used_in_deep_neural_networks_for/
Starting in AI,1593758278,"Can somebody help me out, I'm interested in designing a neural network that receives pairs of photos (original and with effect) and is capable of learn the effect applied to the second photo and can apply this effect in any new photo.",neuralnetworks,MrazV,False,/r/neuralnetworks/comments/hke84s/starting_in_ai/
Know what you can do with NLTK.,1593690270,,neuralnetworks,MarinaChatterjee,False,/r/neuralnetworks/comments/hjvidk/know_what_you_can_do_with_nltk/
On Simulating Neural Damage in Connectionist Networks,1593629787,,neuralnetworks,o_guest,False,/r/neuralnetworks/comments/hjgjkb/on_simulating_neural_damage_in_connectionist/
I'm getting false postive rate as 1.0,1593613402,What's the problem with my CNN,neuralnetworks,kkrishnaa_kkumarr,False,/r/neuralnetworks/comments/hjb5d9/im_getting_false_postive_rate_as_10/
Problem with my convolutional neural network,1593601105,I'm using CNN for anomaly detection. I'm getting different values for accuracy and other metrics for every execution. Can someone help me fix this? Please,neuralnetworks,kkrishnaa_kkumarr,False,/r/neuralnetworks/comments/hj81vt/problem_with_my_convolutional_neural_network/
Predicting Stock Market Prices with a Neural Network (or attempt at least),1593575605,"With the rise of machine learning and neural networks, I decided to try my hand at writing one. Took me a while to decide what to make it do, and even longer to actual create it, but here we are. I decided to try and predict future stock market prices. (And before you bash me for that, you Can't predict something as volatile as the stock market. Not perfectly anyway) I figured that this would be a good learning experience, and maybe, just maybe, it would perform decently well.

With that being said, the project is also on GitHub [HERE](https://github.com/I506dk/Stock-Neural-Network). I used NVIDIA along with some other stocks during testing, but I'll only reference NVDA.

So, being that this is timeseries data, I decided to use the first 75% of the data to train the model on. The last 25% would be used for validation. Then I went through the usual process of importing the data from a text file, normalizing everything before giving it to the model, and then predicting future dates (although it is more of just getting them. You don't really have to predict tomorrow's date. Just look at a calendar).

The architecture of the model consists of a 64 node LSTM layer, a 0.5% Dropout layer, a 64 layer Dense layer, another 0.1% Dropout layer, then finally a 1 node Dense layer as output. Nothing super fancy or complicated.

    # Concluded that the first dropout layer(s) need to be more harsh than the latter ones
    Model_Input = Input(shape=(5, 1), name='Model_Input')
    x = LSTM(64, name='Model_0')(Model_Input)
    x = Dropout(0.5, name='Model_dropout_0')(x)
    x = Dense(64, name='Dense_0')(x)
    x = Dropout(0.1, name='Model_dropout_1')(x)
    x = Activation('sigmoid', name='Sigmoid_0')(x)
    x = Dense(1, name='Dense_1')(x)
    Model_Output = Activation('linear', name='Linear_output')(x)
    model = Model(inputs=Model_Input, outputs=Model_Output)
    adam = optimizers.Adam(lr=0.0005)
    model.compile(optimizer=adam, loss='mse')

One thing I did notice however, was over fitting (Much of it was in previous architectures). Being that this model was going to mostly like be wrong (or just not correct down to the exact penny), I wanted to be as ""accurate"" as I could make it. Hence the multiple dropout layers. But along with that I added early stopping of the epochs to try and stop it before it started to cheat or over fit.

    Early_Stopping = EarlyStopping(monitor='loss', mode='min', patience=130, verbose=1)
    
    # x is equal to the entire dataset after being normalized
    # Entire dataset has shape (number of days worth of data, 5 (columns), 1)
    # y is equal to the closing prices of the dataset
    history = model.fit(x=History_Input_Data, y=History_Closing_Price, batch_size=150, epochs=1150, shuffle=False, validation_data=(Validation_Input_Data, Validation_Closing_Price), verbose=0, callbacks=[Early_Stopping])

I played with different patience values and monitoring loss and val\_loss. The above is what I finally settled on. It performed the best when testing it on multiple stocks. I wanted the results for each stock to be relatively similar as far as accuracy.

Some other pictures of stock graphs are located [HERE](https://drive.google.com/drive/folders/17P6GcquzUNjoXB4fapcqXr7FT3E6ssBc?usp=sharing). These are just the ones I was using. There is also a picture of the loss (black line) and val\_loss (green line) for the NVDA stock symbol. I graphed it while testing just to see if I was getting over fitting.

Other than that there isn't much else to be said. Feel free to correct my mistakes or make suggestions. Everything is welcomed honestly.",neuralnetworks,I506dk,False,/r/neuralnetworks/comments/hj2v7w/predicting_stock_market_prices_with_a_neural/
Predicting Stock Market Prices with a Neural Network (or attempt at least),1593575445,,neuralnetworks,I506dk,False,/r/neuralnetworks/comments/hj2tuk/predicting_stock_market_prices_with_a_neural/
Know how gradient descent algorithm helps to improve performance in gradient boosting models,1593571834,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/hj1y7h/know_how_gradient_descent_algorithm_helps_to/
[N] Yann LeCun Quits Twitter Amid Acrimonious Exchanges on AI Bias,1593563968,"Turing Award Winner and Facebook Chief AI Scientist Yann LeCun has announced his exit from popular social networking platform Twitter after getting involved in a long and often acrimonious dispute regarding racial biases in AI.

Here is a quick read: [Yann LeCun Quits Twitter Amid Acrimonious Exchanges on AI Bias](https://syncedreview.com/2020/06/30/yann-lecun-quits-twitter-amid-acrimonious-exchanges-on-ai-bias/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/hizz74/n_yann_lecun_quits_twitter_amid_acrimonious/
Video combining/upscaling,1593524363,I have a collection of a TV show on DVD where the quality is SD but the video is the full 4:3 aspect ratio it was made as. I also have the Blu-ray version in full HD but it's been cropped down to 16:9. Is there a good program to use training data from the middle of the frame to upscale the outer bands so I can have (basically) full HD but the full 4:3 aspect ratio? Even if it's janky AF I at least want to try,neuralnetworks,nickdrones,False,/r/neuralnetworks/comments/hinj0u/video_combiningupscaling/
Neural Network Optimal Control in Astrodynamics: Application to the Missed Thrust Problem - Acta Astronautica,1593433668,,neuralnetworks,Gereshes,False,/r/neuralnetworks/comments/hhyy3c/neural_network_optimal_control_in_astrodynamics/
Can I get feedback on my NN c++,1593416568,[removed],neuralnetworks,Joey_Shmoey_,False,/r/neuralnetworks/comments/hhvmev/can_i_get_feedback_on_my_nn_c/
Hi guys! Can you help me out? I'm unable to answer this.,1593416359,,neuralnetworks,pseudonym24,False,/r/neuralnetworks/comments/hhvl2e/hi_guys_can_you_help_me_out_im_unable_to_answer/
Can I get some feedback on my Neural Network c++,1593415282,[removed],neuralnetworks,Joey_Shmoey_,False,/r/neuralnetworks/comments/hhvdqb/can_i_get_some_feedback_on_my_neural_network_c/
Trying to get feedback on my custom built Neural Network,1593405443,[removed],neuralnetworks,Joey_Shmoey_,False,/r/neuralnetworks/comments/hhtecu/trying_to_get_feedback_on_my_custom_built_neural/
Revealing Dark Secrets of BERT (Analysis of BERT's Attention Heads) - Paper Explained,1593390878,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/hhpwmh/revealing_dark_secrets_of_bert_analysis_of_berts/
This AI translates code from a programming language to another | Facebook TransCoder Explained,1593265733,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/hgtt8z/this_ai_translates_code_from_a_programming/
Confused by the last layer before a softmax layer.,1593224347,"I'm confused about the setup of the last few layers for classification neural networks.

I've been reading up about it, this is what I know:

The final step is the crossentropy cost function, which takes in 2 vectors, (the output/softmax layer prior and the target vector (which is one-hot encoded) and returns a scalar.

The preceding step, is the output layer, or the softmax layer. I'll just make this layer a vector called L^(N). So the sum(L^(N)) = 1.0. 

This is what I know so far, and will be appreciated if anything can be recorrected. ...My main confusion is the layer preceding the softmax/output layer L^(N),which I'll just call as a vector L^(m).  I would like to know how the input vector that we put into the softmax layer comes from in terms of L^(m). 

From some of the stuff I've seen online, I'm getting the impression that is we do this: L^(N) = T(W\*L^(m) \+ b) where W is a weight matrix, b is a bias vector, and T is the softmax mapping.

But other times, I see that there's a separate step between L^(m) and L^(n), which I will just call  by a vector L^(n-1). And it looks like for this, L^(N-1) = S(W\*L^(m) \+ b), where S is a sigmoid mapping or some other hidden activation function, and then directly from there, we make L^(n) = T(L^(N-1)), where T is the softmax mapping. 

I would like to see whether the second notion is correct or not. Thanks.",neuralnetworks,stravimond,False,/r/neuralnetworks/comments/hgl898/confused_by_the_last_layer_before_a_softmax_layer/
Basic Question: Help Appreciated!,1593204040," 

Hi I have a very basic question regarding Artificial Neural Networks. So, I was reading a book which said that

&gt;ANNs are capable of unsupervised learning from unstructured data, you no longer need to feed AI information one piece at a time. With neural nets, simply unleash them on the internet and the system will do the rest.

Now where I'm scratching my head is, that this statement IS TRUE but the neural net has to be trained from beforehand in order to be correct since it cannot learn on it's own without being provided training data + the correct output. Is my statement here correct?

If my statement here is correct, what is the main difference between ANNs and general Machine Learning Algorithms? Am I correct in saying that

1. ANNs learn on the go (if provided the correct answer in advance) and
2. General ML Algorithms are fed a large amount of data at the same time and patterns are automatically extracted from the data and used as a model to predict future inputs?
3. ANNs can ONLY be trained when given the correct answer in advance

Your help on my query would be greatly appreciated.

Thanks!",neuralnetworks,robgrima,False,/r/neuralnetworks/comments/hgfkto/basic_question_help_appreciated/
Thoughts on a few recent papers that could be useful in neural network tabular regression,1593186315,"I've been reading up on a some of the newer papers and have flagged a few that seem interesting to me. Specifically papers that pertain to general/tabular regression Neural Network performance and not just specifically related to image or text recognition. Was wondering if anyone has had any experience with these new tools yet? Is my understanding of them correct? Any other advancements along the same line that you think would be worth mentioning?

* Mish - [\[1908.08681\] Mish: A Self Regularized Non-Monotonic Neural Activation Function](https://arxiv.org/abs/1908.08681) \- Shows improvement over swish in some cases. 1st and 2nd order derivatives have slightly different properties. Computationally faster than GELU.
* Lisht - [\[1901.05894\] LiSHT: Non-Parametric Linearly Scaled Hyperbolic Tangent Activation Function for Neural Networks](https://arxiv.org/abs/1901.05894)
* Gradient centralization - [\[2004.01461\] Gradient Centralization: A New Optimization Technique for Deep Neural Networks](https://arxiv.org/abs/2004.01461) \- Basically just makes the mean of all weights excluding output layer to 0
* E-Swish - [\[1801.07145\] E-swish: Adjusting Activations to Different Network Depths](https://arxiv.org/abs/1801.07145) \- Adds a term to swish and shows that in some cases a parameterized swish provides better results.
* Radam - [\[1908.03265\] On the Variance of the Adaptive Learning Rate and Beyond](https://arxiv.org/abs/1908.03265) \- Uses smaller learning rates in first few epochs essentially just to get enough samples so that the variance of adaptive LR doesn’t explode and causes detrimental learn rates in the first few steps.
* Lookahead - [\[1907.08610\] Lookahead Optimizer: k steps forward, 1 step back](https://arxiv.org/abs/1907.08610) \- maintains two optimizers/weights, one with a larger and more reactive learn rate, one with a smaller more stable learn rate. Every X epochs, the “slow” weights are set to the “fast” weights.
* Mixup - [\[1710.09412\] mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412) \- “averages” samples as a form of data augmentation. Ex. create a linear interpolation between 2 samples so that if in production the model sees a sample “in the middle” of two samples it has trained on, it’s output will probably be somewhere in the middle of the corresponding outputs.
* [\[1711.00489\] Don't Decay the Learning Rate, Increase the Batch Size](https://arxiv.org/abs/1711.00489) \- Increase batch size instead of decreasing learn rate. Supposed to stabilize learning, smaller updates. How to scale down something like weight-decay? Is this just an alternative that is faster for training, or is it also supposed to provide better performance?",neuralnetworks,Yogi_DMT,False,/r/neuralnetworks/comments/hg9zpx/thoughts_on_a_few_recent_papers_that_could_be/
my chatbot QnA ... need help ! question in comments,1593177692,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/hg7gol/my_chatbot_qna_need_help_question_in_comments/
"A Russian design studio has developed an AI designer and, for over a year, have been passing it off as a human. To date, the network has been used successfully in over 20 of the studio’s commercial projects.",1593158398,,neuralnetworks,fly1nghigh,False,/r/neuralnetworks/comments/hg3hyh/a_russian_design_studio_has_developed_an_ai/
Recreate your images in the style of famous artists,1593122979,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/hfvb7u/recreate_your_images_in_the_style_of_famous/
Lectures on Neural Networks and Deep Learning,1593118929,[removed],neuralnetworks,mr_harshitgupta,False,/r/neuralnetworks/comments/hfu3m5/lectures_on_neural_networks_and_deep_learning/
Is transfer learning possible with scikit-learn's Multilayer Perceptron Regressor (Python)?,1593113915,"I have two data sets. One is huge and cheap, the other is small and very expensive. This seems like a perfect scenario for transfer learning (as I understand it) because I want to use the huge, cheap data to train a model which will then be used to predict the small, cheap data.

My question: **is it possible to perform transfer learning with scikit-learn's neural networks?**

From what I've seen online, scikit does not have a good reputation for neural nets. I understand that there are more robust options available, but I'm familiar with scikit's idioms so if possible I would like to stick with their MLP regressor module. 

The reason I wonder whether it is possible is that I understand transfer learning to involve the modification of at least the output layer of neurons, and I don't know if scikit's model setup is customization in a way that allows for these kinds of changes. Thanks!",neuralnetworks,t-rurl,False,/r/neuralnetworks/comments/hfsm3m/is_transfer_learning_possible_with_scikitlearns/
"[Article] Revisiting Classic Neural Network Architectures: DenseNet, ResNeXt, MnasNet, and ShuffleNet v2",1593095697,"Deep learning as we know it has largely been defined by advances in neural network architectures that took place in the last decade. Inspired by this work, this series takes a deeper look at field-defining deep learning architectures. In this third and final part we take a look at DenseNet, ResNeXt, MnasNet, and ShuffleNet v2v -- all models from the past couple of years. We’ll cover the original papers published about each, including a look at the architecture, training, and comparative results on various tests.

Article link:  [https://blog.paperspace.com/popular-deep-learning-architectures-densenet-mnasnet-shufflenet/](https://blog.paperspace.com/popular-deep-learning-architectures-densenet-mnasnet-shufflenet/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/hfn589/article_revisiting_classic_neural_network/
"Neural Network Using Keras Sequential API : Overview, Structure, and it's Applications",1593082295,,neuralnetworks,redd-griffin-it,False,/r/neuralnetworks/comments/hfk1bt/neural_network_using_keras_sequential_api/
Introduction to Autoencoders. Know how to use autoencoder for extracting decent images from noisy images,1593065645,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/hfgy7f/introduction_to_autoencoders_know_how_to_use/
Detailed explanation of building a basic Convolutional neural network which detects pneumonia in chest x-rays (dataset source included),1593059847,,neuralnetworks,nischalmadhav,False,/r/neuralnetworks/comments/hfg45a/detailed_explanation_of_building_a_basic/
[R] Fourier Feature Mapping Enables MLPs to Learn High-Frequency Functions in Low-Dimensional Domains,1593024577,"Google Research and University of California researchers however found that standard MLPs are poorly suited for low-dimensional coordinate-based vision and graphics tasks and have difficulty learning high-frequency functions. To overcome these issues, they have proposed the use of Fourier feature mapping with MLPs in order to learn high-frequency functions in low-dimensional problem domains.

Here is a quick read: [Fourier Feature Mapping Enables MLPs to Learn High-Frequency Functions in Low-Dimensional Domains](https://medium.com/syncedreview/fourier-feature-mapping-enables-mlps-to-learn-high-frequency-functions-in-low-dimensional-domains-3d7ea03fd1a0)

The paper *Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains* is on [arXiv](https://arxiv.org/pdf/2006.10739.pdf).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/hf6uoy/r_fourier_feature_mapping_enables_mlps_to_learn/
"Hey guys, just wanted to share a gradient descent lecture with some of my animations and some theoretical studies on how gradient descent works in 1D and 2D. Also, the video talks about batch gradient descent method and shows you how to get the job done on Python.",1593016620,,neuralnetworks,bazziapps,False,/r/neuralnetworks/comments/hf4aly/hey_guys_just_wanted_to_share_a_gradient_descent/
Can you code a GAN completely in python?,1593013210,"Hello all,

I have my final project approaching for my DS course and while bouncing ideas around came across generating art with a GAN.  I am extremely interested and was first wondering if this is possible with only Python. 

Also if anyone has any experience with GAN please share insight and or things to watch out for. Anything is great! 

Thank you",neuralnetworks,DrEl1344,False,/r/neuralnetworks/comments/hf37tv/can_you_code_a_gan_completely_in_python/
Need some help fixing this ANN,1593010768,"For a school mathematics project, I made an ANN by working on it from the mathematics aspect. However, when I do run it, I get really odd outcomes. The outputs are rarely correct and when I substitute sigmoid for tanh, the error just converges to the same value (something like 0.125). If there is someone who is willing to help out by looking at the code do pm me (I don't want to post it as the paper will go through turn it in  - a software for plagiarism checking - and I just want to be safe).

&amp;#x200B;

Thanks for your help and sorry about the inconvenience.",neuralnetworks,TNTPA,False,/r/neuralnetworks/comments/hf2hv6/need_some_help_fixing_this_ann/
Making Blurry Faces Photorealistic Goes Only So Far,1592932559,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/heio9y/making_blurry_faces_photorealistic_goes_only_so/
I'm writing a handwriting recognition CNN for a school project and am looking for some resources to help me out,1592916217,"I'm writing it in python. I have a good understanding of how NNs work, but most ""tutorials"" I can find online on CNNs are basically just copy paste lines of keras code, without any helpful explanations on the ins and outs of it. Any help would be appreciated!",neuralnetworks,bolrnly,False,/r/neuralnetworks/comments/hedxi4/im_writing_a_handwriting_recognition_cnn_for_a/
Relationship between anchor box and kmeans,1592883412,,neuralnetworks,promach,False,/r/neuralnetworks/comments/he77me/relationship_between_anchor_box_and_kmeans/
Does anyone know how to get the weights for a radial basis function?,1592862995,"I have been searching for days now and I know that there is a formula, however, everybody uses different notation and don't actually give a coherent formula. I know it has to do with the derivative of the error with respect to the weight but I don't know how to find this. Many people online also talk about linear regression but just say that and nothing else. They never say how to apply it to the network to find the weights. Any help would be appreciated!",neuralnetworks,Nicojwn,False,/r/neuralnetworks/comments/he1qq7/does_anyone_know_how_to_get_the_weights_for_a/
Convolutional Neural Networks in Under 10 Min,1592840368,,neuralnetworks,Physical-Plantain-23,False,/r/neuralnetworks/comments/hdum8d/convolutional_neural_networks_in_under_10_min/
Sentiment analysis web app using streamlit.The sentiment analysis is done with bi lstm,1592830458,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/hdrtrd/sentiment_analysis_web_app_using_streamlitthe/
"[R] Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network . Traditional artificial neurons are insufficient in extracting the interaction information of input data.",1592767952,[removed],neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hdcy8l/r_perfect_artificial_neuron_maybe_perfect_most/
"[R] Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network . Traditional artificial neurons are insufficient in extracting the interaction information of input data.",1592767021,[removed],neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hdcoem/r_perfect_artificial_neuron_maybe_perfect_most/
"Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network",1592744028,"  

[https://www.techrxiv.org/articles/It\_may\_be\_time\_to\_perfect\_the\_neuron\_of\_artificial\_neural\_network/12477266](https://www.techrxiv.org/articles/It_may_be_time_to_perfect_the_neuron_of_artificial_neural_network/12477266)

As we know, artificial neural networks are inspired by biological neural networks. Seventy years ago, people designed artificial neurons by imitating the knowledge about biological neurons at that time. Today, due to the development of biology, we have a relatively good understanding of how the work of neurons, especially dendrites.

I found, at the time of design, the traditional artificial neurons ignored a fact that dendrites participate in pre-calculation in a biological neuron or biological neural network.

More specifically, biological dendrites play a role in the brain pre-processing to the interaction information of input data. This can be illustrated briefly by two tasks in life. For understanding a picture task, biological dendrites play a role in extracting the relationship across parts of an input-picture. For understanding an article or a speech task, biological dendrites play a role in extracting the relationship across parts in an input-word.

Traditional artificial neurons are insufficient in extracting the interaction information of input data. Thus we have designed a lot of convolutional layers. Gang neurons maybe reduce the number of layers in an existing network for the same task.",neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hd6gub/perfect_artificial_neuron_maybe_perfect_most_anns/
"Perfect artificial neuron, maybe perfect most ANNs. It may be time to perfect the neuron of artificial neural network",1592742851,"[https://www.techrxiv.org/articles/It\_may\_be\_time\_to\_perfect\_the\_neuron\_of\_artificial\_neural\_network/12477266](https://www.techrxiv.org/articles/It_may_be_time_to_perfect_the_neuron_of_artificial_neural_network/12477266)

&amp;#x200B;

As we know, artificial neural networks are inspired by biological neural networks. Seventy years ago, people designed artificial neurons by imitating the knowledge about biological neurons at that time. Today, due to the development of biology, we have a relatively good understanding of how the work of neurons, especially dendrites.

I found, at the time of design, the traditional artificial neurons ignored a fact that dendrites participate in pre-calculation in a biological neuron or biological neural network.

More specifically, biological dendrites play a role in the brain pre-processing to the interaction information of input data. This can be illustrated briefly by two tasks in life. For understanding a picture task, biological dendrites play a role in extracting the relationship across parts of an input-picture. For understanding an article or a speech task, biological dendrites play a role in extracting the relationship across parts in an input-word.

&amp;#x200B;

Traditional artificial neurons are insufficient in extracting the interaction information of input data. Thus we have designed a lot of convolutional layers. Gang neurons maybe reduce the number of layers in an existing network for the same task.",neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hd67x1/perfect_artificial_neuron_maybe_perfect_most_anns/
"Perfect artificial neuron, maybe perfect most ANNs",1592738444,"  

It may be time to perfect the neuron of artificial neural network

[https://www.techrxiv.org/articles/It\_may\_be\_time\_to\_perfect\_the\_neuron\_of\_artificial\_neural\_network/12477266](https://www.techrxiv.org/articles/It_may_be_time_to_perfect_the_neuron_of_artificial_neural_network/12477266)

As we know, artificial neural networks are inspired by biological neural networks. Seventy years ago, people designed artificial neurons by imitating the knowledge about biological neurons at that time. Today, due to the development of biology, we have a relatively good understanding of how the work of neurons, especially dendrites.

I found, at the time of design, the traditional artificial neurons ignored a fact that dendrites participate in pre-calculation in a biological neuron or biological neural network.

More specifically, biological dendrites play a role in the brain pre-processing to the interaction information of input data. This can be illustrated briefly by two tasks in life. For understanding a picture task, biological dendrites play a role in extracting the relationship across parts of an input-picture. For understanding an article or a speech task, biological dendrites play a role in extracting the relationship across parts in an input-word.

Traditional artificial neurons are insufficient in extracting the interaction information of input data. Thus we have designed a lot of convolutional layers. Gang neurons maybe reduce the number of layers in an existing network for the same task.",neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hd5cvg/perfect_artificial_neuron_maybe_perfect_most_anns/
"Perfect artificial neuron, maybe perfect most ANNs",1592738413,,neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hd5cnk/perfect_artificial_neuron_maybe_perfect_most_anns/
"Perfect artificial neuron, maybe perfect most ANNs",1592738369,,neuralnetworks,GangLiu,False,/r/neuralnetworks/comments/hd5cec/perfect_artificial_neuron_maybe_perfect_most_anns/
NeuralNetwork Algorithm | Used Artificial Neural Network. See Best Method,1592737032,,neuralnetworks,jainish1367,False,/r/neuralnetworks/comments/hd53b8/neuralnetwork_algorithm_used_artificial_neural/
Where to start training an NN for a card game?,1592689447,"I'm writing my first NN to play a simple card game, I'm familiar with Python and have implemented a working version of the game, but I'm really struggling to find understandable explanations of how to train a NeuralNet to play. I'm trying to set things up in pytorch and I think I want an adversarial neural net (?) but the example given here (https://pytorch.org/tutorials/beginner/fgsm_tutorial.html) seems a million miles away from the problem I'm trying to solve. In particular I have a discrete output (points) rather than continuous, and want to learn by competing against other 'players'

I've tried to read up around the subject, but my main issue is many articles, books, papers go into depth on the mathematics and optimisation, but give few examples; while ones with examples seem to tackle very different tasks like image recognition. I'm really struggling with the translation from mathematical models to code particularly as I'm not very familiar with a number of the concepts and language used. Further setting initial training parameters/architectures seems to be a huge topic with no clear answers and I'm not even sure where to start with that!

I've set up the code for my game, tested it, and created some deterministic players with basic strategies (play high, random-choice, user input etc.) but have no idea where to go next. The game itself is a simplified version of [6nimmit](https://boardgamegeek.com/boardgame/432/6-nimmt) played over a series of rounds

* Each player chooses 1 card from their hand (This is the bit I'm struggling with - I want an NN to make the card choice)
* All cards are played 
* The game state updates
* Players score points depending on changes to the game state  

Points 2,3,4 are already implemented.

The psuedo-code for my game looks something like:

    class Game:
       players = [p1, p2, p3, p4]
       state = [[], [], [], []] # List of lists containing cards played, max 4x5 - could be trivially turned into numpy array if required
     
      def update_state(cards):
            self.state = get_new_state(self.state, cards)
            for player in self.players: 
                 player.score += get_score(self.state, player)
            
    class Player
        hand = [card1, card2, card3. ...]
        score = 0
        def choose_card(state):
             card = ??? # NN decision goes here, yields an integer between (0, len(hand)-1)
             #e.g.  card = random.randint(0, len(hand)-1)  
             return hand.pop(card)

    def play_game():
        game = Game()    
        for turn in range(10): # Game takes place over 10 turns
             cards_chosen = []
             for player in game.players:
                  card= player.choose_card(state)
                  cards_chosen.append(card) 
             game.update_state(cards_chosen)
         return game.player_scores

    scores = play_game()
    print(""Winner is :"", max(player_scores))


I've been reading this article which seems very close to what I want to do: https://arxiv.org/pdf/1808.10442.pdf and has an implementation here: https://github.com/henrycharlesworth/big2_PPOalgorithm but it relies on a particular PPONetwork implementation, which I'm struggling to wrap my head around and seems like it might be unnecessarily complicated for my use case!",neuralnetworks,jollyspiffing,False,/r/neuralnetworks/comments/hcudwc/where_to_start_training_an_nn_for_a_card_game/
Making AI Score 100 Perfect Dunks In Basketball! (Neural Networks + Genetic Algorithm),1592661800,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/hcms5u/making_ai_score_100_perfect_dunks_in_basketball/
"What is Image Recognition? Functions, algorithm and its uses",1592569931,,neuralnetworks,agarwalsimran,False,/r/neuralnetworks/comments/hbzv4i/what_is_image_recognition_functions_algorithm_and/
The relation between more neurons and test error is probably not what you would expect!,1592556820,,neuralnetworks,Giacobako,False,/r/neuralnetworks/comments/hbx9ed/the_relation_between_more_neurons_and_test_error/
Finding Neuron Activations in TensorFlow2.0,1592547941,"0

I was reading [Training and Evaluation](https://www.tensorflow.org/guide/keras/train_and_evaluate) (TensorFlow 2.0 and Python3) for MNIST dataset with code as follows:

    inputs = keras.Input(shape=(784,), name=""digits"")
    x = layers.Dense(64, activation=""relu"", name=""dense_1"")(inputs)
    x = layers.Dense(64, activation=""relu"", name=""dense_2"")(x)
    outputs = layers.Dense(10, activation=""softmax"", name=""predictions"")(x)
    
    model = keras.Model(inputs=inputs, outputs=outputs)
    
    model.compile(
        optimizer=keras.optimizers.RMSprop(),  # Optimizer
        # Loss function to minimize
        loss=keras.losses.SparseCategoricalCrossentropy(),
        # List of metrics to monitor
        metrics=[keras.metrics.SparseCategoricalAccuracy()],
    )
    
    print(""Fit model on training data"")
    history = model.fit(
        x_train,
        y_train,
        batch_size=64,
        epochs=2,
        # We pass some validation for
        # monitoring validation loss and metrics
        # at the end of each epoch
        validation_data=(x_val, y_val),
    )

In this code, if I want to see the activations for the neurons, say  either during each epoch and at the end of training, how can I get each  neuron's (in each layer) activations?

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/hbvkrq/finding_neuron_activations_in_tensorflow20/
How to decide the size of the NN needed for the job?,1592534027,"How would you decide how many layers and how big each one should be for your NN.

For my example, the NN has 21 inputs and 3 outputs, how should I design the hidden layers?

Thanks in advance!",neuralnetworks,Josip_K,False,/r/neuralnetworks/comments/hbsf1k/how_to_decide_the_size_of_the_nn_needed_for_the/
Transfer learning has made it possible to use pre-trained models to achieve better performance with less data and in less time Here is an article if you want to know about transfer learning from scratch.Also it describes how COVID19 can be diagnosed with Transfer learning,1592506317,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/hbkhg9/transfer_learning_has_made_it_possible_to_use/
This is an animated lecture on Gradient Descent. Let me know what you guys think.,1592501894,,neuralnetworks,bazziapps,False,/r/neuralnetworks/comments/hbj9tn/this_is_an_animated_lecture_on_gradient_descent/
How to make DFF more efficient?,1592470259,I am currently writing a school project and I noticed how DFF's are inefficient because of exponentially increasing connections. I read an article that says that there are methods to combat this issue but I couldn't find any online and the article didn't mention any. Does anyone have a tip?,neuralnetworks,Nicojwn,False,/r/neuralnetworks/comments/hbayom/how_to_make_dff_more_efficient/
I know what an epoch is but could someone tell me what conf.debug_steps is in the configuration file? Screenshots attached.,1592455046,"I am running a training stack for training a full float model. I have e PC with no GPU so I am not actually intending to complete the training but rather make sure that the training stack runs. When the conf.debug\_steps is set to 100, the training never starts for the 15 minutes that I have waited for it. I can't wait for it any longer to see if it would take a lot of time and then start. But if I change the conf.debug\_steps to 2, the training starts. I know what the conf.epoch=100 is but my question is: what does the conf.debug\_steps do and what is it? I have searched google but couldnt find anything about it. 

Here it is in the configuration file: [https://imgur.com/dgOzBTA](https://imgur.com/dgOzBTA)

Here it is used in the [train.py](https://train.py) file: [https://imgur.com/LozTrEv](https://imgur.com/LozTrEv)",neuralnetworks,huzaifakhan771,False,/r/neuralnetworks/comments/hb7pnv/i_know_what_an_epoch_is_but_could_someone_tell_me/
Deep Compression: Fine-Tuning,1592375231,"Hey guys, I was reading the paper [Deep Compression](https://arxiv.org/abs/1510.00149) and in ""Trained Quantisation and Weight Sharing"" it was mentioned that:

1. Weights are clustered using K-Means algorithm for each layer   

2. Generate code book (clustered centroid/effective weights)

3. Quantize the weights with code book

4. Retrain code book


My questions are:

1.) what is meant by ""retraining"" in step 4? Does it mean that the clustered resulting network is trained until convergence or only fine-tuned, say for 2-3 epochs?

2 ) what if you skip Quantisation to reduce number of bits per floating point number and only focus on retraining code book (effective weights)?


My understanding so far is that in ""Pruning"" step:

You first train the network, prune lowest p% of lowest magnitude weights in each layer and retrain the resulting network.

Here, ""retraining"" means that you retrain the resulting network until convergence (say using early stopping). I am assuming this since it's not mentioned in the paper.

Correct me if I am wrong.

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/halr7l/deep_compression_finetuning/
A basic clarification on simple feed-forward neural network,1592353203,"Hey Guys, I'm a beginner in neural networks and I am currently working on a problem wherein I have data for some patients with a disease. I have 2 types of data (time-invariant features and some time-series). The goal here is to predict if the patient will recover or die in some time. I do realize that I should be using a sequence model, but to begin with, I tried out a simple feed-forward network. To my dismay, I am unable to achieve even a 100% train accuracy. I have 1000 patients and around 240 variables. I have tried various combinations of hidden units(up to 10000(seems unreasonable)) and layers (5-10) still no hope. My question to the group: My understanding was that I'll be at least able to achieve 100% train accuracy given i have so much flexibility in hidden units and layers am I missing something. 

&amp;#x200B;

Please excuse my typo",neuralnetworks,johnyj01,False,/r/neuralnetworks/comments/hag8to/a_basic_clarification_on_simple_feedforward/
How to Write Your First Neural Network from Scratch,1592340728,,neuralnetworks,Snoo-15519,False,/r/neuralnetworks/comments/hach30/how_to_write_your_first_neural_network_from/
machine learning webapp 🐈 🐕 prediction using flask,1592300406,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/ha0s89/machine_learning_webapp_prediction_using_flask/
Discord Server for Deep Learning Book for beginners,1592294947,"https://discord.gg/qs2KPzy

Here's the link for reading group of Deep Learning book. First we have to make study plan and follow that accordingly. It can be very helpful to be in study group as you can ask questions and solve them like a team. So if anyone is interested please join. it would be really helpful for you and me as well. I'd like to request some senior deep learners to join as well.",neuralnetworks,JastorJ,False,/r/neuralnetworks/comments/h9zlxv/discord_server_for_deep_learning_book_for/
Gated Linear Networks,1592257151,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/h9pi06/gated_linear_networks/
Check out my small cross platform (.net core) GPU accelerated NN training library (open source),1592252951,,neuralnetworks,trenmost,False,/r/neuralnetworks/comments/h9o7hi/check_out_my_small_cross_platform_net_core_gpu/
Cars from Radiator Springs generated by StyleGAN2 deep learning neural network,1592245958,,neuralnetworks,Snoo_72253,False,/r/neuralnetworks/comments/h9lyp3/cars_from_radiator_springs_generated_by_stylegan2/
Neural Nets with no loss functions?,1592241722,[removed],neuralnetworks,marcgem,False,/r/neuralnetworks/comments/h9km32/neural_nets_with_no_loss_functions/
Watch This Before Writing Neural Networks,1592241211,,neuralnetworks,Snoo-15519,False,/r/neuralnetworks/comments/h9kgdr/watch_this_before_writing_neural_networks/
Understanding and Using GANs to denoise Images,1592223726,,neuralnetworks,Hussain_Mujtaba,False,/r/neuralnetworks/comments/h9f9n1/understanding_and_using_gans_to_denoise_images/
[Beginner] Question about best neural network structure and hyperparameters to use for predicting planetary locations,1592156360,"I am trying to create a neural network to predict where planetary bodies will be from one time point to another. To do this I thought an lstm network would be sufficient but it's predicted values are a few hundred billion units off. I've found that activating the layers with SELU, optimizing the model with ADAM, and using MAE give the best results so far but my output values are still incredibly off. To move forward I think the best approach is to normalize the data, but when I've done that in the past my values were off even further. I've discovered that you can't use batch normalization with lstm networks, but I'm not sure if group normalization, instance normalization, or layer normalization is the way to go. Any advice would be appreciated.",neuralnetworks,DenseLink,False,/r/neuralnetworks/comments/h8y2r0/beginner_question_about_best_neural_network/
Is there an easy-to-use neural network program where you just give it raw training data and it gives you an output,1592154050,"Hi! 

I'm looking for a program where you enter a number for how many inputs the neural network has, how many hidden layers there should be, how many neurons in the hidden layers, and how many outputs it should have. You just give it raw training data and how many epochs it should run, batch size, and learning rate. You press a button and it trains and you can later the just give it a raw input data and it gives you the outputs.

If you know any, please let me know. It would be greatly appreciated, Thnx in advance!",neuralnetworks,Esgow,False,/r/neuralnetworks/comments/h8xetk/is_there_an_easytouse_neural_network_program/
Predicting Next Digit given a sequence of digits ranging from 0 to 9,1592151655,"Given a sequence of digits (0-9), predict what the next digit is going to be? Or predict if it’s going to be even or odd?

Ex: 0, 5, 3, ........, 6, 9 (total of 6000 or something)

Ex: 0, 0, 1, ........., 1, 0 (series of 0s and 1s representing odds and evens)

While predicting, it doesn't need to look back at the whole data, instead it should just look back a fixed length of digits (like 10, 15).

What is the best way to formulate this problem? Is it regression or classification?

And what algorithm should I use? (Please also include the activation function, optimizer and loss function to be used)

If possible, share some code in tensorflow or keras.",neuralnetworks,Nikhilbadveli,False,/r/neuralnetworks/comments/h8wqi9/predicting_next_digit_given_a_sequence_of_digits/
"Beginner here , looking for an online tool to create a dataset of image, label ?",1592145731," Hi ! I hope someone can help me out because at this point I dont even know what Im doing .

I am working on a small Edge detection project ( I have a background in GIS ) and this is my first time working with deep learning . I am not looking to get better at it or anything I just need to get this done for school . My goal is to compare results ( canny+segmentation VS semantic segmentation ).I will be using Unet, and my understanding is that I need to train this network using an already segmented dataset , containing 512x512 tiles of (image, label) .

So far, I have a georeferenced satellite image (3bands) with its matching vector file (where I drew the edges ) , I did rasterize the vector file to get a mask.

Now Im completely lost at to how to cut my image and mask to tiles and how to label them. I found some solutions online like geo-label maker that seems appropriate for me but I would like to avoid using code (because I have to learn programming from the beginning and I dont have a lot of time to invest in programming) .

So I guess my question is , is there any online tool that will help me achieve this ? or maybe a software I can download ?

Please feel free to correct anything I just said and to give any information you judge useful . Thank you for reading .",neuralnetworks,toothfairy222,False,/r/neuralnetworks/comments/h8v0gx/beginner_here_looking_for_an_online_tool_to/
neural networks capability,1592074651,"Hello, I was thinking about interesting idea - is it possible to generate a thoughtful text on a certain topic using a neural network. What are your thoughts?   
Thanks in advance for your attention",neuralnetworks,Troy-Harder,False,/r/neuralnetworks/comments/h8dcz0/neural_networks_capability/
Hackathon for COVID-19,1592060208,[removed],neuralnetworks,TheOutcast014,False,/r/neuralnetworks/comments/h891b9/hackathon_for_covid19/
GameGAN: Whole PAC-MAN Game Recreated Using Only AI by NVIDIA. NO GAME ENGINES NEEDED!,1592058848,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/h88n5g/gamegan_whole_pacman_game_recreated_using_only_ai/
"I need help with something, DM me.",1592041868,[removed],neuralnetworks,Vindex0110,False,/r/neuralnetworks/comments/h84llo/i_need_help_with_something_dm_me/
A.I. Teaches MACHINE LEARNING | PART 2,1591967176,,neuralnetworks,thataiguru1,False,/r/neuralnetworks/comments/h7kciq/ai_teaches_machine_learning_part_2/
What happens when you train only the batch norm layers in a network?,1591927671,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/h7bx99/what_happens_when_you_train_only_the_batch_norm/
Neural models of animal learning,1591889006,"I’m looking for collaborators (the good kind).  Others who, like me, are interested in developing biologically plausible models of neuronal functioning, synaptic modification, and nervous system architecture that replicates the decision making and learning capabilities of animals as a pathway toward developing more capable general AI.  Here's the back story.

In 1980 I developed a theory of synaptic modification unlike others around at the time.  It enabled me to write a program on an Apple \]\[ that simulated an animal with a simple nervous system but which had considerable capabilities.  It had reflexes and drives and the ability to learn in accordance with both classical and operant conditioning procedures. All this via simulating a few neurons and their synaptic connections.  You can see a video of a modern version of that [program here](http://www.matheta.com/demonstration-videos/) with an Arduino as a transplanted brain controlling a Roomba.  [This page](http://www.matheta.com/how-it-works/) contains sufficient information to enable anyone interested to understand exactly how the program works.

Not long ago, I tried to extend that program to cover additional forms of learning.  The Apple and Arduino programs only replicated two forms of operant conditioning: positive reinforcement and positive punishment. However, there are five forms as [described here](http://www.matheta.com/wp-content/uploads/2014/04/Operant_conditioning_diagram.png).  The problem is that while the original program emulates the effects of food as a positive reward (which increases the probability of a behavior), it became clear to me that I had to figure out a way that the *cessation of punishment* somehow **also** produces an increased probability of a behavior.  (In the original model, punishment inhibits behavior.)

The [diagram here](http://www.matheta.com/wp-content/uploads/2017/12/NewBrain_edited-1.png) shows a circuit that will do just that: a cessation of a negative stimulus produces an increased probability of any behavior that preceded the cessation.  That was great, but something even more interesting happened.  The neuron “F/T” in the diagram, I discovered, replicated the functioning of the Fear/Terror affect of Sylvan Tomkins’ Affect Theory.  [This is a diagram](http://www.matheta.com/wp-content/uploads/2018/05/Affect-vs-Time_edited-1.png) I made a couple of years ago for presentations explaining Affect Theory.  Nobody unfamiliar with Affect Theory could possibly see the relevance of that diagram, but I soon realized that the circuit above could be directly extended to produce this, a brain that models most of Tomkins’ affects which, he maintains, directs roughly 90% of the behavior of all animals, up to and including humans (maybe only 70% for us.)  This is what I need help to implement.  Even though the model I’ve developed is, in theory, straightforwardly extendable to any number of neurons, the process of representing increasing numbers of independently operating neurons with a linear/sequential program is becoming daunting.

So that’s the bare bones description of what I’m trying to do.  I’d be glad to answer any questions and/or provide more detail.",neuralnetworks,SumOfRoy,False,/r/neuralnetworks/comments/h11d8k/neural_models_of_animal_learning/
Explaining How a Deep Neural Network Trained with End-to-End Learning Steers a Car,1591807043,,neuralnetworks,TheInsaneApp,False,/r/neuralnetworks/comments/h0ehe6/explaining_how_a_deep_neural_network_trained_with/
Understanding the perceptron,1591801418,[removed],neuralnetworks,perceptron_noob,False,/r/neuralnetworks/comments/h0crsj/understanding_the_perceptron/
Deepfake audio of your own in just minutes,1591774073,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/h05zps/deepfake_audio_of_your_own_in_just_minutes/
CNN Filters,1591767067,"I've understood how CNN works, and the functioning of the filters, does a CNN have only one filter per layer? for me, this makes just logical sense, but every time that I see a picture that tries to explain it, it looks like it has multiple filters per layer",neuralnetworks,Federile,False,/r/neuralnetworks/comments/h04k5i/cnn_filters/
A.I. Teaches MACHINE LEARNING,1591717763,,neuralnetworks,thataiguru1,False,/r/neuralnetworks/comments/gzppry/ai_teaches_machine_learning/
Question about CNN,1591706815,"hi guys,

I have this question about how CNN works in TensorFlow. For example, I have these 2 convolutional layers using Keras.

&amp;#x200B;

model.add(Conv2D(filters=32, (3, 3), activation='relu', input\_shape=(28, 28, 1)))  model.add(Conv2D(filters=32, (3, 3), activation='relu'))

&amp;#x200B;

In the first layer, 32 feature maps will be created.

In the second layer, I assume, 32 other feature maps will be created from each feature map result of the first layer, so the total would be 32\*32 feature maps?",neuralnetworks,NaohFoo,False,/r/neuralnetworks/comments/gzmgk2/question_about_cnn/
Zero-shot Text Classification With GPT-2,1591685996,,neuralnetworks,amitness,False,/r/neuralnetworks/comments/gzhxlh/zeroshot_text_classification_with_gpt2/
Separating nNet examples from practical use,1591678791,"I'll admit. I love watching neural networks learn simple tasks like figure out how to play snake or chess or see an entire ecosystem evolve and spring to life..

But I also see a much different conversation being had amongst professionals. It seems that the hobbyist uses them for their own means unrelated to how they might be used professionally.

So, as a student working towards their CS degree and as someone who is fascinated with AI, are nNets worth bothering with if I'm not wanting to become a data scientist?",neuralnetworks,starved_student,False,/r/neuralnetworks/comments/gzgce1/separating_nnet_examples_from_practical_use/
"[Article] Revisiting Classic Neural Network Architectures: ResNet, InceptionV3, and SqueezeNet",1591629185,"Deep learning as we know it has largely been defined by advances in neural network architectures that took place in the last decade. Inspired by this work, this series takes a deeper look at field-defining deep learning architectures. In this second part we’ll look at ResNet, InceptionV3, and SqueezeNet. We’ll cover the original papers published about each, including a look at the architecture, training, and comparative results on various tests.

Article link:  [https://blog.paperspace.com/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/](https://blog.paperspace.com/popular-deep-learning-architectures-resnet-inceptionv3-squeezenet/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/gz15dh/article_revisiting_classic_neural_network/
Autoencoders for image enhancement,1591620334,"Can autoencoders be used to enhance and upscale images instead of just compressing them (as usual)? That is, doing the opposite.

Thanks,",neuralnetworks,UnaiCorzo,False,/r/neuralnetworks/comments/gyynm6/autoencoders_for_image_enhancement/
What is the best neural network to upscale Super Mario 64 textures?,1591596231,,neuralnetworks,mariois64,False,/r/neuralnetworks/comments/gytcen/what_is_the_best_neural_network_to_upscale_super/
What why and how of activation functions!,1591555320,,neuralnetworks,ayush_parhi,False,/r/neuralnetworks/comments/gyhsq0/what_why_and_how_of_activation_functions/
[Article] 8 Simple Techniques to Prevent Overfitting in Machine Learning,1591554942,[removed],neuralnetworks,data_science_share,False,/r/neuralnetworks/comments/gyhomm/article_8_simple_techniques_to_prevent/
Help developing a neural network from scratch using c++,1591535505,[removed],neuralnetworks,joemoore94,False,/r/neuralnetworks/comments/gyc45e/help_developing_a_neural_network_from_scratch/
"Input from field of view, how?",1591532942,"As a hobby project I'm writing an evolution simulator and I've decided on using neural networks to control the creatures' behavior. I understand the basics on how NNs work, but I'm new to actually implementing them, and would like some advice. The creatures would need to react to their surrondings, by gathering input using their senses (sight, hearing, maybe smelling?) and feed that into the NN. 

For example, let's say the creature has evolved eyes. Okay, now it'll be able to see everything in a certain field of view. It could then gather useful information about the creature it's looking at, such as: Does it look dangerous? Does it look tasty? What direction is it moving in? And so on. This information could then be passed on to the NN to make creature react accordingly. Now, the issue I'm facing in the implementation is that I don't know how many creature's it'll see at a specific time. Sometimes there might be no creatures in the field of view, and thus the input vector would be empty. Other times there might be a ton of creatures in the field of view, and the input vector would be very long. For this reason, I can't just have one input neuron per attribute per seen creature, since that would mean the amount of input neurons had to change between each time step. 

Any advice on what to do is appreciated!",neuralnetworks,Episkbo,False,/r/neuralnetworks/comments/gybh70/input_from_field_of_view_how/
Neural network to understand drunk speech.,1591490434,?,neuralnetworks,Tristan401,False,/r/neuralnetworks/comments/gy2kga/neural_network_to_understand_drunk_speech/
Is that a good place to ask for help?,1591372419,"Hello guys! 
Before asking the question i apologize for my bad english.
I'm an informatic engineering student, this semester on of my courses of study was about ML and neural networks, but because of tue Covid19 the lessons were bad and not really helpful. 
In order to pass the exam, i've to project a Neural Network, but I've some problems:

1) I can't find a good source of Datasets in the internet, can some one help me? 

2) What ""type"" of NN is better for me? I mean I can't do a MNIST NN, but I don't want to start something bigger than me, I don't have time :( 

Thanks.",neuralnetworks,Bifrost003,False,/r/neuralnetworks/comments/gx75j6/is_that_a_good_place_to_ask_for_help/
I created Lady Liberty using a neural network. She doesn't look too happy...,1591352550,,neuralnetworks,basu68,False,/r/neuralnetworks/comments/gx1yxm/i_created_lady_liberty_using_a_neural_network_she/
What's usually the most expensive part of training a neural network?,1591299352,"So I was just playing around with training an MNIST network with different batch sizes, and I noticed that if I use a batch size of 10,000 (so only 6 steps in the whole epoch), then an epoch takes about 6 seconds. On the other hand, if I use a batch size of 1 (so 60,000 steps in the whole epoch), then an epoch takes 180 seconds.

I wanted to understand this a bit better. So what exactly is rate-limiting here? Is it that the backward() method is slow, or just that setting up each batch has overhead...? I was curious what you guys thought.

Thanks.",neuralnetworks,TissueReligion,False,/r/neuralnetworks/comments/gwog99/whats_usually_the_most_expensive_part_of_training/
"Best image ""gap-filler""?",1591292372,"Hi, as the title suggests I'm in need of a good image ""gap-filler"". right now I use a gimp plugin that takes pixels from around the selected area and randomly stuffs them into the selected area, so basically when I try to fill in a gap in a brick wall I get a brick-colored mess. I heard that neural networks excel at this ""gap-filling"" in images because they can detect patterns such as stacked bricks, and in short, I'm looking for recommendations for the best program for such use.  


TL:DR - looking for a neural network based program/plugin which I can use for filling gaps in images convincingly.",neuralnetworks,SeicentoDriverMichai,False,/r/neuralnetworks/comments/gwm56y/best_image_gapfiller/
Approximating a function,1591290220,"Hey guys, say I have a function: f(x, y) = x \^ 2 + y \^ 2 and I want to approximate for a given domain of x and y using gradient descent. How do I go about it?

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/gwlg2s/approximating_a_function/
AI learns to control a tank,1591205707,,neuralnetworks,automaton57,False,/r/neuralnetworks/comments/gvyei0/ai_learns_to_control_a_tank/
OpenAI – Learning Dexterity End-to-End - Experiment Report,1591133450,"Today OpenAI published a Weights &amp; Biases Report ([here](https://app.wandb.ai/openai/published-work/Learning-Dexterity-End-to-End--VmlldzoxMTUyMDQ)) on some recent work done by the Robotics team at OpenAI where they trained a policy to manipulate objects with a robotic hand in an end-to-end manner. Specifically, they solved the block reorientation task from our 2018 release ""[Learning Dexterity](https://openai.com/blog/learning-dexterity/)"" using a policy with image inputs rather than training separate vision and policy models (as in the original release).

In the report they describe their experimental process in general and then detail the findings of this specific work. In particular, they contrast the use of Behavioral Cloning and Reinforcement Learning for this task, and ablate several aspects of our setup including model architecture, batch size, etc.

Alex and I happy to discuss this and answer any questions about it.",neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/gvgbms/openai_learning_dexterity_endtoend_experiment/
Are there learning resources on how to effectively train neural networks using case study approaches?,1591117371,[removed],neuralnetworks,cyyeh,False,/r/neuralnetworks/comments/gvb53u/are_there_learning_resources_on_how_to/
What neural network to use for internal combustion engine process calculations?,1591115623,"Hello,

i am doing some research for neural networks to use for engine process calculations. These consist of some Navier-Stockes equations, geometric equations for the flame shape and  phenomenological equations. The inputs are for example the temperature, pressure, ...  when the valves close and the output is the anchor angle, maximum cylinder pressure, ... (just numbers, NO plot for the pressre,... ) .There was already some work done before, where 3 LSTM (100-300 neurons) and 3 (50-300 neurons) Dense layers are used in Keras .

Do You think this setup is suited for such problems?  The prediction was generally pretty good. However extreme conditions, where the flame does not burn that good anymore have fairly high errors.

I already read some literature, thats fairly theoretical. What literature would you recommend? Especially for more information on what model to use and how to predict over-/underfitting in such case.

Is it maybe possible to import 3D-Cad Data for prediction?

Thanks in advance! ",neuralnetworks,eggandbacon_0056,False,/r/neuralnetworks/comments/gval6l/what_neural_network_to_use_for_internal/
"[Article] Revisiting Classic Neural Network Architectures: AlexNet, VGG16, and GoogleNet",1591109846,"Deep learning as we know it has largely been defined by advances in neural network architectures that took place in the last decade. Inspired by this work, this series takes a deeper look at field-defining deep learning architectures. In this first part we’ll look at AlexNet, VGG16, GoogleNet. We’ll cover the original papers published about each, including a look at the architecture, training, and comparative results on various tests. 

Article link:  [https://blog.paperspace.com/popular-deep-learning-architectures-alexnet-vgg-googlenet/](https://blog.paperspace.com/popular-deep-learning-architectures-alexnet-vgg-googlenet/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/gv8sx5/article_revisiting_classic_neural_network/
"The YOLOv4 algorithm. Introduction to You Only Look Once, Version 4. Real Time Object Detection in 2020",1591109068,,neuralnetworks,OnlyProggingForFun,False,/r/neuralnetworks/comments/gv8kxg/the_yolov4_algorithm_introduction_to_you_only/
Top 15 AI Articles You Should Read This Month - May 2020,1591081179,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gv2f7n/top_15_ai_articles_you_should_read_this_month_may/
Gradient based pruning,1591079405,"Guys can you point me towards research papers involving gradient based pruning. The only paper I read in this direction was ""Optimal brain damage"" (LeCun et al.)

Thanks",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/gv223j/gradient_based_pruning/
[N] ICML 2020 Announces Accepted Papers,1591050956,"Organizers of the 37th International Conference on Machine Learning (ICML) sent out their eagerly awaited paper acceptance results today. A total of 1,088 papers out of 4,990 submissions made it to the prestigious machine learning conference. The acceptance rate of 21.8 percent is a slightly lower than 2019’s 22.6 percent (774 accepted papers from 3,424 submissions), and it seems likely the drastic increase in submissions contributed to this.

Here is a quick read: [ICML 2020 Announces Accepted Papers](https://medium.com/syncedreview/icml-2020-announces-accepted-papers-47340c917621)

The complete list is [here](https://icml.cc/Conferences/2020/AcceptedPapersInitial).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/guuqjg/n_icml_2020_announces_accepted_papers/
Help with a problem,1591031214,"Hello everyone.

There is an online challenge where you need a great knowledge of programming (and at the moment, of neural networks).

Me and other people (a team) are trying to ""unravel"" the mystery behind it all. However, we lack the technical knowledge of programming.

It's a very interesting project and I think everyone here would like to participate.

This is just a hobby (it's not paid at all!).

Please, whoever is interested send me a message, so I can send the discord link so we can work together!

Goodbye.",neuralnetworks,IamNotaMirror,False,/r/neuralnetworks/comments/guof0c/help_with_a_problem/
Top 3 Artificial Intelligence Research Papers – May 2020,1590996801,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gug4yv/top_3_artificial_intelligence_research_papers_may/
Is there a website/platform where you can sell/buy datasets for ML?,1590968245,"I was thinking if there is a supply/demand enough for such a platform (where you can buy/sell) to exist. There is a ton of demand, but are there enough people willing to supply?  


Here the data providers can be organizations, or the people generating data themselves.",neuralnetworks,vasa_develop,False,/r/neuralnetworks/comments/gu9kpy/is_there_a_websiteplatform_where_you_can_sellbuy/
Neural nets visualization,1590952736,"Hi!

I often struggle to find a good tool for visualization of my networks. I would like to have a program which can on one hand allow me to build neatly connected blocks, add formulas and draw connections but on the other hand modify elements not use predefined building blocks. Some 3d perspective support would be a nice bonus.

Does anybody know a tool like this? :)",neuralnetworks,derrkater,False,/r/neuralnetworks/comments/gu527h/neural_nets_visualization/
Please help in creating large dataset for training neural network.,1590905676,"I’m a university student currently working on a school project focusing on training an Artificial Intelligence to distinguish between raw shrimp, cooked shrimp, raw chicken chunks and cooked chicken chunks. I am forced to switch into this project due to the current Covid-19 situation which prevents me from accessing the university laboratory, leaving me with quarter the time other students would have for their projects. I am required to provide the AI with at least 1000 images for each of the items mentioned above in order for it to learn about their distinct features. I tried sourcing different images from google and other website such as instagram with the help of tools such as image scrapper but repetition and irrelevant images starts to pop up around the 100th image. To make things worse, each group of images have a specific criteria that needs to be followed. Hence I would like to seek your kind assistance in collecting enough number of images to train the artificial intelligence using Google form. It would be nicer if someone is willing to spread this post. It only takes 4000 people to complete this mission if everyone uploaded 1 image.  
 Please snap a photo of the items listed above if the criteria listed below is satisfied. Link for submission for each item is listed below, sample images is also shown in the link provided.

Cooked chicken

Criteria: The image of cooked chicken chunks should not include cooked shrimp, raw shrimp and raw chicken chunks as it might cause confusion. It is also important that the natural color of the cooked chicken chunks is not covered by condiments.

[https://docs.google.com/forms/d/e/1FAIpQLSetz655iyw5H10nnHGnYK-sDFIm-12ZeneX3fWrVM1UTNGmfA/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSetz655iyw5H10nnHGnYK-sDFIm-12ZeneX3fWrVM1UTNGmfA/viewform?usp=sf_link)

Cooked shrimp

Criteria: The image of cooked shrimp should not include raw shrimp, raw chicken chunks and cooked chicken chunks as it might cause confusion. It is also important that the natural color of the cooked shrimp is not covered by condiments. The shrimp should also be headless. 

[https://docs.google.com/forms/d/e/1FAIpQLSee4tZRV3fxCdJevs16bfYcwUlr07XYfCvquoR-qAshbZUWSA/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSee4tZRV3fxCdJevs16bfYcwUlr07XYfCvquoR-qAshbZUWSA/viewform?usp=sf_link)

Raw chicken chunks

Criteria: The image of raw chicken chunks should not include cooked shrimp, raw shrimp and cooked chicken chunks as it might cause confusion.

[https://docs.google.com/forms/d/e/1FAIpQLSecSzQk8ho3y0ZD6TPrPfrUa3KbZgTEXzjIObMPiW9pkyqwUw/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLSecSzQk8ho3y0ZD6TPrPfrUa3KbZgTEXzjIObMPiW9pkyqwUw/viewform?usp=sf_link)

Raw shrimp

Criteria: The image of raw shrimp should not include cooked shrimp, raw chicken chunks and cooked chicken chunks as it might cause confusion. The shrimp should also be headless. 

[https://docs.google.com/forms/d/e/1FAIpQLScuiQSGMM9U3cbgHdpY0ye-EzOKtUojQh3RiVeQYl-BJrKtdg/viewform?usp=sf\_link](https://docs.google.com/forms/d/e/1FAIpQLScuiQSGMM9U3cbgHdpY0ye-EzOKtUojQh3RiVeQYl-BJrKtdg/viewform?usp=sf_link)",neuralnetworks,ztay0001,False,/r/neuralnetworks/comments/gttum9/please_help_in_creating_large_dataset_for/
Deep Learning for Videos,1590869538,,neuralnetworks,ANil1729,False,/r/neuralnetworks/comments/gtkwfw/deep_learning_for_videos/
New AI technique speeds up language models on edge devices,1590864431,,neuralnetworks,jsamwrites,False,/r/neuralnetworks/comments/gtjfix/new_ai_technique_speeds_up_language_models_on/
How to determine the distance between modes of 1-D signals using ANN,1590848008,"The simplest version of my problem is that I have a series of one-dimensional data sets and I want to develop an ANN to identify two of the modes of the (irregular) distributions and return the distance between them.  I have had moderate luck in using a simple multi-layer perceptron that outputs a single value per n-dimensional input.  I am having difficulty improving beyond a certain point and I was hoping I could find some direction here.

If any one is particularly interested in helping, the more complex version of what I want to do is identify the mode of three-dimensional input when, and only when, those two modes are topologically connected by non-zero values; better still, I want to identify the three-dimensional location of those modes.  I am hopeful that projecting the 3-dimensions onto each axis and starting with the original problem statement above might take me in the right direction.  Any pointers to relevant papers would be very welcome.",neuralnetworks,adamnredwine,False,/r/neuralnetworks/comments/gtexlq/how_to_determine_the_distance_between_modes_of_1d/
Can a neural network count?,1590847307,"I was really disheartened by GPT 3 and that seemingly after all its size and training, it's basically a 175 billion parameter lookup table (explained well here  [https://www.youtube.com/watch?v=SY5PvZrJhLE&amp;t=6s](https://www.youtube.com/watch?v=SY5PvZrJhLE&amp;t=6s) ) and it really hasn't learnt much underlying reasoning (e.g. it can add 2 digit numbers because those math problems are common to find from a web scrape and it would have seen every possible combination a lot of times, but its miserable at 4 digit addition possibly because it's just unlikely to have seen the exact same numbers you are asking it to add before often in the dataset). 

I personally think this is explainable as an inability to generalise. And it made me wonder if a network could even, say, learn how to generalise on predicting a simple function, like, say, carrying on a sine graph. Then I realised that that is a bit complex for a NN as it would have to take in inputs of any size and know what they mean. So now I'm wondering if there even exists a neural network that can count? And by that I mean, count to infinity like a human can - I may need a sheet of paper and it may take a while for longer numbers but I'm pretty confident that  if you gave me a number I could count at least one number higher and I think theoretically given the time I could do that for any number to infinity.

 But I can't think of a NN architecture that could do this. I would be really interested to find out about research in generalisation problems like these. Does anyone know of work on more extreme generalisation in general or if there is a NN that can count?",neuralnetworks,_Idmi_,False,/r/neuralnetworks/comments/gterau/can_a_neural_network_count/
ML Discord starting projects on May 31st,1590788983,"Hello everyone,
The Neural Network Study Group Discord, if you're unaware has recently emerged from Reddit as a Discord server for ML researchers, beginners, and experts to learn and build projects together as a community.

All projects start on May 31st which is in 2 days!

These projects include:

-Discord AI bot
-Heath conditions AI
-Agriculture management AI
-Text to speech w/ Youtube data
-A research project on reinforcement learning 
-And a Unity evolution simulation AI

If you would like to contribute to a project or just the community as a whole, please come on board: https://discord.gg/pP8KTBR",neuralnetworks,John_Lins,False,/r/neuralnetworks/comments/gt22lg/ml_discord_starting_projects_on_may_31st/
Neural modeling of the first kind,1590756467,"Given the state of the world, I’m writing out of a great deal of optimism that someone among you might find my question of interest.

Essentially, I’m looking for an informed judgement, a valid disconfirmation if possible, of a theory of synaptic modification I developed when in graduate school (described here: [Matheta.com](http://matheta.com/).)  Of course, to provide such a judgment would necessarily require putting in the time and effort to sufficiently understand what is a fairly simple theory that has many implications and ramifications.   It soon became apparent, however, that the “neura(l)” in the name of this reddit refers mainly to neurons of the Deep Learning kind whereas I’m interested in modeling the functioning and capabilities of the real, “wet” neurons of animal nervous systems.  So, if anyone knows a more suitable subreddit (or something similar) where I might ask, that would be great.  I'm glad to provide more information if that would be helpful.

If, on the other hand, there is interest here, I’m more than willing to engage to the extent anyone’s interested.

Thanks in advance.",neuralnetworks,SumOfRoy,False,/r/neuralnetworks/comments/gssdog/neural_modeling_of_the_first_kind/
This Week in AI - Issue #19 | Rubik's Code,1590736798,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gso5c5/this_week_in_ai_issue_19_rubiks_code/
Upscaling The Matrix with AI! (Technical Showcase),1590711462,,neuralnetworks,380W,False,/r/neuralnetworks/comments/gsi7px/upscaling_the_matrix_with_ai_technical_showcase/
Reverse engineering youtube algorithm?,1590698870,"I was having a discussion today with my friends about how the YouTube algorithm keeps recommending stuff from ages ago that some how blows up all of a sudden with views. Seeing as youtube use a neural net to work out what videos to recommend (going off what the Internet says anyways), I was wondering if it were possible to create a neural net that could cheat the youtube algorithm by learning what it likes. The network would take in all the recommended videos information for a new user of YouTube, such as length of video, description, title, tags, age of video etc; Then output the optimum details for a video to be recommended (title, description, tags, duration) etc. I'm fairly sure if this were possible someone would of made it by now but I can't find one. New to neural nets so I might be totally wrong. If so I apologise. Thanks",neuralnetworks,Tcmaxwell2,False,/r/neuralnetworks/comments/gseh4l/reverse_engineering_youtube_algorithm/
Poly-YOLO that increase accuracy of YOLOv3 and has a masking functionality,1590689793,[removed],neuralnetworks,ChaosesX,False,/r/neuralnetworks/comments/gsbi6a/polyyolo_that_increase_accuracy_of_yolov3_and_has/
How would the NEAT algorithm actually be implemented in a physical system?,1590687895,"I'm having trouble understanding the applicability of the NEAT genetic algorithm.

Say we have a physical system, like a two wheeled inverted pendulum robot that we want to train to stay balanced. How would the NEAT algorithm actually be trained and applied to this physical system?

Would the physical system be modelled in a computer simulation on which the GA would train? I don't see how running multiple physical experiments by uploading code to the physical system for numerous generations would be practical.

Just wondering if anyone has any insight/experience with NEAT.",neuralnetworks,Cogitarius,False,/r/neuralnetworks/comments/gsaw1q/how_would_the_neat_algorithm_actually_be/
Need help creating my first neural network,1590614850,"Hi guys 
Am kinda new to all the machine learning stuff.
I just got into andrew ng's course on coursera and along with it going for a udemy course on machine learning and it's going fine, am learning alot and having fun along the way.
I always wanted to create a neural network for a flappy bird clone am going to build.
Can someone tell me what material/subjects should I be comfortable with to create such neural network?
Thanks guys

(Sorry for any grammar mistake, and sorry if I seem newbie (I am, but am learning along the way ;) ))",neuralnetworks,hhhhhxDD,False,/r/neuralnetworks/comments/grsr8h/need_help_creating_my_first_neural_network/
Open Source Code for neural network for photo editing,1590589739,"Hey

Anybody working on anything to do with photo editing?

More specifically I'd be interested to hear from people who can or have worked on/have code for neural network that can turn selfies into manga/anime/cartoon characters.

Anything open source out there already for this kind of thing?

Something decent though as I have seen a few things on the market already but don't look great.",neuralnetworks,2020Corp,False,/r/neuralnetworks/comments/grkqjw/open_source_code_for_neural_network_for_photo/
Siemens Sustainability AI Hackathon 2020,1590565574,Thinking about participating in the Siemens Sustainability AI Hackathon 2020. What are your thoughts about this?,neuralnetworks,bitteryberry,False,/r/neuralnetworks/comments/grfam4/siemens_sustainability_ai_hackathon_2020/
Python play coc I am going to do something different on youtube like sentdex i just need your help to make me completed with this project. This will be from beginner level and i will explain every line of code .,1590560080,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gre5l3/python_play_coc_i_am_going_to_do_something/
Keras beginner with RNN problems,1590553909,"I'm pretty new to this community and any help is appreciated.

I've been trying to make a PAC MAN AI and after little success with convolutional networks I've decided to try using a recurrent neural network since it makes sense to have it try and predict the next controller input based upon the previous inputs and screen data. 

The problem is I'm having a lot of trouble figuring out how to give an lstm layer image data specifically a shape of (212,168,1). All of the tutorials I find are just sequences of individual numbers. 

Is there any good resources for this? Am I just approaching this problem incorrectly? 

So far I've really been enjoying learning about keras and tensor flow, and I'm really comfortable with convolutional and dense neural networks, but I've just been stuck on recurrent neural networks all day.",neuralnetworks,Automite,False,/r/neuralnetworks/comments/grctt3/keras_beginner_with_rnn_problems/
[Question] Starting my first project with Neural Network,1590529135,"Hello,   
as the title says i am making my first neural network, that is I have a class on my college on the subject. We were given the freedom to choose what out NN would do/learn and i thought that teaching a computer to recognise if a person is wearing a mask (medical) or not in a picture. I did a bit of studying and I understand the general concept behind NN but i have trouble starting to write the code behind it. So i would appreciate any tips on how to maybe start with some really simple codes to start with and build from there to the point that i need for the project...  
Thanks &lt;3",neuralnetworks,Alonenever01,False,/r/neuralnetworks/comments/gr69kx/question_starting_my_first_project_with_neural/
What next to study after image classification ?,1590506835,,neuralnetworks,ANil1729,False,/r/neuralnetworks/comments/gqz4cf/what_next_to_study_after_image_classification/
Is there an artificial neural network example without using toolbox?,1590499617,"Hi,

I want to learn how artificial neural networks work. I want to solve a simple problem without using any toolbox. Is there any example you can advice which can guide me to do this?",neuralnetworks,pear4,False,/r/neuralnetworks/comments/gqx15b/is_there_an_artificial_neural_network_example/
Complete tutorial to learn about NN,1590481566,"Sometimes people ask me about neural networks, activation functions, layers, etc

I think this tutorial is a great way to understand Neural Networks and their parts.

[https://www.neuraldesigner.com/learning/neural-networks-tutorial](https://www.neuraldesigner.com/learning/neural-networks-tutorial)   


Hope you like it",neuralnetworks,jsanrom,False,/r/neuralnetworks/comments/gqt41u/complete_tutorial_to_learn_about_nn/
10 Practical project with MACHINE LEARNING may for your next project or COLLEGE STUDENTS,1590479157,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gqsnof/10_practical_project_with_machine_learning_may/
How to teach computers to describe events?,1590469866,Is there any online tutorial,neuralnetworks,rousse101,False,/r/neuralnetworks/comments/gqqt5g/how_to_teach_computers_to_describe_events/
"Nvidia GameGAN artificial intelligence recreated Pac-Man from scratch after ingesting 50,000 gameplay",1590435955,,neuralnetworks,jsamwrites,False,/r/neuralnetworks/comments/gqhuh0/nvidia_gamegan_artificial_intelligence_recreated/
I'm too confused .I don't have anyone to guide me in this field. If someone could guide me here. I would be very grateful. Thanks.,1590405888,[removed],neuralnetworks,jango2in1,False,/r/neuralnetworks/comments/gq9cal/im_too_confused_i_dont_have_anyone_to_guide_me_in/
Neural Network based BEng Project,1590403692,"Hello r/neuralnetworks, I’m looking into the possibility of doing my final year project applying neural networks to aerodynamic design. I’ve not done much coding apart from somewhat basic matlab, and I’m completely new to neural networks.

Can anyone recommend any resources to begin some research over summer? Purely so I can gauge whether or not a project like this is within my capabilities. 

Thanks",neuralnetworks,_teeps,False,/r/neuralnetworks/comments/gq8wxo/neural_network_based_beng_project/
What is Dropout regularization in Deep Learning,1590395108,,neuralnetworks,ANil1729,False,/r/neuralnetworks/comments/gq791k/what_is_dropout_regularization_in_deep_learning/
Create Deepfakes in 5 Minutes with First Order Model Method,1590392803,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gq6sof/create_deepfakes_in_5_minutes_with_first_order/
Humble Book Bundle: Definitive Guides to All Things Programming by O'Reilly (pay what you want and help charity),1590348990,,neuralnetworks,big_clips,False,/r/neuralnetworks/comments/gpvuqv/humble_book_bundle_definitive_guides_to_all/
Introduction to Deep Learning,1590335671,"Headstart your Deep learning journey with this simple explanation:👇🏻

https://highontechs.com/deep-learning/introduction-to-deep-learning/",neuralnetworks,kuldeeprp,False,/r/neuralnetworks/comments/gps0qk/introduction_to_deep_learning/
Make a network to play games?,1590310336,"Whenever I see tutorials on how to make an AI that plays games they only seem to play games they make with python instead of already made games like GTA or CIV, are there any good tutorials about this?",neuralnetworks,Redstoner7,False,/r/neuralnetworks/comments/gpmepx/make_a_network_to_play_games/
Neural Nets for MULTIPLICATION | fully explained,1590289932,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gpid1b/neural_nets_for_multiplication_fully_explained/
Pseudo Labelling,1590265051,,neuralnetworks,userinterfaceme,False,/r/neuralnetworks/comments/gpbtj3/pseudo_labelling/
Neural network minimizes error but outputs the same values for any input,1590249556,"I've been trying to develop a neural network that can recognize handwritten english uppercase letters. As an input it takes 28x28 images that are transformed into vectors of 28\*28 values between 0.0 and 1.0 (each value represents a pixel's intensity after converting image to grayscale). These are 28\*28 input neurons, there are one or more hidden layers, and finally 26 output neurons where each neuron represents one english character. So far so good.

I've implemented my own neural network in C# which handles forward- and backpropagation using weighted sum as an input function and sigmoid as an activation function. I've tested this implementation for several problems of different scale: simple XOR function, iris flower data set and wheat seeds data set. After training for several hundred epochs, my neural network scored over 90% using a separate test set.

However, I can't get my neural network to work with letter recognition. My training set consists of 200 images of each letter (provided as normalized vectors). The problem is that after several epochs my neural network calculates the same output for any input. However, the network still minimizes the error every iteration. It seems to me that it just found its own weird way of finding the minimum of the cost function.

I've tried several solutions:

* changing the number of hidden layers (1-3)
* changing the number of neurons in hidden layers (5-1000)
* changing the learning rate (0.005-0.9)
* trying different activation functions (tanh, ReLU)
* scaling images (7x7, 14x14)
* shuffling training set
* changing values of input vectors (0.0-1.0, -1.0-1.0)

but the problem persists.

At this point I'm starting to question that a ""classic"" implementation of a neural network like mine is capable of solving this problem reliably. I'm at a dead end and looking for any tips!

I've been thinking of implementing separate neural networks for each letter so that each neural network outputs just one value - how probable it is that the input represents a given letter. However, I'm not sure if this will make any difference.",neuralnetworks,justcosmic,False,/r/neuralnetworks/comments/gp7btx/neural_network_minimizes_error_but_outputs_the/
An API like keras for C++ ?,1590206293,"Does anyone know of an API similar to keras for C++? 

I have been working on convolution neural nets in python for over a year now and I want to code my work in c++ (although, it's apparently quite tedious).  

Keras is brilliant but I can't seem to find anything like it for c. Does anyone know if there is an API out there? I am looking for something with good community support, good docs etc. This is literally just for a bit of fun for me. Thanks on advance :) 

P.S I have nothing against python. It's lovely.",neuralnetworks,SpaceGirl92,False,/r/neuralnetworks/comments/goybr1/an_api_like_keras_for_c/
Real Time Object Detection with yolov3 in tensorflow 2.2,1590184798,[removed],neuralnetworks,emadboctor,False,/r/neuralnetworks/comments/goss8d/real_time_object_detection_with_yolov3_in/
multi-classification architecture and general questions,1590184042,[removed],neuralnetworks,roboto_delgato,False,/r/neuralnetworks/comments/goskc7/multiclassification_architecture_and_general/
[PROJECT] for starters who are willing to put their feet in COMPUTER VISION and AI stuff. CHECK IT OUT,1590138487,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gofx5j/project_for_starters_who_are_willing_to_put_their/
Classify web pages with neural net?,1590107933,"So I am working on a java project and I'm have a webcrawler that retrieves and downloads the html from websites it finds. You input a word, such as the word Java, and my program then searches google for websites and downloads the html data.

So now I am trying to use a Neural Net to predict if the website is what I am looking for, yes I am aware that I am going to need a lot of data and have to manually feed the network. Where I am stuck is with converting the html into numbers for the neural net.

At first I though about just converting the html into plain text and then assign a number to each letter, and just have a document that is made up of a bunch of numbers that make up words. I am unsure though how a neural net would view this, would all these numbers be just one input or would each word be an input to the neural net, which then would cause the problem of having an unspecified number of inputs, and if the net allows this as one input would this be a viable way of classifying if a website is what I want or not?",neuralnetworks,Cal24D,False,/r/neuralnetworks/comments/go92xc/classify_web_pages_with_neural_net/
I made a library to download images from a Google Image search to train neural networks,1590100745,"Well, I called it Process Google Dataset. This idea already exists, but the unique thing about my solution is that it works on any OS (that can run Chrome and Python), and it is easy to install and use. There are 2 parts to my solution. The first involves downloading a very lightweight Chrome extension, searching a term on Google Image Search, and activating the tool to parse through a search result, and downloads a list of urls of all the images it can see.

Then the Python script simply loops through that list and downloads them.

Downsides:

1. If Google changes the layout of how they display their search results, I would need to update my tool.

Future Improvements:

1. Telling the tool how many pictures to get (currently, it won't stop until you tell it to).
2. Sometimes images are in base64 format (from source), so either skip those or encode them in the Python script.

Any feedback is appreciate :)

Repo URL: [https://github.com/masesk/process-google-dataset](https://github.com/masesk/process-google-dataset)",neuralnetworks,masesk,False,/r/neuralnetworks/comments/go6zuk/i_made_a_library_to_download_images_from_a_google/
Common sub-networks,1590068112,"Guys, I am pruning a neural network (CNN and Dense) and for different sparsity levels, I have different sub-networks. Say for sparsity levels of 20%, 40%, 60% and 80%, I have 4 different sub-networks.

Now, I want to evaluate the similar connections between them. Any idea how to visualize this or compute this?

I am using Python 3.7 and TensorFlow 2.0.

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/gnwyds/common_subnetworks/
[PROJECT] FOR beginners to dirty your hands with !! [Github],1590062086,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gnvhj7/project_for_beginners_to_dirty_your_hands_with/
Train accurate models on GPU without caring about missing values &amp; categorical encoding using CatBoost!,1590058293,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/gnunvy/train_accurate_models_on_gpu_without_caring_about/
Granny Neural Network Advice,1590050168,"First time on reddit, sorry if I'm breaking any rules. My granny is blind and doesn't know English, so during the lock down I would like to code out a simple speech recognition app for her.

Idea is to have her record the names of people she wants to call (\~10), and some commands (like call, open Radio, etc), and then (after a trigger) for the system to recognize the word she is speaking now. 

So the question to you guys is: Would using a neural network to recognize a spoken phrase among pre-recorded ones be a good idea? I don't have much experience with them, but it is either this or trying to do a similarity search, or maybe converting the audio files into images and testing it like that.

Any advice/thoughts are welcome :)",neuralnetworks,just_sissy,False,/r/neuralnetworks/comments/gnt2ny/granny_neural_network_advice/
Catboost Tutorial on Google Colaboratory with free GPU,1590036755,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/gnqir9/catboost_tutorial_on_google_colaboratory_with/
Real time plotting in python,1590023103,"What do you use for real time plotting in python? As an example, plotting the error and accuracy during training. I’ve done it before a couple years ago using matplotlib but remember it being a headache. What’s the general concensus on the best way to do this?",neuralnetworks,dopamine_dealer_,False,/r/neuralnetworks/comments/gnn6rd/real_time_plotting_in_python/
Similarities between mycorrhizae nutrient distribution &amp; neural networks,1589999748,"Aloha Everyome, I’ve been doing some interesting experiments. I’ve been exposing my inoculated jars to varying levels of magnetic fields using N-52 Rare earth magnets. I’ve made lots of interesting observations and had some fascinating ideas (mostly after ingestion🤙🏻) One in particular is the process by which fungus moves nutrients around root structures the way it facilitates movement in neural networks. I would be stoked if there was a fellow traveler who could check/verify my lunacy here:

https://youtu.be/2aOw0AHJLmU
Warm Mahalos

[View Poll](https://www.reddit.com/poll/gnh0z8)",neuralnetworks,quietki11r,False,/r/neuralnetworks/comments/gnh0z8/similarities_between_mycorrhizae_nutrient/
wanna learn nn?,1589994976,,neuralnetworks,_brightprogrammer_,False,/r/neuralnetworks/comments/gnfguu/wanna_learn_nn/
Relationship between activation function and output,1589972116,"I'm not sure if this is the right  place to ask this but I can't understand the b part of this question in  my assignment. I have completed the 1st part and understand the maths  behind it, but the 2nd part has me stumped.

I  looked up ridge functions and they basically map Real vectors to a  single Real value from what i understood, for that reason i considered  that the activation function has to be one that ranges over the Real  numbers, but that still doesn't clear my doubts.

I  don't need a full answer just an explanation of the question will be  very helpful, here's some text from the book i'm referring(Russel and  Norvig), though I couldn't really grasp how this would help me choose an  activation function.

*Before  delving into learning rules, let us look at the ways in which networks  generate complicated functions. First, remember that each unit in a  sigmoid network represents a soft threshold in its input space, as shown  in Figure 18.17(c) (page 726).  With one hidden layer and one output   layer,  as in Figure  18.20(b),  each  output  unit  computes  a  soft-thresholded linear combination of several such functions.  For  example,  by adding two opposite-facing soft threshold functions and  thresholding the result, we can obtain a “ridge” function as shown in  Figure 18.23(a).  Combining two such ridges at right angles to each  other (i.e., combining the outputs from four hidden units), we obtain a  “bump” as shown in Figure 18.23(b).*

&amp;#x200B;

https://preview.redd.it/eryhdvbvhwz41.jpg?width=1271&amp;format=pjpg&amp;auto=webp&amp;s=8b2877ad98cac661b4db9db364479670761b0845",neuralnetworks,i_am_the_kiLLer,False,/r/neuralnetworks/comments/gn993l/relationship_between_activation_function_and/
Importance of network architecture,1589968546,"Hey guys, I am interested in resources (research papers, etc) for the importance of neural networks architecture while training it.

What role does the architecture play for the training and subsequent performance of it on unseen data.

Can you please refer me to such resources?

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/gn8kg8/importance_of_network_architecture/
Modify YOLOv3 backbone from DarkNet to AdderNet,1589966934,,neuralnetworks,promach,False,/r/neuralnetworks/comments/gn89fh/modify_yolov3_backbone_from_darknet_to_addernet/
Speed detection project . [Project],1589958750,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gn6on2/speed_detection_project_project/
I created speed measuring project which can measure speed even if low lights and fast motion.,1589958113,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gn6jyo/i_created_speed_measuring_project_which_can/
Simplest way to vectorize backpropagation?,1589939369,"Can someone point me to a good source on backpropagation vectorization? I am still learning about NN and want to develop one, completely from scratch. The forward prop is quite simple. But the many combinations that arise in the backprop, specially if the output layer has many neurons, make it kind of hard to grasp it all.",neuralnetworks,6thUser,False,/r/neuralnetworks/comments/gn2adr/simplest_way_to_vectorize_backpropagation/
"I hate English essays and creative writing and want to make an artificial learning program to do it for me. Is this possible, if so how hard would it be? Here is a picture of my friends cat for you to enjoy.",1589935433,,neuralnetworks,connor-owen,False,/r/neuralnetworks/comments/gn17df/i_hate_english_essays_and_creative_writing_and/
Hyperparameter Search using Weights &amp; Biases Library,1589920500,"I've written a demo kernel on Hyperparameter Search for FasterRCNN using the Weights &amp; Biases library. W&amp;B allows you to automated a HP sweep and the kernel demonstrates that on the Global Wheat Detection challenge. If you like it or found it useful feel free to give an upvote!  
[https://www.kaggle.com/kshen3778/hyperparameter-search-for-fasterrcnn](https://www.kaggle.com/kshen3778/hyperparameter-search-for-fasterrcnn)",neuralnetworks,kshen3778,False,/r/neuralnetworks/comments/gmwp6z/hyperparameter_search_using_weights_biases_library/
Difference between concatenation and addition,1589905678,,neuralnetworks,rat22s,False,/r/neuralnetworks/comments/gmrvlb/difference_between_concatenation_and_addition/
About human NN,1589899044,"The fact is that you are a bunch of neurons, each of which activates ""eat"" output neuron in any incomprehensible situation.",neuralnetworks,nane___h,False,/r/neuralnetworks/comments/gmprpb/about_human_nn/
Is there any website about how to teach neural networks of all Brodmann areas to learn?,1589874350,,neuralnetworks,rousse101,False,/r/neuralnetworks/comments/gmk4hw/is_there_any_website_about_how_to_teach_neural/
I created project which can calculate distance between motion center and fixed point. what interesting can we do with it ?,1589866675,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gmiklc/i_created_project_which_can_calculate_distance/
I created distance counter. What else intresting can we do with this.,1589865509,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gmibqa/i_created_distance_counter_what_else_intresting/
"Apart from talking, driving or general things for lazy humans what thing do we want artificial intelligence to accomplish?",1589855948,,neuralnetworks,trisqly,False,/r/neuralnetworks/comments/gmg0v8/apart_from_talking_driving_or_general_things_for/
Neural network as function approximator for PID control,1589837686,"Hi, I am new to neural networks and I would like to build a simple one that from a set of inputs (let's say: size of the robot and its mass) outputs the PID control gains of the robot. I have a big database of these correspondencies and I would like to train a simple neural network as an exercise to get into NN. 
I need to use pytorch since it is what it is used in my lab. Do you have any examples able to do this?
I think it should be quite a simple problem for NN to solve

Thanks!",neuralnetworks,minidiable,False,/r/neuralnetworks/comments/gmashf/neural_network_as_function_approximator_for_pid/
I made a library to download images for neural networks from a Google Image search,1589829091,,neuralnetworks,masesk,False,/r/neuralnetworks/comments/gm7yha/i_made_a_library_to_download_images_for_neural/
"what context"" bigger on the inside "" means in artificial intelligence deep learning , machine learning?",1589811192,[removed],neuralnetworks,Doctor_who1,False,/r/neuralnetworks/comments/gm270e/what_context_bigger_on_the_inside_means_in/
[Tutorial] Designing and Implementing the Controller for Neural Architecture Search,1589810583,"This tutorial covers how to design and implement a controller for neural architecture search in Python. Topics covered include designing the controller architecture, accuracy predictors, training the controller, sampling architectures, and obtaining the predicted accuracies.  

Article link: [https://blog.paperspace.com/neural-architecture-search-controllers/](https://blog.paperspace.com/neural-architecture-search-controllers/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/gm20qy/tutorial_designing_and_implementing_the/
Another cool machine learning project using my old drawing project . Not like the tracking but for now loving it. Any suggestion how can i improve tracking. Good workout for beginners.,1589788831,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/glx9h9/another_cool_machine_learning_project_using_my/
Receptive Field Arithmetic for Convolutional Neural Networks,1589787687,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/glx1d6/receptive_field_arithmetic_for_convolutional/
How to Learn Convolutional Neural Network Theory?,1589765272,"I have learned the theory behind classical neural networks through the book ""Make Your Own Neural  Network"" by Tariq Rashid, who explains the mathematics behind classical neural networks in a simple way. However, I have not been able to find a resource that explains that mathematics behind convolutional neural networks and recurrent neural networks that are explained simply, without seeing huge mathematical formulas that I cannot understand. Does anybody have a free online resource that teaches convolutional neural network theory (or recurrent neural network theory) in an intuitive and simple manner, building up from the basics?",neuralnetworks,RohanJ2006,False,/r/neuralnetworks/comments/glrvrl/how_to_learn_convolutional_neural_network_theory/
Ml implementation,1589721927,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/glfdbh/ml_implementation/
Time dependent neural networks,1589721645,"When humans view an image for example we are able to classify it better the more time we are given to study it. With neural networks it's time independent, an image classification does not change over time viewing the image. Are there any neural networks out there that increase their classification accuracy over time viewing the same picture? Maybe this is a stupid question but I couldn't find any papers that talk about this.",neuralnetworks,fsrock,False,/r/neuralnetworks/comments/glfaui/time_dependent_neural_networks/
Automatic gestures controlled vlc player with machine learning and opencv,1589721516,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/glf9r3/automatic_gestures_controlled_vlc_player_with/
Intuition for ReLU,1589686781,"it's hard for me to wrap my head around my relu activation is so successful and works so well.

I understand that it solves the vanishing gradiant problem. Ok

But really, it's just a simple function that filters out negative values.

The way I understand things:
No activation: linear regression
Any activation: introduce non linearity, more learning.

But why Relu??

Can anyone provide a non mathematical intuition for why it works?",neuralnetworks,justtryin2018,False,/r/neuralnetworks/comments/gl8jjv/intuition_for_relu/
CNN + LSTM Hybrid Network for Vehicle collision Prediction moments before it happens!,1589678096,"&amp;#x200B;

[results](https://i.redd.it/okx8vpsj78z41.gif)

Code and How: [GitHub](https://github.com/perseus784/Vehicle_Collision_Prediction_Using_CNN-LSTMs). Please Star If you would like.",neuralnetworks,Perseus784,False,/r/neuralnetworks/comments/gl6f2y/cnn_lstm_hybrid_network_for_vehicle_collision/
Where to start as a beginner?,1589655911,[removed],neuralnetworks,Czaggy,False,/r/neuralnetworks/comments/gl0a8u/where_to_start_as_a_beginner/
Where to begin as a newbie?,1589654997,[removed],neuralnetworks,Czaggy,False,/r/neuralnetworks/comments/gl006e/where_to_begin_as_a_newbie/
Bad cases of neural network application,1589637361,[removed],neuralnetworks,ggsmdmf,False,/r/neuralnetworks/comments/gkv1nu/bad_cases_of_neural_network_application/
Looking for guidance on neural networks physically implemented with CMOS Technology.,1589629444,"Hello everyone!

Could anybody point me towards a direction where I could read more about CMOS neural networks?

Thanks in advance!",neuralnetworks,Notrevealingmyname,False,/r/neuralnetworks/comments/gktbos/looking_for_guidance_on_neural_networks/
I tried making OPENCV project for counting number of people BUT how can i add more features to it ? Any suggestions.,1589616138,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gkr01o/i_tried_making_opencv_project_for_counting_number/
Are my backpropagation equations valid,1589580812,"def ReLU(x): return np.maximum(0, x)

def dReLU(x): return 1. \* (x &gt; 0)

def sigmoid(x): return 1 / (1 + np.exp(-x))

def sigmoidPrime(x): return sigmoid(x) \* (1 - sigmoid(x))

def cost(x, y): return np.sum((y - x) \*\* 2)

def dcost(x, y): return (x - y)

hidden\_weights = np.random.normal(scale=0.1, size=(2, 2))

output\_weights = np.random.normal(scale=0.1, size=(1, 2))

hidden\_biases = np.random.normal(scale=0.1, size=(2, 1))

output\_biases = np.random.normal(scale=0.1, size=(1, 1))

def train(rate, hw, ow, hb, ob, i, o):

input\_activations  = ReLU(i)

hidden\_sums = np.dot(hw, input\_activations) + hb

hidden\_activations = ReLU(hidden\_sums)

output\_sums = np.dot(ow, hidden\_activations) + ob

output\_activations = sigmoid(output\_sums)

output\_deltas = dcost(output\_activations, o) \* sigmoidPrime(output\_sums)

hidden\_deltas = np.dot(ow.T, output\_deltas) \* dReLU(hidden\_sums)

ob -= rate \* output\_deltasow -= rate \* np.dot(output\_deltas, hidden\_activations.T)

hb -= rate \* hidden\_deltashw -= rate \* np.dot(hidden\_deltas, input\_activations.T)

&amp;#x200B;

I tried implementing these equations in python:

[https://sudeepraja.github.io/Neural/](https://sudeepraja.github.io/Neural/)

&amp;#x200B;

Could anyone tell me if I did it right?

&amp;#x200B;

Thanks",neuralnetworks,Shadow_710,False,/r/neuralnetworks/comments/gkipv1/are_my_backpropagation_equations_valid/
Image Segmentation,1589550051,"Hey Guys, I have a question: I have a dataset having many images. Where each image has labels to explain it's components. An example of such an image is attached.

&amp;#x200B;

https://preview.redd.it/nmdcgcntmxy41.jpg?width=1006&amp;format=pjpg&amp;auto=webp&amp;s=232e365a150159e8742a1f2720e213134b5cfc59

I know about Conv nets for image classification but have little idea of how to approach this problem. Should  I be looking into image segmentation? Can you please provide tutorials where I can read sample code and then experiment? I am using Python 3.7 and TensorFlow 2.0.

Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/gk9b4a/image_segmentation/
I created no of visitors entered or out with OPENCV,1589546956,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gk8iad/i_created_no_of_visitors_entered_or_out_with/
SOS ( Exam question ) :'(,1589546261,"You can pretty much say i'm far from being a smart student since I can't answer this very basic question ( i spent 3 days on it :'( ) 
My question is :
How to build a neural netwok ( schema) taking into account 3 types of cells/units : one reprenseting the output and two cells representing each two types of inputs (an excitatory input and an inhibitory input) that calculates the XOR function. 
 

PS: please explain it in the easiest possible way this is litteraly my first week studying neural netwoks. 

Thanks a lot !!",neuralnetworks,heroiina,False,/r/neuralnetworks/comments/gk8c8i/sos_exam_question/
This Week in AI - Issue #18 | Rubik's Code,1589541024,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gk75b8/this_week_in_ai_issue_18_rubiks_code/
Drawing in air with just python opencv check it out,1589504033,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gjz7a0/drawing_in_air_with_just_python_opencv_check_it/
classification of odd and even natural numbers with neural networks?,1589490903,,neuralnetworks,foadsf,False,/r/neuralnetworks/comments/gjv90o/classification_of_odd_and_even_natural_numbers/
[R] Google ‘Data Echoing’ Accelerates Neural Network Training,1589488480,"A team of researchers at Google Brain recently proposed a “data echoing” technique that enables these time-consuming upstream training stages to also benefit from accelerators. Introduced in the paper *Faster Neural Network Training with Data Echoing,* the technique reuses intermediate outputs from earlier pipeline stages so that idle accelerator capacity can be reclaimed.

Here is a quick read: [Google ‘Data Echoing’ Accelerates Neural Network Training](https://medium.com/syncedreview/google-data-echoing-accelerates-neural-network-training-a02ee87d7fdb)

The paper *Faster Neural Network Training with Data Echoing* is on[ arXiv](https://arxiv.org/pdf/1907.05550.pdf).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/gjugwm/r_google_data_echoing_accelerates_neural_network/
Draw on air openCV,1589470211,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gjojtf/draw_on_air_opencv/
GLaDOS reads the Navy Seal Copypasta [using 15.ai neural network],1589470173,,neuralnetworks,Darkhan17,False,/r/neuralnetworks/comments/gjojem/glados_reads_the_navy_seal_copypasta_using_15ai/
Optical flow object tracking,1589448674,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gjj21z/optical_flow_object_tracking/
Hi every one I am a newbie and I have a question.,1589436190,"For context:
My brother is a 19 y/o photographer with 3 diplomas recognized in the whole of the EU. 
This morning after I posted a picture of the view found just outside my home(attached here) I got a message from Google saying: 
""Your new panorama is done"" and I didn't know what it was about and I open it and I was shocked. It was so much better than what I made(also attached). 
The question is:
Is it possible to make a deep learning program that analyses photos and makes them better than my brother can? And if so is anyone willing to help me do it?",neuralnetworks,ChristianVibesonly,False,/r/neuralnetworks/comments/gjgij3/hi_every_one_i_am_a_newbie_and_i_have_a_question/
Conv-2 Lottery Ticket Hypothesis numpy pruning,1589360374,"Hey Guys, I coded [Conv-2](https://github.com/arjun-majumdar/Lottery_Ticket_Hypothesis-TensorFlow_2/blob/master/Conv_2_CIFAR10_Magnitude_based_Pruning_Gaussian_Glorot_initializations.ipynb) [The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635) in TensorFlow 2.0 in conjunction with numpy absolute magnitude based pruning technique .

Let me know what you all think.

Stay safe!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/givdc9/conv2_lottery_ticket_hypothesis_numpy_pruning/
"22 worked examples in machine learning (energy, medicine, banking, retail...)",1589353040,,neuralnetworks,datapablo,False,/r/neuralnetworks/comments/gituw8/22_worked_examples_in_machine_learning_energy/
Creating deep fakes is not a big deal now !,1589341264,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gir6op/creating_deep_fakes_is_not_a_big_deal_now/
On detecting adversarial examples: building an adversary detection network.,1589309648,"I'm trying to understand how an adversary detection network would **work**. In section 3.2 of [this research paper](https://arxiv.org/pdf/1702.04267.pdf) \- I've found some sort of an explanation as to how it might be structured + how the probabilities (that the input is adversarial) would be worked out.

**What I understand** is that first the classification network is trained with the regular dataset and adversarial examples are also generated for each data point of the dataset using some method eg. DeepFool. 

As a result, we have a binary classification dataset consisting of the original data + corresponding adversarial examples of each data point.

**What I don't understand: How does this dataset, twice the size of what we had before, help us on making an adversary detection network?** **How do we input something into this so trained network and get the probability** *(within a range of values - determined by what activation function we use, of course)* **that the new input was adversarial?** 

As long as I understand how the adversary detection network works I have some sort of an idea how it would be useful tool for a DNN as probably a subnetwork branching off the main network at some layer.

This is purely based on research papers; I'm not trying to put any of this into practice yet. 

If anyone has experience in this field (cybersecurity and ML) please offer me your intelligence - a clue could help.",neuralnetworks,sealion420,False,/r/neuralnetworks/comments/gihpvv/on_detecting_adversarial_examples_building_an/
Our weird behavior during the pandemic is messing with AI models,1589298140,,neuralnetworks,jsamwrites,False,/r/neuralnetworks/comments/gidmo6/our_weird_behavior_during_the_pandemic_is_messing/
Pytorch: getting error while trying to set max_size argument for torchtext.vocab.Vocab object in Colab,1589286210,[removed],neuralnetworks,Mandala16180,False,/r/neuralnetworks/comments/gia7jm/pytorch_getting_error_while_trying_to_set_max/
Found a video for neural networks to add two numbers!,1589252282,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gi2ogt/found_a_video_for_neural_networks_to_add_two/
Not sure if this is the right place for this... but the new live action Pokemon movie is going to be LIT if they use this Tensorflow port of pix2pix.,1589226186,"[https://affinelayer.com/pixsrv/?fbclid=IwAR3eZVZasnv5R7\_f8GMkHcZs5tZ7VtlhgEzOTk0BmaY-wM3NAZbiSkAn5j4](https://affinelayer.com/pixsrv/?fbclid=IwAR3eZVZasnv5R7_f8GMkHcZs5tZ7VtlhgEzOTk0BmaY-wM3NAZbiSkAn5j4)

https://preview.redd.it/2i1vdd7lv6y41.png?width=2438&amp;format=png&amp;auto=webp&amp;s=d19dd09cf5139b322a334980070f1d245feed01a",neuralnetworks,DesignerGeezus,False,/r/neuralnetworks/comments/ghuw5c/not_sure_if_this_is_the_right_place_for_this_but/
Best way to train a feedforward network for multiple-choice text classification?,1589209356,"Hello all,

Computer science PhD student here doing work in ML and NLP. For my current research project, I’m attempting to train a feedforward network to perform multiple-choice text classification, i.e., choose one text or the other, based on a dense representation for each text. The representation is aggregated from underlying hidden states for several predictions of a BERT-based model fine-tuned on a lower-level task (flagging plausibility conflicts within each text), so I’m wanting to add a feedforward network as a classification head on this aggregated representation.

My question is: what is the best practice to input the representation for the texts for multiple-choice text classification? I have tried concatenating them and subtracting them (instead of summing because a small part of the representation also consists of the lower-level labels for each text which have meaning when subtracted, but not when summed). I get decent results for both (~85%), but a bit better with concatenating.

I could try summing the representations, then just concatenating the portions of the input dealing with layers. But I’m wondering if I’m missing a better way to do this. Hopefully this post isn’t too cryptic. Thanks!",neuralnetworks,shanestorks,False,/r/neuralnetworks/comments/ghp6zn/best_way_to_train_a_feedforward_network_for/
A general overview of neural networks by Chris Olah,1589203348,,neuralnetworks,datapablo,False,/r/neuralnetworks/comments/ghnfqe/a_general_overview_of_neural_networks_by_chris/
Differentiation of analog and digital made photos - beginner question,1589199601,"I'm looking for a way to detect/differentiate color nuances and grain in photos to differentiate analog pictures from digital ones. Is a convolutional neural network the best way to go there? Where to start learning, as a beginner? Had basic python and neural networks in university, but that was all theory and nothing practical. We just talked about object differentiation with edge detection using filters, but this wouldn't be possible for a detection of small color shifts in pictures. 

Sorry if this ist a dumb question, just don't know where to start.",neuralnetworks,35mmperseconds,False,/r/neuralnetworks/comments/ghmhxb/differentiation_of_analog_and_digital_made_photos/
What is the input shape for the google wavenet model ?,1589182863,[removed],neuralnetworks,SelectNobody,False,/r/neuralnetworks/comments/ghj5o6/what_is_the_input_shape_for_the_google_wavenet/
Python 3.9 - The Shape of Things to Come,1589179906,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ghiko8/python_39_the_shape_of_things_to_come/
Found this for those who wanna know how to get started with machine learning.,1589177851,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/ghi5qz/found_this_for_those_who_wanna_know_how_to_get/
"I'm looking for volunteers to join me in the Mars City State Design Competition organized by the Mars Society. If you have engineering, academics, or 3D artist background you might want to join r/NexusAurora. The sub is open for enthusiasts as well. Project updates will be published daily",1589145212,"Nexus Aurora project is a collaboration between volunteers aiming to submit a proposal for the [Mars City State Design Competition](https://www.marssociety.org/news/2020/02/11/mars-city-state-design-competition-announced/) organized by The Mars Society by the end of June 2020. Together with YouTuber Spaceman Dave, I intend to gather together a crew of volunteers in various domains: engineering, logistics, aeronautics, medical, programming, design, etc. We have to design a viable city for 1 million people on Mars.

Volunteers:

* [Mars or Bust](https://www.youtube.com/channel/UC5k8uqbPSsNlZm7cFcke1EA) \- Engineering, Coordination
* [Visual School](https://visual.school/) \- Graphics, Coding
* Lots of engineers and designers on Discord already

Official website coming soon! Until then, join [r/NexusAurora](https://www.reddit.com/r/NexusAurora/) subreddit if you want to contribute or simply to stay updated.",neuralnetworks,VisualCoding,False,/r/neuralnetworks/comments/gh9qme/im_looking_for_volunteers_to_join_me_in_the_mars/
The History Machine: Filtering History's Bias with Neural Networks,1589143327,,neuralnetworks,ObjectiveGround5,False,/r/neuralnetworks/comments/gh95ci/the_history_machine_filtering_historys_bias_with/
I have created simple tutorial for ann digit recognition from scratch.,1589103115,,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/ggyg43/i_have_created_simple_tutorial_for_ann_digit/
To those interested in building NNs from scratch and building AI applications with a community. A Discord.,1589070401,"The Discord ""Neural Network Study Group"" has emerged from Reddit with the intentions of following Sentdex's ""Neural Networks From Scratch"" series.


Since then, the community has grown and changed.


The members are a combination of beginners and experts.


Our goal is to acquire the knowledge to develop an AI application from the ground up as a community.


https://discord.gg/pP8KTBR",neuralnetworks,John_Lins,False,/r/neuralnetworks/comments/ggr6hg/to_those_interested_in_building_nns_from_scratch/
Question - Baseline model for regression testing(in Rapidminer),1589057406,"

Hi all - so I have a project where I need to use a baseline model(regression etc) and then improve this model with DL.  The data consists of tweets that ultimately provide a target variable of “sentiment” that gives a score of -1,0,1  that correspond to neg, neutral and pos.  Using an 80/20 split, what would be the simplest and quickest model to baseline?  Apologies if my question is a bit confusing as im relatively new to this field",neuralnetworks,war_duck,False,/r/neuralnetworks/comments/ggneig/question_baseline_model_for_regression_testingin/
[Question] I have a Network idea which I’m sure exists but would like to be pointed in the right direction as to what it may be,1589039867,"My background in neural networks consists of a math course where we derived a feed-forward multi-layer deep neural network using stochastic gradient descent. At the end of the course I touched in convolution but not applied to any network in specific , just the general idea ( which I’m guessing is important to understand convolutional neural networks maybe?) 

Anyways I was wondering what kind of network would function in this manner, and I’ll do my best to explain. The application could be for , let’s say stock market prediction. So the initial  network may try to be to extrapolate trends based on sole stock market data from the past, possibly in a stochastic way too, so it only looks at certain time period intervals randomly. Then there’s a “layer?” Above this with a greater bias, which looks at news articles with positive or negative phrases of certain companies, and correlates this with the date in the “layer?” Below it. And finally there’s a layer above this one whose cost function is to minimize loss of the amount invested, this having the highest bias. 

Please correct me if my terms are off. My background is in applied math and I’ve only taken one machine learning course, but I’d like to get into the field. What search terms could I research to find something similar to the design of a network I just explained ? It’s not something with sequential layers, rather layers being subsets of each other. 

Thank you.",neuralnetworks,cristian1928_UF,False,/r/neuralnetworks/comments/gghuy7/question_i_have_a_network_idea_which_im_sure/
Animal Crossing Wild World/City Folk 2 AM Extended using MuseNet,1588989645,,neuralnetworks,coocoojax,False,/r/neuralnetworks/comments/gg6mt0/animal_crossing_wild_worldcity_folk_2_am_extended/
[Question] sklearn perceptron activation function,1588955585,"I'm looking at sklearn perceptron, and I don't understand which activation function it is using.  My notes say heaviside is the most common.  Does anyone know what the sklearn perceptron function is using?  Or am I thinking about activation functions completely wrong.",neuralnetworks,statquestions11,False,/r/neuralnetworks/comments/gfwepw/question_sklearn_perceptron_activation_function/
TTNet: Real-time temporal and spatial video analysis of table tennis,1588944433,,neuralnetworks,TheInsaneApp,False,/r/neuralnetworks/comments/gft7nj/ttnet_realtime_temporal_and_spatial_video/
[Question] I asked a stranger a question on their neuralink but it keeps giving me a message failed to send error,1588926814,nonVerbal message sending.neuralink,neuralnetworks,neutrinoPoints,False,/r/neuralnetworks/comments/gfpbe6/question_i_asked_a_stranger_a_question_on_their/
This Week in AI - Issue #17 | Rubik's Code,1588924325,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gfot7i/this_week_in_ai_issue_17_rubiks_code/
Recurrent neural network loss spikes up,1588886967,"I've been looking into artificial neural network for a year now so I understand the concepts of backpropagation in ANNs. I already successfully made a feed forward neural network and now I'm working on a recurrent neural network, which is giving me a bit of trouble.

What I basically can do, is reuse the code for that FFNN and just add a hidden state with a corresponding backpropagation method BPTT (Backpropagation through time).

My goal is to make an RNN, which can predict next sine value, based on previous ones. Right now I'm using 2 layers, input and output with 1 neuron each.

I'm deriving everything successfully, I even checked it on paper, but for some reason loss first goes from 0.4 down to 0.03 (which is great), but then eventually starts rising up and even reaches 0.4 again. After thousands of iterations, loss then keeps bouncing between 0.12 and 0.09.

Why does this happen? What does this tell me? Do I have to stop the learning process when loss is reasonably small? Also loss doesn't go down, when I increase number of layers. It only works for 2 layers for some reason.

Thanks.",neuralnetworks,Nejako,False,/r/neuralnetworks/comments/gffnmz/recurrent_neural_network_loss_spikes_up/
Evolution creature learning,1588884458,"Anybody know if there’s a subreddit for this game and if not , should I make one",neuralnetworks,DylanRFP6,False,/r/neuralnetworks/comments/gfev9b/evolution_creature_learning/
1. Cryptocurrency Price Prediction Using Deep Learning,1588876409,,neuralnetworks,abhinav_sagar,False,/r/neuralnetworks/comments/gfca6b/1_cryptocurrency_price_prediction_using_deep/
Pink Floyd - Automatic Willpower (Album generated using OpenAI's Jukebox neural network),1588867980,,neuralnetworks,OnlyTheForum,False,/r/neuralnetworks/comments/gf9hpu/pink_floyd_automatic_willpower_album_generated/
I am looking to hire someone who knows reinforcement learning.,1588855887,Dm me for details please. Thank you,neuralnetworks,Rith_97,False,/r/neuralnetworks/comments/gf5y8e/i_am_looking_to_hire_someone_who_knows/
Accurate tide level estimation: A deep learning approach,1588800267,,neuralnetworks,amin_mlm,False,/r/neuralnetworks/comments/gesxuc/accurate_tide_level_estimation_a_deep_learning/
AI Generates SharinGAN - Part 2 (Tried To Get Better Results),1588787096,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/geopv9/ai_generates_sharingan_part_2_tried_to_get_better/
Types of Neural Networks,1588753050,,neuralnetworks,agarwalsimran,False,/r/neuralnetworks/comments/gefyez/types_of_neural_networks/
Using Neural Networks to Find Answers in Tables,1588680207,,neuralnetworks,palegoat11,False,/r/neuralnetworks/comments/gdwhol/using_neural_networks_to_find_answers_in_tables/
Spiking Neural Network w/ Growing topology,1588673246,,neuralnetworks,inboble,False,/r/neuralnetworks/comments/gdv28a/spiking_neural_network_w_growing_topology/
Top 15 AI Articles You Should Read This Month - April 2020,1588662859,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gdt24e/top_15_ai_articles_you_should_read_this_month/
Neural Networks Explained in One Minute,1588587416,,neuralnetworks,TheInsaneApp,False,/r/neuralnetworks/comments/gd8zyd/neural_networks_explained_in_one_minute/
"Human Trajectory Prediction using LSTM, do they use CNN in the first stage?",1588578068,"Hey I was reading about Human Trajectory prediction using LSTM, most of the time they have human skelton or human point(from top down view) already given. I wonder if I want to implement similar thing, I need for sure a CNN to get body features, right? and then feed it to an LSTM.  


paper example for my concern:

[https://www.researchgate.net/publication/327010364\_Scene-LSTM\_A\_Model\_for\_Human\_Trajectory\_Prediction](https://www.researchgate.net/publication/327010364_Scene-LSTM_A_Model_for_Human_Trajectory_Prediction)

&amp;#x200B;

if my approach is wrong please correct me.

Thought: in this case the CNN is the encoder and the LSTM is the decoder",neuralnetworks,ComfortableCod,False,/r/neuralnetworks/comments/gd76wu/human_trajectory_prediction_using_lstm_do_they/
Top 3 Artificial Intelligence Research Papers,1588575856,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gd6rf1/top_3_artificial_intelligence_research_papers/
What is input shape for y,1588495948,I have basic doubt that what is suitable input shape of labels (y). For a neural networks like ann and cnn?,neuralnetworks,Pawan315,False,/r/neuralnetworks/comments/gcndz4/what_is_input_shape_for_y/
How to use ALBERT for Chinese?,1588441790,"I’m trying to use the Chinese model for albert to train a rnn, and i’ve never implemented word embeddings before. All the guides i’ve found online have been pretty confusing, does anyone have any good guides/walkthroughs for this process? working in colab btw",neuralnetworks,stonkpab,False,/r/neuralnetworks/comments/gcams1/how_to_use_albert_for_chinese/
AI Generates a New Sharingan | Using GAN To Generate SharinGAN,1588425038,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/gc67p9/ai_generates_a_new_sharingan_using_gan_to/
Feeding OECD bullshit to Transformer,1588368173,"So i have this assignment on the concept of OECD risk management and I was curious if a neural net could help me write some of the required bullshit filler since I have to hand in 45 pages and there's a need for maybe 7 page being super exhaustive. This is the result.

Neural net: 
https://talktotransformer.com/

Prompt: 

&gt;This report examines what countries have achieved in terms of strengthening resilience through better risk management and identifies persisting challenges. It focuses on the importance of creating an institutional environment that engages all stakeholders and allows them to build resilience against future shocks. This report has contributed to the development of the OECD Recommendation on the Governance of Critical Risks.

Output:
&gt;Executive Summary

Table of Contents

Executive Summary

Introduction

Budgetary authorities play an important role in the overall economy, but as they have evolved and adjusted to changes in the nature of public expenditure, government spending has become more widely dispersed and opaque. Low efficiency and a lack of incentives are more evident in the budgeting of emerging markets than in developed economies, which tend to spend significantly more


Now this network does quite poorly when you feed it LOTR or Sci fi, but if you give it governement reports it is fucking flawless :D Is it me or are governement reports mostly bullshit ?",neuralnetworks,icecoldpopsicle,False,/r/neuralnetworks/comments/gbqq3k/feeding_oecd_bullshit_to_transformer/
This Week in AI - Issue #16 | Rubik's Code,1588322166,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/gbeflx/this_week_in_ai_issue_16_rubiks_code/
[D] AI21 Labs Asks: How Much Does It Cost to Train NLP Models?,1588274364,"AI21 Labs Co-CEO, Stanford University Professor of Computer Science (emeritus), and [AI Index](https://aiindex.org/) initiator Yoav Shoham describes the motivation for the project. “It started with an inquiry we got at the AI Index. I started jotting down a quick answer and realized it deserved a longer one. I also realized we had a lot of the expertise at AI21 Labs. So we spun up a small effort to put this report together, to benefit the community.”

The team compared three different-sized Google BERT language models on the 15 GB Wikipedia and Book corpora, evaluating both the cost of a single training run and a typical, fully-loaded model cost. The team estimated fully-loaded cost to include hyperparameter tuning and multiple runs for each setting: “We look at a somewhat modest upper bound of two configurations and ten runs per configuration.”

* $2.5k — $50k (110 million parameter model)
* $10k — $200k (340 million parameter model)
* $80k — $1.6m (1.5 billion parameter model)

Read more: [AI21 Labs Asks: How Much Does It Cost to Train NLP Models?](https://medium.com/syncedreview/ai21-labs-asks-how-much-does-it-cost-to-train-nlp-models-9061f8660ead)

The paper *The Cost of Training NLP Models: A Concise Overview* is on [arXiv](https://arxiv.org/pdf/2004.08900.pdf).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/gb2961/d_ai21_labs_asks_how_much_does_it_cost_to_train/
I trained a GAN to fart...,1588255710,"My friends and I were talking at a party a while back about farts being funny (#sophistocated) and eventually arrived at this idea. If you think farts are funny then I think you will get a few luls out of this. I suck at websites so this is basically just a link that will serve you .wav files that contain farts. On the backend, I just have the network generating new clips to be served when needed.

Some of them are very juicy, some squeaky, and some kinda violent sounding. Just keep on hitting the link for a new fart. Listen to a few, we had to go to some pretty dark corners of the internet to get the training data.

[www.fart.ai](http://www.fart.ai/)",neuralnetworks,linux_developer,False,/r/neuralnetworks/comments/gawc1a/i_trained_a_gan_to_fart/
I trained a GAN how to fart.,1588254889,"My friends and I were talking at a party a while back about farts being funny (#sophistocated) and eventually at this idea. If you think farts are funny then I think you will get a few luls out of this. I suck at websites so this is basically just a link that will serve you .wav files that contain farts. On the backend, I just have the network generating new clips to be served when needed.

Some of them are very juicy, some squeaky, and some kinda violent sounding. Just keep on hitting the link for a new fart. Listen to a few, we had to go to some pretty dark corners of the internet to get the training data.

www.fart.ai",neuralnetworks,linux_developer,False,/r/neuralnetworks/comments/gaw3iu/i_trained_a_gan_how_to_fart/
[Question] What dimension should my network have?,1588240087,"Good day fellow redditors,

I am trying to create my first own simple neural network with numpy, but I am having issues with the dimensions of it. I have looked up a few tutorials, therefore I have a basic understanding of the topic.

The situation is as following: 
I have two input variables in my dataset, x1 (value between 2 and 5) and x2 (value between -2 and +2).
I have one output variable y (value between 2 and 5).

How many input nodes, outputs nodes, hidden layers do I need? What is the dimension of my weight array?
My idea so far was dividing the values in 0.1 steps, so (4.9-2.0)/0.1 = 29 output nodes.

I have two input variables thus I want to use two input nodes.
Furthermore, I have read that the number of hidden layer nodes should be somewhere between the number of input and output nodes.
With my suggestion the weight array should have the size [number of hidden layers][2].

Does this make any sense to you? I am open to any suggestions.

Sorry for my poor English, I am not a native speaker.

Thank you in advance",neuralnetworks,MakeMeCrasy,False,/r/neuralnetworks/comments/gaspid/question_what_dimension_should_my_network_have/
"Nomenclature: the total ""kernel size"" for a deep CNN network is called...?",1588232597,"When we define a layer in a convolutional neural network, the number of (generally, adjacent) spatial elements used to compute one output element is called the ""kernel size.""

Suppose that we have a deep(ish) CNN with two layers, whose kernel sizes are 3 and 5.  If we look at the output of one element of such a model, it would depend on 7 input elements.  What word should we use for that number?

Older machine learning papers sometimes use the term ""window"" to describe the local span of a model which is trying to learn spatial information.  However, such models are not necessarily built with layered architectures.  Would ""window"" be an appropriate term for what I'm describing?  ""Total kernel size"" doesn't seem like the right term, because there are multiple kernels involved in a deep CNN.",neuralnetworks,bwllc,False,/r/neuralnetworks/comments/garbm6/nomenclature_the_total_kernel_size_for_a_deep_cnn/
"Algorithms, Islands &amp; Nook Miles: AI Workshop Will Be Held in ‘Animal Crossing’",1588193101,"This has been a sad year for global artificial intelligence conferences, all of which have been impacted by the COVID-19 pandemic. Organizers have attempted to put together virtual events, but it remains challenging to replicate the networking experience that draws participants to these academic and industry gatherings. A number of, shall we say, “creative” proposals have emerged — like having researchers meet in the video game *Animal Crossing*.

Yesterday in the r/MachineLearning [subreddit](https://www.reddit.com/r/MachineLearning/comments/g9urkz/r_animal_crossing_ai_workshop_call_for_abstracts/), a lighthearted announcement appeared from the [“Animal Crossing Artificial Intelligence Workshop](http://acaiworkshop.com/index.html) (ACAI)”calling for abstracts. The post rapidly went viral in the community.

Read more: [Algorithms, Islands &amp; Nook Miles: AI Workshop Will Be Held in ‘Animal Crossing’](https://medium.com/syncedreview/algorithms-islands-nook-miles-ai-workshop-will-be-held-in-animal-crossing-a35cf3c4d05)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/gahblf/algorithms_islands_nook_miles_ai_workshop_will_be/
Training Google API Neural Network,1588132861,"Hey everyone, I recently started a project using the [Google Speech to Text API.](https://cloud.google.com/speech-to-text)  I've always been interested in NN's but I don't have the time to learn them in depth yet. But I was curious as to whether I could train the Neural Networks more in the google API. 

Like I said I haven't learned much in depth on NN's yet so this could just be a dumb question. But with the fact that the NN is in the google cloud, am I right in saying you can't train it further? Also I know you can give it keywords to bias towards, but that isn't what I mean.",neuralnetworks,Upvoteme12345,False,/r/neuralnetworks/comments/ga2f2n/training_google_api_neural_network/
Could an AI become president?,1588083719,[removed],neuralnetworks,MAOU_42,False,/r/neuralnetworks/comments/g9o2v9/could_an_ai_become_president/
Would it be possible to raplace Trump with an AI?,1588083648,[removed],neuralnetworks,MAOU_42,False,/r/neuralnetworks/comments/g9o24j/would_it_be_possible_to_raplace_trump_with_an_ai/
Would it be possible to replace trump with an AI?,1588083441,[deleted],neuralnetworks,[deleted],False,/r/neuralnetworks/comments/g9nzz4/would_it_be_possible_to_replace_trump_with_an_ai/
Is it possible to do object detection training with cropped images instead of bounding boxed images (see details for dataset example)?,1588077297,"Example: https://github.com/Horea94/Fruit-Images-Dataset/blob/master/Training/Eggplant/14_100.jpg

Full dataset: https://github.com/Horea94/Fruit-Images-Dataset",neuralnetworks,whereistimbo,False,/r/neuralnetworks/comments/g9mcz8/is_it_possible_to_do_object_detection_training/
I need to make a Text To Speech Neural Network project and I don't have an idea where to begin. Could you guys help me?,1588026374,"I hope I came to the right place. I have a project for school to create a neural network that will take text and output speech. I have found some datasets but the only thing I know about neural network is that I'm probably going to use python and numpy.

&amp;#x200B;

Thanks.",neuralnetworks,aBraM_aBraM,False,/r/neuralnetworks/comments/g9axm4/i_need_to_make_a_text_to_speech_neural_network/
"I am starting my new series on Neural Networks in C#, Unity. This first episode covers a basic backpropagation perceptron and a brief introduction to hyperparameters such as learning create and weight decay!",1588004873,,neuralnetworks,NICO_THE_PRO,False,/r/neuralnetworks/comments/g93zdm/i_am_starting_my_new_series_on_neural_networks_in/
Collaborative Filtering with Machine Learning and Python,1587972496,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/g8vzeb/collaborative_filtering_with_machine_learning_and/
binary Convolutional neural network predict always 1!,1587856164,"hello redditers!

`classifier = Sequential()`  
`classifier.add(Convolution2D(32, 3, 3, input_shape = (64, 64, 3), activation = ""relu""))`  
`classifier.add(MaxPooling2D(pool_size = (2, 2)))`  
`classifier.add(Flatten())`  
`classifier.add(Dense(units = 128, activation= 'relu'))`  
`classifier.add(Dense(units = 1, activation = 'sigmoid'))`  
`classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])`

`train_datagen = ImageDataGenerator( rescale = 1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)`

`test_datagen = ImageDataGenerator(rescale=1./255)`   
`training_set = train_datagen.flow_from_directory( 'dataset/training_set', target_size=(64,64), batch_size=32, class_mode='binary')`

`test_set = test_datagen.flow_from_directory( 'dataset/test_set', target_size=(64, 64), batch_size=32, class_mode='binary')`

`classifier.fit_generator( training_set, steps_per_epoch=8000, epochs=10, validation_data=test_set, validation_steps=800)`

`classifier.save('catvsdogs_classifier.h5')`

&amp;#x200B;

I copied this code from a tutorial on the internet, I was happy finishing the training after couple of days of weird terminal freezes

anyway when I try to use another code to predict I got always 1.This CNN classify images to cats or dogs.(&gt;= 0.5 is dog and &lt; is cat)normally they use softmax layer as last layer but since we're dealing with binary CNN, one sigmoid node should do the job,Interesting: when I try to print the full connected relu layer it gives me this result:

[Relu Output of Predicting](https://preview.redd.it/wmj6ps3so1v41.png?width=681&amp;format=png&amp;auto=webp&amp;s=f688cb8e30ea706146f9cbcf0831c6e336049c22)

this looks broken! weights should be really small to get a value between -3 and 3.can someone's eye see where is the ugly sh\*t happening?

Another thing made me wonder, training data's classes are only defined as different folders in the system, and not mentioned at all in the code, so I suppose keras is smart enough to pick this clue

Thank you for reading",neuralnetworks,ComfortableCod,False,/r/neuralnetworks/comments/g83e07/binary_convolutional_neural_network_predict/
Help figuring out chain rule with two layer neural network.,1587818696,,neuralnetworks,raidicy,False,/r/neuralnetworks/comments/g7sxoh/help_figuring_out_chain_rule_with_two_layer/
Is there a way of training a NN on incorrect or “wrong” training data?,1587786857,"I.e. by specifying the error yourself?

I am thinking of making a small simulation game where each agent has a neural network to decide what their next action is. In my head I am imagining that after each action taken by an agent I would be able to “score” how good that action was and train their decision-making network on that (I.e “online” learning)

This might mean if they did something that was very good for them then that would be akin to normal backward propagation where you compute error based on the difference between what the network WOULD have generated and what it SHOULD have generated (and adjust weights and biases on that).

But if an agent takes, say, a very incorrect action then I’d like to say “that was wrong, the error is THIS much” and train the network in a sort of “the answer is anything but this” or a “the weights and biases need to move AWAY from this value” kind of way.

Is this a thing? Does this have a name? Do any conventional ML programming libraries do this? Is this any different than me just ALWAYS training them in what they _should_ have done? Or any different from only training them when they are right? 

Struggling a bit to find study material for this kind of approach. I’d _rather_ not write a NN from scratch (I’ve done it before) as I’m really looking for something I can have a high confidence of being correct 😅",neuralnetworks,peabnuts123,False,/r/neuralnetworks/comments/g7n05x/is_there_a_way_of_training_a_nn_on_incorrect_or/
Neural Networks in Scratch,1587767884,,neuralnetworks,ipagera,False,/r/neuralnetworks/comments/g7i8rh/neural_networks_in_scratch/
"[R] Amazon Introduces ResNeSt: Strong, Split-Attention Networks",1587761410,"The ResNet (residual neural network) neural network debuted in 2015, and quickly proved itself — winning the prestigious CVPR 2016 Best Paper Award. ResNet also took first place on three tasks in the ImageNet competition and aced the detection and segmentation tasks in the COCO competition. Over the past four years, the [ResNet paper](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) has been cited over 40,000 times, and many variations of the network have appeared.

The latest ResNet improvement comes courtesy researchers from Amazon and UC Davis, who this week unveiled their Split-Attention Networks, ResNeSt. The new network inherits ResNet’s concise and universal features and shows significant performance improvement without a large increase in the number of parameters, surpassing previous models such as ResNeXt and SEnet.

Here is a quick read: [Amazon Introduces ResNeSt: Strong, Split-Attention Networks](https://medium.com/syncedreview/amazon-introduces-resnest-strong-split-attention-networks-a2d7d6deb8b0)

The paper *ResNeSt: Split-Attention Networks* is on [arXiv](https://arxiv.org/pdf/2004.08955v1.pdf).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/g7gcde/r_amazon_introduces_resnest_strong_splitattention/
What's a good super resolution neural network? There are so many out there. I tried vdsr and srgan. Any advice ?,1587761058,,neuralnetworks,redjohn1991,False,/r/neuralnetworks/comments/g7g8nf/whats_a_good_super_resolution_neural_network/
HOW MACHINE LEARNING IS BEING USED TO ERADICATE MEDICATION ERRORS,1587749809,,neuralnetworks,analyticsindiam,False,/r/neuralnetworks/comments/g7cq8z/how_machine_learning_is_being_used_to_eradicate/
This Week in AI - Issue #15 | Rubik's Code,1587718156,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/g74y7n/this_week_in_ai_issue_15_rubiks_code/
"[D] Who Invented Backpropagation? Hinton Says He Didn’t, but His Work Made It Popular",1587689211,"One might think that news of the 2019 Honda Prize being awarded to Dr. Geoffrey Hinton “for his pioneering research in the field of deep learning in artificial intelligence (AI)” would prompt the machine learning community to toast the man they call the “Godfather of Deep Learning.” Instead, the gloves came off and what ensued was an unexpected Internet dust-up.

Jürgen Schmidhuber started it. In a blog[ post](http://people.idsia.ch/~juergen/critique-honda-prize-hinton.html#I), the Scientific Director of The Swiss AI Lab IDSI called out the Honda Prize for crediting Hinton with inventing backpropagation, among other things. Schmidhuber argued that “**Hinton has made significant contributions to artificial neural networks (NNs) and deep learning, but Honda credits him for fundamental inventions of others whom he did not cite.**”

Schmidhuber identified what he said were “six false and/or misleading attributions of credit to Dr. Hinton” in the press release. “I’ll point out,” he wrote, “that Hinton’s most visible publications failed to mention essential relevant prior work — this may explain some of Honda’s misattributions.”

The 6,300 word document, ***Critique of Honda Prize for Dr. Hinton*****, was published on Tuesday on the The Swiss AI Lab IDSIA (Istituto Dalle Molle di Studi sull’Intelligenza Artificiale) website. The opening line reads: “We must stop crediting the wrong people for inventions made by others.”**

Today, Hinton, University Professor Emeritus at the University of Toronto, responded on [Reddit](https://www.reddit.com/r/MachineLearning/comments/g5ali0/d_schmidhuber_critique_of_honda_prize_for_dr/fo8rew9?utm_source=share&amp;utm_medium=web2x), “**I have never claimed that I invented backpropagation.** David Rumelhart invented it independently long after people in other fields had invented it. It is true that when we first published we did not know the history so there were previous inventors that we failed to cite. **What I have claimed is that I was the person to clearly demonstrate that backpropagation could learn interesting internal representations and that this is what made it popular.”**

Read more: [Who Invented Backpropagation? Hinton Says He Didn’t, but His Work Made It Popular](https://medium.com/syncedreview/who-invented-backpropagation-hinton-says-he-didnt-but-his-work-made-it-popular-e0854504d6d1)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/g6yq7v/d_who_invented_backpropagation_hinton_says_he/
Forecasting with a simple Neural Network,1587683420,,neuralnetworks,saqibahmed515,False,/r/neuralnetworks/comments/g6x4tn/forecasting_with_a_simple_neural_network/
Machine Learning-based Anti-spoofing Methods To Improve Facial Recognition Software,1587667022,,neuralnetworks,Data-Power,False,/r/neuralnetworks/comments/g6s5em/machine_learningbased_antispoofing_methods_to/
AMA with the HuggingFace team tomorrow,1587585874,[removed],neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/g68akw/ama_with_the_huggingface_team_tomorrow/
Techniques for improving inference FPS performance?,1587567281,"I am looking to get an idea of how the inference performance is  improved in production level deployed CNN models. I know there is a need  of a vaguely powerful GPU for faster inference. But is there any other  performance optimization techniques to make the model run faster? 

I can understand by seeing many blogs and courses that the modern  resnets, inceptions, mobilenets improve faster performance in the  architecture itself. Is there any other way of improving it in transfer  learning process with a pretrained model?",neuralnetworks,pikadhu,False,/r/neuralnetworks/comments/g62ja6/techniques_for_improving_inference_fps_performance/
XOR Neural Network Producing Same Output For All Input Sets,1587563139,"Hi all, I tried to create my own basic neural network library in C++. It works great with the common (1,0,0)-&gt;1 neural network with 3 inputs and 1 output. However, when I try to create the XOR problem, it produces the same output for all input sets. I've tried adding biases, however this didn't change anything. If anyone could take a look at my code it would be greatly appreciated.

Main.cpp = [https://pastebin.com/kBgwXnVZ](https://pastebin.com/kBgwXnVZ) 

NeuralNetwork.cpp = [https://pastebin.com/zLAGxEUd](https://pastebin.com/zLAGxEUd)

NeuralNetwork.h = [https://pastebin.com/pyv2i0Vs](https://pastebin.com/pyv2i0Vs) 

&amp;#x200B;

Many thanks for any replies!",neuralnetworks,mrbean42,False,/r/neuralnetworks/comments/g61dhi/xor_neural_network_producing_same_output_for_all/
Using NeuroEvolution AI mastering Flappy Bird,1587527292,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/g5u3at/using_neuroevolution_ai_mastering_flappy_bird/
First NN project issues/questions,1587510688,"I've just started looking into nns for a machine learning project for school, and i really find those interesting. So after getting a basic understanding of layers, weights, biases, activation function ecc ecc i've started. 
The project consists in a car with 5 sensors (left right front left-front right-front) that has to complete various random tracks after being trained.
For the training i've choose a genetic algorithm.
Current projects works like this : 
5 inputs (sensors) 1 hidden layer of 4 neurons and 2 outputs: acceleration and steering (note that while the steering uses a tanh activation function for a range of -1, 1 in output, the acceleration uses a sigmoid for a 0,1 range so it only goes forward).
Every population has 50 cars, after all the car have crashed the top 2 are selected as parents for the next gen (fitnes is calculated by distance + avg of sensors (so the car that stays the most the center has better score) + time alive).
I also keep a copy of the best nn since the start of the session, so the parents for the next gen are random combination of these 3. Doing the crossover i only select randomly from a parent the whole weight matrix non randon values within the matrix.

After i have the new 50, i mutate them with a 3% chance of mutation where i add a value in range (-0.05, +0.05).

and repeat.
now, i've waited since generation 4000ish and they were barely able to do a lap on an nascar like track (oval).
when i put them on a mountain like road theymanage to do the first left corner but dont learn to turn right immediatey for the next corner ... waited for 1000ish generations on this one 

1)am i doing something theoretically wrong?
2)the values in the weights matrices have to be always in the -1, 1 range? 
3) are 4 neurons in the hidden layer enough?
 
1 week into the project and i can't make progress...",neuralnetworks,cosmindaxgugoasa,False,/r/neuralnetworks/comments/g5pxtf/first_nn_project_issuesquestions/
How does the talktotransformer website work so fast?,1587508759,[removed],neuralnetworks,parrot15,False,/r/neuralnetworks/comments/g5pe4q/how_does_the_talktotransformer_website_work_so/
what is the deepest neural network you have ever seen?,1587449864,[removed],neuralnetworks,labbitzy,False,/r/neuralnetworks/comments/g59yh9/what_is_the_deepest_neural_network_you_have_ever/
What do you think of this?,1587439575,,neuralnetworks,BeefPieSoup,False,/r/neuralnetworks/comments/g57p42/what_do_you_think_of_this/
Simple neural network demonstration using seven-segment display,1587419919,,neuralnetworks,plun9,False,/r/neuralnetworks/comments/g52ajw/simple_neural_network_demonstration_using/
Guide to Content-Based Recommendation Systems,1587372678,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/g4pa8q/guide_to_contentbased_recommendation_systems/
Difference between Artificial Intelligence Machine Learning and Neural Networks,1587370251,,neuralnetworks,sohaib0343,False,/r/neuralnetworks/comments/g4otut/difference_between_artificial_intelligence/
5 steps to learn Deep Learning,1587368694,,neuralnetworks,Sohaib0301,False,/r/neuralnetworks/comments/g4oiwd/5_steps_to_learn_deep_learning/
Ways to demonstrate that the model works.,1587343939,"I just started the world of NN, my programming teacher asked us for a small prototype and what I am wondering is how to show him that the model works correctly? I only know that there is an ""Accuracy"" indicator that can tell us how well it works, but my teacher will not settle for just that.

What ways do you recommend to demonstrate that my model works?
#noob",neuralnetworks,hokadekegi-3658,False,/r/neuralnetworks/comments/g4j4vm/ways_to_demonstrate_that_the_model_works/
"Difference between AI, Machine learning and neural network",1587333231,,neuralnetworks,Sohaib0301,False,/r/neuralnetworks/comments/g4g925/difference_between_ai_machine_learning_and_neural/
MLP vs RBF neural network,1587328031,I need to do two classification problems with 21 inputs and 3 outputs for one of them and 10 for the other. Which would be a better choice of architecture for my neural network?,neuralnetworks,satanicape,False,/r/neuralnetworks/comments/g4eruf/mlp_vs_rbf_neural_network/
Which websites are the best to publish NN articles?,1587269380,"I aim to blog/write semi-academic, slightly nonformal, amateur articles on NN, putting roughly equal focus on neuroevolution as well as ""mathematical""learning""(backprop, PSO, RL, evolution strategies, etc)

I have a wordpress blog &amp; Medium's recommendation system might help spreading it to new readers, but I'd like to hear some suggestions",neuralnetworks,Opposition69420,False,/r/neuralnetworks/comments/g41ale/which_websites_are_the_best_to_publish_nn_articles/
NN noob seeking guidance,1587251517,"I just recently started my inquiry into what NNs are and how they work. I have understanding of how the different layers and their neurons can be interconnected in different ways and how the random weights and biases affects the network and how the network learns through the back-propagation algorithm and how that’s often optimized through gradient descent. What I want is a book or an article  recommendation or a brief explanation on how to design the architecture of the NN...like how many hidden layers to use, what kind of connection to use (dense or others), how we choose “what” the neurons in the hidden layers should represent. I’d really appreciate it if you can suggest a resource that has the answers to these questions. Thank you!",neuralnetworks,SolomonicScrotum,False,/r/neuralnetworks/comments/g3x3li/nn_noob_seeking_guidance/
Stephen Wolfram: The Path to a Fundamental Theory of Physics May Begin With a Hypergraph,1587164192,"Physics is the most fundamental of the sciences, dealing with matter and energy. But despite centuries of study, scientists still struggle with the basic question of how the universe works — in other words, we still lack a truly fundamental theory of physics.

And that’s something Stephen Wolfram has been thinking about for nearly 50 years. Known for his work in computer science, mathematics, and theoretical physics, Wolfram announced this week that he may have found a path that leads to a fundamental theory of physics, and that it is “beautiful.”

Read more: [Stephen Wolfram: The Path to a Fundamental Theory of Physics May Begin With a Hypergraph](https://medium.com/syncedreview/stephen-wolfram-the-path-to-a-fundamental-theory-of-physics-may-begin-with-a-hypergraph-c1fd124b6e62)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/g3cihz/stephen_wolfram_the_path_to_a_fundamental_theory/
[Question] Are CNNs always permutation invariant ?,1587133117,"Hey folks, 

I am currently working with Multi Layer Perceptrons and Convolutional Neural Networks for image classification of the MNIST handwritten numbers data set. 

I trained both the MLP and the CNN on the permuted data, in which I have permuted all 28x28 pixels of each image. I then retrained both networks. In theory  the MLP should have an accuracy similar to the original not permuted image set (which the results support) and the CNN should perform worse ( which the results do not support)

So when examining results , the CNN does not perform worse on the permuted vs original not permuted image data. I was curious if it could be because the images are rather small 28x28 and have only one color Chanel since they are grayscale.

Any idea on why the CNN performs equally as good on original vs permuted data? It should not be permutation invariant in theory...",neuralnetworks,deniz_sen,False,/r/neuralnetworks/comments/g32uz2/question_are_cnns_always_permutation_invariant/
This Week in AI - Issue #14 | Rubik's Code,1587113484,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/g2y8u4/this_week_in_ai_issue_14_rubiks_code/
"So I made this post in a Facebook group looking for cool thoughts and insight, and got blasted by idiotic replies instead.",1587092269,,neuralnetworks,Firmlygrasp1t,False,/r/neuralnetworks/comments/g2tnpp/so_i_made_this_post_in_a_facebook_group_looking/
What would be the best type of NN for this?,1587079501,"I have a data set which contains live data (i.e. there are serveral sets of floats, each one with an increment in time), and a confidence rating. I want to train a neural network on this data set so i can give it a new batch of live data it hasnt seen before, and then have it generate its own confidence rating. What would be the best type of neural network to use for this?",neuralnetworks,Natekomodo,False,/r/neuralnetworks/comments/g2q9gm/what_would_be_the_best_type_of_nn_for_this/
Build a Neural Network from scratch tutorial. Lets form a study group 📖,1587072952,"I was thrilled to see that Sentdex is releasing a video series titled, ""Neural Networks From Scratch""

https://youtu.be/Wo5dMEP_BbI

The best way to acquire a deeper understanding of ML &amp; AI is by developing an understanding of its functionality at a lower level.

Sentdex is also incorporating the mathematics of AI in the course which should come in handy.

This is the first video in the series that was realised so far, and I would like to build a tiny study group on Discord: https://discord.gg/R33aGt

Let's help each other better understand this incredible technology!",neuralnetworks,John_Lins,False,/r/neuralnetworks/comments/g2o84r/build_a_neural_network_from_scratch_tutorial_lets/
Do you guys know a good platform or forum for AI lovers to share projects or to start collaborative projects?,1587060553,,neuralnetworks,mateorandulfe,False,/r/neuralnetworks/comments/g2k73u/do_you_guys_know_a_good_platform_or_forum_for_ai/
OpenAI Puts CV Models Under Their Microscope,1587059486,"OpenAI yesterday unveiled its Open AI Microscope, which provides visualizations of every significant layer and neuron in eight of today’s most popular computer vision (CV) models. Interactions between neurons indicate the abilities of neural networks, and with machine learning trending toward increasingly complicated neural networks it is important for researchers to be able to quickly and easily conduct a closer inspection of these thousands of interactions. This is where AI Microscope comes in.

Just as biologists gain insights into organisms by putting model specimens under their microscopes, AI Microscope was designed to help researchers analyze the features that form inside leading CV models.

A quick read: [OpenAI Puts CV Models Under Their Microscope](https://medium.com/syncedreview/openai-puts-cv-models-under-their-microscope-95234ae75244)

Check more [Open AI Microscope](https://microscope.openai.com/models) info.",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/g2ju4r/openai_puts_cv_models_under_their_microscope/
Neural Networks Basic Cheat Sheet,1587039733,,neuralnetworks,TheInsaneApp,False,/r/neuralnetworks/comments/g2dvxa/neural_networks_basic_cheat_sheet/
I need answer for this question because i cant understand . For B usually i use tensorflow,1587028380,,neuralnetworks,ar_abel,False,/r/neuralnetworks/comments/g2bjmm/i_need_answer_for_this_question_because_i_cant/
"I am just starting to learn CNNs, and everything kind of seems alright to me, but the last part of the output of the code. Once training is complete, what are these values? I mean, are we not supposed to get the output that says ""10% - dog, 70% - cat, 20% - elephant"" (just an example)?",1586986043,,neuralnetworks,lalayev,False,/r/neuralnetworks/comments/g215eh/i_am_just_starting_to_learn_cnns_and_everything/
Is there a standard way to give a first guess or limit the range of output of an ANN?,1586966411,"I have an ANN that I am trying to develop to estimate certain parameters for input arrays where the parameters I want to estimate have certain physical meaning with hard limits.  It seems to me like there should be some clear way to set limits on the output.  I'm sure I could come up with something, but it seems to me like this must be a pretty standard problem and I'd like to approach it is a standard way.  What do you recommend?",neuralnetworks,adamnredwine,False,/r/neuralnetworks/comments/g1uqth/is_there_a_standard_way_to_give_a_first_guess_or/
‘Active Neural Slam’ Uses Classical and Learning Approaches to Explore 3D Spaces,1586962946,"The new paper *Learning to Explore Using Active Neural Slam* from researchers at Carnegie Mellon University, Facebook AI Research, and University of Illinois at Urbana-Champaign, introduces Active Neural SLAM, a modular and hierarchical approach to learning policies for exploring 3D environments.

Here is a quick read:[‘Active Neural Slam’ Uses Classical and Learning Approaches to Explore 3D Spaces](https://medium.com/syncedreview/active-neural-slam-uses-classical-and-learning-approaches-to-explore-3d-spaces-e41e8c8e794b)

The original paper Learning to Explore Using Active Neural Slam is [here](https://arxiv.org/pdf/2004.05155.pdf)

To visit their GitHub page, click [here](https://github.com/devendrachaplot/Neural-SLAM/)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/g1tlfd/active_neural_slam_uses_classical_and_learning/
Some help with R Studio,1586910164,"I need to determine the best nesting of laser cutting parts in steel sheets with minimun waste of raw material. My inputs will be the parts and sheets areas, and the respectives quantities. Is it possible to determine wich part, the quantity and the size of the steel sheet using neural network in R studio?",neuralnetworks,mauricioavpo,False,/r/neuralnetworks/comments/g1hbkm/some_help_with_r_studio/
What is the advantage of Adam over Nadam? Isn´t Nadam just way better as it prevents noise impact?,1586855961,,neuralnetworks,mateorandulfe,False,/r/neuralnetworks/comments/g12chc/what_is_the_advantage_of_adam_over_nadam_isnt/
Self-Labelling: Automatically Generate Labels for Unlabeled Images,1586799871,,neuralnetworks,amitness,False,/r/neuralnetworks/comments/g0nrzt/selflabelling_automatically_generate_labels_for/
What is the difference between Adadelta and RMSprop optimisers? Don't they use the same equations?,1586794840,,neuralnetworks,mateorandulfe,False,/r/neuralnetworks/comments/g0m5ir/what_is_the_difference_between_adadelta_and/
Introduction to Recommendation Systems,1586762745,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/g0e6oz/introduction_to_recommendation_systems/
Making AI Play Flappy Bird | Using Unity and NEAT,1586719848,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/g0334e/making_ai_play_flappy_bird_using_unity_and_neat/
Neural Networks from Scratch - P.1 Intro and Neuron Code (Python),1586714733,,neuralnetworks,RambleFeed,False,/r/neuralnetworks/comments/g01jy9/neural_networks_from_scratch_p1_intro_and_neuron/
Making AI Play 3D Flappy Bird - Using NEAT,1586713099,[deleted],neuralnetworks,[deleted],False,/r/neuralnetworks/comments/g0128t/making_ai_play_3d_flappy_bird_using_neat/
Kernel Trick in Support Vector Machine (SVM),1586688192,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/fzulhe/kernel_trick_in_support_vector_machine_svm/
Trained Tensorflow on Bach music,1586683957,"Hi.

I’ve tried to generate a music with a neural network, and I’m pretty happy with a result.

For preprocessing midi files I took the idea from this article https://towardsdatascience.com/generate-piano-instrumental-music-by-using-deep-learning-80ac35cdbd2e

After that I’ve used a textgenrnn to train the network, since the preprocessed midi is definitely looks like a simple text with some structure behind it.

And, as a last step, I’m using a scale quantize on the generated output of the network, and generated music becomes more harmonic. After that I create a midi file from the data.

You can try to play with it here:
http://neurobach.herokuapp.com/",neuralnetworks,Ahineya_it,False,/r/neuralnetworks/comments/fztscb/trained_tensorflow_on_bach_music/
Neural Network with multiple outputs in Keras,1586619031," I am fairly new to developing NNs in Tensorflow, and am trying to build a NN in Keras with two different output paths where the first path informs the second. The first path passes its loss to the end of the second path, like so:

1. Pass through layer A then layer B, calculate loss as L(Step 1) and back-propagate error. (easy)
2. Pass through layer A then layer C, calculate loss **incorporating the loss from step 1** as L(Step 2)−λL(Step 1), and back-propagate error. (not sure how to do this)

I think I need to save the error from step 1, and use it to build a customized loss function for step 2. Any guidance would be appreciated.",neuralnetworks,lamps19,False,/r/neuralnetworks/comments/fz7kox/neural_network_with_multiple_outputs_in_keras/
I am new to neural networking and I had a cool idea.,1586582657,"If scientists finally figured out how human emotion works, does that mean we could find a way for an ai to feel emotion? If that is possible, we could put a group of them together as a social gathering. The robots would develop a real conscious, I know it sounds crazy. But if we had the knowledge, is it possible?",neuralnetworks,GenesisX_,False,/r/neuralnetworks/comments/fyx6fw/i_am_new_to_neural_networking_and_i_had_a_cool/
How Google is Teaching a Robot Dog to Learn to Move like a Real Dog,1586579167,,neuralnetworks,subhamroy021,False,/r/neuralnetworks/comments/fyweh3/how_google_is_teaching_a_robot_dog_to_learn_to/
This Week in AI - Issue #13 | Rubik's Code,1586506235,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fybjuj/this_week_in_ai_issue_13_rubiks_code/
does anyone recognize this deceased John Doe journal page as being neural network related?,1586390631,,neuralnetworks,Locomule,False,/r/neuralnetworks/comments/fxhxbc/does_anyone_recognize_this_deceased_john_doe/
What is special about NN models compared to quantitive methods?,1586360388,"Given a classification problem , you  inspect  variables and analytically find relationships between these variables to come out with a quantitive method to classify the input. 

The same problem can be solved by NN. You analyze the variables (features) and see how they effect the output of the quantitive method just as above (according to my current understanding of NN ) , then feed these variables to a NN , and you add the quantitive method output as a label for the NN model to learn from.     

What are the benefits of using NN if the quantitive method that does the job of the NN is used to teach the NN model ?",neuralnetworks,Beginner4ever,False,/r/neuralnetworks/comments/fx8ooj/what_is_special_about_nn_models_compared_to/
Machine Learning: AI learns to play Tetris using Convolutional Neural Network,1586281791,,neuralnetworks,ssusnic,False,/r/neuralnetworks/comments/fwp1f4/machine_learning_ai_learns_to_play_tetris_using/
⭕ Best Known Convolutional Neural Networks,1586280176,,neuralnetworks,TheInsaneApp,False,/r/neuralnetworks/comments/fwoj8a/best_known_convolutional_neural_networks/
How to know if a known position is in the view field of rgb-d image,1586279034,"Hello I‘m doing my bachelor thesis, and I want to tell if a known position (x,y,z) in the field of view of rgb-d camera, The position of the camera is known and the target position is known, but how can I know if the target position in the area of that camera?",neuralnetworks,ComfortableCod,False,/r/neuralnetworks/comments/fwo61m/how_to_know_if_a_known_position_is_in_the_view/
EduNN - Custom Deep Learning Framework for Education,1586158080,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fvubh2/edunn_custom_deep_learning_framework_for_education/
LEGO Identification cNN HELP!,1586109263,"I’m trying to create a lego piece identification convolutional neural network.

To start a figured I’d try it with two LEGO pieces to see what kind of accuracy I could get. I chose two drastically different pieces and figured the accuracy would be somewhat decent. The pieces I chose are a [2x4](https://images-na.ssl-images-amazon.com/images/I/51KFlwhpEWL._AC_SX425_.jpg) and [2x2](https://images-na.ssl-images-amazon.com/images/I/51um5qVFMrL._AC_SX466_.jpg) which as shown by the images are pretty different. The accuracy I received was absolutely terrible. The data set I made and trained on had 300 images of each piece so I get why the results weren’t great considering 1000 is usually a minimum but the results were so sub-par that I’m questioning whether that was only contributing factor to the bad accuracy. I tried making a different data set with better lighting and the same background versus the original which had a couple of different backgrounds. The results from this weren’t very different but this one had even fewer photos so I guess that would make sense. 

Here’s my big problem: A LEGO piece has a limited number of angles it can be photographed at (100 photos can capture about every angle of a lego piece). What am I supposed to do with the next 900+ photos? If I end up with multiple photos of the same angle won’t that mess up the neural network? I’m a beginner with deep learning which I’m sure is apparent from this so I would love some insight from someone who has some experience with cNN’s and who might be able to help me.",neuralnetworks,gotparticles,False,/r/neuralnetworks/comments/fvhzin/lego_identification_cnn_help/
Could you please give your opinion on this training progress of a ConvNet?,1585966775,"Training progress for validation and loss is here;

[https://imgur.com/RPPdx8M](https://imgur.com/RPPdx8M)

Hyperparameters:

Several convolutional layers (4) with Maxpooling, ReLU, fully-connected, Softmax and cross-entropy. Four clases. Data set is 1064 images plus augmentation, split in 70% for training, 20% for test. Minibatch=64 (I tried 32 too and got similar results). Learning rate=0.01, dropout =0.5 at the fc layer. Trained using a CUDA enabled Nvidia Quadro GP100 GPU. I believe I have the overfitting under control but I would like to know your opinion guys. I am begguiner in NN. Could you please let me know your opinion? Thanks guys!",neuralnetworks,cacahuatte,False,/r/neuralnetworks/comments/fulm4u/could_you_please_give_your_opinion_on_this/
10 Open Source Data Science Projects to Make you Industry Ready!,1585918255,,neuralnetworks,subhamroy021,False,/r/neuralnetworks/comments/fu7uqj/10_open_source_data_science_projects_to_make_you/
This Week in AI - Issue #12 | Rubik's Code,1585901236,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fu4dh7/this_week_in_ai_issue_12_rubiks_code/
Visualizing Molecular Structures for Machine Learning,1585825285,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/ftkegh/visualizing_molecular_structures_for_machine/
Rethinking Softmax with Cross-Entropy,1585825198,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/ftkdr3/rethinking_softmax_with_crossentropy/
Where would I learn how to feed in arbitrary data into neural networks?,1585779888,"I am looking to pipe files and binary streams into encoder / decoders and and probably RNNs. Most of the resources I'm finding are for transfer learning for images on existing nets and parameters, or stuff for NLP that is very text / language centric. Is there a Tensorflow / PyTorch course for people who have a lot of dev experience and want to use it for unusual stuff?",neuralnetworks,__GG,False,/r/neuralnetworks/comments/fta38l/where_would_i_learn_how_to_feed_in_arbitrary_data/
What types of Neural Networks would usually use the Cosine Similarity Loss Function?,1585730431,,neuralnetworks,mateorandulfe,False,/r/neuralnetworks/comments/fsvxhz/what_types_of_neural_networks_would_usually_use/
"I'm starting a daily drawing challenge using prompts influenced by coronavirus infection spread through a NN, INKQUARANTINE!",1585711444,"&amp;#x200B;

https://preview.redd.it/r9faaea6k4q41.png?width=2951&amp;format=png&amp;auto=webp&amp;s=d804be666202d31bd8b47b6a72995f9868e36adf

Hello!

I've been pretty bored lately (as some of us are too), and thought about doing this challenge for myself so I can get to draw consistently again while tinkering with data in the process. I'm a pretty big fan of daily challenges, so I tried giving it a little spin.

For those who don't know, they are friendly platforms for people to join and make one drawing each day, inspired by a prompt assigned to the specific date.

Well, the prompts given in this variation change as the quantity of coronavirus-infected rises (positive mood) or lowers (negative mood), which is fed into a AttnGAN. The dataset is updated daily, it's the one in data.europa dot eu (I don't know if links are allowed). This way, we can draw the boredom away as the virus diminishes it's spread and we get more hopeful and serene prompts as a reward!

As a personal exercise I prepared some social network stuff, but it's intended to just be a local thing in my group of artist friends (And before anyone asks, I did ask for Jake Parker's blessing).

I'm currently using the slope of the difference between each day's total data worldwide (a week in advance, so I don't have redownload and parse the csv every day) to calculate a vector that will assign the range that feeds the NN, but it seems like the dataset isn't exactly on point (every country has different guidelines for counting the infected) which can lead to some pretty big spikes in data and kind of weird results. 

Lastly, this NN stuff is just a hobby, I'm mainly an animator! So any wisdom will be greatly appreciated. The system is working at the moment (I have to filter it heavily, but it's acceptable), yet I feel like it could be waaay more effective. I'll begin to upload prompts soon, hopefully. If anyone cares, it's ""inkquarantine\_"" on IG.

&amp;#x200B;

TL:DR: Dataset isn't the most reliable, looking for a better method of interpretation. Should be noted that programming isn't my main skill",neuralnetworks,clovelt,False,/r/neuralnetworks/comments/fsroj4/im_starting_a_daily_drawing_challenge_using/
Security Camera using Tensorflow.js - Tutorial + Live Demo,1585655307,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/fsbiu1/security_camera_using_tensorflowjs_tutorial_live/
Top 15 AI Articles You Should Read This Month - March 2020,1585553936,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/frmh20/top_15_ai_articles_you_should_read_this_month/
Top 3 Artificial Intelligence Research Papers – March 2020,1585553867,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/frmgla/top_3_artificial_intelligence_research_papers/
[Need Help] Learning Machine Learning,1585550644,"Hello Guys,

because of Covid19 my Machine Learning Course will no longer be held, but i still have my written exam this Semester. I would like to learn Machine Learning for my Exam but i still really want to learn it practical because i always wanted to learn it but never found the time.

So my Question is if you guys could recommend me any Ressources. I dont really know where to start because no matter where i start i feel very overwhelmed. I pref Lynda / Video2Brain Videos or something Interactive.

The Script contains the Following Topics so maybe that helps you guys helping me hehe.

(I loosly translate them because english is not my native language)

\- Assoziationsrules (Apriori-Algo.)

\- Data Preperation

\- Clustering (Hirarchial, K Middle)

\- Linear Regression

\- Neuronal Networks (Gradient stuff, Classification, ...) alot of sub topics

\- Decesion Trees and ID3-procedure

Its a very theoretical Course and there normally would be a hands on in MatLab where we would implement everything but thats canceld because of COVID

Ty in advcaned",neuralnetworks,kngholdy,False,/r/neuralnetworks/comments/frlv5c/need_help_learning_machine_learning/
The Perception–Action Hierarchy and its Implementation Using Binons (Binary Neurons),1585508348,"Elsevier publications have been delayed so I have posted a preprint of my paper:

The Perception–Action Hierarchy and its Implementation Using Binons (Binary Neurons)

It's online at my website: http://www.adaptroninc.com/BasicPage/p-hierarchy

Binons form a growing feedforward neural network.",neuralnetworks,BrettNMartensen,False,/r/neuralnetworks/comments/frb41w/the_perceptionaction_hierarchy_and_its/
[Source code with demo] Here is my python implementation of Deep Q-learning for playing Tetris,1585492881,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/fr6vk0/source_code_with_demo_here_is_my_python/
Neural network for game,1585466419,"hi i dont much about neural networks, but ive been workin on one for a game and i'm wondering if im doing this right?

im using brain.js ( [https://github.com/BrainJS/brain.js](https://github.com/BrainJS/brain.js) )

&amp;#x200B;

and im using a feedforward network,

1 data looks like this

`""input"": {`

	`""point2_dis"": 0.50329857720269,`

	`""point5_dis"": 0.030343967013889,`

	`""point6_dis"": 0.50329857720269,`

	`""point1_dis"": 0.030343967013889,`

	`""point3_dis"": 0.061344740125868,`

	`""point4_dis"": 0.0071625094943576`

`}, ""output"": {`

	`""y"": 0.5,`

	`""x"": 0.5`

`}`

`},`

&amp;#x200B;

the inputs are ray traces from inside my game, and it will tell you the distances from 4 walls

output is a players position in 3d space subtracted by predicted players position (predicted by the player's velocity)

&amp;#x200B;

so i want to know are these inputs/outputs good?

when i train this w/ about 30k samples, i get around a 0.01+ erroring rating. im getting data every 1 second in the game so..

ok so my real question is are my inputs and outputs good",neuralnetworks,Riddle1336,False,/r/neuralnetworks/comments/fr1y9w/neural_network_for_game/
Trading AI (Deep- vs Machine Learning),1585414692,[removed],neuralnetworks,dumalab,False,/r/neuralnetworks/comments/fqnvmb/trading_ai_deep_vs_machine_learning/
Distilling Task Specific Knowledge from BERT into Simple Neural Networks (paper explained),1585410691,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/fqmnxf/distilling_task_specific_knowledge_from_bert_into/
Semantic segmentation for self driving cars,1585341379,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/fq55b6/semantic_segmentation_for_self_driving_cars/
any projects with CV+Covid ?,1585320006,"Some folks created a CV/AI based detector for spotting large groups of people and to ensure social distancing. Link here: [https://www.youtube.com/watch?v=BIpapSWRsBE&amp;feature=youtu.be](https://www.youtube.com/watch?v=BIpapSWRsBE&amp;feature=youtu.be)

It seems cool, do you know any other projects that use CV for easing the burden of COVID?",neuralnetworks,jasabdom,False,/r/neuralnetworks/comments/fpy1cp/any_projects_with_cvcovid/
This Week in AI - Issue #11 | Rubik's Code,1585305888,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fputiu/this_week_in_ai_issue_11_rubiks_code/
No need to switch from Jupyter to any IDE! A visual debugger for Jupyterlab is here,1585305093,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/fpuo8h/no_need_to_switch_from_jupyter_to_any_ide_a/
This guy has just won the internet. Quarantine goals,1585289227,[removed],neuralnetworks,Candid-Trade,False,/r/neuralnetworks/comments/fprubh/this_guy_has_just_won_the_internet_quarantine/
Incredibly disappointed in the USFCA Deep Learning Part 1 certificate (fast.ai in person course),1585261926,"Sorry if this post is out of the usual here, but I need to vent. I am withdrawing after two lectures (net loss of $600 dollars, $1400 refunded). The short of it, if you are considering taking this course, wait for the free version. There is zero advantage to paying for it.

Specific complaints - 

* Questions are answered by committee. You need 5+ ""likes"" on the forum to get a response. This means a lot of questions are not answered.

* The syllabus was not offered ahead of time. It was not until the second lecture that it became clear that this course was a hybrid of a deep learning course and machine learning course. It also sounded like other students expected the full fast.ai book to be covered, but we are only doing a few chapters. 

* Videos are very rough. They are live streaming them with the intention of editing them down into better videos for the free course. This means in the second lecture we lost an hour (of a 2.5 hour class) to things that weren't Deep Learning or ML. So you pay for early access to a worse version, this does not make much sense to me.

* My biggest complaint is there isn't really a professor or knowledgeable TA that you can ask ""how would I ..."" questions to. I have some ideas I would like to implement that are outside the scope of ""turn it into an image and use resnet on it"", but these questions do not get voted up.

* It did not seem like it would be graded on anything. It is a certificate course, but there wasn't any project or assignment. Just sit through lectures (no attendance) and run some notebooks (no accountability). What is this certifying that you have done?

I have done an undergrad and graduate degree and this is definitely bottom 3 in terms of classes I've taken. It is mostly due to the forum format and the lack of an expert to speak to. Jeremy is obviously very good at what he does, and the free (fast.ai) resource is amazing, but some rough livestreams and a forum are not worth $2000. Really just hire a couple TAs for students to talk to and this would have been fine.",neuralnetworks,__GG,False,/r/neuralnetworks/comments/fpl7l4/incredibly_disappointed_in_the_usfca_deep/
Predicting Protein Structures Using Deep Learning,1585253922,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/fpitcy/predicting_protein_structures_using_deep_learning/
What is the difference between a neural network and multiple if else statements?,1585224374,"I don't get it. Why should we use/train neural networks when they are just a bunch of if else statements? Why would we then need to train a network in this case?

&amp;#x200B;

Say for instance, I am doing a self driving car project; If red light, apply breaks; Pedestrain detected? Apply brakes etc. 

How does neural networks come into play for autonomous self driving cars? Genuine question!!  Thanks guys",neuralnetworks,PythonArt,False,/r/neuralnetworks/comments/fpadqg/what_is_the_difference_between_a_neural_network/
Multi Matrix Deep Learning with GPU's - its role in artificial intelligence,1585220902,,neuralnetworks,Albertchristopher,False,/r/neuralnetworks/comments/fp9p21/multi_matrix_deep_learning_with_gpus_its_role_in/
This guy has just won the internet. Quarantine goals,1585201954,,neuralnetworks,BothZookeepergame1,False,/r/neuralnetworks/comments/fp68xk/this_guy_has_just_won_the_internet_quarantine/
Training N Networks with the Same Training Data,1585198761,"Hi,

I'm working on a project and I've realized that training N neural networks over the same training data will often result in different outputs for the same input for each of the N networks. 

I'm trying to do timeseries prediction and I used the forecast library of the R language as benchmark which uses N networks to make the forecast (20 by default) and averages out the outcome. 

In my own implementation using Keras, I've noticed that averaging out the predictions of my N networks has worse results than using only the result of one network. Also my forecasts seem to have better results than the averaged out forecast library's predictions. 

My general question is since a training process results in many different networks is there a better method for aggregating the outputs of many different networks or (as a special aggregation case) to select the best output from the group of N outputs?

Thank you",neuralnetworks,ikoukas,False,/r/neuralnetworks/comments/fp5liv/training_n_networks_with_the_same_training_data/
Mixed Outputs and in a Neural Network and Defining their Significance,1585197692,"Hello, 

I'm working on a project and have come to understand that sometimes setting the inputs of a training process and the desired outputs isn't enough. My intuition tells me that if you train a network with extra transformations of the initial data as extra inputs and also extra knowledge as outputs that aren't your useful outputs could make the useful outputs better even though the extra outputs won't be used. 

The problem with that (as I understand it) is that if for example the extra knowledge is in the form of 1000 outputs and your useful outputs are only 3, the loss function that the training tries to minimize will mostly try to minimize the loss associated with the 1000 outputs and ""assign"" less significance to the error of the 3 desired outputs (i.e. a category outputs) than the one we want. 

How can we declare in such a ""mixed"" network that some of the outputs are categories, some are float values and that their significance is different? (I'd prefer that such a thing would be described using Keras, if possible)

Thank you",neuralnetworks,ikoukas,False,/r/neuralnetworks/comments/fp5djn/mixed_outputs_and_in_a_neural_network_and/
A Template for COVID-19 Research,1585166944,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/fox85v/a_template_for_covid19_research/
Intro to Pytorch Lightning,1585165750,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/fowuj4/intro_to_pytorch_lightning/
I Used Deep Learning To Detect Naruto (Anime Series) Hand Signs [I Made This],1585159004,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/foutr7/i_used_deep_learning_to_detect_naruto_anime/
"Building a scalable online product recommender with Keras, Docker, GCP, and GKE",1585158055,,neuralnetworks,hszafarek,False,/r/neuralnetworks/comments/foujop/building_a_scalable_online_product_recommender/
You can reduce fraud by 80% and speed up the underwriting process in times with a car damage recognition solution &gt;&gt;,1585055010,,neuralnetworks,benjamin_brook,False,/r/neuralnetworks/comments/fo4r4x/you_can_reduce_fraud_by_80_and_speed_up_the/
Multi Matrix Deep Learning with GPUs,1585041184,,neuralnetworks,Albertchristopher,False,/r/neuralnetworks/comments/fo223x/multi_matrix_deep_learning_with_gpus/
Braille Characters (Language for the visually impaired) to Speech using Convolutional Neural Network,1585023415,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/fnyq3r/braille_characters_language_for_the_visually/
Effects of Weight Initialization on Neural Networks,1584958608,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/fnhmj1/effects_of_weight_initialization_on_neural/
Detection of COVID-19 in chest X-Rays with Deep Learning,1584951356,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fnge6k/detection_of_covid19_in_chest_xrays_with_deep/
[Resource] An overview of neural architecture search and the state of the art,1584907939," A thorough overview of the field of neural architecture search, and the state of the art.

This article divides the subject of neural architecture search into three areas of active research: how the search space is designed, how the search strategy is designed (either with Bayesian optimization, reinforcement learning, evolutionary algorithms, network morphism, or game theory), and different ways to speed up architecture performance estimation (specifically with low fidelity estimations, learning curve extrapolation, and one-shot learning).

Article link: [https://blog.paperspace.com/overview-of-neural-architecture-search/](https://blog.paperspace.com/overview-of-neural-architecture-search/)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/fn6aum/resource_an_overview_of_neural_architecture/
what is sparse softmax cross entropy loss and how to calculate it using tensorflow,1584898874,,neuralnetworks,SnakeEyedJoe,False,/r/neuralnetworks/comments/fn3ks0/what_is_sparse_softmax_cross_entropy_loss_and_how/
Convolutional neural network from scratch help,1584835703,"I want to learn how to code a convolutional neural network from scratch (in java, that's what I know best). I already made several FFNNs in the past for data classification, and that was all from scratch (forward pass, calculating derivatives).

Question is, where can I find resources for learning convolutional neural network maths, especially backpropagation. I would really appreciate any help.

I'm also thinking about starting to learn RNN instead of CNN, because it's easier? What do you think?",neuralnetworks,Nejako,False,/r/neuralnetworks/comments/fmq3vn/convolutional_neural_network_from_scratch_help/
Representing Scenes as Neural Radiance Fields for View Synthesis,1584756160,,neuralnetworks,junglist313,False,/r/neuralnetworks/comments/fm7f4h/representing_scenes_as_neural_radiance_fields_for/
Any ideas to make a neural network prototype in unity or python with easy to gather data or an unsupervised learning?,1584748731,"So in Catalonia, Spain before accessing university we have to do a TDR (Treball de Recerca) what in English you would call an investigation project, basically we have to look for a theme we're really interested in and make a kind of research paper explaining how it works in-depth and sometimes make a prototype. It's a really important mark of all the two last years of high school and if I do an impressive project I have the opportunity to submit it to some contests and have my research paper posted publicly on google along with my name.

The thing is that I choose to investigate Artificial Neural Networks and now I'm on a very early stage of the project where I have to think about what to do as the prototype, I've searched a bit about the subject and know the basics, that there are different learning methods and kinds of neural networks, that there are ways of training the program with data and some times you can just make the program learn by itself with little to no data.

**It's worth noting that:**

* I would like to make something not too complex i'm not that experienced on programming.
* It would be cool to make the data myself with the help of other students asking for their help or through google forms, gathering data, in general, is valued since my school believes it's an important part of the scientific method. ---- I could only do this in case it wouldn't take too much time.
* I've seen that there are people that have made cars learn how to get by a circuit with genetic algorithms in unity, I think this is pretty cool but I don't know if it's too complex to make or academically suitable plus they ask us to develop something useful or that helps the scientific community (taking into account the limitations of a highschool student).
* I'm allowed to use data libraries or libraries in general as long as I don't copy-paste the code but again, they value the gathering of data and the idea is to make the neural network myself not just use TensorFlow for example.
* Plz don't kill me for what I'm about to say: I'll just start to learn derivatives in math class on a few months, I know neural networks require them and a strong understanding of math in general but I'm really passionate about this topic and don't mind to learn before my classmates do.
* We have two courses to do this project but our teachers and older classmates recommend us to have it done by the end of summer so that when all the exams come on the following course we just have to finish writing the paper.
* I'm not doing this completely alone, I have a tutor that will help me and grade my work and have a contact working on the local computer science university that's willing to help me with some technical doubts I may have. My tutor is a physics teacher I choose because he is the only teacher that has at least tried to boot up a programming language IDE on my high school and he's the kind of teacher everyone likes, I've been having some problem on explaining what all of this is about so if you could also help with that I'd be really grateful.
* I follow youtube channels like Jabrils and Dan Shiffman from the coding train just in case you wanted to mention them I wanted you to know I know them, also to say that I tried to replicate \[this coding challenge\]([https://www.youtube.com/watch?v=KtPpoMThKUs](https://www.youtube.com/watch?v=KtPpoMThKUs)) and it helped me understand very well how neural networks work but didn't get to make the network itself.

that's all, thanks in advance to all of you, it's my first time posting on this sub so if it doesn't fit here please tell me and if possible in which other sub this would fit.",neuralnetworks,didicayu,False,/r/neuralnetworks/comments/fm5kub/any_ideas_to_make_a_neural_network_prototype_in/
My First Single Neural Network (Perceptron) using JavaScript,1584716447,,neuralnetworks,oFlamingo,False,/r/neuralnetworks/comments/flwf1e/my_first_single_neural_network_perceptron_using/
FlashText : A library faster than Regular Expressions for NLP tasks,1584703211,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/fltda2/flashtext_a_library_faster_than_regular/
This Week in AI - Issue #10 | Rubik's Code,1584694341,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/flrrpp/this_week_in_ai_issue_10_rubiks_code/
"GAN, CNN or Autoencoder to reconstruct missing ECG leads?",1584690971,"I've been looking around on what type of network to recreate a signal(ECG) based on a different signal of the same type(one ECg lead to another ECG lead). Thus, it's not about upscaling or denoising. On the below picture, I have marked 4 red fields. That's the data I have and I intend to recreate the entire picture (probably column 1, then column 2). 

Though, when reading around in articles, I can't judge well why I would choose GAN, CNN or Autoencoders over one another (or a whole different type of network?). My intuition says GAN, because of what I've seen people use it to when recreating something completely new. But I have also seen just a single article try the same as me, and they used a CNN.

https://cdn.discordapp.com/attachments/288390761509027842/688038783395823654/unknown.png",neuralnetworks,TDuncker,False,/r/neuralnetworks/comments/flr6tk/gan_cnn_or_autoencoder_to_reconstruct_missing_ecg/
Elon Musk Neuralink is here,1584689602,,neuralnetworks,isreejesh,False,/r/neuralnetworks/comments/flqy7o/elon_musk_neuralink_is_here/
Backpropigation Help,1584589617,"Hello all, 
I’m currently working on my first neural network; a handwriting recognition program. I currently have the 2 classes created (activation neuron with all 784 weights) and hidden layer neurons which have 30 weights each. I have 2 hidden layers and currently have the structure of the code to process the input and receive the output, but I have no idea how to correctly train a multilayer neural network. Can someone help me with understanding how to train this neural network; and how to properly adjust every single weight?
Thank you",neuralnetworks,WeebDestroyer1000,False,/r/neuralnetworks/comments/fl3d3s/backpropigation_help/
How to find the best learning rate with few experiments?,1584574611,,neuralnetworks,IsoLigt,False,/r/neuralnetworks/comments/fkzpb9/how_to_find_the_best_learning_rate_with_few/
Eliminate Multicollinearity using Lasso Regression (Regularization Methods),1584545317,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/fkrbk2/eliminate_multicollinearity_using_lasso/
PIRL: Learn Image Representations Immune to Geometric Transformation,1584493648,,neuralnetworks,amitness,False,/r/neuralnetworks/comments/fkgx2z/pirl_learn_image_representations_immune_to/
Me at the Zoo Upscaled with an AI,1584493289,,neuralnetworks,KujoReddit00,False,/r/neuralnetworks/comments/fkgtwr/me_at_the_zoo_upscaled_with_an_ai/
"Monna Lisa upscaled, this is not photoshop",1584481720,,neuralnetworks,KujoReddit00,False,/r/neuralnetworks/comments/fkdtbk/monna_lisa_upscaled_this_is_not_photoshop/
Top AI Algorithms in Healthcare,1584348158,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fjhsjf/top_ai_algorithms_in_healthcare/
ELECTRA Pre-training Text Encoders as Discriminators Rather Than Generators (paper explained),1584327749,,neuralnetworks,deeplearningperson,False,/r/neuralnetworks/comments/fjdye0/electra_pretraining_text_encoders_as/
JSML (JAVASCRIPT MACHINE LEARNING),1584292461,"So I created a community about machine learning. The thing is it is all about JavaScript stuff.
THIS COMMUNITY IS JUST STARTING NOW SO WE NEED SOME MODERATORS. YOU COULD BECOME ONE OF THEM.
Join r/jsml NOW!",neuralnetworks,xtpgtpgx,False,/r/neuralnetworks/comments/fj4om8/jsml_javascript_machine_learning/
NN visualization,1584289932,[removed],neuralnetworks,julrog,False,/r/neuralnetworks/comments/fj3zsg/nn_visualization/
Best Programming Languages for AI and Machine Learning,1584199679,,neuralnetworks,Ramesh_Sethi,False,/r/neuralnetworks/comments/fik1zv/best_programming_languages_for_ai_and_machine/
Too Lazy to import numpy &amp; pandas everytime! Here is how Pyforest can help!,1584195540,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/fij1an/too_lazy_to_import_numpy_pandas_everytime_here_is/
Help Needed - Image Labeling Bounty - Compensation Available,1584124383,"&amp;#x200B;

https://preview.redd.it/o4r0i4qkhhm41.png?width=740&amp;format=png&amp;auto=webp&amp;s=69cc794cdbf83d45e408578b2f772d9136bab21a

Greetings! We are offering the opportunity for AI, Machine Learning, and Data Science enthusiasts and professionals to join our image tagging bounty. It is completely free to join, and you can earn compensation (paid in cryptocurrency) for completing the bounty. No previous technical experience is required.  

For this challenge, we are asking the community to appropriately classify a simple image based on what you see within the image. Each image will only a few seconds to complete, and the entire bounty can be completed within a few hours.  You will simply draw a box around the object pictured in the image, and then select from a drop down menu what you see in the image. We would love it if you joined us!  

**Your time is valuable, and that's why we're giving 15,000 KAT to every participant who successfully completes the bounty.**  

**🤖Bounty Details**

👉Registration begins: 7:30 PM UTC+7, March 13th, 2020 

👉Registration closes: 11:59 PM UTC+7 March 27th, 2020 

👉Labeling Period Begins: 00:00 AM UTC+7, March 28th, 2020

👉Labeling Period Deadline: 11:59 PM UTC+7, April 12th, 2020  

&amp;#x200B;

✍️To sign up, visit the official bounty page by visiting  [https://app.kambria.io/bounty/5e68936c54c01e8af7952989](https://app.kambria.io/bounty/5e68936c54c01e8af7952989) 

If you would like to participate in this bounty, please register and sign up at your earliest convenience. **Spots are limited, so do not wait!**",neuralnetworks,Freeme62410,False,/r/neuralnetworks/comments/fi4d3w/help_needed_image_labeling_bounty_compensation/
Not sure if I hate it or love it 🤔,1584064963,,neuralnetworks,JessicaLostInSpace,False,/r/neuralnetworks/comments/fhs4sz/not_sure_if_i_hate_it_or_love_it/
It sure if I hate it or love it 🤔,1584064892,,neuralnetworks,JessicaLostInSpace,False,/r/neuralnetworks/comments/fhs472/it_sure_if_i_hate_it_or_love_it/
Are neural networks just statistic?,1584063688,"I've been doing some of this in college and I have to say I'm a bit disappointed. I expected some magical thing, but it kind of seems like a tool for statistics. If it rained 4/5 last thursdays, the odds of rain next thursday are 80%. Do you need a neural network for that? These two pictures pretty much sum up my thoughts so far:

 [https://images-cdn.9gag.com/photo/aOYA1mE\_460s.jpg](https://images-cdn.9gag.com/photo/aOYA1mE_460s.jpg) 

 [https://miro.medium.com/max/700/1\*x7P7gqjo8k2\_bj2rTQWAfg.jpeg](https://miro.medium.com/max/700/1*x7P7gqjo8k2_bj2rTQWAfg.jpeg)",neuralnetworks,ATCWannabeme,False,/r/neuralnetworks/comments/fhrtyy/are_neural_networks_just_statistic/
StyleGAN2 Distillation for Feed-forward Image Manipulation,1584028942,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/fhilvp/stylegan2_distillation_for_feedforward_image/
Lagrangian Neural Networks,1584028819,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/fhikla/lagrangian_neural_networks/
Stumped by a simple NN problem - intro to AI class and having a hard time understanding how to start this problem...I’m not looking for the the answer but if anyone can provide just the starting point or a resource that can show me the way it would help tremendously.,1583978974,,neuralnetworks,war_duck,False,/r/neuralnetworks/comments/fh8yfl/stumped_by_a_simple_nn_problem_intro_to_ai_class/
Chinese Researchers Use CNNs to Classify 3000-Year-Old Oracle Bone Scripts,1583966048,"To better understand the form of Chinese characters used on oracle bones from over 3,000 years ago, a group of Chinese researchers recently applied a multi-regional convolutional neural network (CNN) to classify oracle bone rubbings. Their study has been [published](https://ieeexplore.ieee.org/document/9004518/keywords#keywords) by journal *IEEE Computer Graphics and Applications*.

Read more: [https://medium.com/@Synced/chinese-researchers-use-cnns-to-classify-3000-year-old-oracle-bone-scripts-b3404e3771d7](https://medium.com/@Synced/chinese-researchers-use-cnns-to-classify-3000-year-old-oracle-bone-scripts-b3404e3771d7)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/fh5sad/chinese_researchers_use_cnns_to_classify/
Tensorflow2.1 code2seq implementation,1583964676,,neuralnetworks,kolkir,False,/r/neuralnetworks/comments/fh5fh4/tensorflow21_code2seq_implementation/
"1D convolution for neural networks, part 9: Stride",1583955239,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/fh2va8/1d_convolution_for_neural_networks_part_9_stride/
Distill's new publication explore Human Interpretable AI,1583882962,,neuralnetworks,Rick_grin,False,/r/neuralnetworks/comments/fgndin/distills_new_publication_explore_human/
Where can I find sources to get deeper understand of neural networks?,1583842293,"I have been using Keras and Tensorflow for the last couple of months and read francois's book - "" Deep Learning with Python"".

&amp;#x200B;

Anyway at this point I still feel like I'm playing with lego, but I want better and deeper understanding.

I have the mathematical background and knowledge, but I need good sources to look for.

I prefer some books or papers and not just summarized YouTube videos.",neuralnetworks,new_one_7,False,/r/neuralnetworks/comments/fgcu3s/where_can_i_find_sources_to_get_deeper_understand/
Visual Summary of SimCLR: State of the Art Self-Supervised Learning Method,1583804666,,neuralnetworks,amitness,False,/r/neuralnetworks/comments/fg63zk/visual_summary_of_simclr_state_of_the_art/
"1D convolution for neural networks, part 1: Sliding dot product",1583767420,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ffwa96/1d_convolution_for_neural_networks_part_1_sliding/
Custom Keras Callback for visualization,1583756696,"Normally, inorder to visualize the training steps of a tf.keras model, we use Tensorboard callback. But having Tensorboard open while training can take up a lot of RAM. Also, in order to view the graph, we need to run the server everytime. Another way would be to use the history object returned by .fit() method. We need to manually plot them once training is over. Also, it only returns values for every epoch, and not for every batch.

I have made a Custom Keras Callback ( [GitHub link](https://github.com/Gautam-J/Other-Python-Scripts/tree/master/Custom Keras Callback/utils.py) ), that tracks metrics per batch, and automatically plots them, and saves it as a .PNG file. It also saves the model automatically, once training is over. This is useful, if you want to explicitly stop your training process after some time. Even you press Ctrl+break / Ctrl+C during training, it still plots the data, saves the graph and also saves the model before closing the program. I personally find this to be really useful, as I don't have to write a separate code for plotting my training curves.",neuralnetworks,Gautam-j,False,/r/neuralnetworks/comments/fftv1x/custom_keras_callback_for_visualization/
What do you consider relevant criteria when comparing NN frameworks?,1583744780,"I have just started working on a thesis, in which I've set out to compare the neural network frameworks that are in use nowadays. After reading a couple very uninformative articles online, I am not quite sure how to properly go about it. In the articles, the differences between frameworks were described very vaguely, as if written by somehow who doesn't actual do anything computer science related and only's heard some of the fancy sounding words.

Anyway, if you were to work on a new project and had to choose a framework to use, based on what parameters and qualities would you make your choice? And can NN frameworks even be fairly compared without specifying, what exactly the framework is going to be used for? I imagine different frameworks have different performance in different types of NNs.

I am pretty new to NNs in general, so I would appreciate your advice!",neuralnetworks,PanVidla,False,/r/neuralnetworks/comments/ffrwse/what_do_you_consider_relevant_criteria_when/
The Overview of Artificial Intelligence in Medicine,1583743361,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ffrp9c/the_overview_of_artificial_intelligence_in/
The Neurobaby Project - Open development of Artificial General Intelligence (AGI),1583732953,"Tired of reading about all the progress that Google, Deepmind and a host of billion dollar companies are making with respect to Artificial General Intelligence and not being able to contribute?

Why should they have all the fun?

[The Neurobaby Project](https://www.reddit.com/r/Neurobaby_AGI/) subreddit is a place that is open to everyone who wants to help.  It provides a roadmap with semi-structured problems that when completed will hopefully result in AGI.  

We are on the verge of getting there but we need new architectures and new ideas. Let's figure us out.

Come help me make a baby!",neuralnetworks,mustgoplay,False,/r/neuralnetworks/comments/ffq3pu/the_neurobaby_project_open_development_of/
"Ultra-precise, mind-controlled prosthetic hand for amputees via RPNI neural interface",1583510672,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/fefk2i/ultraprecise_mindcontrolled_prosthetic_hand_for/
SLIDE is first algorithm for training deep neural nets faster on CPUs than GPUs,1583509706,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/fefbbz/slide_is_first_algorithm_for_training_deep_neural/
"Does any one know how to draw neural network of the below equation? I have to show the network, the weights and bias. Also any link where I can learn how to draw neural network.",1583489564,,neuralnetworks,Tab_IM,False,/r/neuralnetworks/comments/febh7j/does_any_one_know_how_to_draw_neural_network_of/
This Week in AI - Issue #8 | Rubik's Code,1583488909,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/febdoe/this_week_in_ai_issue_8_rubiks_code/
Human Experimentation in Philadelphia,1583464428,,neuralnetworks,symmetryskew,False,/r/neuralnetworks/comments/fe794z/human_experimentation_in_philadelphia/
Need brain tumour datasets to create CNN for my final year project. Please links to download directly..,1583423468,,neuralnetworks,Sharling375,False,/r/neuralnetworks/comments/fdx63x/need_brain_tumour_datasets_to_create_cnn_for_my/
How to filter pictures using a convolutional neural network?,1583422563,"Hi all, 

I'm kinda stuck on a step in my project and would like some insights. For my project I have to classify passenger trains, freight trains and no trains from webcam pictures. The webcam is static so it never moves and it saves pictures every second regardless if a train passed. So, the saved pictures are roughly 90-95% with no trains. Right now, I'm trying to just sort pictures with and without train. My guess is I have to overfit because of the static webcam? The network **must** know the difference of the environment of the webcam pictures without or with train. The trees, houses, roads, signs, ect... on the pictures **never** changes regardless if a train passes. But overfitting is horrible and sorting the trains in passenger and freight. How do I approach this?",neuralnetworks,TomasoJew,False,/r/neuralnetworks/comments/fdwxtu/how_to_filter_pictures_using_a_convolutional/
"Does any one know how to draw neural network of the below equation? It was my exam question and I have no idea how to solve it. I have to show the network, the weights and bias.",1583415423,,neuralnetworks,Tab_IM,False,/r/neuralnetworks/comments/fdvcgp/does_any_one_know_how_to_draw_neural_network_of/
Sharing a nice site that is friendly for beginners on ANN and ML and more,1583406933,,neuralnetworks,rabbitcarrots,False,/r/neuralnetworks/comments/fdtrzs/sharing_a_nice_site_that_is_friendly_for/
Seeking Microscopic Image Classifier,1583353626,"I have been taking pictures with my hobby-level microscope for a couple of years. Each time, I have to refer to online sources to first classify, then identify all of the crazy junk I put under the lens. I have also been interested in CNNs, and open source deep learning platforms like MobileNet, and ImageNet. 

I am an amateur programmer, familiar with flavors of C, java, and various scripting languages. I have tinkered with some fun little neural networks in Processing. I am confident that I can hack together something that will work, but I want to focus my efforts.

I would like to take the core of the MobileNet classifier, and train it on microscopic images. First on categories, like Diatom, Pollen, Bacteria, etc. Eventually I would continue the training with additional shapes, and classification categories. The goal would be to feed in an image, and get a list of cropped images out along with the classification that the AI chose.

Where do I start with this? Are there better open source packages that are pre-trained on a microscopic image database?

Thanks for your time!",neuralnetworks,DM-Norritt,False,/r/neuralnetworks/comments/fdj59v/seeking_microscopic_image_classifier/
Build a First Neural Network,1583346988,,neuralnetworks,OpenDataSciCon,False,/r/neuralnetworks/comments/fdhgmv/build_a_first_neural_network/
What Algo to use in this case,1583340169,[removed],neuralnetworks,abdelmajidlra1,False,/r/neuralnetworks/comments/fdfp8w/what_algo_to_use_in_this_case/
"Machine learning beats BTC/USDT on unseen data, even with transaction fees and slippage (article) + free dataset that used to be behind paywall (data)",1583282120,"**This is the link to Towards Data Science:**  
[https://towardsdatascience.com/machine-learning-beats-btc-usdt-on-unseen-data-even-with-transaction-fees-and-slippage-caa5e7a40caf?source=friends\_link&amp;sk=8feb7976e93ae96f024e289d5294c4ea](https://towardsdatascience.com/machine-learning-beats-btc-usdt-on-unseen-data-even-with-transaction-fees-and-slippage-caa5e7a40caf?source=friends_link&amp;sk=8feb7976e93ae96f024e289d5294c4ea)

**This is a link to kaggle** with a dataset (BTC/USDT data) that used to be behind paywall (9k euro on sites like kaiko) but I am releasing it now for free - my way to say thank you to this community.

[https://www.kaggle.com/michalbalcerak/1min-btcusdt-hitbtc-data-with-volumes-and-summary](https://www.kaggle.com/michalbalcerak/1min-btcusdt-hitbtc-data-with-volumes-and-summary)",neuralnetworks,Reddit_Rabbit_Cat,False,/r/neuralnetworks/comments/fd4knt/machine_learning_beats_btcusdt_on_unseen_data/
A great explanation of the Maths behind the Perceptron/ Neural net,1583147752,,neuralnetworks,alexcpn,False,/r/neuralnetworks/comments/fcaloo/a_great_explanation_of_the_maths_behind_the/
Top 3 Artificial Intelligence Research Papers – February 2020,1583136697,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/fc8xz9/top_3_artificial_intelligence_research_papers/
[D] Sentiment analysis image,1583129597,"Hi , 
I am trying sentiment analysis of images.
I have 4 classes - Hilarious , funny very funny not funny.
I tried pre trained models like VGG16/19 densenet201 but my model is overfitting getting training accuracy more than 95% and testing around 30 
Can someone give suggestions what else I can try?
Training images - 6K",neuralnetworks,ambuje12,False,/r/neuralnetworks/comments/fc7sjb/d_sentiment_analysis_image/
Thoughts on the ONNX format?,1583052948,"Hi all!, So i prefer training/creating my models in PyTorch over TenserFlow hovewer most places use TenserFlow for production and also i'd like to use my model in many frameworks like ML.net, the solution for this is to convert your models to ONNX format, the thing is, how ""good"" is this format, what's the % of models that i can convert to fully work in this open format?",neuralnetworks,Flamyngoo,False,/r/neuralnetworks/comments/fbrg4h/thoughts_on_the_onnx_format/
How can I improve the test accuracy of my CNN in PyTorch?,1583000294,"I'm a beginner with PyTorch and ML and I would like to know the techniques and strategies used to improve the network performance on the test dataset.

Currently, I have two network architecture:

1 - ConvNet1

    # experiment 1
    # 3 convolutional layers and 2 linear layers
    class ConvNet1(nn.Module):
        
        def __init__(self, num_classes=10):
            super(ConvNet1, self).__init__()
    
            self.layer1 = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
            
            self.layer2 = nn.Sequential(
                nn.Conv2d(16, 24, kernel_size=4),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
    
            self.layer3 = nn.Sequential(
                nn.Conv2d(24, 32, kernel_size=4),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
    
            self.dropout = nn.Dropout2d(p=0.3)
            
            self.fc1 = nn.Linear(32*29*29, 120)
    
            self.relu = nn.ReLU()
    
            self.fc2 = nn.Linear(120, 10)
    
            
        def forward(self, x):
    
            x = self.layer1(x)
    
            x = self.layer2(x)
    
            x = self.layer3(x)
    
            # print(out.shape)
            
            x = x.view(-1, 32*29*29)
    
            x = self.fc1(x)
    
            x = self.relu(x)
    
            x = self.fc2(x)
    
            return x
    
 and

2 - ConvNet2

    # experiment 2
    # 1 convolutional layer and 1 linear layer
    class ConvNet2(nn.Module):
    
        def __init__(self, num_classes=10):
            super(ConvNet2, self).__init__()
    
            self.layer1 = nn.Sequential(
                nn.Conv2d(3, 16, kernel_size=3),
                nn.ReLU(),
                nn.MaxPool2d(kernel_size=2),
                nn.Dropout2d(p=0.3))
    
            self.fc1 = nn.Linear(258064, 120)
    
        def forward(self, x):
            x = self.layer1(x)
    
            x = x.view(-1, 16 * 127 * 127)
    
            x = self.fc1(x)
    
            return x

Surprisingly, the ConvNet2 network performs much better than ConvNet1 even if its architecture is simpler. When I train for 10 epochs, ConvNet1 has 41% accuracy and ConvNet2 has 78%. Not really sure why, though.

What would you do to ConvNet2 (or ConvNet1?) to improve its accuracy?",neuralnetworks,pythonistaaaaaaa,False,/r/neuralnetworks/comments/fbgla6/how_can_i_improve_the_test_accuracy_of_my_cnn_in/
Conditioning versus multi-term loss thoughts?,1582912618,"Let's say I have a problem where I am trying to predict two continuous values given the same input. It seems there are two ways I could do this, either I could have two output nodes and use some loss function to aggregate their respective loss, or I could ""condition"" by having one output but placing the property of interest (perhaps a binary label to differentiate) as a feature to guide the model towards predicting one of the properties. Does anyone know of any literature that explores these differences? It would seem the second case would give you more predictive power as you are now able to tell earlier layers the explicit conditional information, while in the first case all of the features will need be shared between the two properties and the differentiation comes at the end. Thanks for any thoughts!",neuralnetworks,dlovelan,False,/r/neuralnetworks/comments/faypcv/conditioning_versus_multiterm_loss_thoughts/
This Week in AI – Issue #7 | Rubik's Code,1582879487,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/farsvf/this_week_in_ai_issue_7_rubiks_code/
Tesseract or neural network to identify handwritten and computer types images?,1582724525,"I am wanting to use my computer to recognize numbers from images.

Those images are both computer generated (for example a screenshot of a number) or handwritten (for example a picture of some handwriting on as piece of paper).

I am somewhat familiar with using tensorflow + keras with python to build a model to recognize hand written digits. And I have also briefly used tessaract for OCR purposes.

**Question:**

&gt;Is it possible to use one method that suited to recognizing both hand written and computer generated numbers? If so which one?",neuralnetworks,easycub2,False,/r/neuralnetworks/comments/f9tdwc/tesseract_or_neural_network_to_identify/
FastMRI leverages adversarial training to remove image artifacts,1582716322,,neuralnetworks,PowerOfLove1985,False,/r/neuralnetworks/comments/f9rt3w/fastmri_leverages_adversarial_training_to_remove/
CNN - Convolution output question,1582693766,"First time poster here.

Say I have an image 28x28 pixels (MNIST) and two filter layers, first being 20x3x3 and second being 10x3x3.

The first convolutional operation will result in an ouput of shape 20x28x28 (if using padding), correct?

Then the shape is 20x14x14 after maxpooling.

The second convolutional operation output is what shape? 10x20x14x14?",neuralnetworks,xakumazx,False,/r/neuralnetworks/comments/f9o5df/cnn_convolution_output_question/
PyTorch C++ Tutorials from Beginner to Advanced,1582684831,"Link: [https://github.com/prabhuomkar/pytorch-cpp](https://github.com/prabhuomkar/pytorch-cpp)

People who want to learn more, please mark down issues of tutorials they need. We are in the process of adding more tutorials for this :)

 Mark down on issues with a bug report if you face any challenges while running the code.

Also, please show some love by starring or forking ⭐️",neuralnetworks,op_prabhuomkar,False,/r/neuralnetworks/comments/f9m76w/pytorch_c_tutorials_from_beginner_to_advanced/
Deep Learning | How to Train your Deep Neural Network?,1582632527,,neuralnetworks,gajanand_edu,False,/r/neuralnetworks/comments/f99b67/deep_learning_how_to_train_your_deep_neural/
T5 is the new SoTA NLP model by Google: Text-To-Text Transfer Transformer,1582598948,,neuralnetworks,Rick_grin,False,/r/neuralnetworks/comments/f939p9/t5_is_the_new_sota_nlp_model_by_google_texttotext/
Code Review Help!,1582577430,"I am competing in the Numerai tournament as a way to get better at coding, and I have developed a very elementary neural network. Can someone with a little more do review it and let me know your thoughts? Thank you in advance!!

[GitHub Page](https://github.com/ethantjones97/son_of_big_e/blob/master/numerai1.ipynb)",neuralnetworks,Big_E_Jones,False,/r/neuralnetworks/comments/f8xwf2/code_review_help/
Deploying Machine Learning Models – pt. 3: gRPC and TensorFlow Serving,1582533361,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/f8oden/deploying_machine_learning_models_pt_3_grpc_and/
Using Matlab for ANN - How to get weights from the GUI from data already saved?,1582466797,"Hello everyone. How are you? 

I am new to ANN. So my problem is: I already trained and gotten by outputs, and errors, and evaluating accuracy using root mean square error (RSME) and correlation (Rsq) by looking at the different number of neurons to get the lowest error. 

My issue is: I already did this, and I did not realize you can get weights and bias from it. 

I used Matlab 2018a, and the nntool function. 

I already did my dataset months ago and I was wondering if there is a way to determine the weights and biases when I already trained the network. 

All the scientific papers I read, they did not explain this or include this. There was a random paper, from a journal where people wouldn't normally cite from, due to low or nonexistent impact factor (IF) who made sure to put this in. And it makes sense to include it. 

Any advice is appreciated. Thank you.",neuralnetworks,negentropicentity,False,/r/neuralnetworks/comments/f89ulh/using_matlab_for_ann_how_to_get_weights_from_the/
What is the best way to train images that have text written on it? Ask,1582455615,[removed],neuralnetworks,ambuje12,False,/r/neuralnetworks/comments/f882ua/what_is_the_best_way_to_train_images_that_have/
Look back window,1582451207,Im doing a school project on sfock prediction and i need to use look back windows in my model. Im wondering how that can be done. I cant find anything on google,neuralnetworks,tinashejm4,False,/r/neuralnetworks/comments/f87h9b/look_back_window/
How to use neural network to generate a (near) real image of a black hole,1582407555,,neuralnetworks,amin_mlm,False,/r/neuralnetworks/comments/f7zbc7/how_to_use_neural_network_to_generate_a_near_real/
Why is my neural network not affected by inputs?,1582395354,"Hi there! I'm currently working on a school project and I would really appreciate any advice I could get.
I have been trying to program a neural network. In my program, each element of the output array represents a move a player can take. The greatest value is the move that is made.
The values of the output do consistently vary when the input changes. However, no matter what the input is, one output element is always greatest, even if their numerical values do change, meaning the player consistently repeats the same motion. 

Here is my neural network code:


        
    class NeuralNetwork:
    
    #depths is an array containing the number of nodes in each layer
        def __init__(self, depths):
    
            self.depths = depths
            self.weights = []
            self.biases = []
    
    #initalises weights and biases
            for i in range(len(depths)-1):
                self.weights.append(np.random.rand(depths[i],depths[i+1]))
    
            for i in range(len(self.depths)):
                self.biases.append(np.random.rand(depths[i]))
    
        def feedforward(self, input):
            output = input
    
    #iterates through weights and biases, multiplying and adding
            for i in range(len(self.depths)-1):
                output = np.dot(output,self.weights[i])
                output += self.biases[i+1]
    
            return output
    
    
    #NN represents a neural network with an input of size two, a hidden layer with two nodes, and an output of size one
    depths = [2,2,2]
    NN = NeuralNetwork(depths)
    
    input = [0.5,0.5]
    output = NN.feedforward(input)

If I change input, or use a new randomised network, one element in output always remains largest.

I would really appreciate if anyone would be willing to take the time to review what I've done here (I've had no experience doing this before) and possibly given an explanation as to why this outcome is occurring.",neuralnetworks,acousticewe,False,/r/neuralnetworks/comments/f7wcgi/why_is_my_neural_network_not_affected_by_inputs/
Is there some maximum height of stacked LSTMs before it’s pointless?,1582344188,I see a lot of posts about stacking 3 or 4 layers. Does it ever make sense to go really deep with it? Maybe 10 layers?,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/f7n94w/is_there_some_maximum_height_of_stacked_lstms/
This Week in AI – Issue #6 | Rubik's Code,1582275580,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/f78hay/this_week_in_ai_issue_6_rubiks_code/
Text book recommends?,1582240905,"Hi,

I am looking for a recommendation for 3 types of books.

1. general Deep learning
2. specialized for computer vision deep learning
3. multi-agent systems ML

Any or all recommendations welcome :-)",neuralnetworks,bardpeter,False,/r/neuralnetworks/comments/f71sp1/text_book_recommends/
Multi Matrix Deep Learning with GPUs,1582198018,,neuralnetworks,Albertchristopher,False,/r/neuralnetworks/comments/f6rqnq/multi_matrix_deep_learning_with_gpus/
Outputting the details (e.g. weights) of a neural network (R/python),1582081793,"Does anyone know if its possible to produce a summary file that displays the details of all the neurons, weights, layers of the neural network? As well as performance stats (convergence)? Thanks!",neuralnetworks,jj4646,False,/r/neuralnetworks/comments/f648ie/outputting_the_details_eg_weights_of_a_neural/
Deploying Machine Learning Models – pt. 2: Docker &amp; TensorFlow Serving,1581928732,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/f55ulr/deploying_machine_learning_models_pt_2_docker/
Genetic Algorithm teaches a Neural Network to beat a 2D car game,1581917992,"Hi, I created a 2D car game in pyglet, python for learning algorithms to learn to play it. Then I created a genetic algorithm to teach a neural network to learn to play it. The neural network inputs are the wall distances around the car, as a lidar would see on the top of the car. It outputs the moving directions. Let me know your thoughts.
https://github.com/swordey/Genetic2DCarGame",neuralnetworks,swordythomas,False,/r/neuralnetworks/comments/f53zxo/genetic_algorithm_teaches_a_neural_network_to/
Is it possible to re-train an existing weight to your custom class?,1581861833,I'm doing a project on Darkflow using yolov2 and it could be a problem if it can't be done.,neuralnetworks,owlletyouknow,False,/r/neuralnetworks/comments/f4r1d5/is_it_possible_to_retrain_an_existing_weight_to/
Deep learning: How to estimate the weight and the failure load of a spaghetti bridge,1581802171,,neuralnetworks,amin_mlm,False,/r/neuralnetworks/comments/f4g2ji/deep_learning_how_to_estimate_the_weight_and_the/
Speedup Pandas 10x using Python’s Datatable package,1581772421,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/f498pw/speedup_pandas_10x_using_pythons_datatable_package/
"Are neural networks in use in the weather industry, yet?",1581692610,To predict weather and such.,neuralnetworks,sunwoo-yang,False,/r/neuralnetworks/comments/f3tdop/are_neural_networks_in_use_in_the_weather/
"Classify Texts with TensorFlow and Twilio to Answer Loves Me, Loves Me Not",1581539850,,neuralnetworks,lizziepika,False,/r/neuralnetworks/comments/f2xner/classify_texts_with_tensorflow_and_twilio_to/
How to recognize overlapping digits?,1581404765,"I've got a set of images with overlapping digits which need to be recognized. The task seems a good fit for neural networks but the issue is that they are used to have inputs as single digits but in case of overlapping digits it's impossible to extract each digit separately.

Is it possible at all to recognize images like this?

Here are examples of images:
[1](https://ibb.co/hLhmS7m)
[2](https://ibb.co/CmQ50rD)
[3](https://ibb.co/zhZK43V)
[4](https://ibb.co/wQyVh7h)",neuralnetworks,_WhiteAngel_,False,/r/neuralnetworks/comments/f24t5g/how_to_recognize_overlapping_digits/
The mathematics for behind RNAs,1581396558,"Hello guys, I'm very fascinated for the ideia of replicate the neural architeture (even being rudimentary comparatively), but I actually interested in how the mathematics is important in this research field, beacause I see a lot tutorials teachin things like: ""how to make a neural networks for recognition of cats using that model"" and do not explain the background this algorithms(proofs mathematics of how it works).

Basically my question is how much mathematics is nescessary for develop computer system using RNNs and create new architeturals models and if we are coming to plateau for the models ?

^(sorry my english, I'm still learn...)",neuralnetworks,NetonD,False,/r/neuralnetworks/comments/f23c3k/the_mathematics_for_behind_rnas/
"DeepSpeed - deep learning optimization library that makes distributed training easy, efficient, and effective",1581384879,,neuralnetworks,Jewpiter,False,/r/neuralnetworks/comments/f20u4y/deepspeed_deep_learning_optimization_library_that/
Faster R-CNN,1581361014,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/f1ux9s/faster_rcnn/
MIT Introduction to Deep Learning | 6.S191,1581360977,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/f1uwy4/mit_introduction_to_deep_learning_6s191/
Fast and Efficient Training of Neural Networks (feat. CodeEmporium),1581360930,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/f1uwi5/fast_and_efficient_training_of_neural_networks/
Architecture ideas needed,1581332399,,neuralnetworks,paland3,False,/r/neuralnetworks/comments/f1oqnx/architecture_ideas_needed/
Can someone give a explanation of this Tweet from Elon Musk in detail?,1581325064, [https://twitter.com/elonmusk/status/1224182478501482497?ref\_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1224182478501482497&amp;ref\_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps%253A%252F%252Ftwitter.com%252Felonmusk%252Fstatus%252F1224182478501482497%26widget%3DTweet](https://twitter.com/elonmusk/status/1224182478501482497?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1224182478501482497&amp;ref_url=https%3A%2F%2Fpublish.twitter.com%2F%3Fquery%3Dhttps%253A%252F%252Ftwitter.com%252Felonmusk%252Fstatus%252F1224182478501482497%26widget%3DTweet),neuralnetworks,concard88,False,/r/neuralnetworks/comments/f1nphs/can_someone_give_a_explanation_of_this_tweet_from/
Deploying Machine Learning Models – pt. 1: Flask and REST API,1581323352,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/f1nh6l/deploying_machine_learning_models_pt_1_flask_and/
AI Platform,1581322820," 

Recently, Google dedicates considerable attention to its AI tools along with its new end-to-end AI platform. The new platform gives users vast opportunities within the whole lifecycle of Machine Learning project.

[https://cloud.google.com/ai-platform/](https://cloud.google.com/ai-platform/)",neuralnetworks,alfi_solomons_,False,/r/neuralnetworks/comments/f1nejf/ai_platform/
Understanding Region of Interest — Part 2 (RoI Align and RoI Warp),1581270051,,neuralnetworks,burnpiro,False,/r/neuralnetworks/comments/f1c06b/understanding_region_of_interest_part_2_roi_align/
Recommend me a book about neural network please,1581263845,"I read this article (https://medium.com/coinmonks/the-mathematics-of-neural-network-60a112dd3e05) and I love how the guy explained how neural networks works without treating it as blackbox system. I am no expert in math, but I think I need to learn about neural network more by looking clearly on how neural network works even on math.",neuralnetworks,whereistimbo,False,/r/neuralnetworks/comments/f1ahnv/recommend_me_a_book_about_neural_network_please/
Speedup Python Pandas with RAPIDS GPU-Accelerated Dataframe Library called cuDF on Google Colab!,1581145022,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/f0o32o/speedup_python_pandas_with_rapids_gpuaccelerated/
Understanding GauGAN: Debugging Training &amp; Deciding If GauGAN Is Right For You,1581110744,"This is the last part in a four part series on GauGAN (find parts 1-3 [here](https://blog.paperspace.com/)). We'll look at common issues with training and how to handle them, and how to decide whether GauGAN can suit your needs from a business perspective.

Article link:  [https://blog.paperspace.com/debugging-gaugan-training-and-business-considerations/](https://blog.paperspace.com/debugging-gaugan-training-and-business-considerations/) 

You can also test out GauGAN with free GPUs at: [https://ml-showcase.paperspace.com/projects/gaugan](https://ml-showcase.paperspace.com/projects/gaugan)",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/f0h6vg/understanding_gaugan_debugging_training_deciding/
This Week in AI – Issue #4 | Rubik's Code,1581066141,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/f07yo5/this_week_in_ai_issue_4_rubiks_code/
A Visual explanation of Region of Interest (RoI) from Fast R-CNN,1581018106,,neuralnetworks,burnpiro,False,/r/neuralnetworks/comments/ezxpmg/a_visual_explanation_of_region_of_interest_roi/
What's Next After NN and DL?,1580880417,"I just watched this:  [https://www.youtube.com/watch?v=bfmFfD2RIcg](https://www.youtube.com/watch?v=bfmFfD2RIcg)   
I always want to know what comes after when learning the present so what's next after neural networks? After deep learning? To pose my challenge, I read on quora that we don't know but...really?",neuralnetworks,PixieDust404,False,/r/neuralnetworks/comments/ez4fxg/whats_next_after_nn_and_dl/
Someone used neural networks to upscale a famous 1896 video to 4k quality,1580859218,,neuralnetworks,khunshan,False,/r/neuralnetworks/comments/eyzfjj/someone_used_neural_networks_to_upscale_a_famous/
Understanding GauGAN: Model Evaluation Techniques &amp; Comparative Performance,1580742545,"One of the hardest things about working with GANs is their evaluation. In this post we'll look at different evaluation metrics for GauGAN, how to calculate the Frechet Inception Distance (FID), and how GauGAN performs compared to other algorithms like Pix2PixHD, SIMS, and CRN.

Article link: [https://blog.paperspace.com/gaugan-evaluation-techniques](https://blog.paperspace.com/gaugan-evaluation-techniques)

This is the third part in a four-part series on GauGAN. For an in-depth look at the GauGAN architecture and its loss functions, check out [Part 1](https://blog.paperspace.com/nvidia-gaugan-introduction/) of the series. [Part 2](https://blog.paperspace.com/gaugan-training-on-custom-datasets/) covers adapting GauGAN to train on your own custom datasets.",neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/ey8gbd/understanding_gaugan_model_evaluation_techniques/
Top 3 Artificial Intelligence Research Papers – January 2020,1580718070,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ey3yta/top_3_artificial_intelligence_research_papers/
Are neural networks able to predict quantitative outputs?,1580697863,"I'm working on a research project in which I'm researching different ML algorithms and then implementing them into an algorithm to predict MLB scores. Before I invest time into learning about neural nets, I wanted to check to see if this was a realistic goal.

&amp;#x200B;

TIA",neuralnetworks,a1_jakesauce_,False,/r/neuralnetworks/comments/ey01fx/are_neural_networks_able_to_predict_quantitative/
Deep dream web interface,1580506601,,neuralnetworks,AutismMDMA_LSD,False,/r/neuralnetworks/comments/ewvezy/deep_dream_web_interface/
[News] DeepMind &amp; UCL Introduce New Model and Test Set for Inference,1580499703,"Now, DeepMind and University College London (UCL) have introduced a new deep network called MEMO which matches SOTA results on Facebook’s bAbI dataset for testing text understanding and reasoning, and is the first and only architecture capable of solving long sequence novel reasoning tasks.  
Read more: [DeepMind &amp; UCL Introduce New Model and Test Set for Inference](https://medium.com/syncedreview/deepmind-ucl-introduce-new-model-and-test-set-for-inference-582d9446e66e)",neuralnetworks,rockyrey_w,False,/r/neuralnetworks/comments/ewtpmc/news_deepmind_ucl_introduce_new_model_and_test/
This Week in AI – Issue #3 | Rubik's Code,1580457470,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ewl3jr/this_week_in_ai_issue_3_rubiks_code/
Adaptive Compute Recurrent Neural Network Tutorial in PyTorch,1580261078,,neuralnetworks,clam004,False,/r/neuralnetworks/comments/evfgnq/adaptive_compute_recurrent_neural_network/
If Van der Waals was a neural network,1580237460,,neuralnetworks,elcric_krej,False,/r/neuralnetworks/comments/ev9nwe/if_van_der_waals_was_a_neural_network/
"I found this site while searching, and I wanted to know if the algorithms used where good for general use of neuronal network?",1580236035,,neuralnetworks,H-L_echelle,False,/r/neuralnetworks/comments/ev9bnx/i_found_this_site_while_searching_and_i_wanted_to/
Using saved weights on different NN structures?,1580222397,"Hey everyone, my question may have an obvious answer but here it goes. 

Suppose you have two NN's (N1 and N2) which are very similar, but differ in the dimension of the output layer (say we've added a single output). 

Would it be possible to train N1 and use those weights on N2? Some edges in N2 would have no weight, but I'm thinking that the weights might still be useful. I'm working on a very fluid project and I am constantly changing the structure of the NN, as well as the entire environment. 

Thanks in advance!",neuralnetworks,SupMathematician,False,/r/neuralnetworks/comments/ev6emi/using_saved_weights_on_different_nn_structures/
Keras-rl: Input Format for observations,1580199122,"Hello everybody,

I am quite new to the topic of neural networks and I have recently started with my first project in this area and maybe you could help me with a question in regards to the input format for my observations

Would it be better for the model if I would have a observation which is implemented as one integer variable which can have the values from 1 to 20 or would it be better to implement 20 bool variables which can have the values 1 or 0?   

I am using the following code

`self.model = Sequential()`

`self.model.add(Dense(512, activation='relu', input_shape=env.observation_space))`

`self.model.add(Dropout(0.2))`

`self.model.add(Dense(512, activation='relu'))`

`self.model.add(Dropout(0.2))`

`self.model.add(Dense(512, activation='relu'))`

`self.model.add(Dropout(0.2))`

`self.model.add(Dense(nb_actions, activation='linear'))`",neuralnetworks,Matze999,False,/r/neuralnetworks/comments/ev2pzd/kerasrl_input_format_for_observations/
Wrapping Your Head Around Gradient Descent (with comics!),1580176096,,neuralnetworks,flytehub,False,/r/neuralnetworks/comments/euyj0x/wrapping_your_head_around_gradient_descent_with/
Double DQN with TensorFlow 2 and TF-Agents,1580114139,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/eul8xv/double_dqn_with_tensorflow_2_and_tfagents/
Updating Biases,1580078244,"I found a paper that details a neural network for playing checkers that I have been trying to implement; however, I'm having trouble with how to update the biases. What would be the best way to go about this? Also, do I understand correctly that each weight and bias has its own self-adaption perimeter? The paper can be found here [http://www.cs.nott.ac.uk/\~pszgxk/courses/g5baim/papers/checkers-002/TNNKChellapillaAndDBFogelText.pdf](http://www.cs.nott.ac.uk/~pszgxk/courses/g5baim/papers/checkers-002/TNNKChellapillaAndDBFogelText.pdf).",neuralnetworks,ThomasCornwall,False,/r/neuralnetworks/comments/euea5g/updating_biases/
DeepMind’s Alternative to Backpropogation seems like it will transform the way we create neural networks.,1580018270,"Google’s latest DeepMind innovation is super interesting. Sideways introduces an alternative to backpropagation which is much more efficient. I’m curious to see how accurate the new method is for applications other those mentioned in the article.

Does anyone here have any experience with it or can speak to its potential in other applications?

[This is where I read about it. ](https://www.zdnet.com/google-amp/article/google-deepminds-sideways-takes-a-page-from-computer-architecture/)",neuralnetworks,khaloffle,False,/r/neuralnetworks/comments/eu2ywa/deepminds_alternative_to_backpropogation_seems/
Understanding GauGAN Part 2: How To Train GauGAN on Custom Datasets,1579910866, [https://blog.paperspace.com/gaugan-training-on-custom-datasets/](https://blog.paperspace.com/gaugan-training-on-custom-datasets/),neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/etj2af/understanding_gaugan_part_2_how_to_train_gaugan/
Artificial intelligence to improve resolution of brain magnetic resonance imaging,1579900418,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/etgpwm/artificial_intelligence_to_improve_resolution_of/
This Week in AI – Issue #2,1579853637,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/et7hx0/this_week_in_ai_issue_2/
YOLACT++,1579782222,,neuralnetworks,dvof,False,/r/neuralnetworks/comments/esscl5/yolact/
Plotting hidden layer output,1579776707,What is the best way to visualise the activation output of each hidden layer?,neuralnetworks,saladbowltie,False,/r/neuralnetworks/comments/esrh77/plotting_hidden_layer_output/
Just crossposting in case someone here can help out :),1579766673,,neuralnetworks,new_login_who_dis,False,/r/neuralnetworks/comments/esq2xr/just_crossposting_in_case_someone_here_can_help/
An Intuitive Explanation of Convolutional Neural Networks,1579764544,,neuralnetworks,Aspentatious,False,/r/neuralnetworks/comments/esps84/an_intuitive_explanation_of_convolutional_neural/
Reformer: The Efficient Transformer,1579713265,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/esezdp/reformer_the_efficient_transformer/
Jupyter Notebook Kernel Dying Running CIFAR Image Classification,1579708108,"I am working through this CIFAR Image Classification model in Jupyter Notebook and when I try and run the following code the kernel dies every time. 

    cnn_model.fit(X_train, y_train, batch_size=5, epochs=5, shuffle=True) 

**I am getting this error:**

    I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: SSE4.1 SSE4.2 AVX AVX2 FMA
    To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.

I tried the following but it didn't work 

    import os 
os.environ[""CUDA_VISIBLE_DEVICES""]=""-1""     
import tensorflow as tf
    

Does anyone know how to resolve this? I am using a MacBook Pro Mid 2014. 

Any help or a little direction would be amazing!",neuralnetworks,JessieGibson,False,/r/neuralnetworks/comments/esdtmm/jupyter_notebook_kernel_dying_running_cifar_image/
"When I was learning machine learning for the first time, the exact manner in which convolutional neural networks worked always evaded me, largely because they were only ever explained at an introductory level in tutorials. So, I made an animated video explaining exactly how CNNs work. Hope it helps!",1579703022,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/escmlr/when_i_was_learning_machine_learning_for_the/
A.I. Generated News - We Can't Afford to Poop on our own Homes,1579694851,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/esb0ap/ai_generated_news_we_cant_afford_to_poop_on_our/
Trying to program a simple neural network in C,1579665013,"Hi,

I would like to program a simple neural network in C so I can use it for signal detection on a DSP. I'm currently a beginner so I have no idea how to make it works. Is there any good ressources I can learn from so I can make this simple project? Where can I start?

Thanks for the help!",neuralnetworks,Heimdell_Irsei,False,/r/neuralnetworks/comments/es6e4w/trying_to_program_a_simple_neural_network_in_c/
Which kind of Artificial Neural Network is the best for analyzing and/or predicting volcanoes seismic signals?,1579657947,I am working on a project in which we want to treat seismic data for a volcano to be able to determine potential hazards. We are beginners in this area and we are not sure which topology or which type of ANN is the best for these cases.,neuralnetworks,ferynando7,False,/r/neuralnetworks/comments/es4ym6/which_kind_of_artificial_neural_network_is_the/
[News] Google Introduces Flax: A Neural Network Library for JAX,1579635752,"Google recently introduce Flax — a neural network library for JAX that is designed for flexibility. Flax can train neural networks by forking an example from its official GitHub repository.  

Source: [https://medium.com/syncedreview/google-introduces-flax-a-neural-network-library-for-jax-84bdc6f8f160](https://medium.com/syncedreview/google-introduces-flax-a-neural-network-library-for-jax-84bdc6f8f160) 

The *Google Research: Flax* repository is on [GitHub](https://github.com/google-research/flax/tree/prerelease).",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/erzu8a/news_google_introduces_flax_a_neural_network/
"Rap Written and rapped by A.I. - ""Why Can't AI Stand Alone?"" (Remix)",1579633695,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/erzc3s/rap_written_and_rapped_by_ai_why_cant_ai_stand/
Artificial Life using Spiking Neural Networks and Neuroevolution,1579521054,,neuralnetworks,inboble,False,/r/neuralnetworks/comments/erc98m/artificial_life_using_spiking_neural_networks_and/
Double Q-Learning with Python,1579507866,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/erad37/double_qlearning_with_python/
A.I. Generated News - George Washington Invented Stinky Pen and Ducks Attack Man,1579481979,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/er5gho/ai_generated_news_george_washington_invented/
Understanding GauGAN: Unraveling Nvidia's Landscape Painting GANs,1579451063, [https://blog.paperspace.com/nvidia-gaugan-introduction/](https://blog.paperspace.com/nvidia-gaugan-introduction/),neuralnetworks,hellopaperspace,False,/r/neuralnetworks/comments/eqyc62/understanding_gaugan_unraveling_nvidias_landscape/
Questions about neuronal network,1579385193,"I am wondering about how to make a neuronal network (don’t respond by code but explanation).  The mains things I am wondering about are these points:

1-  what are the values of the inputs (I think it’s between 0 and 1, but I am not sure)?
2- what operation do I use to process the data (It may be a multiplication, but again I’m not sure)?
3- how do I change the values in the “black box” and how do I know which value to change and how to know if I add to the “black box” value or subtract?",neuralnetworks,H-L_echelle,False,/r/neuralnetworks/comments/eqn5vu/questions_about_neuronal_network/
Looking for advice on extending the XOR neural network to learn a 2D image space,1579320077,I've got an neural net with two input nodes and 1 output node - all with values 0-1. How can I intelligently choose the number of nodes in the hidden layers required to learn the training data?,neuralnetworks,Dr_Octahedron,False,/r/neuralnetworks/comments/eqbjoq/looking_for_advice_on_extending_the_xor_neural/
"A.I. Generated News - Santa is ""eating"" Babies! and Elon Musk's Prank Goes Wrong",1579306447,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/eq8z64/ai_generated_news_santa_is_eating_babies_and_elon/
"A.I. Generated News - Santa is ""eating"" Babies! and Elon Musk's Prank Goes Wrong",1579305255,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/eq8q8g/ai_generated_news_santa_is_eating_babies_and_elon/
This Week in AI - Issue #1,1579290622,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/eq5elp/this_week_in_ai_issue_1/
Double Deep Q-Learning to Get the Most out of your DQN - DataHubbs,1579254737,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/epydo2/double_deep_qlearning_to_get_the_most_out_of_your/
EmotionCues: AI Knows Whether Students Are Paying Attention,1579203754,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/epoe3w/emotioncues_ai_knows_whether_students_are_paying/
How To Tutorial by A.I. - How to Drive a Car,1579183297,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/epjp9f/how_to_tutorial_by_ai_how_to_drive_a_car/
An Adversarial Approach for the Robust Classification of Pneumonia from Chest Radiographs,1579173449,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/epi0hq/an_adversarial_approach_for_the_robust/
Optimize Response Time of your Machine Learning API in Production,1579173075,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ephyh1/optimize_response_time_of_your_machine_learning/
"[Help for training] Neural network converging to unnacceptable error, then diverging then re-converging again to the same point",1579167403,"Hi,

I am self-learning about neural networks and tried to build one to test myself. Although the network ""kinda works"", it does not learn the way i think it should. And at that point, I should need some help.

Some informations about the network before going into the details of the problems :

* The network has 1 input layer of 19 neurons ; 2 hidden layers of 10 neurons each; 1 output layer made of a single neuron;
* I use Tanh as the activation function. i also tried using Relu and sigmoid but it always gives me the problem I will further describe ;
* I have a data set of a very large amount of data that covers every input possible ;
* All the weights are randomly initialized ;
* I set the learning rate at 0.0003. 

That being said, the problem is the following (graph 1 show the evolution of the error; the dots are the errors, the red line is the moving average if the error). At first, the network gives me random answers to my problem, wich is quit normal. But then, it converge to a point where the error is constant and unnacceptable.

*Processing img h8yxlgxc14b41...*

After that convergence point, it diverges back to give me random answers. An interesting point is that the moving average stays approximately the same (\~0.5), but it gives me more often a good answer (see graph 2 : classes with low error are the most represented).

*Processing img yw5izune14b41...*

After this divergence, it converge back to point of unnacceptable error.

&amp;#x200B;

I honestly don't know what is going wrong, and what I can do to go through this problem. If anyone has any idea, i would be really happy.

Thanks",neuralnetworks,sk0lopandre,False,/r/neuralnetworks/comments/eph5h5/help_for_training_neural_network_converging_to/
How to decide on number of nodes in hidden layers for a network with very few input and output nodes?,1579151525,Reading up on this it seems most sources are saying the size of the hidden layers should be between the size of the input layer and the size of the output layer. This assumes the size of output layer is smaller than the size of the input layer. Should that be the case? And what if your network has like 1 input node and 1 output node. How would you decide the number of nodes in that case?,neuralnetworks,Dr_Octahedron,False,/r/neuralnetworks/comments/epeqjc/how_to_decide_on_number_of_nodes_in_hidden_layers/
Neural networks with few input nodes,1579132997,"I’m working on a net which learns the brightness levels of an image given x and y co-it’s. 2 input nodes (with range 0-1) and 1 output node (with range 0-1). This should be an easy enough problem to solve I would have thought, just an extension of the XOR net, but training doesn’t seem to be reliable. I’ve tried with many network topologies but I think 2 hidden layers should be all I really need. Unsure how to choose number of nodes in each hidden layer. 3-5 seems to work best but I can only train to about 85%, sometimes gets stuck at 60%. I’m using leaky relu activation function. Does anyone have any experience with this sort of problem?",neuralnetworks,Dr_Octahedron,False,/r/neuralnetworks/comments/epaxgk/neural_networks_with_few_input_nodes/
Building a Lie Detector for Images,1579123754,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ep8s1u/building_a_lie_detector_for_images/
How Google researchers used neural networks to make weather forecasts,1579116176,,neuralnetworks,jonfla,False,/r/neuralnetworks/comments/ep6xl2/how_google_researchers_used_neural_networks_to/
How do neurons apply non-linear transformations?,1579109699,"My understanding is this: the purpose of the convolutional, pool, and fully connected layers is to apply transformations so the data is linearly separable for the output layer. Let's say our neural network is a few fully connected layers then the output layer. Since each neuron applies activation_function([weight] * [input] + [bias]) how is a non-linear transformation applied by each neuron? I was told this non-linearity came from the activation function but ReLU is linear (besides at 0).",neuralnetworks,mooman996,False,/r/neuralnetworks/comments/ep5dqm/how_do_neurons_apply_nonlinear_transformations/
"Any activation function, similar to sigmoid/tanh, that classifies for *very small* values?",1579096801,,neuralnetworks,Hubba_Bubba_Lova,False,/r/neuralnetworks/comments/ep2nia/any_activation_function_similar_to_sigmoidtanh/
Parse website into specific format with machine learning,1579094064," 

Hi,

I would like to extract informations from this website f.e

[http://www.stylusstudio.com/edifact/D96A/INVOIC.htm#part4.1.1](http://www.stylusstudio.com/edifact/D96A/INVOIC.htm#part4.1.1)

Starting at 4.1.1 Header section

I want it to be parsed into this structure:

class segments:

0020 BGM (link) -&gt;

class positions:

010 -&gt;

class group :

C002 Document/Message Name

class elements :

1001,

1131,

3055,

....

I could write a parser that just goes through the website and reads in all the information with this structure.

But is there a lazy solution with machine learning ?",neuralnetworks,Sharkoon1,False,/r/neuralnetworks/comments/ep26hc/parse_website_into_specific_format_with_machine/
Hitting the Gym With Neural Networks: Implementing a CNN to Classify Gym Equipment,1579024254,,neuralnetworks,hszafarek,False,/r/neuralnetworks/comments/eoong6/hitting_the_gym_with_neural_networks_implementing/
Starting with Neural Networks,1579006865,"Hey everyone,

I recently wanted to learn how to build a neural network as a hobby project. I just need a way to start learning how they work, and how to develop one. I am not expecting a response, but some help would be nice.

Thanks,
idripntear",neuralnetworks,idripntear,False,/r/neuralnetworks/comments/eokyqq/starting_with_neural_networks/
Find The Most Important Neural Network Hyperparameters In Seconds,1578951798,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/eoawdv/find_the_most_important_neural_network/
"AI Listens to Panda Love Sounds, Predicts Mating Success",1578933619,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/eo6jhg/ai_listens_to_panda_love_sounds_predicts_mating/
Introduction to Double Q-Learning,1578906086,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/eo1pbo/introduction_to_double_qlearning/
"Suggestions for a model that predicts words given partially completed strings, like ""__e__ts a_r_t__ _ea_ _r__ing""",1578885172,"I've been learning neural networks for several weeks now, and I've seen a lot of similar problems to this. However, they all move sequentially through text, predicting the next word or character. This problem is kind of different, in that the model is not primarily sequential, and it will probably need to behave like both a word predictor and a character predictor.

It doesn't have to be remotely perfect. I'm only trying to generate occasional high-certainty predictions, and retroactively use some of those predictions to make further predictions. It's also OK to just predict parts of words, like prefixes, suffixes, and fractional components of words generated from a dictionary. Letters are progressively filled in and then fed into the model in multiple passes, so predictions will get easier over time. Space characters are filled in, so word boundaries are known from the start. Most words are expected to remain incomplete for the majority of passes.

I'm assuming it will use tokens between 40 and 100 characters long. It seems like it will need to be recurrent and use a bidirectional LSTM. Maybe an attention mechanism. But I can't really wrap my head around the differences between how this model should work compared to a standard sequential autocomplete or text generation example.

Any input would be helpful.",neuralnetworks,AlrightThatsIt,False,/r/neuralnetworks/comments/enyag4/suggestions_for_a_model_that_predicts_words_given/
How Super Resolution Works,1578877264,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwnst/how_super_resolution_works/
How to Design a Neural Network | 2020 Edition,1578877223,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwnie/how_to_design_a_neural_network_2020_edition/
NEATO - Neural Evolution of Augmenting Topologies Observable in Minecraft,1578877043,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwm77/neato_neural_evolution_of_augmenting_topologies/
Go-Explore: a New Approach for Hard-Exploration Problems,1578876998,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwlvh/goexplore_a_new_approach_for_hardexploration/
Machine learning shapes microwaves for a computer's eyes,1578876957,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwlkg/machine_learning_shapes_microwaves_for_a/
Wave physics as an analog recurrent neural network,1578876913,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwl8h/wave_physics_as_an_analog_recurrent_neural_network/
"Deep learning, 3-D technology to improve structure modeling, create better drugs",1578876887,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwl1u/deep_learning_3d_technology_to_improve_structure/
Viewpoint: Physics Insights from Neural Networks,1578876835,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/enwknr/viewpoint_physics_insights_from_neural_networks/
Trouble with the input shape while fine-tuning VGG16,1578862544,,neuralnetworks,Atam55,False,/r/neuralnetworks/comments/entare/trouble_with_the_input_shape_while_finetuning/
Tutorial for YOLO Darknet,1578836744,"Hi,

I'm a beginner programmer and I would like to learn how to use object detection. My goal is to detect objects (playing cards) on a computer screen. I have already created a test file with 500 playing cards (black and white). Now I need to learn how to use darknet to train a neural network to handle the detection. 

I have created a project and I have downloaded this repositery into my project. 

https://github.com/thtrieu/darkflow

Any tips how I can start the training process. I'm very new to this.
I'm running Win10 with AMD Ryxen 2700. So far I have only created the test files and installed tensorflow and opencv as instructed in the darknet installation.",neuralnetworks,Usurper__,False,/r/neuralnetworks/comments/ennm1v/tutorial_for_yolo_darknet/
How to visualize the filter images Yolo Darknet,1578824152,"I train darknet Yolo for custom image data, how can I know what my model seen in my images

&amp;#x200B;

I have weight file, cfg and lable file with me.",neuralnetworks,danish-shaikh,False,/r/neuralnetworks/comments/enlvpy/how_to_visualize_the_filter_images_yolo_darknet/
"When making an auto encoder for an image, is there a process similar to convolutions but working to expand the layer space.",1578810489,"I’ve been working on making neural networks from scratch using no 3rd party libraries and I’ve gotten as far as to have a general neural network model. I understand using convolutions at the early stages of the image processing, but on the opposite end how do I go from a small layer space to large layer space( 200 -&gt; 20000) like the pixels of a 200-200 image?",neuralnetworks,SikinAyylmao,False,/r/neuralnetworks/comments/enk2ai/when_making_an_auto_encoder_for_an_image_is_there/
Neural Network bot/signals,1578664382,"Hi, I'm working on a bot to trade using neural networks. It is currently in development. If you want you can see the generated signals of laterization, long or short.

It is based on train images generated every minute by looking at the BTC / USD quote. The train is run weekly. Currently the train is over 50,000 images.

It is still in full development, if you are interested in taking a look at the signals this is the link: [https://vafed.it/signals.php](https://vafed.it/signals.php).

To stay up to date you can follow this post on [https://bitcointalk.org/index.php?topic=5213806.0](https://bitcointalk.org/index.php?topic=5213806.0)",neuralnetworks,stempelo,False,/r/neuralnetworks/comments/emr70f/neural_network_botsignals/
"PracticalAI 2.0 | ‘Learn, Explore and Build’ ML Models Online",1578613578,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/emi94s/practicalai_20_learn_explore_and_build_ml_models/
‘Brains Are Amazing’ — Neuroscientists Discover L2/3 Human Neurons Can Compute the XOR Operation,1578596215,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/eme4ey/brains_are_amazing_neuroscientists_discover_l23/
Rap Song with lyrics written by A.I. (Talk To Transformer),1578574986,,neuralnetworks,Ubizwa,False,/r/neuralnetworks/comments/em9cbl/rap_song_with_lyrics_written_by_ai_talk_to/
"New Projects See GPT-2 Summarizing Movies, Playing Chess",1578519751,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/elzf9i/new_projects_see_gpt2_summarizing_movies_playing/
A Neural Network Guitar Pedal,1578510522,,neuralnetworks,dizzykiwi3,False,/r/neuralnetworks/comments/elx7sp/a_neural_network_guitar_pedal/
NEATO - Neural Evolution of Augmenting Topologies Observable in Minecraft,1578496025,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/eltvmm/neato_neural_evolution_of_augmenting_topologies/
NEAT evolved networks always converging to the same local maxima,1578438977," I have been trying to implement NEAT,  and I am testing my implementation by trying to evolve a XOR network as  mentioned in the original paper.

&amp;#x200B;

The  funny thing is that it always evolves OR instead  of XOR, sometimes it comes realy close to XOR (and I mean almost perfectly, as in the best fitness score being 0.001 or less away from the maximum achievable)  but then fitness plummets and quickly goes back to evolving perfect OR networks.

&amp;#x200B;

TBH  my code is rather messy, and I wouldn't ask anyone to try debugging it  for myself. My real question is, have you ever had a similar problem  with NEAT, or any other genetic algorithm? I'm looking for clues as to  where did I go wrong.

&amp;#x200B;

 Thanks in advance!",neuralnetworks,elSenorMaquina,False,/r/neuralnetworks/comments/eljm1e/neat_evolved_networks_always_converging_to_the/
How to Design a Neural Network | 2020 Edition,1578428596,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/elh49f/how_to_design_a_neural_network_2020_edition/
Microsoft Releases NNI V1.3 for AutoML Algorithms and Training,1578422290,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/elfluq/microsoft_releases_nni_v13_for_automl_algorithms/
Research Survey about Security in Machine Learning,1578401186,[removed],neuralnetworks,oliver133322,False,/r/neuralnetworks/comments/elb3zi/research_survey_about_security_in_machine_learning/
Facebook AI | Text-Based Generation of Interactive Worlds,1578341956,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ekzn4h/facebook_ai_textbased_generation_of_interactive/
Vision-Based AI Model Solves Sudoku at a Glance,1578340795,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ekzctq/visionbased_ai_model_solves_sudoku_at_a_glance/
Let's Do This,1578187023,,neuralnetworks,hisairnessag,False,/r/neuralnetworks/comments/ek5rjb/lets_do_this/
[Google Brain Object detection] EfficientDet: Scalable and Efficient Object Detection implementation by Signatrix GmbH,1578175649,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/ek3bvl/google_brain_object_detection_efficientdet/
How to make such model?,1578158433,"I want to make a model such that when we input parent images we get output as child's image .
What is the approach to solve this problem? What other information I should have inorder to solve this problem?",neuralnetworks,RstarPhoneix,False,/r/neuralnetworks/comments/ejzdhi/how_to_make_such_model/
A quick introduction into machine vision?,1578137391,"Hi!

So, I am applying for a job in a company that deals with neural-networks-powered AI for industrial robots with a focus on machine vision. Now, I have a solid grasp of general neural network basics, but I have never done anything related to computer vision.

So, my question is whether you could recommend me sources where I could read up on how computer vision works before my interview on Tuesday. I would like to know what's up, if they bring it up. Also, if you've ever applied for a job in a company that deals with NNs, do you have any general tips on what to expect in an interview?

Many thanks!",neuralnetworks,PanVidla,False,/r/neuralnetworks/comments/ejvlm0/a_quick_introduction_into_machine_vision/
What are some of the unpopular neural networks and their training techniques.,1578124369,"Obviously  deep learning is now very popular that employ neural networks as its  substrate. But what are some of unpopular neural networks and their  training techniques or even neural networks that are not trained in its  mainstream meaning.

Off the top we have following which are not as popular as DNNs but still are well known,

* RBM and Hopfield nets
* SOM
* SOINN
* CPPN
* GNG
* NEAT/HyperNEAT
* Evolution strategies

One I could think of that is buried under is a string matching NN - [Optimal neural network algorithm for  on-line string matching](https://ieeexplore.ieee.org/document/718523)

Please share some of the interesting networks from the past.",neuralnetworks,paarulakan,False,/r/neuralnetworks/comments/ejtsjh/what_are_some_of_the_unpopular_neural_networks/
Gaussian Error Linear Unit Activates Neural Networks Beyond ReLU,1578085737,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ejlsmk/gaussian_error_linear_unit_activates_neural/
MLP: all outputs go to 1.0 for every one of the 3 categories . HELP,1578039882,"Hello everyone , I have a project for the university .We need to create an MLP with 2 hidden layers. I did everything as stated in the theory the professor gave us and what the videos said. My problem is that after I train the network with my 3k training examples , if I put one of those examples again the result for the 3 categories are C1=1.0 C2=1.0 C3=1.0 , instead of 2 zeros and 1 one.. heres the code , I think its well written and i double checked for mistakes In code , I think its something logical. Sorry for asking for help that way but I have no time left to spare. Thank you

P.s Every variable is defined so i can change em.  
Layer 1 and 3 have sigmoid and layer 2 can change between tanh and linear.

In my main i just use the above functions . Also I deleted the print functions so they don't disturb you. Here are some of the results and then the code:

LAYER1 OUTPUTS:  
NEURON0: 1.000000  
NEURON1: 1.000000  
NEURON2: 1.000000  
NEURON3: 1.000000  
NEURON4: 1.000000

LAYER2 OUTPUTS:  
NEURON0: 1.000000  
NEURON1: 1.000000  
NEURON2: 1.000000

LAYER3 OUTPUTS:  
C0: 1.000000  
C1: 1.000000  
C2: 1.000000

\#include &lt;stdio.h&gt;  
\#include &lt;math.h&gt;  
\#include &lt;stdlib.h&gt; //rand()  
\#define I 6000 // # of inputs(total examples)  
\#define N I/2 // # of examples for the training set  
\#define d 2 // # of x(inputs) in each example  
\#define K 3 // # of categories  
\#define H1 5 // # of layer 1 neurons  
\#define H2 3 // # of layer 2 neurons  
\#define FUNCTION 0 // type of fuctions used for layer 2 (), 0=tanh 1=linear  
\#define n 0.1 // learning rate  
\#define L 1 //  
//---------------------------------------------------  
// Structs  
//---------------------------------------------------  
struct example  
{  
float xi\[d\];  
float category\[K\]; //for example {1.0 , 0.0 , 0.0}  
int type; // C1=1 C2=2 C3=3  
};  
struct neuronL1  
{  
float b; //bias  
float weights\[d\];  
float y;  
float output;  
float delta;  
};  
struct neuronL2  
{  
float b; //bias  
float weights\[H1\];  
float y;  
float output;  
float delta;  
};  
struct neuronL3  
{  
float b; //bias  
float weights\[H2\];  
float y;  
float output;  
float delta;  
};  
struct example examples\[I\];  
struct neuronL1 layer1\[H1\]; //Hidden 1  
struct neuronL2 layer2\[H2\]; //Hidden 2  
struct neuronL3 layer3\[K\];  
//---------------------------------------------------  
// Prototypes  
//---------------------------------------------------  
float sigmoid(float x);  
float sigmoidDerivative(float x);  
void forwardpass(struct example example);  
void backprop(struct example example);  
void weightUpdate(struct example example);  
void initializeWeights();  
void activateL1(struct example example);  
void activateL2();  
void activateL3();  
void loadExamples();  
void printExamples();  
void printWeights();  
void printOutput();  
void printY();  
//---------------------------------------------------  
// Sigmoid function  
//---------------------------------------------------  
float sigmoid(float x)  
{  
float exp\_value;  
float return\_value;  
/\*\*\* Exponential calculation \*\*\*/  
exp\_value = exp((double)-x);  
/\*\*\* Final sigmoid value \*\*\*/  
return\_value = 1 / (1 + exp\_value);  
return return\_value;  
}  
//---------------------------------------------------  
// Sigmoid derivative function  
//---------------------------------------------------  
float sigmoidDerivative(float x)  
{  
float return\_value = sigmoid(x) \* (1 - sigmoid(x));  
return return\_value;  
}  
//---------------------------------------------------  
// Forward Pass  
//---------------------------------------------------  
void forwardpass(struct example example)  
{  
activateL1(example);  
activateL2();  
activateL3();  
}  
//---------------------------------------------------  
// Back Prop  
//---------------------------------------------------  
void backprop(struct example example)  
{  
// calculate delta for each layer 3 neuron  
for (int i = 0; i &lt; K; i++)  
{  
layer3\[i\].delta = (layer3\[i\].output - example.category\[i\] ) \* (layer3\[i\].output \* (1 - layer3\[i\].output));  
}  
// calculate delta for each layer 2 neuron  
for (int i = 0; i &lt; H2; i++)  
{  
layer2\[i\].delta = 0;  
// for each neuron in the output layer 3  
for (int j = 0; j &lt; K; j++)  
{  
layer2\[i\].delta += layer3\[j\].delta \* layer3\[j\].weights\[i\];  
}  
if (FUNCTION == 0)  
{  
layer2\[i\].delta \*= (1 - pow(layer2\[i\].output, 2)); //1 - tanh\^2(x)  
}  
else if (FUNCTION == 1)  
{  
layer2\[i\].delta \*= 1; //1  
}  
}  
// calculate delta for each layer 1 neuron  
for (int i = 0; i &lt; H1; i++)  
{  
layer1\[i\].delta = 0;  
// for each neuron in the output layer 2  
for (int j = 0; j &lt; H2; j++)  
{  
layer1\[i\].delta += layer2\[j\].delta \* layer2\[j\].weights\[i\];  
}  
layer1\[i\].delta \*= (layer1\[i\].output \* (1 - layer1\[i\].output)); // sigmoid \* (1 - sigmoid)  
}  
}  
//---------------------------------------------------  
// Weights Update  
//---------------------------------------------------  
void weightUpdate(struct example example)  
{  
//updating layer 3 (output)  
for (int i = 0; i &lt; K; i++)  
{  
// for each weight from layer 2 (hidden) to the output layer 3  
for (int j = 0; j &lt; H2; j++)  
{  
layer3\[i\].weights\[j\] = layer3\[i\].weights\[j\] + (n \* layer3\[i\].delta \* layer2\[j\].output);  
}  
layer3\[i\].b = layer3\[i\].b + (n \* layer3\[i\].delta); //updating the bias  
}  
//updating layer 2 (hidden)  
for (int i = 0; i &lt; H2; i++)  
{  
// for each weight from layer 1 (hidden) to the layer 2(hidden)  
for (int j = 0; j &lt; H1; j++)  
{  
layer2\[i\].weights\[j\] = layer2\[i\].weights\[j\] + (n \* layer2\[i\].delta \* layer1\[j\].output);  
}  
layer2\[i\].b = layer2\[i\].b + (n \* layer2\[i\].delta); //updating the bias  
}  
//updating layer 1 (output)  
for (int i = 0; i &lt; H1; i++)  
{  
// for each weight from layer 2 (hidden) to the output layer 3  
for (int j = 0; j &lt; d; j++)  
{  
layer1\[i\].weights\[j\] = layer1\[i\].weights\[j\] + (n \* layer1\[i\].delta \* example.xi\[j\]);  
}  
layer1\[i\].b = layer1\[i\].b + (n \* layer1\[i\].delta); //updating the bias  
}  
}  
//---------------------------------------------------  
// Initialize the Weights  
//---------------------------------------------------  
void initializeWeights()  
{  
//Initialiaze layer 1 weights(w1 &amp; w2) from 0 to 1 and then it's bias (b)  
for (int i = 0; i &lt; H1; i++)  
{  
for (int j = 0; j &lt; d; j++)  
{  
layer1\[i\].weights\[j\] = ((float)rand()) / RAND\_MAX;  
}  
layer1\[i\].b = ((float)rand()) / RAND\_MAX; //bias  
}  
//Initialiaze layer 2 weights from 0 to 1 and then it's bias (b)  
for (int i = 0; i &lt; H2; i++)  
{  
for (int j = 0; j &lt; H1; j++)  
{ //TODO: check if it needs to be from 1 to H1(including H1)  
layer2\[i\].weights\[j\] = ((float)rand()) / RAND\_MAX;  
}  
layer2\[i\].b = ((float)rand()) / RAND\_MAX; //bias  
}  
//Initialiaze layer 3 weights from 0 to 1 and then it's bias (b)  
for (int i = 0; i &lt; K; i++)  
{  
for (int j = 0; j &lt; H2; j++)  
{ //TODO: check if it needs to be from 1 to H1(including H1)  
layer3\[i\].weights\[j\] = ((float)rand()) / RAND\_MAX;  
}  
layer3\[i\].b = ((float)rand()) / RAND\_MAX; //bias  
}  
}  
//---------------------------------------------------  
// Activate Layer 1  
//---------------------------------------------------  
void activateL1(struct example example)  
{  
//for every neuron in Layer 1  
for (int i = 0; i &lt; H1; i++)  
{  
// y = wn\*xn + ... + w1\*x1 + w1\*x1 +b  
// where:  
// n is d(Number of example's xs)  
// and x are the inputs of each example  
for (int j = 0; j &lt; d; j++)  
{  
layer1\[i\].y += layer1\[i\].weights\[j\] \* example.xi\[j\];  
}  
layer1\[i\].y += layer1\[i\].b; //bias  
// Oi = sigmoid(y)  
layer1\[i\].output = sigmoid(layer1\[i\].y);  
}  
}  
//---------------------------------------------------  
// Activate Layer 2  
//---------------------------------------------------  
void activateL2()  
{  
for (int i = 0; i &lt; H2; i++)  
{  
// y = wd\*xd + ... + w1\*x1 + w1\*x1 +b  
//where:  
// d is H1(Number of neurons in the previous layer)  
// x1 to xd is the outputs of Layer 1  
for (int j = 0; j &lt; H1; j++)  
{  
layer2\[i\].y += layer2\[i\].weights\[j\] \* layer1\[j\].output;  
}  
layer2\[i\].y += layer2\[i\].b; // add bias  
// o=tanh(y)  
if (FUNCTION == 0)  
{  
layer2\[i\].output = tanh(layer2\[i\].y);  
}  
// o=y  
else if (FUNCTION == 1)  
{  
layer2\[i\].output = layer2\[i\].y;  
}  
}  
}  
//---------------------------------------------------  
// Activate Layer 3(Outputs)  
//---------------------------------------------------  
void activateL3()  
{  
for (int i = 0; i &lt; K; i++)  
{  
for (int j = 0; j &lt; H2; j++)  
{  
layer3\[i\].y += layer3\[i\].weights\[j\] \* layer2\[j\].output;  
}  
layer3\[i\].y += layer3\[i\].b;  
layer3\[i\].output = sigmoid(layer3\[i\].y);

}  
}  
//---------------------------------------------------  
// MAIN  
//---------------------------------------------------  
int main()  
{  
loadExamples();  
//printExamples();  
initializeWeights();  
// printing the weights  
//printWeights();  
//activating the network  
forwardpass(examples\[0\]);  
// printing the output  
//printY();  
//printDesired(examples\[0\]);  
//printOutput();

// trainning the network  
for (int i = 0; i &lt; I/2; i++)  
{

//activating the network  
forwardpass(examples\[i\]);  
backprop(examples\[i\]);  
weightUpdate(examples\[i\]);  
}  
//activating the network  
//forwardpass(examples\[0\]);

//printWeights();  
// printing the new output  
printDeltas();  
printY();  
printOutput();

return 0;  
}

EDIT: The Output I gave you before the code is after the 3000 examples . And then reinserting example\[0\] (the first one) . So It looks like it satisfies all the categories.",neuralnetworks,alecfilios2,False,/r/neuralnetworks/comments/ejclxv/mlp_all_outputs_go_to_10_for_every_one_of_the_3/
Artificial Intelligence in 2020,1578014598,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ej7urq/artificial_intelligence_in_2020/
‘SketchTransfer’ Draws Understanding From Detail Invariance,1577997149,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ej3sl1/sketchtransfer_draws_understanding_from_detail/
In need of some LSTM coding help/advice,1577879281,"Hi!

Could you guys maybe take a look at [this](https://stackoverflow.com/questions/59523363/mql5-lstm-library-output-of-example-script-determining-prediction-value) stackoverflow question? I didn't want to post it directly here for the sake of blowing up this post with too much information/content.  


I have a feeling that I am missing something super simple but not exactly sure what implementation I have to use to get the correct prediction value.  


Thanks for your time in advance! :)",neuralnetworks,thirstynipples69,False,/r/neuralnetworks/comments/eihklt/in_need_of_some_lstm_coding_helpadvice/
2019 in Review: 10 AI Papers That Made an Impact,1577814367,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ei64yv/2019_in_review_10_ai_papers_that_made_an_impact/
Do you view the world through a Neural Network lens?,1577718797,"Since discovering Deep Learning etc I now find myself viewing the world in terms of Neural Networks.

We have a smallholding / farm with animals .. and I now see an animal as a large collection of overlaid Neural Networks ... rather than cuddly complex creatures.

Am I the only one whose view of animal - and perhaps human - behavior has been changed through learning about Neural Networks?",neuralnetworks,ExFAANG,False,/r/neuralnetworks/comments/ehn718/do_you_view_the_world_through_a_neural_network/
Convolutional Neural Networks Explained…with American Ninja Warrior,1577717093,,neuralnetworks,hszafarek,False,/r/neuralnetworks/comments/ehmuye/convolutional_neural_networks_explainedwith/
NeurIPS 2019 Outstanding Machine Learning Paper Awards,1577695029,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ehjj04/neurips_2019_outstanding_machine_learning_paper/
I created an artificial agent that learns to balance a double pendulum using neuroevolution,1577629052,,neuralnetworks,Giacobako,False,/r/neuralnetworks/comments/eh6dj4/i_created_an_artificial_agent_that_learns_to/
Help: selecting features for MLP classificaiton; training a rejection class,1577621830,"Suppose I have an object A, a background B and a set of various undefined objects X.

The objects A \[ and X\] are positioned in front of the unevenly lit background. Because of the uneven lighting the image frequently isn't segmented perfectly. The regions found through segmentation are then used to calculate features for classification. The Multilayer Perceptron has to recognize A.

So far training with samples from A and B and outputs A and B lead to elements of X being lumped randomly into one of the two classes. Adding output X and training samples for X didn't help matters much because the samples can't cover all possible X.

Now my questions:

What's a reasonable approach to selecting features so that no objects B or X are ever classified as A? How well would this translate to distinguishing two objects from the background and set X?

Will manually adding objects X from the testpictures wrongly classified as A to the sample set of X lead to overfitting?

I'd highly appreciate someone taking their time to clear this up for me 'cause I'm stuck.",neuralnetworks,gloriousfalcon,False,/r/neuralnetworks/comments/eh5bwk/help_selecting_features_for_mlp_classificaiton/
Google Colab Training,1577615360,"Hello,
I am training my model on google colab for the first time and I cannot make it past epoch 30 because it keeps disconnecting on me. I have a couple of queations Id be really grateful if you could help with. 

1. First of all do you have any solution so it could stop disconnecting that often? 
If im not in the tab it disconnects within minutes and if i am it still does in about 1 hr or so if I dont engage with the screen somehow. 
I have tried the javascript code for the browser concole that autoclicks but it still doesnt work for me. I wake up or come to it disconnected.

2. I have noticed a semi-disconnection too (if thats a thing). Basically the status is Busy, all other cells look like [ ], but the cell that outputs epochs and metrics still goes on and it even autosaves. When i try to click on Busy and try to connect, it says it cannot reconnect. 
What is this? Should i leave it to continue like that or should i restart training? 

3. So because I have restarted it many times this week in attempt to reach at least half of my epochs, I saw today that the disk (at ram/disk button) is pretty filled up. Which means it has been saving stuff from these half runs of the code.
 Is there anyway i can start where it left off? Some magic to start at the epoch it disconnected at would be great.

Also sorry if this is more about Colab and not neural networks but the Google Colab subreddit seemed pretty dead.

Thank you for any little help you could provide",neuralnetworks,Teabx,False,/r/neuralnetworks/comments/eh4j7x/google_colab_training/
Need help with NEAT algorithm for a game AI,1577529962,"I'm making a game. This game simulates natural selection and neural evolution.

Each cell has some basic parts which have various inputs and outputs. Each cell also has a simple brain based on the NEAT algorithm. One of the core features of NEAT is crossover, which allows what I'd consider to be sexual reproduction. Two networks can combine to create a child network. There is also something called an innovation number, which tracks which mutations occurred and in what order. This helps align the two parent networks in the case they have slightly different topologies. 

In my game, at least in the early phases of development, I don't plan to have sexual reproduction in the game at all. Since there is no sexual reproduction, there will never be a case where two parent networks combine into a child network. Instead, upon reproducing, the new child network will simply be passed through a function that determines probabilities of individual values or topology being mutated.

This leads me to believe that the innovation number and the entire idea of combining two parent networks is useless for this particular case. It would definitely make programming a bit easier if I didn't have to worry about it.

Do you think the innovation number is still necessary?",neuralnetworks,Tristan401,False,/r/neuralnetworks/comments/egpfzh/need_help_with_neat_algorithm_for_a_game_ai/
2019 In Review: 10 Open-Sourced AI Datasets,1577471076,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/egeqrl/2019_in_review_10_opensourced_ai_datasets/
How do we determine quality of text produced by a NN?,1577433655,Is there like a scoring system?,neuralnetworks,CornellGen,False,/r/neuralnetworks/comments/eg8e6r/how_do_we_determine_quality_of_text_produced_by_a/
"MIT Deep Learning 2020 Course is starting Mon, Jan 6, 2020 (video and slides to be provided online)",1577416077,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/eg5en6/mit_deep_learning_2020_course_is_starting_mon_jan/
Facebook PointRend: Rendering Image Segmentation,1577377406,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/efxd0a/facebook_pointrend_rendering_image_segmentation/
Birdsong clef,1577314168,"Did anyone try to build a birdsongs classification ? 
Cheers",neuralnetworks,aymane12-,False,/r/neuralnetworks/comments/efn133/birdsong_clef/
Neural Network learns to play Club Penguin with Genetic Evolution!,1577302404,,neuralnetworks,mattberrycrunch,False,/r/neuralnetworks/comments/efkoni/neural_network_learns_to_play_club_penguin_with/
A new deep learning model for EEG-based emotion recognition,1577296677,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/efji4x/a_new_deep_learning_model_for_eegbased_emotion/
What are the Limits of Deep Learning? Going Beyond Deep Learning,1577296627,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/efjhr6/what_are_the_limits_of_deep_learning_going_beyond/
2019 in Review: 10 Essential AI YouTube Channels,1577292833,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/efipke/2019_in_review_10_essential_ai_youtube_channels/
Labeling objects in videos based on kinematics?,1577267299,"Apologies if this does not fit the sub - I will remove if required.
In my PhD research I track the movement of rat whiskers. Current solutions use classical vision algorithms to trace whiskers in each label, and track (label) their movement across pairs of frames.
As whiskers look extremely similar (more so than e.g. pedestrians) the information for labeling is mostly in position + movement dynamics and not so much appearance. This is classically solved with association filters. Their number is also not apriori known.
Are there networks to trace and label an unknown objects based on their movement without using appearance information?",neuralnetworks,BestBoyCoop,False,/r/neuralnetworks/comments/efeq6b/labeling_objects_in_videos_based_on_kinematics/
How does an Artificial Neural Network Learn? (2019) | Eduonix,1577212194,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ef54jl/how_does_an_artificial_neural_network_learn_2019/
Engineers are Pushing Machine Learning to the World’s Humblest Microprocessors,1577212143,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ef544f/engineers_are_pushing_machine_learning_to_the/
Ultimate Guide to Deep Q-Learning with TF-Agents,1577131213,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/eepo4f/ultimate_guide_to_deep_qlearning_with_tfagents/
Randomly Wired Neural Networks: A Breakthrough In Neural Architecture Search,1577111508," Connections between neurons in a physical neural system are not the same from birth. Now, a similar technique is used in several popular neural networks like ResNet. More information about randomly wired networks in this article.",neuralnetworks,alfi_solomons_,False,/r/neuralnetworks/comments/eelajx/randomly_wired_neural_networks_a_breakthrough_in/
Help to design NN for text re-writing?,1577104512,"Hello,

I am working on a LSTM based neural network to rewrite a user entered search query \[UQ\] to a reformulated query \[WPQ\] (well performing query)

Example -

&amp;#x200B;

|User Query \[UQ\]|Well Performing Query \[WPQ\]|
|:-|:-|
|Ladies Pregnancy dress|Maternity Dress|
|Indian Earrings|Jhumka|

&amp;#x200B;

As explained in this paper - [https://cse.iitkgp.ac.in/\~pawang/papers/sigir19.pdf](https://cse.iitkgp.ac.in/~pawang/papers/sigir19.pdf)

I am facing difficulty to design the model. Is it possible to assist me with this?

Thanks in advace.",neuralnetworks,redditaddict07,False,/r/neuralnetworks/comments/eek30b/help_to_design_nn_for_text_rewriting/
Silly question: can a transformer be used for any sequential data or does it have to be text?,1576933826,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/edpr2t/silly_question_can_a_transformer_be_used_for_any/
DeepInsight,1576898274,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/edkhmb/deepinsight/
Multi length arrays,1576894757,"I have a neural network, and the data I want to train it on is compromised of arrays of different lengths. But it breaks every time I try and run it. Is there a why to fix this?
I'm programming in Python
The activation function is a Sigmoid as of now, but I'm going to change it to a ReLu. 
Any and all help will be appreciated.
Cheers! 
Happy Holidays!",neuralnetworks,KilroyWasHere189,False,/r/neuralnetworks/comments/edjuow/multi_length_arrays/
ICLR 2020 Accepted Papers Announced,1576886032,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/edi6ep/iclr_2020_accepted_papers_announced/
Neural Networks for Noobs: It's Not as Complicated as You Might Think,1576864767,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/eddkrx/neural_networks_for_noobs_its_not_as_complicated/
"NeurIPS 2019｜Amii, Mila, and Vector Researchers Discuss AI in Canada and More",1576861022,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/edcrc4/neurips_2019amii_mila_and_vector_researchers/
Mixed Precision Training,1576854069,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/edb9xs/mixed_precision_training/
A CNN that takes several rows in a CSV file as a single input,1576850505,"I have extracted facial features from several videos in the form of facial action units(AU). These features span for several seconds and hence Take several rows in a CSV file (each row containing AU data for each frame of the video). Originally, I had multiple CSV files as input but, as advised previously, I have concatenated and condensed the data into a single file. My CSV columns look like this:

Filename | Label | the other columns contain AU data

Filename contains individual ""ID"" that help keep track of a single ""example"". The label column contains 2 possible value. Either ""yes"" or ""no"".

I've been unable to find any code that can help me out in this scenario. It might be a case of 3DCNN but the codes and/or tutorials that I've seen are for videos. Can someone please help me out. I'll really appreciate the assistance.",neuralnetworks,HudakSSJ,False,/r/neuralnetworks/comments/edakm9/a_cnn_that_takes_several_rows_in_a_csv_file_as_a/
"RNN of 2 nodes can approximate spring vibration to arbitrarily high precision by both self weights being slightly less than 1 and both weights weight from other node being plus/minus epsilon, same as sine/cosine being eachothers derivatives. That is a fractal. How can RNN approximate mandelbrot?",1576824668,,neuralnetworks,BenRayfield,False,/r/neuralnetworks/comments/ed6iuw/rnn_of_2_nodes_can_approximate_spring_vibration/
"Creative, Aspiring A.I.?",1576812128,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ed4afu/creative_aspiring_ai/
Intelligence May Not Be Computable,1576789484,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/eczee7/intelligence_may_not_be_computable/
Intelligence May Not Be Computable,1576789207,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/eczc2w/intelligence_may_not_be_computable/
Hierarchical self-organizing maps for unsupervised pattern recognition,1576775397,,neuralnetworks,inboble,False,/r/neuralnetworks/comments/ecw3r4/hierarchical_selforganizing_maps_for_unsupervised/
2019 in Review: 10 AI Failures,1576773071,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ecvk3n/2019_in_review_10_ai_failures/
Neural networks as non-leaky mathematical abstraction,1576758934,,neuralnetworks,elcric_krej,False,/r/neuralnetworks/comments/ecsm32/neural_networks_as_nonleaky_mathematical/
A video over the brief history of AI as well as an analysis of wether neural networks will one day be able to replicate human like consciousness,1576732000,,neuralnetworks,TheScienceVerse,False,/r/neuralnetworks/comments/ecoj2t/a_video_over_the_brief_history_of_ai_as_well_as/
One Shot Learning Object Detection using Turi Create,1576700377,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/echzt7/one_shot_learning_object_detection_using_turi/
A Quick Introduction to Vanilla Neural Networks,1576690472,,neuralnetworks,hszafarek,False,/r/neuralnetworks/comments/ecfq5a/a_quick_introduction_to_vanilla_neural_networks/
Visual Dialogue State Tracking Improves Question Generation,1576686142,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/eceriq/visual_dialogue_state_tracking_improves_question/
Introducing FlyteHub — Open Source AI That Scales.,1576647133,,neuralnetworks,flytehub,False,/r/neuralnetworks/comments/ec8c3w/introducing_flytehub_open_source_ai_that_scales/
Deep Learning breakthrough made by Rice University scientists,1576639974,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ec720c/deep_learning_breakthrough_made_by_rice/
This robot helps you lift objects — by looking at your biceps,1576639847,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ec710p/this_robot_helps_you_lift_objects_by_looking_at/
Deep Double Descent,1576639808,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ec70rn/deep_double_descent/
This object-recognition dataset stumped the world's best computer vision models,1576639692,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ec6zzz/this_objectrecognition_dataset_stumped_the_worlds/
DeepMOT,1576629535,,neuralnetworks,capricornfati,False,/r/neuralnetworks/comments/ec4w7l/deepmot/
I synthesized Morgan Freeman’s voice for a video I made for Nvidia,1576622126,,neuralnetworks,ahadcove,False,/r/neuralnetworks/comments/ec391u/i_synthesized_morgan_freemans_voice_for_a_video_i/
Is Neural Net file compression possible?,1576544564,"I'd like to know if it possible to create a Neural Net that compresses files as much as possible. I've only just started looking to machine learning, however I am proficient in programming. My guess is it would work by creating a checksum for the input file, getting the current file size, doing something to the file (I'll call this process #), rechecking the file size, do # in reverse, then compare original checksum to previous one.

I'm sure someone must have attempted this before, but I can't find anything on it. If anyone has any idea/starting point for me to look into, that's be great. 

Thanks!

Also, this is sort of a cross post from my post on /r/tensorflow asking the same question.",neuralnetworks,Tcmaxwell2,False,/r/neuralnetworks/comments/ebocd2/is_neural_net_file_compression_possible/
Composing Neural Networks,1576519811,"I'm not so familiar with the field, but I work more broadly in theoretical computer science. A large focus of my field is the idea of \`compositionality', i.e. how we can construct complex things by the plugging together of smaller things. In general, the process of composition is complex, whereas the objects plugged together may themselves be simple.   


I was wondering if this approach might have any traction with resepct to neural networks. I'm not sure exactly what form this would take, but there seem to be two ""obvious"" ways of plugging neural networks together: by placing the underlying graph of ""neurons"" one on top of another (depth-stacking), or by placing them side-by-side (width). Of course, plugging the underlying graphs together is totally uninteresting - the interesting part has to be how one might re-calculate the weights and thresholds of the two neural networks once they are composed. This is just an example, I stress.",neuralnetworks,Feral_P,False,/r/neuralnetworks/comments/ebiqkb/composing_neural_networks/
PyTorch Deep Learning Framework: Speed + Usability,1576513851,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ebhcs6/pytorch_deep_learning_framework_speed_usability/
Help with Neural Network talk,1576510031,"Hi, I am planning on doing talk at my school and have created a Powerpoint on Neural Networks however I am in no way qualified in Neural Networks/AI etc.. and as such I was wondering if I could get some feedback on the powerpoint. Feedback in any capacity would be greatly appreciated.

[https://1drv.ms/p/s!Anpcdo40FuuikhgIAYRMGF\_ykxHb?e=VgtX38](https://1drv.ms/p/s!Anpcdo40FuuikhgIAYRMGF_ykxHb?e=VgtX38)",neuralnetworks,Al4xdalegend,False,/r/neuralnetworks/comments/ebgit9/help_with_neural_network_talk/
Damage Inspection with AI - Automating Claims Processing for Insurance,1576489953,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/ebd66v/damage_inspection_with_ai_automating_claims/
Top 5 Deep Learning Research Papers in 2019,1576483741,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ebcbly/top_5_deep_learning_research_papers_in_2019/
Word sequence decoding in NLG,1576377956,[https://towardsdatascience.com/word-sequence-decoding-in-seq2seq-architectures-d102000344ad](https://towardsdatascience.com/word-sequence-decoding-in-seq2seq-architectures-d102000344ad)?,neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/eatd6a/word_sequence_decoding_in_nlg/
PyTorch Intermediate Tutorials for Deep Learning in C++,1576302877,"Hi C++ and Machine Learning/Deep Learning Folks!

My project [C++ Implementation of PyTorch Tutorials for Deep Learning Researchers](https://github.com/prabhuomkar/pytorch-cpp) is updated with Intermediate Tutorials containing:

* Convolutional Neural Network
* Deep Residual Network
* Recurrent Neural Network
* Bidirectional Recurrent Neural Network
* Language Model (RNN-LM)

Thanks to [Markus Fleischhacker](https://github.com/mfl28) for his daily contributions which made this happen. Show some love by starring/forking/contributing to more tutorials/reporting issues. Cheers!",neuralnetworks,op_prabhuomkar,False,/r/neuralnetworks/comments/eag1jz/pytorch_intermediate_tutorials_for_deep_learning/
Need help with building a variational autoencoder,1576293816,,neuralnetworks,Piperipheral,False,/r/neuralnetworks/comments/eaejb2/need_help_with_building_a_variational_autoencoder/
Neural Network -&gt; Not a Neural Network,1576266613,"Hey, fellow neural networkers.  I have a question that has been bouncing around in my head, but I have had no luck finding an answer to, even with Google.

Can a trained neural network be translated into a 'standard' program that is not a neural network and give the exact same results?  

I know that very simple neural networks are the exact same as binary operators.  I also understand that a neural network with many nodes and hidden layers can be far more complex.  But has anyone tried to show that a neural network is the equivalent of a non-neural network?",neuralnetworks,emeri1md,False,/r/neuralnetworks/comments/ea8v0t/neural_network_not_a_neural_network/
AI Motherboard?,1576265775,"Hi,
I'm looking forward to a new pc and browsing for viable motherboards I stumbled over mining motherboards with lots of pcie x1 (and a single x16) slots .
Could I use one for AI programming?",neuralnetworks,Ramox_Phersu,False,/r/neuralnetworks/comments/ea8oev/ai_motherboard/
Challenge On! Tech Giants Unite to Fight Deepfakes With AI,1576254705,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ea67m6/challenge_on_tech_giants_unite_to_fight_deepfakes/
NeurIPS 2019 | The Numbers,1576193977,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e9vnx1/neurips_2019_the_numbers/
Questions on some basic deep learning stuff.,1576178123,"I recently came across temperature softmax
             , when do I use it,  
                temperature value a constant ( how do I choose it)
                 learnable parameter ( how do I define the function )
I want to train my own face recognition using triplet loss
         Do I need to get the face of whom I want to recognise or is the.                     model able to learn simalirity between faces and hence any arbitrary     dataset will do.
Eventually do a knowledge distillation on vgg face ? Will it work


Please stop my mind from going crazy.",neuralnetworks,champulal,False,/r/neuralnetworks/comments/e9s3q6/questions_on_some_basic_deep_learning_stuff/
AI Reimagines Ancient Chinese Poetry,1576167967,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e9prbw/ai_reimagines_ancient_chinese_poetry/
My response to Dask CUDA,1576152011,,neuralnetworks,DoDzilla_AI,False,/r/neuralnetworks/comments/e9mne1/my_response_to_dask_cuda/
"Could someone please explain the difference between Keys, Values and Queries when doing Multi-Headed Attention",1576100471,"I'm a bit confused as to how the Keys, Values, and Queries are different when doing multi-headed attention. Could you please provide a conceptual distinction. Thanks!",neuralnetworks,mutatedmonkeygenes,False,/r/neuralnetworks/comments/e9dotv/could_someone_please_explain_the_difference/
"Would it be possible to create a neural network for battling pokemon, such as with the Pokemon Showdown! battle simulator?",1576086854,,neuralnetworks,Toasterkid13,False,/r/neuralnetworks/comments/e9ag3q/would_it_be_possible_to_create_a_neural_network/
"Hey everyone! I created a Youtube channel called Backit - where I make c# projects (AI learns to drive, understanding perceptron, etc.)! I love working on it and sharing more with the community - so it would mean a lot if everyone could check it out ♥️",1576085099," About my projects:

I started to learn about AI - and [in my latest video](https://youtu.be/UfX7XkDBoFk) I teach cars to drive with the help of genetic algorithm and neural network. I uploaded the code to [Github](https://github.com/martenq1337/COPS_AI) \- so it is available for everyone. I just wanted to mention this, because [another subreddit liked](https://www.reddit.com/r/learnmachinelearning/comments/e7s1q7/if_you_are_interested_in_genetic_algorithm_and/) it.

If you are interested about the basics of AI, I [made a video](https://youtu.be/Bfj-TizP_Pw) about perceptron, which is a single layer neural network - and I also made an implementation in C#.

I would like to expand the circle of people who can provide useful knowledge to the world for free - so if it aroused your interest, [here you can find my channel](https://www.youtube.com/channel/UCKJxd4PwFOvJVUi49KaoYLw).

Have a nice day!",neuralnetworks,lacaai,False,/r/neuralnetworks/comments/e9a1o3/hey_everyone_i_created_a_youtube_channel_called/
"How a company can automate digitization of menu cards using OCR and deep learning, why the problem is a difficult one to crack and what it takes to build a modern pipeline.",1576068891,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/e96quf/how_a_company_can_automate_digitization_of_menu/
Layering activation functions.,1576009869,How effective is using multiple activation functions? Such as using a Sigmoid then a tanh?,neuralnetworks,KilroyWasHere189,False,/r/neuralnetworks/comments/e8w7v4/layering_activation_functions/
Japanese Unicorn Preferred Networks Migrates Its DL Platform to PyTorch,1575995245,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e8stqg/japanese_unicorn_preferred_networks_migrates_its/
"Counting Cards Using Machine Learning and Python - RAIN MAN 2.0, Blackjack AI - Part 1",1575947581,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/e8ktfj/counting_cards_using_machine_learning_and_python/
"Raspberry Pi 3 vs Raspberry Pi 4 Performance with TensorFlow, TF Lite, &amp; Coral USB Accelerator",1575947542,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/e8kt4y/raspberry_pi_3_vs_raspberry_pi_4_performance_with/
Deep learning compute system is billed as world's fastest,1575947482,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/e8kson/deep_learning_compute_system_is_billed_as_worlds/
Neural networks extract information from sparse datasets,1575947369,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/e8kruv/neural_networks_extract_information_from_sparse/
Kaolin: The first comprehensive library for 3-D deep learning research,1575947321,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/e8krj1/kaolin_the_first_comprehensive_library_for_3d/
Question about Radial Basis Function Network,1575945550,"Hi, I need help in using Radial Basis Function Neural Network for 2D coordinate transformation. I'm a complete newbie and only read papers about it, however they didn't really share which software they use so I'm at a lost here. Would like to conduct the same research, and of course for a different coordinate systems.   
The questions I'd like to ask:  
1. What software should I use?  
2. How does RBFN convert a certain coordinate system to another?  


Hoping for kind and nice replies. Thank you.",neuralnetworks,TheRealSlimDiCaprio,False,/r/neuralnetworks/comments/e8kec7/question_about_radial_basis_function_network/
Having problems performing a Dot product on 2 vectors,1575923185,"I get the first error when trying to Dot the self.wih vector (100 hidden nodes, 1024 input nodes) with my input vector (3x32x32). I figured this was because the number of columns weren't matching, So I tried reshaping, by doing 'train\_data\[ 'X' \]\[ : , : , : , i \].reshape(3,1024)', which results in the error in the second image in red. I think the overflow error is because it is generating a number out of bounds, but I'm not sure how to go around this.

Please can someone give me some hints on how to fix this?

&amp;#x200B;

https://preview.redd.it/8mxy742h2o341.png?width=992&amp;format=png&amp;auto=webp&amp;s=5acabe4445c9ac98749e290461ce72c4e02e4116",neuralnetworks,kennnnnnnnnnnnnnn,False,/r/neuralnetworks/comments/e8fhbm/having_problems_performing_a_dot_product_on_2/
Playing Space Invaders Blind | RL &amp; Cross Modality Transfer,1575918739,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e8efvp/playing_space_invaders_blind_rl_cross_modality/
2019’s Top Machine and Deep Learning Research Papers,1575918697,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/e8efiz/2019s_top_machine_and_deep_learning_research/
Creating Custom TensorFlow Dataset,1575881991,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/e87ji5/creating_custom_tensorflow_dataset/
RPG with 'dungeon master ai' and 'story engine' in the works using neural network and machine learning,1575877765,"https://www.youtube.com/watch?v=tw6CUVk4mn0

The Story Engine that the studio is teasing us with is smart enough to act as your human DM. It would track the repercussions of your actions across the open-world game, it would have NPCs react and respond to your character in an evolving way.

Here is what they said in the video about it:

""Julian and I have worked in multiple situations where we had to design story telling apps that would build a story around your actions. Nobody in the RPG industry as far as I know have spent the time that we have spent inside the educational industry trying to do this and we have spent a lot of time modifying prototyping and building a story engine that we think will change how an RPG is done. We want to do what it felt like when a DM would sit across from you and react to what you did.

It started in infancy at Arena and was very primitive, Daggerfall was more involved and then every once in a while, years after, it was obvious that a story engine would be needed, and it would get more advanced every time I was on educational products. At this point i've been tinkering with it on and off for a few decades. It's gotten pretty good at this point. We hope it will be able to drive stories that would simulate a real person, Dungeon Master specifically, and continually keep the player with some decent stories but also make sure that there is continuation to these stories that would affect the world in a meaningfull way and in a persistant way so there is meaning in the short term to the quests and meaning after the quest is completed. That's the idea and it looks like after all that time, the technology will be able to do something like that.""",neuralnetworks,bugsixx,False,/r/neuralnetworks/comments/e86y0q/rpg_with_dungeon_master_ai_and_story_engine_in/
I how can I figure out the hidden layers for a given task?,1575858020,"I cant find an answer, can anyone link a source where they have examples and answers?",neuralnetworks,Imagine-existance,False,/r/neuralnetworks/comments/e83gku/i_how_can_i_figure_out_the_hidden_layers_for_a/
Convolutional Neural Networks (CNNs) explained,1575835329,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/e7ylbd/convolutional_neural_networks_cnns_explained/
What boolean function is being computed here?,1575796431,,neuralnetworks,guacforlife13,False,/r/neuralnetworks/comments/e7rpf5/what_boolean_function_is_being_computed_here/
[NN idea] Remix songs. Hear me out on this,1575795141,"Making this from scratch would be hard. 
I assume that one would need a very large training set, consisting of original songs and their remixes.
I'm not sure if one could specify multiple outcome genres.

The other method I thought up is that if Deezer can do it in the one direction with Spleeter, maybe one could do it in the other direction as well. If we took the Neural Network behind Spleeter and somehow reversed it, it would theoretically work to splice together vocals+drums+instruments+other into finished mp3s.

It seems like the resulting NN would only need some minor tweaking, or even just preprocessor code, in order to be able to take let's say the isolated drum track from song 1, and have it play whenever the drums in song 2 (no pun intended) would play, instead of them.

I'm thinking of something like: A Neural Network that can replace instrument tracks in songs with other instruments.

It seems like it should be possible, but I'm just rambling. Maybe someone finds this though provoking or sth.",neuralnetworks,precision1998,False,/r/neuralnetworks/comments/e7rjjh/nn_idea_remix_songs_hear_me_out_on_this/
Have some basic questions on neural networks,1575795128,"Hey! I’m trying to wrap my head around neural networks but I still have some questions. 

\-What happens from layer to layer?

\-What is the advantage of having multiple layers?

\- Why are there less inputs than layers? 

\- What does each neuron in a layer represent?

If you could help me out with any of these questions, that would be great!",neuralnetworks,IncomprehensibleFact,False,/r/neuralnetworks/comments/e7rjh7/have_some_basic_questions_on_neural_networks/
"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers.",1575721642,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/e7dr5a/this_video_goes_over_a_model_that_predicts_the/
Weekly Papers | Fine-Grained Neural Architecture Search; Adversarial Examples Boost Image…,1575665595,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e74dug/weekly_papers_finegrained_neural_architecture/
"Is it better to train a new model from scratch or using weights from a previous, loosely related, model?",1575660682,"Posting this more for discussion and hearing out various arguments, rather than for getting a one-size-fits-all answer — which likely doesn’t exist.

So, say, you trained a deep net to classify plants based on an image (not my case, so I’m making that up), and it does it well now. Then you want to train another network, say, to classify animals. Will you be better off training it from scratch, or using the weights from the plant classification network as a starting point?

Arguments for starting from existing work: The net has probably learned some basic stuff like zooming/rotation/recognizing basic shapes in its first layers.

Arguments against starting from existing work: As some of the net’s mechanisms might by now be very “plant-specific”, it might end up in a local minimum when trying to classify animals, based on these specific mechanisms.

What do you think? Have you ever faced a similar problem in your work? How did you end up doing it?",neuralnetworks,vzakharov,False,/r/neuralnetworks/comments/e73axz/is_it_better_to_train_a_new_model_from_scratch_or/
Neurosymbolic AI Advances State of the Art on Math Word Problems,1575660016,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/e735pr/neurosymbolic_ai_advances_state_of_the_art_on/
How do i use the SVHN datasets?,1575639713,"I am trying to use the svhn dataset in jupyter notebook. found here:
http://ufldl.stanford.edu/housenumbers/

The dataset contains 73257 images, each of size 32x32x3

I have loaded the train and test .mat files using scipy.io.loadmat(), but then how do i look at each image as a 1 dimensional vector of size 32x32x3?",neuralnetworks,kennnnnnnnnnnnnnn,False,/r/neuralnetworks/comments/e6ywhc/how_do_i_use_the_svhn_datasets/
The only Tesseract and OpenCV guide to building OCR engines in Python you'll ever need,1575606730,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/e6tn5j/the_only_tesseract_and_opencv_guide_to_building/
Opinions on how to scale residual connection to different dimensions [keras example],1575606122,"Context is a deep CNN in python/keras with tensorflow. Let's assume 1D for simplicity.

In my understanding, the purpose of a residual connection is to 1) allow the gradient to flow more easily and/or to 2) allow deeper layers to not make the model worse (I.E. they can just zero out and the previous layer pass through in the worst case).

If we are going between layers that have the same number of filters, it's a simple addition between the more shallow layer and the current layer)..

    merged_layer = add([old_layer, cur_layer])

If the current layer has, say, 2x more filters (common), traditionally we add a projection layer of kernel size 1:

    projected = Conv1d(old_layer.shape[-1].value, 1)
    merged_layer = add([projected, cur_layer])

Another possibility I realized is just repeating the layer we wish to pass along:

    concat = concatenate([old_layer, old_layer], axis=2)
    merged_layer = add([concat, cur_layer])

Note: if there was a pooling layer that reduced the size of the filters, we'd need to apply that to the residual connection in both cases.  Also, such doesn't add learnable parameters to the model.

It seems like the latter is more in the spirit of the concept of a residual connection.  Is there a reason I am not thinking of for doing the former?",neuralnetworks,elbiot,False,/r/neuralnetworks/comments/e6tj33/opinions_on_how_to_scale_residual_connection_to/
Deep Learning Improves Structural Health Monitoring of Civil Infrastructure,1575563069,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e6ji4j/deep_learning_improves_structural_health/
Statistical Mechanics of Deep Neural Networks,1575513917,"We present our progress towards a new Theory of Deep Learning

[https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/](https://calculatedcontent.com/2019/12/03/towards-a-new-theory-of-learning-statistical-mechanics-of-deep-neural-networks/)",neuralnetworks,calculatedcontent,False,/r/neuralnetworks/comments/e6ammq/statistical_mechanics_of_deep_neural_networks/
Personal Invisibility Cloak Stymies People Detectors,1575478413,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e62223/personal_invisibility_cloak_stymies_people/
How can I make an NN that takes multiple csv as input with various/specific columns being used for learning. I have only 2 Labels.,1575468884,"Hi. I want to make an NN (most likely a 3D one) which will take multiple csv files as input. Those files only have 2 labels and output will also be of the same 2 Labels. Each file has multiple columns that contain the data and can reach upto 800 rows of data. It's not the same for each file.

Can someone please provide a link to such a program that can accomplish this or something similar? The help will be greatly appreciated.",neuralnetworks,HudakSSJ,False,/r/neuralnetworks/comments/e5ztc1/how_can_i_make_an_nn_that_takes_multiple_csv_as/
Can Simple Neuron Interactions Capture Complex Linguistic Phenomena?,1575326947,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e571ia/can_simple_neuron_interactions_capture_complex/
Why neural nets have become a big business,1575314383,,neuralnetworks,jonfla,False,/r/neuralnetworks/comments/e53x8u/why_neural_nets_have_become_a_big_business/
is there an image likeness/matching/sorting programs that are for windows?,1575286842,"I dont know if this is the right forum, but if not does anyone know where is?

&amp;#x200B;

I am a competent end user of software but zero coding skills, i'm  code-phobic for some reason, but massively productive and creative with software. 

&amp;#x200B;

I really would love a simple program that uses an image 'A' and sorts through a folder and finds images 'B' with similar likeness, based on large shapes and/or colour. 

&amp;#x200B;

&amp;#x200B;

I have heard that this is basically what neural networks are good at, and wondered if there is a program that someone on windows could use?  (even if fairly slow and using single x64 cpu)

&amp;#x200B;

&amp;#x200B;

I have thousands of artworks and would like to make an animation with them, by using an animation (image A) and finding matches from a folder that could be used in that frames place",neuralnetworks,laseluuu,False,/r/neuralnetworks/comments/e4xwrs/is_there_an_image_likenessmatchingsorting/
I made a video how did I train AI cops to catch the criminal. I used genetic algorithm + NN for training. It also includes Github link to the project.,1575224967,,neuralnetworks,lacaai,False,/r/neuralnetworks/comments/e4lc9d/i_made_a_video_how_did_i_train_ai_cops_to_catch/
I made a video about COPS AI in Unity - Github link included,1575217484,,neuralnetworks,lacaai,False,/r/neuralnetworks/comments/e4ji6s/i_made_a_video_about_cops_ai_in_unity_github_link/
Neural Network to draw an image?,1575171799,"Hey guys, I have seen some Twitter posts of neural networks that can take a bunch of movie scripts and generate a script of their own. Would it be possible to do something similar with a picture? I think it would be funny to see the results if we fed it a bunch of meme formats or something and tried to make its own, or a bunch of video game characters and it made its own protagonist and villain. Any ideas where I might want to start with something like this? It would be my first NN project.",neuralnetworks,hamhub7,False,/r/neuralnetworks/comments/e49pa2/neural_network_to_draw_an_image/
Training Text Classifier to detect SPAM using CreateML,1575133502,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/e3zl0h/training_text_classifier_to_detect_spam_using/
"A Deep Neural Network based face attributes detection web application developed using PyTorch, Flask, ReactJS, and Firebase Storage.",1575127681,,neuralnetworks,thevatsalsaglani,False,/r/neuralnetworks/comments/e3y1el/a_deep_neural_network_based_face_attributes/
Deepfake resources,1575087573,"I have an idea for using Deepfakes in education. I've been peddling it around Universities as a possible PhD project. So far I've received positive replies about the idea. I'd like to begin experimenting now, before the programs start in the fall. I have some pointers to some big papers in the field (e.g. fake Obama, GANs, the dancing deepfake, etc.), but I would certainly welcome more recommendations. My main concern is getting some hardware to make rapid iteration tractable. I have access to my University's cluster, but I would rather have direct access to a solution to make renders and do experiments without potential delay's. I'll be making an appointment with the people who run the cluster going forward to ask their opinion, but I thought I would ask you for your recommendations first, to get a lay of the land.

Thank you.",neuralnetworks,GetsTrimAPlenty,False,/r/neuralnetworks/comments/e3qjl2/deepfake_resources/
Question - Network structure,1575057202,"Hello, 

I have a very basic question about Neural Networks. I just started on a project to create a basic Neural Network algorithm and is making some basic research into the method. I studied an article about neural networks and didn't understand a sentence. It says that "" The final model used the QuickProp learning rule with Sigmoid transfer function with one hidden layer and a 40-7-1 network structure"". I was mostly wondering, what the netowork structure is referring to, is it taking 40 inputs, using 7 filters and creating one output or what exactly does it mean? 

I am a total beginner, and new to this subreddit, so please forgive me if this is against the rules or stupid.

Link to paper:  [https://link-springer-com.proxy.shh.fi/article/10.1007/s11162-017-9473-z](https://link-springer-com.proxy.shh.fi/article/10.1007/s11162-017-9473-z)",neuralnetworks,Thelastgoodemperor,False,/r/neuralnetworks/comments/e3jux8/question_network_structure/
NEAT Slime volleyball. They are not that good...,1575049794,,neuralnetworks,yinyongjohn,False,/r/neuralnetworks/comments/e3i1v7/neat_slime_volleyball_they_are_not_that_good/
"This video goes over a breast cancer diagnosis model that uses neural networks, implemented in Python.",1575034447,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/e3esp7/this_video_goes_over_a_breast_cancer_diagnosis/
The Age Of AI: How Will Brain Augmentation Affect Human Identity?,1574935800,,neuralnetworks,Ronex60,False,/r/neuralnetworks/comments/e2w4bk/the_age_of_ai_how_will_brain_augmentation_affect/
Best line by line code explanation of multi-headed attention in Transformer Networks I have seen in Pytorch. Uses highschool level language to explain graduate school level deep learning,1574887404,,neuralnetworks,lillythetaylor,False,/r/neuralnetworks/comments/e2mgiy/best_line_by_line_code_explanation_of_multiheaded/
Google &amp; Johns Hopkins University | Can Adversarial Examples Improve Image Recognition?,1574877606,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e2jv1k/google_johns_hopkins_university_can_adversarial/
Productivity of Human Study Methods,1574876420,"It's that time of the year where [Kairos](https://en.wikipedia.org/wiki/Kairos) is king. Single or low double digits on courses which cost hundreds of dollars! Instead of impulsively signing up for something that might not be useful, I'm looking for input on your personal study experience and what methods you used to obtain knowledge and experience for various use-cases.

I would like to know:

1. How did you study ANN/DL/ML? (university, paid-e-learning, youtube series, github)
2. How productive/useful did you find your resources/course? (lecturer's notes were epic, hardware/software was expensive or too slow, course was not practical enough, not enough training data)
3. Part time or full time and duration (roughly) to complete the course? (days, months, years)

Optional:

* Software stack(s) used? (Keras - via PlaidML, TensorFlow - SYCL implementation, PyTorch)
* Reason for studying? (student - just part of degree, student - specialized in data science, professional - company paid for it, hobbyist - procrastinating on doing taxes)
* Free courses recommend sponsoring over paid course? (Michael Nielsen, no Udemy is more productive)
* Courses or books on your todo list? (Deep Learning for Programmers by - Nikola Živković)
* Background prior to learning? (pure mathematics, computer science, actuarial science)
* How do you stay up to date with latest ANN/DL/ML research and tools?
* Which industry are you in?",neuralnetworks,fr33will,False,/r/neuralnetworks/comments/e2jjwb/productivity_of_human_study_methods/
Single Headed Attention RNN: Stop Thinking With Your Head,1574868873,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/e2hmnv/single_headed_attention_rnn_stop_thinking_with/
MarioNETte: Few-Shot Identity Preservation in Facial Reenactment,1574796995,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/e2358l/marionette_fewshot_identity_preservation_in/
"Transformer deconstructed, it's components explained in non-jargon language with toy examples in gory bottom to top detail, and assembled back together into a chatbot. A PyTorch Deep NLP tutorial with Deep Reinforcement Learning applied to dialogue on the way",1574788793,,neuralnetworks,clam004,False,/r/neuralnetworks/comments/e2133m/transformer_deconstructed_its_components/
Why does this not work?,1574719937,"import numpy as np  


class NeuralNetwork():  


def \_\_init\_\_(self):  
np.random.seed(1)  


self.synaptic\_weights = 1 \* np.random.random((3, 1)) - 1  
 def sigmoid(self, x):  # don't worry about the green siggaly line  
 return 1 / (1 + np.exp(-x))  


def sigmiod\_derivative(self, x):  
 return x \* (1 - x)  


def train(self, training\_inputs, training\_outputs, training\_iterrations):  
 print(""training"")  
 for interrations in range(training\_iterrations):  
outputs = self.think(training\_inputs)  
error = training\_outputs - outputs  
adjustments = np.dot(training\_inputs.T, error \* self.sigmiod\_derivative(outputs))  
 self.synaptic\_weights += adjustments  


def think(self, inputs):  
inputs = inputs.astype(float)  
 print('Inputs:')  
 print(inputs)  
output = self.sigmoid(np.dot(inputs, self.synaptic\_weights))  
 print('Output.think', output)  
 return output  


if \_\_name\_\_ == '\_\_main\_\_':  
neural\_network = NeuralNetwork()  
 print('Random synaptic weights:')  
 print(neural\_network.synaptic\_weights)  


   training\_inputs = np.array(\[\[4, 0, 0\],  # is robot  
 \[7, 3, 1\],  # isn't a robot  
 \[5, 8, 1\]\])  


   training\_outputs = np.array(\[\[1, 0, 3\]\]).T  # returns one if it thinks there is a robot// returns zero if it  
\# doesn't  
\# think that there is robot  
 neural\_network.train(training\_inputs, training\_outputs, 50000)  
 print('Synaptic weights after training:')  
 print(neural\_network.synaptic\_weights)  


   A = str(input('Input 1:'))  
B = str(input('Input 2:'))  
C = str(input('Input 3:'))  
 print('New situation data =', A, B, C)  
 print('Output data')  
 print(neural\_network.think(np.array(\[A, B, C\]).T))  


&amp;#x200B;

&amp;#x200B;

Output:

Output.think \[\[0.99909367\]

 \[0.99999987\]

 \[1.        \]\]

Synaptic weights after training:

\[\[ 1.75130241\]

 \[ 1.61614528\]

 \[-1.24006264\]\]

&amp;#x200B;

&amp;#x200B;

\*\*the one in the output should be a three",neuralnetworks,TheBeaver14633,False,/r/neuralnetworks/comments/e1nqlc/why_does_this_not_work/
Deep Learning for Programmers Ebook,1574675738,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/e1dwof/deep_learning_for_programmers_ebook/
"Training and Deploying a Multi-Label Image Classifier using PyTorch, Flask, ReactJS and Firebase data storage Part 1: Multi-Label Image Classification using PyTorch",1574663911,,neuralnetworks,thevatsalsaglani,False,/r/neuralnetworks/comments/e1c3s4/training_and_deploying_a_multilabel_image/
Help needed,1574614092,"Is there anyone who can help me understand a project and explain each individual line of code? The project is on glaucoma detection. I'm trying to do something similar to it but first I need to understand an example project. I'm fairly new to deep learning, so it would be very helpful if someone can explain what's going on over a videochat/voice call. I'll send over the code of anyone is interested. Thank you.",neuralnetworks,salmanc2,False,/r/neuralnetworks/comments/e10wsv/help_needed/
Different deep learning based OCR methods and Graph Convolutional Networks for automatic ID card information extraction,1574571301,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/e0u1r5/different_deep_learning_based_ocr_methods_and/
GAN for genome editing?,1574550231,"So my knowledge of GANs is limited but I had an interesting idea. All ethics aside, could you potentially train a GAN to create a genome based on features you want a child to have? Rather than editing each base pair directly, you would just tell the GAN what features you want and it would spit out a genome sequence that you would implement with some advanced version of CRISPR. Im not saying its a good idea or even ethically permissable, just interesting.",neuralnetworks,minimiles01,False,/r/neuralnetworks/comments/e0pr1h/gan_for_genome_editing/
Training CoreML Object Detection model from scratch using CreateML,1574529353,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/e0ksan/training_coreml_object_detection_model_from/
Some guidance for a begginer?,1574365871,"I have very little knowledge of neural networks, but i am trying to learn and i'm using [Brain.js](https://github.com/BrainJS/brain.js) to develop a simple neural network on javascript. I'm using a LSTM network and want to make it able to produce short pieces of text based off my training data. However, i'm struggling figuring out how many neurons/layers do i need and how big my training set should be. For now, best results that i've gotten have been using three hidden layers with 6/10/6 neurons, and a small set of 50 pieces of text around \~300 characters long, with a learning rate of 0.03. This setup seems to at best get down to a learning error of 0.06 after roughtly 200 iterations, but it stops improving after that and results are nowhere near desired (nonsensical, repeating same words etc).

Which direction should i go to improve this? Which parameters should i change? Any help or suggestions are greatly appreciated. Also, i have about 20.000+ pieces of learning data available, it's just that i've narrowed it to 50 to better understand what is going on.",neuralnetworks,Astro_BB,False,/r/neuralnetworks/comments/dzoscr/some_guidance_for_a_begginer/
Google Brain’s Nicholas Frosst on Adversarial Examples and Emotional Responses,1574359288,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dzn4zy/google_brains_nicholas_frosst_on_adversarial/
Big RNNs Achieve SOTA Performance in Video Prediction,1574265473,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dz3yy7/big_rnns_achieve_sota_performance_in_video/
Looking for a good advanced book about the latest research in neural networks,1574225614,"I am looking for a book that would review the latest research about neural networks. The book that would go into advanced concepts and topics that as much as possible.

&amp;#x200B;

I am absolutely proficient with programming, so I am not interested in coding examples.",neuralnetworks,dark_prophet,False,/r/neuralnetworks/comments/dyx1qj/looking_for_a_good_advanced_book_about_the_latest/
First personal project. I took the classic game of Snake and gave it a genetic algorithm.,1574205385,,neuralnetworks,Boostie204,False,/r/neuralnetworks/comments/dyspz8/first_personal_project_i_took_the_classic_game_of/
Can Bots Surpass the ‘Realism’ of Human Dialogue?,1574192681,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dypkau/can_bots_surpass_the_realism_of_human_dialogue/
Initial Help Story Creation,1574137747,"Hi everyone

I'm experimenting in the last few month with all kinds of neural networks. I was mostly playing around with image recognition and tensorflow.

Now I'd love to make something else. I would like to make a story telling neural network, which can create different stories out of its training data and in the future maybe change on user input. Unforunately I really don't know how thisntechnically should look like. 
I'm searching help?! Hopefully someone could help me understand the network I try to create.",neuralnetworks,Yingrjimsch,False,/r/neuralnetworks/comments/dyfg9k/initial_help_story_creation/
Any reason to have multiple hidden layers which have differing node counts?,1574105628,[removed],neuralnetworks,starved_student,False,/r/neuralnetworks/comments/dy83nu/any_reason_to_have_multiple_hidden_layers_which/
Huawei Tops ETH Zurich 2019 Smartphone Deep Learning Rankings,1574094208,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dy5dsk/huawei_tops_eth_zurich_2019_smartphone_deep/
Transfer Learning with TensorFlow 2 – Model Fine Tuning,1574066955,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/dy0my8/transfer_learning_with_tensorflow_2_model_fine/
Looking for sample projects using tensorflow,1573898622,"I’m looking for a couple of sample projects using tensorflow to play with while learning about neural networks.

I was thinking of taking and modifying the following kinds of games.

- snake game
- virtual car driving

Then I plan to take what I understand and build a tic tac toe game to see how deeply my mental neural network gets the design and how things work.

Any good suggestions projects on github?",neuralnetworks,IWuvComputers,False,/r/neuralnetworks/comments/dx51ma/looking_for_sample_projects_using_tensorflow/
The Beauty of Deep Neural Networks,1573853345,,neuralnetworks,bil-sabab,False,/r/neuralnetworks/comments/dwx86z/the_beauty_of_deep_neural_networks/
Weekly Papers | EMNLP 2019 Best Paper; Facebook XLM-R and More!,1573838510,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dwtvtn/weekly_papers_emnlp_2019_best_paper_facebook_xlmr/
World first ever computer RPG with 'dungeon master ai' and 'story engine' in the works using neural network and machine learning,1573805815,"https://www.youtube.com/watch?v=tw6CUVk4mn0

What the developers want to achieve is basicaly like there would be a human dungeon master in the game that reacts to your actions in the game.",neuralnetworks,bugsixx,False,/r/neuralnetworks/comments/dwnr9e/world_first_ever_computer_rpg_with_dungeon_master/
Platform support: TF only vs. Pytorch for object classification?,1573677806,"I made a post about a few things I'd like to do:

* How many squirrels appeared in front yard? Compare squirrel sightings this week with same week last year.
* How many cars passed in front of street? Compare car traffic this week vs. last week. Maybe even classify SUV vs sedans.
* How many people came up to the door. Send MQTT message when people detected in certain frame.

I'm very new to this, so will probably just be using pre-trained models from other people's tutorials.

I've narrowed my hardware choices to between Pi4+CoralUSB or Jetson Nano.  They're about the same price, and both seem easily accessible.  The item I'm not sure about is how important it is to have support for platforms other than TensorFlow.  For the things I want to do, are there projects that rely on Pytorch that are far supperior to TensorFlow projects?  The Jetson Nano has very good support for other platforms, but Google's Coral USB stick only supports TensorFlow Lite.  


Asking for feedback before committing to making a purchase.",neuralnetworks,androidusr,False,/r/neuralnetworks/comments/dvxylu/platform_support_tf_only_vs_pytorch_for_object/
Looking for a good guide to neural networks for beginners (preferably using MATLAB),1573640666,"I have become very interested in neural networks and machine learning in particular and would like to know where I can find good resources on getting started on these and beginning to understand them 

I have a background in engineering with a speciality in materials and software and have briefly  used neural networks to predict kinetics of reactions but that was some time ago and I would like to get back into it as I find machine learning (particularly neural networks) quite interesting. 

Does anybody have any good resources or beginner guides that would be helpful to me. 

Thanks in advance!",neuralnetworks,flor_sol,False,/r/neuralnetworks/comments/dvpv1h/looking_for_a_good_guide_to_neural_networks_for/
OCR review and moderation - Human in the loop workflows for deep learning solutions,1573623064,"[article link](https://nanonets.com/blog/human-in-the-loop-ai/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=hil&amp;utm_content=nn)

an interesting read on how digitization is affecting society, how automation is changing the nature of work and how human in the loop solutions can bridge the gaps. 

[human in the loop OCR moderation](https://i.redd.it/idusxdvh3ey31.gif)",neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/dvn21a/ocr_review_and_moderation_human_in_the_loop/
Actually what are timesteps? (TF Keras),1573609124,"I'm trying to do a sequence classification problem using a Conv1D-based network in TensorFlow.

I have a dataset with eight features. I have around 54,000 rows of these eight features, and I have five output classes. The data streaming is at 200 hertz. How on earth do I complete the timesteps part? I've been struggling for several weeks now in getting the dimensions to actually match and the arrays to match the expected functions - can someone explain how I split my data into timesteps, what dimensions they should be for the Conv1D, and how I go from there?

What even are timesteps? I can't find a solid explanation anywhere.",neuralnetworks,id_like_to_go_to_a,False,/r/neuralnetworks/comments/dvkc14/actually_what_are_timesteps_tf_keras/
[AI application with source code] Let your machine play Street Fighter!,1573599620,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/dvi5nr/ai_application_with_source_code_let_your_machine/
I'm looking for success/failure stories applying unsupervised document embedding techniques,1573561273,"Hey everyone! :)

As the title says, I am looking for both success stories and disappointing failures of applications of **modern** unsupervised document embedding techniques on actual problems (as opposed to academic benchmarks, toy datasets, academic evaluation tasks, etc.). The main focus is naturally on industry uses for business/product problems, but I would also love to hear about cases from government bodies, non-profits, use in research (with empirical measurement and where document embedding is one of the tools, not the subject of research) and any other ""real life"" use. I would love to hear about your experience, but connecting me to people you know or even hinting me towards companies or projects you know used these techniques (or tried to) would also be of tremendous help.

What's in it for you? Well, I'm preparing a talk for [the data science track of the CodeteCON #KRK5 conference](https://codetecon.pl/en/#program) based on my [literature review-y blog post on document embedding techniques](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d?source=friends_link&amp;sk=158194696b5fe4cad9605f4648eb2a83), and while I feel I have a pretty good overview of the academic papers, benchmarks and SOTA status up until the most recent stuff to come out in the field at this point in time, I can't say the same for uses in the industry; I have a partial view from my experience in one ongoing project to actually use this, and experience shared by some of my data scientist friends (all in Israel, naturally) - most of it, so far, by the way, is that averaging (good) word embeddings is a very tough ""baseline"" to beat.

This is why I thought reaching out to get a better sense of things in the industry world-wide, and enriching my talk with the status of actual successes and industry applications will give people attending my talk more value, and will serve my attempt to make my talk a status report on the topic.

And (coming back to WIIFM) naturally (I think), I intend to share any (share-able) knowledge I accumulate not only in my talk, but also by adding a section dedicated to it to [the aforementioned blog post](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d?source=friends_link&amp;sk=158194696b5fe4cad9605f4648eb2a83), and maybe even by writing an extended post around it (if enough interesting trends and issues come up). So, hopefully, if you are (like me) interested in this, we might also end up getting, together, a nice overview of where the industry stands at the moment.

What **modern** techniques (so no variants of bag-of-words or topic modeling techniques) am I talking about? These are the ones that I know of (I'd love to hear about others!):

* n-gram embeddings
* Averaging word embeddings (including all variants, e.g. SIF)
* Sent2Vec
* Paragraph vectors (doc2vec)
* Doc2VecC
* Skip-thought vectors
* FastSent
* Quick-thought vectors
* Word Mover’s Embedding (WME)
* Sentence-BERT (SBERT)
* GPT/GPT2 (can also be supervised)
* Universal Sentence Encoder (can also be supervised)
* GenSen (can also be supervised)

Thank you and cheers,  
Shay :)",neuralnetworks,shaypal5,False,/r/neuralnetworks/comments/dv98fu/im_looking_for_successfailure_stories_applying/
A great and thorough explanation of convolutional neural networks,1573556560,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/dv8his/a_great_and_thorough_explanation_of_convolutional/
(Totally computer generated) Here's what you get when a neural network posts a comment.,1573508175,"The Neural Networks Project: An introduction to Deep Learning for Computer Vision: Deep Neural Networks (CNNs) have been instrumental in making computers smarter for years. Now we are exploring the potential for them to help you look better too! Here are some more resources to get you started:
 
A Quick Primer on Deep Learning:
 
Deep Learning for Computer Vision
 
The Neural Networks Project is an open research project at the University of California, San Diego. We have designed and built deep neural networks to help us make computer vision systems smarter and more powerful.
 
Deep Neural Networks (CNNs) have been instrumental in making computers smarter for years. The image above is the one we get from a neural network that ""tweets"" a bunch of random tweets, and the image on the right is the same tweet that the neural network posted. In this case, we know that the neural network is not just doing a simple word-matching task: It's actually trying to come up with a new image based on the data that was provided to it. The neural network's goal is to predict what image it will create, and it's just doing that. But how is this accomplished?
 
In the second case, we can see how this works. We want the neural network to predict the location of a photo (using the word ""london""), which it is already able to do. We also don't know what's in that photo, so we need to provide some additional information to the neural network.
  
Google has just published an AI-driven image recognition system called WaveNet. It's a pretty big deal — the technology is capable of ""understanding human language"" according to a release.
 
To be clear, the WaveNet system doesn't understand words like ""fish"" or ""man"" or ""dog."" It's more like a machine that can understand sentences, and when it's given an image like the one above, it can tell it's a cat — because it recognizes that it's a face. (See a video below.)
 
A WaveNet neural network will be able to post a photo like the one above, where it's a cat with a human. (Image: Google Brain)
 
The neural network in the photo above was trained to recognize a cat, and when asked to recognize ""cat,"" it's able to do so. It doesn't know it's a cat...",neuralnetworks,Capn_Crusty,False,/r/neuralnetworks/comments/duzh8m/totally_computer_generated_heres_what_you_get/
Neural Network Tutorial 1 - Introduction to Neural Network,1573489268,,neuralnetworks,ProgrammingKnowledge,False,/r/neuralnetworks/comments/duun4j/neural_network_tutorial_1_introduction_to_neural/
AlphaStar: Grandmaster level in StarCraft II using multi-agent reinforcement learning,1573484749,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dutlel/alphastar_grandmaster_level_in_starcraft_ii_using/
Attention Is All You Need,1573484604,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dutk4i/attention_is_all_you_need/
Solving the three-body problem faster using a deep neural network,1573484525,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dutjej/solving_the_threebody_problem_faster_using_a_deep/
Microsoft + Snow Leopard Trust: Protecting A Threatened Species,1573484459,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dutiwb/microsoft_snow_leopard_trust_protecting_a/
Building An AI (Neural Networks | What Is Deep Learning | Deep Learning Basics),1573484405,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dutii2/building_an_ai_neural_networks_what_is_deep/
Transfer Learning with TensorFlow 2,1573467641,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/duqjd4/transfer_learning_with_tensorflow_2/
The Greatest Problem facing neural networks today,1573466129,"The Greatest Problem facing neural networks today is their lack of a good memory. For any task, it is crucial to be able to recall the best actions you took at any particular moment. That is, you should be able to recall in your mind, at any given time, all the actions you have taken during a task and how to apply them to the next. It's hard enough for an experienced computer programmer or computer scientist to think of what they want to write in a programming language in their head, and it would be much harder for a neural network. If a neural network gets its memory working, it would be able to write code, and thus do much of what programmers do. In this paper, I present a way to do that with a neural network.

The idea is that, by having the network do its own backtracking to remember where it has been,  it would be able to learn from its mistakes, and the best of the best would be able to get the best answer faster. This would let them do things like play chess or improve their programming skills. In fact, we could see a future in which all AI development would happen in this way.  it can now perform some functions, and so can become a neural network, a program that executes on a computer. I call this idea 'the memory paradox'.

This is not to say that the memory paradox is a proof of the correctness of AI or anything like that. It's not that the memory paradox has something to do with the correctness of AI. The memory paradox shows that a machine-like neural network, which learns what it does and remembers its experiences, could be a program that executes on a computer. It would be a program that knows how to code itself. The neural net would also understand that programming is a process of finding the best program and then learning how to implement it.

So it's not surprising that the neural net would recognize the memory paradox in a particular example. But that would have no bearing on whether AI was correct or not. The memory paradox is simply a demonstration that the neural net doesn't understand how programming is supposed to work.

That was a long answer, so I'll try to wrap this up quickly. Here are my answers to the most common objections to AI:

""This isn't real AI.""",neuralnetworks,ske_louse,False,/r/neuralnetworks/comments/duqaye/the_greatest_problem_facing_neural_networks_today/
2020 AI Residency Guide,1573319240,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dtymvc/2020_ai_residency_guide/
Energy consumption,1573308487,"What would be the best method to predict heat consumption, day ahead. E. Consumption is defined by outside conditions (radiation, wind, temperature, rain).",neuralnetworks,Borovc,False,/r/neuralnetworks/comments/dtwcgl/energy_consumption/
How to choose what's the best method?,1573297672,"at the moment I am writing my thesis. For one problem I am trying to solve I got a huge graph where every node has some values and every edge has some weights.

 Now I want to look at completly different set of nodes with different values and want to predict what their edges will look like depending on the values and what the network ""learned"" from the previous data where the edges where known. (example below)

My Problem is: I dont know what method fits me the best. Deep Reinforced Learning, Boltzman-Stuff, Clustering, Gradient-Boosting and so on.   


**How am I able to decide whats the best for my dataset without testing everything? Do you know of any books trying to compare different approaches on a scientific level?**

&amp;#x200B;

&amp;#x200B;

Example:  


**I know the following:**

* City A   
population: 5000  
industry: big  
country: germany  

* City B  
population 3000  
industry: small  
country: austria  

* Edge A-&gt;B  
weight 1000  


Imagine I got 1000 Datasets like this.

&amp;#x200B;

**Now I get two cities which were not in the set before:**  


* City B  
population: 4000  
industry: medium  
country: germany  

* City C  
population 5000  
industry: big  
country: germany  


and my goal now is to predict the weight of the edge from B to C, depending what I learned from the above.",neuralnetworks,LasagneAlForno,False,/r/neuralnetworks/comments/dtun4v/how_to_choose_whats_the_best_method/
Hauntingly beautiful response from Elon Musk's neural network prompt,1573288076,,neuralnetworks,Netras,False,/r/neuralnetworks/comments/dttc7n/hauntingly_beautiful_response_from_elon_musks/
Google T5 Explores the Limits of Transfer Learning,1573148931,"A Google research team recently published the paper Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer, introducing a novel “Text-to-Text Transfer Transformer” (T5) neural network model which can convert any language problem into a text-to-text format.

Synced invited Samuel R. Bowman, an Assistant Professor at New York University who works on artificial neural network models for natural language understanding, to share his thoughts on the “Text-to-Text Transfer Transformer” (T5) framework.

 [https://medium.com/syncedreview/google-t5-explores-the-limits-of-transfer-learning-a87afbf2615b](https://medium.com/syncedreview/google-t5-explores-the-limits-of-transfer-learning-a87afbf2615b)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dt1a29/google_t5_explores_the_limits_of_transfer_learning/
Derivative of convolution &amp; softmax layers,1573147867,"Basically, how do you go on about calculating the derivative of the cost function for a conv/softmax layer? Is there an analytical method, or do you approximate it?",neuralnetworks,PetrosOratiou,False,/r/neuralnetworks/comments/dt10e3/derivative_of_convolution_softmax_layers/
arXiv search: generating tags from papers w/ NLP,1573085317,[https://medium.com/@weights\_biases/generating-tags-from-arvix-an-intro-to-nlp-and-experiment-tracking-e79127604207](https://medium.com/@weights_biases/generating-tags-from-arvix-an-intro-to-nlp-and-experiment-tracking-e79127604207),neuralnetworks,c0rpus_call0sum,False,/r/neuralnetworks/comments/dsp4rd/arxiv_search_generating_tags_from_papers_w_nlp/
Introduction to Hyperparameter Sweeps,1573078042,[https://www.wandb.com/articles/introduction-hyperparameter-sweeps](https://www.wandb.com/articles/introduction-hyperparameter-sweeps),neuralnetworks,c0rpus_call0sum,False,/r/neuralnetworks/comments/dsnhor/introduction_to_hyperparameter_sweeps/
ProtoPNet Recognizes Birds and Shows Us How in Real Time,1573077190,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dsna22/protopnet_recognizes_birds_and_shows_us_how_in/
Numerical gradient descent vs gradient calculation and propagation,1573056120,"I'm fairly new to gradient based optimisation but was wondering about something.

Why  is not done that weights are changed, maybe in staged fashion, and then  an approximation of the gradient of error with respect to weights is  calculated?

I know it won't be as  accurate as taking the derivative of the error function and propagating  that backward but considering how SGD is favoured surely some  approximation in gradients is workable? It would allow for  non-differentiable error functions to be optimised, requiring far less  calculation and possibly even avoids the need for back propagation of  errors?

Does anyone know of this being done or is it just sub-optimal?

I  have heard in passing that this is how optimisation was basically done  before the discovery of BP so is that why it has been left behind?",neuralnetworks,In_for_a_pound,False,/r/neuralnetworks/comments/dsib13/numerical_gradient_descent_vs_gradient/
crop weed detection,1573050942,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dsh4pq/crop_weed_detection/
How to read text/watermark from image.,1573029790,Is there any python package to read text/watermark from set of images,neuralnetworks,kotesh_nitrkl,False,/r/neuralnetworks/comments/dsdkuz/how_to_read_textwatermark_from_image/
Do Deep Neural Networks ‘See’ Faces Like Brains Do?,1572980903,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ds3vqy/do_deep_neural_networks_see_faces_like_brains_do/
"AI, Deep Learning, Automation and the Insurance industry",1572947281,"This article will take you through all there is to know about using deep learning to drive smoother customer onboarding processes and automating claims processing in the insurance industry.

[article link](https://nanonets.com/blog/ai-in-insurance/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=ainsu&amp;utm_content=nn)

&amp;#x200B;

https://i.redd.it/sh218l1bauw31.gif",neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/drx0fq/ai_deep_learning_automation_and_the_insurance/
Eyes #ddgart,1572885266,,neuralnetworks,psenough,False,/r/neuralnetworks/comments/drk34g/eyes_ddgart/
Duplicate or similar image detection using vgg16,1572794019,I have a folder having images. Now how can i find similar or duplicate images from the folder or database,neuralnetworks,kotesh_nitrkl,False,/r/neuralnetworks/comments/dr1olu/duplicate_or_similar_image_detection_using_vgg16/
Check that 2 voices are saying the same thing,1572716705,"Hi,

I just started working on a program that should be able to change one person's voice into the voice of another with only 2 short recordings. I thought it will work by generating a ""voice profile"" from a recording (using triplet loss and GRUs). After having the voice profiles of each person, I need to convert an original sentence with voice 1 to the same sentence but with voice 2. I can easily make sure that the sentence with voice 2 sounds like voice 2, but I don't know how to make sure that the sentence with voice 2 is got the same content of the original sentence.  
I don't want to change the timing or the accent of the sentence. Also, if possible I want to preserve the pitch **changes** too (like someone is trying to sing).  
Do you have any idea how I can check this?  
Thank you!",neuralnetworks,amitgoren,False,/r/neuralnetworks/comments/dqnlwe/check_that_2_voices_are_saying_the_same_thing/
I need YOUR help with the NN project!,1572716426,"Hi there,

I am a senior in high school here in Ukraine, Eastern Europe and I am developing the project for the Ukrainian Scientific Academy (tournament) among youth, the project's idea is to create a useful and easy-to-use interface to create neural networks, with NO coding needed from the user making his own unique networks.

Please find the attached work-in-progress version of the project gif below. (NOTE: functions and objects are subject to change, there will be export/import, and everything the manually scripted NN has, but with convenient UI)

&amp;#x200B;

https://i.redd.it/la08m5ls6bw31.gif

My concern is what to do with the project after the tournament because for me the project seems like a great idea and it would be a stupid move to just throw it away. If you guys have any idea where this project will be really useful, please leave your comment. I was thinking about suggesting schools to implement it, but I am more interested in consumer usage. Any suggestions for the new features are also appreciated.

If anyone wants to join the project and build it together - feel free to dm me.

Thanks for your time.",neuralnetworks,Treedye,False,/r/neuralnetworks/comments/dqnjj0/i_need_your_help_with_the_nn_project/
Neural network reconstructs human thoughts,1572621322,,neuralnetworks,AlphaMaleMBC,False,/r/neuralnetworks/comments/dq5ky4/neural_network_reconstructs_human_thoughts/
https://m.techxplore.com/news/2019-10-neural-network-reconstructs-human-thoughts.html,1572621258,,neuralnetworks,AlphaMaleMBC,False,/r/neuralnetworks/comments/dq5kff/httpsmtechxplorecomnews201910neuralnetworkreconstr/
Any ideas for podcast name?,1572593589,"i have 've decided to create a podcast about deep learning ، machine learning ، artifical inteligence , neuroscience ,....... which interviews with experts . But unfortunately I can't choose a suitable title for it. Could anyone choose some titles for it ? plase hellp me",neuralnetworks,Doctor_who1,False,/r/neuralnetworks/comments/dq1063/any_ideas_for_podcast_name/
Choosing a loss function for binary labelling that doesn't punish indecisiveness,1572550584,"Hey so I'm trying to get a keras neural net to predict a player action in a game of poker. Right now it's a very simple label where the player can either raise or do something else. The thing that I can't get working is that I don't want the model to predict either way. My understanding is that binary crossentropy will encourage decision making from the net by punishing 0.5 prediction values and tending towards 0 or 1.

![img](khe9i39ugxv31 ""The distribution that I want to predict"")

The reason that I don't just offer the net these exact probability labels is because I want it to later generalise more appropriately and so I wouldn't have enough data, because in situations with a low sample size the neural won't be able to use those data points.

In sum, given the data point Ace King suited (or AKs), I would like to predict 95% or so. Given 87s I would like to predict 30%. I have tried binary crossentropy but it seems to get a little stuck on decisions such as K8s, which it predicts with a large 0.47.

model.add(layers.Dense(data\_cols \* 4, activation='relu',

input\_shape=(train\_data.shape\[1\],)))

model.add(layers.Dense(data\_cols \* 4, activation='relu'))

model.add(layers.Dense(data\_cols \* 2, activation='relu'))

model.add(layers.Dense(data\_cols \* 2, activation='relu'))

model.add(layers.Dense(data\_cols, activation='relu'))

model.add(layers.Dense(data\_cols, activation='relu'))

model.add(layers.Dense(1, activation='relu')) # should this be sigmoid?

model.compile(optimizer=opt, loss='binary\_crossentropy', metrics=\['accuracy', 'mae', 'binary\_accuracy'\])

I also tried to use mean absolute error in order to avoid a non-linear loss but my model seems to be unable to predict anything other than always raise (since raises make up 84% of the data set).",neuralnetworks,santiweight,False,/r/neuralnetworks/comments/dpsnvl/choosing_a_loss_function_for_binary_labelling/
Stock Predictor,1572510088,"Hello guys
I‘m trying to build a ANN which longs/shorts stock in a profitable manner for a data science project.

I‘m using the Keras Sequential model with the softmax activation function in the last layer to get the probabilities for whether to short/long or do nothing. 

Now here is the PROBLEM: for the unscaled data the prediction is almost always the same, so I rescaled it to have mean 0 and variance 1. But what do I do with new data, which I want a prediction for? Do I rescale it using the mean from the original dataset or calculate a new mean based on the new and old data? 

Thx, help is much appreciated.",neuralnetworks,maibaum_m,False,/r/neuralnetworks/comments/dpkim7/stock_predictor/
Whitepaper: AI birth to boom,1572435573,[removed],neuralnetworks,Krishanuu,False,/r/neuralnetworks/comments/dp5izf/whitepaper_ai_birth_to_boom/
Undergraduate Pathway?!?,1572386163,"I can't find many sources on what to study as an undergraduate to get into graduate pathways such as artificial intelligence, neural networks, and particularly computational neuroscience. 

Can someone please help me? 
I am currently a computer science major at a community college and have been ruthlessly fascinated by neuroscience since middle school. I just started learning computer programming and noticed how fitting it is to analyze neural pathways with this new lens of computer progeamming. 

How can I get out of this tier of education and into the world of neural networks?",neuralnetworks,Ruofkrof,False,/r/neuralnetworks/comments/dowzfj/undergraduate_pathway/
"How do I determine the output vectors for these 4 input vectors?, it's actually a BAM...",1572282548,,neuralnetworks,AlphaMaleMBC,False,/r/neuralnetworks/comments/dobs1s/how_do_i_determine_the_output_vectors_for_these_4/
[Announcement] Free GPUs for ML/DL Projects,1572265010,"Hey all,

I hope this is an appropriate subreddit for this. I just wanted to share this awesome resource for anyone learning or working with machine learning or deep learning. [Gradient Community Notebooks](https://gradient.paperspace.com/free-gpu) from Paperspace offers a free GPU you can use for ML/DL projects with Jupyter notebooks. With containers that come with everything pre-installed (like [fast.ai](http://fast.ai/), PyTorch, TensorFlow, and Keras), this is basically the lowest barrier to entry in addition to being totally free.

They also have an [ML Showcase](https://ml-showcase.paperspace.com/) where you can use runnable templates of different ML projects and models. I hope this can help someone out with their projects :)",neuralnetworks,nevereallybored,False,/r/neuralnetworks/comments/do80w7/announcement_free_gpus_for_mldl_projects/
Business Value of Artificial Intelligence,1572260181,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/do7886/business_value_of_artificial_intelligence/
Feedback and suggestions on my first neural network project,1572168415,"Hi to everybody.

I'm working on a project to make prediction over a time-series dataset(length about 2300 entries), I am using LSTM artificial neural networks.

I decided to split the full dataset in train\_set (2000 entries) and test\_set(about 300 entries). I used windows with 30 entries and two features(shape of 30, 2) from time t-30 until t, I desire to predict the next values of the series, at time t+1 for both the features.

I ""normalized"" the values for each window divideding each one by the value of the first row value\[0\] and the result subtracted by 1.

To make the prediction, I repeated 10 times a loop over a test\_set window with 30 entries In which I appended the result of the prediction to the window, discarding the oldest entry.

I got the best result with a network with 3 layers with 100 neurons, with 0.3 dropout, activation = ""relu"", trained with 30 epochs, but I think that probably It can do better.

The summary of my model with the command:

     model.summary()",neuralnetworks,ouyeahh,False,/r/neuralnetworks/comments/dnr3gr/feedback_and_suggestions_on_my_first_neural/
Neural networks vector notation,1572156292,"I have studied the mathematics behind neural networks such as gradient descent, chain rule and finding the partial derivative of loss/error function such as cross-entropy, sigmoid etc.
Now, I want to understand the vector notation which is used while implementing a neural network from scratch using numpy framework. Where can I find a good resource for the vector notation part of it?
Thanks!",neuralnetworks,grid_world,False,/r/neuralnetworks/comments/dnpkgq/neural_networks_vector_notation/
neural network feature sensitivity and feature optimization.,1572109784,"I have an interesting regression problem where I'm trying to predict the water level (i.e water volume) in a low-lying, coastal detention basin. I'm using a recurrent neural network with a time series of environmental forcing conditions like precipitation, tide and air temperature to predict water level in the detention basin (paraphrasing here because it's actually quite a complicated system that we've failed to model numerous times using process-based numerical models).

The kicker is that there's a flow control structure allowing the basin to drain to the ocean, but as you might expect, it can only drain when conditions are right. I've included the historical record of gate operations for the flow control structure as a model feature, and was able to predict water levels in the basin quite accurately.

My question now is whether there's a conventional method within the field for determining the sensitivity of the trained RNN to each feature? (i.e. I want to know which features or combinations of features have the greatest relative influence on the water level?).  Furthermore, is there a conventional approach to determining optimal operations of the flow control structure to minimize water level in the basin?

Sorry if these are stupid questions. I'm new to ML so I still have a lot to learn.",neuralnetworks,cfharter,False,/r/neuralnetworks/comments/dngog8/neural_network_feature_sensitivity_and_feature/
Google AI Targets Video Understanding With Speedy ‘TinyVideoNet’ and Other Approaches,1572025814,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dn11c4/google_ai_targets_video_understanding_with_speedy/
What are the best autoencoder architectures for large images?,1572025614,"I have a relatively small set of greyscale images that are relatively large (about 500x600 pixels). The goal of my work is to do classification, but since my set of images is not too big, I thought it would be best to use an autoencoder and use a classifier on the latent code. Can anybody guide me to an overview page / reference that gives me appropriate architectures for such images? I have tried my own convolutional autoencoder architecture, but it gets stuck in optimisation when I go too deep. 

Some additional information is that I'm looking to classify the 'amount of blood vessels' in medical images, which can be very small and very large, so it would be nice if the information on the small blood vessels is kept, but it's not a problem if this cannot be done.",neuralnetworks,HenanL,False,/r/neuralnetworks/comments/dn0zgp/what_are_the_best_autoencoder_architectures_for/
[D] Any references on Super-Resolution on Graph using Graph (Convolutional )Neural Network?,1571931329,"Hi everyone,

I was wondering if there is any research/ paper on doing super-resolution on graphs -- going from coarse resolution mesh/ graph to fine resolution mesh/ graph. Any pointers/ references/ thoughts would be appreciated. Thanks a lot.

Q.) How to do DeConvolution on Graph ?

NOTE: I'm considering unstructured or multi-resolution mesh as a graph and applying graph (conv) neural network to it.",neuralnetworks,pradeep_sinngh,False,/r/neuralnetworks/comments/dmi44z/d_any_references_on_superresolution_on_graph/
Where can I find images of drones?,1571914261,"I'm building a system which involves detecting drones using a neural network. I'm unable to find large sources of image files of drones in an environment.

Can someone help out?",neuralnetworks,CultureImaginary,False,/r/neuralnetworks/comments/dmenyf/where_can_i_find_images_of_drones/
5 Facial Recognition Trends and Market Predictions 2019,1571912387,,neuralnetworks,ankur_bansal123,False,/r/neuralnetworks/comments/dmedjt/5_facial_recognition_trends_and_market/
Skin Cancer Classification Neural Network Case Study,1571844988,,neuralnetworks,lady_monsoon,False,/r/neuralnetworks/comments/dm10hr/skin_cancer_classification_neural_network_case/
Accelerating Sales Growth with Artificial Intelligence,1571833693,,neuralnetworks,akira_chaudhary,False,/r/neuralnetworks/comments/dlykty/accelerating_sales_growth_with_artificial/
What are some of the best methods for reducing overfitting in a CNN?,1571770532,[removed],neuralnetworks,murpheylawe,False,/r/neuralnetworks/comments/dlmi7g/what_are_some_of_the_best_methods_for_reducing/
The Origins of Neural Networks,1571754592,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dlipwv/the_origins_of_neural_networks/
A Self-Organizing Map for Multiclass Classification,1571720387,,neuralnetworks,inboble,False,/r/neuralnetworks/comments/dlcz56/a_selforganizing_map_for_multiclass_classification/
AI &amp; Architecture,1571600785,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dkopx6/ai_architecture/
AI &amp; Architecture,1571581676,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dkkatr/ai_architecture/
Understanding Resistive Switching,1571581636,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dkkajv/understanding_resistive_switching/
Deep learning method that transforms shapes to be presented at SIGGRAPH Asia,1571581591,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dkka7z/deep_learning_method_that_transforms_shapes_to_be/
Intel Movidius Neural Compute Stick,1571574916,Anyone here worked with NCS before? I am having trouble with understanding the output from the stick.,neuralnetworks,a_noob__,False,/r/neuralnetworks/comments/dkj1rq/intel_movidius_neural_compute_stick/
Visualizing how a network learns hand-written Digit recognition,1571565803,,neuralnetworks,trenmost,False,/r/neuralnetworks/comments/dkhpjq/visualizing_how_a_network_learns_handwritten/
Artist needing help understanding NN possibilities,1571535084,"I'm wondering if it's possible to train a NN to output a series of images (i.e. movie) from an input of a single still image by training it to associate movies with still images. 

Input: Still image

Output: Movie (series of images)

Training data available: Movies, still images

If anyone can explain to me the possibilities, difficulties, etc I would greatly appreciate it. I have no background in coding or computer science. I'm an artist working on a collaboration with my university's CSE department. I need to be able to write a proposal that clearly explains what I'm trying to achieve, and I also need to know whether it's even possible. Thank you!",neuralnetworks,unknoahble,False,/r/neuralnetworks/comments/dkczr6/artist_needing_help_understanding_nn_possibilities/
Simulating Neural Network - Digit Recognition - VISUALIZATION,1571506722,,neuralnetworks,Timbelion,False,/r/neuralnetworks/comments/dk6z9l/simulating_neural_network_digit_recognition/
Loss function for autoencoder with both continuous and discrete variables,1571475792,"I am trying to train an autoencoder. I have preprocessed my data, such that continuous variables are min-max scaled (now they are on interval 0-1) and categorical data are dummy variables. In output layer I am using sigmoid (does this make sense?). What loss function should I then use for the training ? For categorical outcomes I would normally use cross entropy and for continuous probably MSE, but I want to predict sth on interval 0-1 (thus sigmoid). Any ideas? Thanks.",neuralnetworks,green_pikachu,False,/r/neuralnetworks/comments/dk185v/loss_function_for_autoencoder_with_both/
Best architecture for guessing the second half of a (very simple) image,1571401303,"I have a very large set of very simple (say, 16x16, BW) images, along the lines of:

https://i.redd.it/8sbzsylikat31.png

I need a network that takes the left part of it and restore the right part. In doing so, it is preferrable that it does not just select one best-fitting image (as a GAN would do), but makes a grayscale “map” indicating probabilities that there would be a pixel in a given place (VAE?), like this:

&amp;#x200B;

https://i.redd.it/cl81gnsrkat31.png

Or even like this:

&amp;#x200B;

https://i.redd.it/dqiqujrwkat31.png

What is the best ANN architecture to do this?

(I know the problem might not seem to make sense, but it does to me.)

(Obviously, the term “image” here is just for convenience. We might just as well call it a 16x16 matrix.)

Thanks in advance for all the responses!",neuralnetworks,vzakharov,False,/r/neuralnetworks/comments/djmpp8/best_architecture_for_guessing_the_second_half_of/
AI and deep learning to tackle traffic congestion,1571389478,,neuralnetworks,aaku-101,False,/r/neuralnetworks/comments/djktam/ai_and_deep_learning_to_tackle_traffic_congestion/
How would I go about getting inputs for a nueral network to play a game?,1571353081,"For a game like Minecraft, Snake, etc. where the network doesn't have direct access to the code of the game, how could I give it inputs?",neuralnetworks,Cheeeeeeeeeeesy,False,/r/neuralnetworks/comments/dje93o/how_would_i_go_about_getting_inputs_for_a_nueral/
Sotabench: Benchmarking Open Source Models Directly From GitHub,1571326154,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dj7zjw/sotabench_benchmarking_open_source_models/
AR Meets AI | Analyzing Industrial Use cases and Business Benefits,1571299467,,neuralnetworks,Firdosali001,False,/r/neuralnetworks/comments/dj37el/ar_meets_ai_analyzing_industrial_use_cases_and/
Data Input Shape for CNN for Sale Price Prediction,1571232527,"I’ve been working on this project where I’ve collected a Data set of roughly 1500 homes and their respective sale price (within the last 6 months). For each home I have on average 31 jpg photos. My objective is to create a CNN to output a continuous predicted intrinsic estimate value of the home. For the time being, I only want the jpg files to be the input. I’ll expand on this after I get this smaller scale model working. 

I’ve been successful in the past putting a CNN into production, but only with input of one jpg. The issue I’m having here is that I have non-fixed number of jpgs for each home. 

I’m already familiar on how to transform the jpg into a multi-dimensional numpy with keras preprocessing, I’m mostly curious about suggestions for the overall architecture of the network. 

Here are my current options:
1. Create an empty multi dimensional numpy tensor that takes in max of 50 jpgs. Then for each jpg, preprocess into a tensor and insert it into the fixed dimension potentially leaving part of the tensor sparse.  Shape would be (num_photos x RGB x pixel_height x pixel_width) or (50 x 3 x 150 x 150) for example. Feed this into a feedforward network to get a baseline model. Downside is that it only take max of 50 photos. In rare cases, I have more than 50 photos.
2. Maybe create an RNN or LSTM layer that reads in multiple photos 

Suggestions/advice much appreciated",neuralnetworks,jjbourne712,False,/r/neuralnetworks/comments/dipbuf/data_input_shape_for_cnn_for_sale_price_prediction/
AI learn to play my game,1571156300,,neuralnetworks,lacaai,False,/r/neuralnetworks/comments/dia6yh/ai_learn_to_play_my_game/
How to Train a Progressive Growing GAN in Keras for Synthesizing Faces,1571126956,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/di4opw/how_to_train_a_progressive_growing_gan_in_keras/
29 Skills for Being a Successful Data Scientist,1571124786,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/di4d2j/29_skills_for_being_a_successful_data_scientist/
Accelerating Deep Learning by Focusing on the Biggest Losers,1571020379,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dhkca2/accelerating_deep_learning_by_focusing_on_the/
ReLU as a literal switch,1571009830,"An electrical switch is n volts in n volts out when on. Off 0 volts.

ReLU then is a  switch since it has exactly the same horizontal line (off), 45 degree (on) graph .

The weighted sum of some weighted sums is still a linear system.

With a ReLU neural network for a *particular*  input each ReLU switch is exactly in the on or off state.  The various  weighted sums wired together in a particular manner.  There is a  particular linear projection in effect from the input of the neural  network to the output.

Since ReLU switches state (on/off) at zero there is no sudden discontinuity of output for gradual changes of input.

A ReLU neural network is a seamless switched system of linear projections.

For  a particular input and particular output neuron the system of weighted  sums connecting the input to the output neuron can be condensed into a  single equivalent weighted sum.

There  are various metrics you can apply to that such as the angle between the  input and the equivalent weight vector.  If the angle is close to 90  degrees and the output of the neuron is large then the length of the  weight vector must be large (math of dot product),  Then  the output is  very sensitive to noise in the input (within the zone  where none of the  switches change.)  If the angle is close to zero then you actually get  the opposite effect, you get some averaging type error correction  (+CLT.)

There are efficient  algorithms for doing certain weighted sums (dot products) that you can then see are valid to incorporate such as the FFT: [https://github.com/S6Regen](https://github.com/S6Regen)

I deleted this from machinelearning where it caused a brain fail in one of the moderators.",neuralnetworks,sean5677,False,/r/neuralnetworks/comments/dhie1a/relu_as_a_literal_switch/
"How is the output of a maxpool with window:1x2, stride:2 calculated?",1571003542,[removed],neuralnetworks,Prior_Transportation,False,/r/neuralnetworks/comments/dhh5xd/how_is_the_output_of_a_maxpool_with_window1x2/
"It's just a beginning, but I took a very simple neural network and generated the weights with a quantum computer.",1570973365,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/dhalg3/its_just_a_beginning_but_i_took_a_very_simple/
"Does anyone have experience with training transfromers, Bert, alBert?",1570882167,"So does anyone know what I could feasibly train on a single RTX 2080?

How much would it cost to train a transformer for english to french translation? AWS, cloud TPU

Ballpark figures please, if the answer depends on some variable could you give an answer for a specific case?

Also, when training on a GPU, is 11GB+ recommended mainly due to the batch size required for convergence?",neuralnetworks,kajobkajob,False,/r/neuralnetworks/comments/dgubor/does_anyone_have_experience_with_training/
I can't believe it.... GPT2 774M,1570817374,,neuralnetworks,orenog,False,/r/neuralnetworks/comments/dgixz3/i_cant_believe_it_gpt2_774m/
How Neural Network recognize MNIST number dataset! Only 10% of weights are show.,1570803859,,neuralnetworks,Timbelion,False,/r/neuralnetworks/comments/dgfww6/how_neural_network_recognize_mnist_number_dataset/
Can you model a NN on an FPGA?,1570785377,"I've just learned about what FPGAs are, and I'm wondering if a NN model can be programmed onto an FPGA, so that when a specific input is given, it will give the predicted output. I don't mean use an FPGA to train the NN, but embedding a pre-trained model onto the FPGA to use for whatever. 

I have always like the idea of NNs being embedded in hardware, so that when the features get put into the system (as electrons), they just travel through wires and at the output electrons come out representing the prediction, fast and efficient compared to running a CPU which requires more complexity such as having machine code and being able to load and store data etc.",neuralnetworks,FIREATWlLL,False,/r/neuralnetworks/comments/dgcj5f/can_you_model_a_nn_on_an_fpga/
"Wait for cuDNN to ""finish""",1570754024,"I have a Pyhton/TensorFlow/CUDA/cuDNN model I am running.
If I run it once it is fine.  If I run the script immediately after it finishes I can (not always) get cuDNN errors.
When the error occurs I can usually wait a minute or so and then it will start working again.
Is there some way I can check if the cuDNN DLLs are ""done"" and know it will be OK to run my next script?",neuralnetworks,FerKerf1,False,/r/neuralnetworks/comments/dg798c/wait_for_cudnn_to_finish/
Why would my single layer NN model a cubic function?,1570749224,"I made a very simple genetic algorithm that randomly instantiates, and then mutates / does crossovers of neural networks to improve them. They each have one input node, 50 hidden nodes, and 1 output node. I test their fitness by taking the negative square of the error from the correct output.

I tried to train it to multiply a number by 5. I base fitness on their accuracy on random numbers from 0 to 1, I didn't train it for any other numbers. This worked well, and it achieves 100% accuracy after only a few hundred generations.

Then I was curious how this would extrapolate to other values, so I graphed the best model's predictions on new inputs up to like 100 and after adjusting the scale, the model's predictions grow directly proportionally to the cube of the input.

My idea is that maybe the fitness function is wrong because I'm basing fitness on the absolute value of the difference from the correct output, when maybe I should be taking a percent difference from the correct output? So maybe trending towards larger values, for some reason in a cubic fashion, as the input gets larger maximizes fitness on average?",neuralnetworks,vvvvvvvwvvvvvvv,False,/r/neuralnetworks/comments/dg68ut/why_would_my_single_layer_nn_model_a_cubic/
The Joy of Neural Painting,1570704465,,neuralnetworks,bil-sabab,False,/r/neuralnetworks/comments/dfw7oe/the_joy_of_neural_painting/
Data driven Frontend development using RNN and Markov Chains,1570655351,,neuralnetworks,iammowgoud,False,/r/neuralnetworks/comments/dfngei/data_driven_frontend_development_using_rnn_and/
"Watch Out, MIT’s New AI Model Knows What You’re Doing Behind That Wall",1570641206,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dfk4m8/watch_out_mits_new_ai_model_knows_what_youre/
Master Thesis Questionnaire,1570619613,"Hey guys, I'm developing a terrain generator that allows authoring using a neural network.  
I've  got some basic results and I am comparing them to a run of the mill  algorithm just to gauge people's perception regarding the realism of the  structures created.

The text is  in Portuguese but I'll translate and since the tests is visual, there's  no need to read all the questions ( they are all the same).

*""Comparação de realismo entre resultados de métodos generativos""* : Realism comparison between results of generative methods.  
*""Este  questionário tem como objetivo perceber qual a perceção dos  utilizadores relativamente ao realismo dos mapas gerados entre estes   dois métodos. Não existem respostas correctas.""* : This  questionnaire tries to understand the perception of the user regarding  the realism of the maps generated by these two algorithms. There are no  correct answers.  
*""Entre a imagem A e B, qual delas representa um terreno mais realista?""*  : Between image A and B, which one represents a more realistic terrain?

There's only two options and the test is visual so it doesn't take long: [https://forms.gle/QDbNbfGUZBvmXd4X6](https://forms.gle/QDbNbfGUZBvmXd4X6)

Thanks in advance guys!",neuralnetworks,Rogerjak,False,/r/neuralnetworks/comments/dffizj/master_thesis_questionnaire/
Difference between neural network and complex network?,1570544106,"Hi, doing a complex network + svm for classification and prediction. I am finding limited resources for complex networks. What is the difference between complex networks and neural networks. Networkx would be the lib I’m using for the complex network. What would be the difference between networkx complex networks and Keras neural nets? Thank you",neuralnetworks,AlphonseWestwood,False,/r/neuralnetworks/comments/df0jol/difference_between_neural_network_and_complex/
Neural network tutorial in Unity 3d,1570472065,,neuralnetworks,Timbelion,False,/r/neuralnetworks/comments/denbos/neural_network_tutorial_in_unity_3d/
Can you be Data Scientist and Software Developer at the same time?,1570469134,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/demmoh/can_you_be_data_scientist_and_software_developer/
Project suggestions in matlab for neural networks,1570446542,I need project suggestions for final year masters taking the the subject of artificial neural networks. I'm planning to do in MATLAB kindly suggest me some doable project within a month,neuralnetworks,joelbiju6,False,/r/neuralnetworks/comments/dehxxy/project_suggestions_in_matlab_for_neural_networks/
"If neural networks are really to emulate our brains, the equivalent of preprocessing has to occur after the input layer.",1570363659,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/de2dsc/if_neural_networks_are_really_to_emulate_our/
A fast neural network approach for direct covariant forces prediction in complex multi-element extended systems,1569909779,"
A fast neural network approach for direct covariant forces prediction in complex multi-element extended systems

Paper on fast neural networks for direct, covariant atomic force prediction is now out, published in Nature Machine Intelligence 

@NatMachIntell 

Neural network force field (NNFF) is a method for performing regression on atomic structure–force relationships, bypassing the expensive quantum mechanics calculations that prevent the execution of long ab initio quality molecular dynamics (MD) simulations.

Here, authors show a staggered NNFF architecture that exploits both rotation-invariant and -covariant features to directly predict atomic force vectors without using spatial derivatives, and we demonstrate 2.2× NNFF–MD acceleration over a state-of-the-art C++ engine using a Python engine. 

#physics #physicists #deeplearning #AI #neuralnetworks #artificialintelligence #atomicphysics 

ArXiv:
https://arxiv.org/abs/1905.02791

Nature Paywall:
https://www.nature.com/articles/s42256-019-0098-0",neuralnetworks,aiforworld2,False,/r/neuralnetworks/comments/dbp040/a_fast_neural_network_approach_for_direct/
Variational Autoencoders - EXPLAINED!,1569896065,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dbmgd7/variational_autoencoders_explained/
Create High Resolution GAN Faces with Pretrained NVidia StyleGAN and Google CoLab,1569895197,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/dbm9x7/create_high_resolution_gan_faces_with_pretrained/
The Wait is Over — TensorFlow 2.0 Released!,1569882361,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/dbjfp5/the_wait_is_over_tensorflow_20_released/
How to make an image generation NN ?,1569864205,"From what I understand you need a GAN ? Do you guys have good documentation/tutorials on how to make one ?

Also, is there no software where you just input like images and resolutions and a NN learns to make similar images ? It seems like the first thing to do to me, but I haven’t found any yet, and I probably am not good enough to make one myself.",neuralnetworks,Falquano,False,/r/neuralnetworks/comments/dbex50/how_to_make_an_image_generation_nn/
Could a neural network recreate a certain persons face with limited input data?,1569856857,"I need a high resolution picture of my grandfather for a portrait painting I’m making. Unfortunately,  he died a few years ago and all of the pictures of him are low resolution and/or not ideal for a portrait.

Could I use a neural network to upscale an existing picture of him or generate an entirely new one based on ideal lighting circumstances?",neuralnetworks,stejac8636,False,/r/neuralnetworks/comments/dbd45b/could_a_neural_network_recreate_a_certain_persons/
Visualizing a Neural Network Controlling an Interplanetary Spacecraft Trajectory,1569847274,,neuralnetworks,Gereshes,False,/r/neuralnetworks/comments/dbayu4/visualizing_a_neural_network_controlling_an/
Top 7 TED Talks about Machine Learning,1569838073,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/db9cb6/top_7_ted_talks_about_machine_learning/
Help in CNN for aerial images,1569808411,"Hey guys, last year i took a class in computer vision field and started to like it, nowdays i'm trying to get some personal project with drones + computer vision and I got by recommendation the CNN approach, I have some questions about the ""how to"" of preparing the dataset stage. Searching for datasets I got the  Cars Overhead With Context ([https://gdo152.llnl.gov/cowc/](https://gdo152.llnl.gov/cowc/)) and spend a day in looking through the READMEs provided.  


My question is how do i prepare these for my CNN? In the folders i have everything separated and labeled by category, for example:   


Utah\_AGRC/train/neg.Utah\_AGRC-HRO\_15.0cm\_12TVL220180-CROP.05573.03863.030.png

&amp;#x200B;

(1) \`neg\` means this is a negative sample. It would say \`car\` otherwise.

(2) 12TVL220180-CROP is the original image name (see file\_name\_translate.txt)

(3) 05573.03863.030 means this image was taken centered from the pixel offset 05573,03863 in the image. The 030 means this patch was rotated 30 degrees from its original.  


Ok, so far so good, I understand how the label was made but do i need to modificate anything more? The train images are all 256x256, they are ready to be passed to my network? What are crucial for this first step? In the dataset they provide the network used (made in caffe framework) but since i'm trying to learn tensorflow i'll try to replicate in there.  I don't have much experience in networks (practical) but i have some knowledge of the theorical CNN's, MLP and some AI.  


Here is some example of one train image: [https://imgur.com/qkLnwqY](https://imgur.com/qkLnwqY)  


Thanks in advance, and if anyone is trigged by some possible ""school work"" relax, i don't wan't any code, i just wan't some advices on how to procced after getting the dataset etc.",neuralnetworks,gabrieldomene,False,/r/neuralnetworks/comments/db4p5j/help_in_cnn_for_aerial_images/
An RNN that don't have a specific input at a time?,1569792074,"Hi!
I'm interested in building a software that can write in my own handwriting.

(I'll upload the picture in a minute)
This is Hebrew (the same letters are circled with the same color).
As you can see, handwritten letters differ, even if it's the same letter - and I need to replicate that.

I believe the best way to do that is to have an RNN (in a different way, that's the catch) the letters and outputs how much (not where to) to move ""the pencil"".
The problem is: I cannot input a single letter at a time,(because if a word have 5 letters there will be only 5 movements, which isn't enough even for a single letter). You might think that inputting the same letter multiple times can be a solution, but despite it being inelegant, the amount of time/movements required for each letter are different.

I need some way to input the network the current letter that it's writing (letters - because it will be in cursive).

* Of course the RNN will have a slight delay.

I guess there's a type of neural networks which can suit the problem, but I don't know what is it. Do you have any idea? 
I'm open for any other idea of implementing the software.

It's kinda hard to explain, I hope you'll understand.
Thank you!!!",neuralnetworks,amitgoren,False,/r/neuralnetworks/comments/db1fpc/an_rnn_that_dont_have_a_specific_input_at_a_time/
"I made a video about perceptron, which is a single layer neural network in C# with a step by step example",1569785049,,neuralnetworks,lacaai,False,/r/neuralnetworks/comments/daztye/i_made_a_video_about_perceptron_which_is_a_single/
Whats going wrong?,1569784124,"hi all, noob here. im working on a small school project (a simple neural network) in python and am failing miserably. i am trying my best, but i have no clue whats happening with my train() function and don't know why my cost wont go bellow 0.25. any help would be much appreciated. also, im kinda new to python so my codes a bit of a mess. also, the reason im not using numpy and writing all the functions myself if because my school requires me to use a wierd version of python to run on a microbit.

&amp;#x200B;

    import random
    import math
    
    training_input = [[1, 0, 0, 1], [0, 1, 1, 1], [0, 0, 0, 1], [1, 1, 1, 0]]
    training_output = [1, 0, 0, 1]
    
    syn0 = [[random.random() for i in range(4)] for i in range(4)]
    syn1 = [[random.random() for i in range(4)] for i in range(1)]
    
    bias0 = [random.random() for i in range(4)]
    bias1 = [random.random() for i in range(1)]
    
    random.seed()
    
    def matrix_multiply(matrix, vector):
        result = [0]*len(matrix)
        for i in range(len(matrix)):
            for ii in range(len(matrix[i])):
                result[i]+=vector[ii]*matrix[i][ii]
        return result
    
    def sigmoid(num, deriv=False):
        return 1/(1+math.exp(-num))
    
    def sigmoid_p(x):
        return sigmoid(x) * (1-sigmoid(x))
    
    def add_bias(vector, bias):
        result = vector
        for i in range(len(vector)):
            result[i] += bias[i]
        return result
    
    def transpose(matrix):
        return [[matrix[c][r] for c in range(len(matrix))] for r in range(len(matrix[0]))] 
    
    def train():
        syn0 = [[random.random() for i in range(4)] for i in range(4)]
        syn1 = [[random.random() for i in range(4)] for i in range(1)]
    
        bias0 = [random.random() for i in range(4)]
        bias1 = [random.random() for i in range(1)]
    
        b = random.random()
    
        costs = []
        
        iterations = 10000
        learning_rate = 0.2
        costs = [] 
    
        print('start')
        for i in range(iterations):
            ri = random.randint(0, len(training_input) - 1)
            inp = training_input[ri]
            
            l0 = add_bias(matrix_multiply(syn0, inp), bias0)
            for ii in range(len(l0)):
                l0[ii] = sigmoid(l0[ii])
            l1 = add_bias(matrix_multiply(syn1, l0), bias1)
            for ii in range(len(l1)):
                l1[ii] = sigmoid(l1[ii])
            out = l1[0]
            pred = sigmoid(l1[0]) # networks prediction
            target = training_output[ri]
            cost = (pred - target)**2
    
            dcost_dpred = 2 * (pred - target)
            dpred_dout = sigmoid_p(out)
    
            dcost_dout = dcost_dpred * dpred_dout
            
            for r in range(len(syn0)):
                for c in range(len(syn0[r])):
                    dcost_w = dcost_dout * inp[c]
                    syn0[r][c] = syn0[r][c] - learning_rate * dcost_w
            for r in range(len(syn1)):
                for c in range(len(syn1[r])):
                    dcost_w = dcost_dout * inp[c]
                    syn1[r][c] = syn1[r][c] - learning_rate * dcost_w
            for n in range(len(bias0)):
                dcost_b = dcost_dout * inp[c]
                bias0[n] = bias0[n] - learning_rate * dcost_b
            for n in range(len(bias1)):
                dcost_b = dcost_dout * inp[c]
                bias1[n] = bias1[n] - learning_rate * dcost_b
        print('done')
    
    train()",neuralnetworks,A-pan-weeb,False,/r/neuralnetworks/comments/dazm83/whats_going_wrong/
What type of NN is appropriate for learning to trade in financial markets?,1569784072,"Let me preface this by saying that I am painfully aware that it's not as easy as ""make NN, teach it to trade, become a millionaire."" 

I created an ""emulator"" of a cryptocurrency broker.  This emulator records every live trading event (e.g. trades, additions and subtractions to order books), and can play them back. The emulator can also correctly handle placing new limit or market orders, etc.  I initially made the emulator to test and optimize a trading strategy that I've been using for a couple of years, but I would love to use it to train a neural network.

I would like to train a neural network to use the quantities and time spans between the additions and deletion of orders (in the order book) to filter out some of my strategy's trades (if it determines that the trade will be unprofitable) inside of this emulator. Alternately, if the NN could just give an ""up"" or ""down"" signal, that would also be useful.

I know that there are types of neural networks specifically for time series prediction, but I am not sure that is what I need since additions/deletions to an order book are more complicated than a time series. 

I am not a bad programmer, but deep learning has such as steep learning curve that I am not sure where to begin or even what types of neural networks to research. If anyone has some tips or links to resources that could help me out, I would really appreciate it.",neuralnetworks,mfurlend,False,/r/neuralnetworks/comments/dazlsz/what_type_of_nn_is_appropriate_for_learning_to/
Where to get activated value in backpropagation?,1569653528,"I'm working on designing my own backpropagation code to actually learn it, but I'm having problems deciphering where I'm supposed to be gathering the values to backpropagate. My current issue is here:

`∂C0/∂a^L = 2(a^L - y)`

Where `C0` is the cost, and `a^L` is the activated output of the output layer. Y is the target value.

My issue getting the activated output of the output layer. Where am I supposed to get this? To my knowledge, I can only get that value if I were to record the data from forwarding, but how would `a^L` be able to apply to adjusting values for the whole network when the inputs change?

Am I misunderstanding what `a^L` is supposed to be referring to?",neuralnetworks,RisingEarth,False,/r/neuralnetworks/comments/dacnrs/where_to_get_activated_value_in_backpropagation/
Hands-On Neural Networks with TensorFlow 2.0 - book OUT NOW!,1569518821,,neuralnetworks,pgaleone,False,/r/neuralnetworks/comments/d9mtyo/handson_neural_networks_with_tensorflow_20_book/
How to do transfer learning on Darknet YOLOv3,1569413123,"I've started getting into object detection in image. I have YOLOv3 neural network with Darknet framework. The network is pre-trained from COCO data set. Now I need to do some transfer learning in order to try to make the results better. What I have so far:

I've created my custom data set and did transfer learning following this guide: https://github.com/ultralytics/yolov3/wiki/Example:-Transfer-Learning

After few hours it spit out some .pt file. From what I gathered .pt is PyTorch format but my program uses .weights which is Darknet format. First I've tried to find how to convert .pt to .weights and I stumbled upon this: https://github.com/marvis/pytorch-caffe-darknet-convert but apparently this works only with YOLOv2.

Then I tried to look for something on how to transfer learn using Darknet but with no luck.

Since then I'm stuck and can't move forward, can any of you give me some tips and pointers on how to achieve transfer learning on YOLOLv3 + Darknet (or how to convert .pt to .weights since I already know how to create .pt)? Thanks!",neuralnetworks,muliku,False,/r/neuralnetworks/comments/d92bjv/how_to_do_transfer_learning_on_darknet_yolov3/
Transforming CRM Operations With Artificial Intelligence,1569407962,,neuralnetworks,tech_rebel,False,/r/neuralnetworks/comments/d91gp7/transforming_crm_operations_with_artificial/
Do NN actually think?,1569014497,"From what I understand of them, random neurons will be activated and thru some type of feedback will make itself always choose the right sequence of neurons and never (or very rarely) choose the wrong sequence because the weights make it so the chance of that particular link being chosen is 0%.

It’s very possible I’m completely wrong on this so bear with me :)",neuralnetworks,chilller6,False,/r/neuralnetworks/comments/d71cf4/do_nn_actually_think/
A Few Neurons are enough to train an artificial self-driving car,1568969570,,neuralnetworks,Mesode,False,/r/neuralnetworks/comments/d6s349/a_few_neurons_are_enough_to_train_an_artificial/
Machine Learning for your apartment hunt. Part 1,1568901181,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/d6ecyk/machine_learning_for_your_apartment_hunt_part_1/
Landmark recognition in Cloud Mail.ru using deep learning: how and why,1568900961,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/d6ebai/landmark_recognition_in_cloud_mailru_using_deep/
Generative Adversarial Networks Are Allowing AI To Enter Creative Fields,1568857126,,neuralnetworks,BlastPalace,False,/r/neuralnetworks/comments/d6730i/generative_adversarial_networks_are_allowing_ai/
Quantum Chemistry Breakthrough: DeepMind Uses Neural Networks to Tackle Schrödinger Equation,1568833155,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/d62enx/quantum_chemistry_breakthrough_deepmind_uses/
"How we made landmark recognition in Cloud Mail.ru, and why",1568818680,,neuralnetworks,pvl18,False,/r/neuralnetworks/comments/d5yz60/how_we_made_landmark_recognition_in_cloud_mailru/
Tabula Rasa - Rethinking the intelligence of machine minds,1568743001,,neuralnetworks,hoopism,False,/r/neuralnetworks/comments/d5klyv/tabula_rasa_rethinking_the_intelligence_of/
Help with a problem,1568728901,"I'm new to neural networks and was trying to create an MNIST data set and imported everything that I needed but I got this and I couldn't find a solution on google.

KeyError  Traceback (most recent call last) [/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py](https://localhost:8080/#) in get\_loc(self, key, method, tolerance)    

**2656** try: -&gt; 2657 return self.\_engine.get\_loc(key)    

**2658** except KeyError:  pandas/\_libs/index.pyx in pandas.\_libs.index.IndexEngine.get\_loc() 

pandas/\_libs/index.pyx in pandas.\_libs.index.IndexEngine.get\_loc()  

pandas/\_libs/hashtable\_class\_helper.pxi in pandas.\_libs.hashtable.PyObjectHashTable.get\_item() 

 pandas/\_libs/hashtable\_class\_helper.pxi in pandas.\_libs.hashtable.PyObjectHashTable.get\_item()  

KeyError: 1198 During handling of the above exception, another exception occurred:  

KeyError  Traceback (most recent call last) 4 frames

[/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py](https://localhost:8080/#) in get\_loc(self, key, method, tolerance) 

**2657** return self.\_engine.get\_loc(key)    

**2658** except KeyError: -&gt; 2659 return self.\_engine.get\_loc(self.\_maybe\_cast\_indexer(key))    

**2660** indexer = self.get\_indexer(\[key\], method=method, tolerance=tolerance)    

**2661** if indexer.ndim &gt; 1 or indexer.size &gt; 1:  pandas/\_libs/index.pyx in pandas.\_libs.index.IndexEngine.get\_loc()  pandas/\_libs/index.pyx in pandas.\_libs.index.IndexEngine.get\_loc()

pandas/\_libs/hashtable\_class\_helper.pxi in pandas.\_libs.hashtable.PyObjectHashTable.get\_item()  pandas/\_libs/hashtable\_class\_helper.pxi in pandas.\_libs.hashtable.PyObjectHashTable.get\_item()  

KeyError: 1198

&amp;#x200B;

Can someone tell me what I did wrong and what I should do to fix this problem.  Thank you!

Sorry if this is a waste of time.",neuralnetworks,Ploply1527,False,/r/neuralnetworks/comments/d5hf9a/help_with_a_problem/
does anyone know anything about this?,1568652886,"I am not the best CSE, on my way to my bachelors. but I am absolutely fascinated by this. how much goes into making this? how much of it is code that exists for free or has an applicable explanation of the alg? i keep looking and it just seams niche enough. Where do I get started?",neuralnetworks,shabutie8,False,/r/neuralnetworks/comments/d53g98/does_anyone_know_anything_about_this/
Image Optimization with Machine Learning,1568616639,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/d4x0fc/image_optimization_with_machine_learning/
Interview with a Deep Learning expert - The Technically Speaking Podcast,1568614885,,neuralnetworks,grtgbln,False,/r/neuralnetworks/comments/d4wrb5/interview_with_a_deep_learning_expert_the/
Learn how to morph faces with a Generative Adversarial Network! (And methods for latent space exploration),1568606371,,neuralnetworks,Um__Actually,False,/r/neuralnetworks/comments/d4vfww/learn_how_to_morph_faces_with_a_generative/
Would it be possible to use deep learning and a neural network to identify if someone else starts posting on someone else's account?,1568583875,"I have been thinking of a way to detect shills and PR firms from ruining Reddit and other online places.

They essentially buy accounts someone else used to build a history with then start posting their own stuff, would it be possible to use deep learning to read an accounts comment history and detect if a new person takes over an account? 

I know humans can do this in forensics to the point it has been used in criminal law since there should be a change in grammar and everyone has a unique way of saying things.

Would anyone be interested in trying to develop something like this with me?",neuralnetworks,kdcj,False,/r/neuralnetworks/comments/d4r384/would_it_be_possible_to_use_deep_learning_and_a/
Machine Learning Complete Elite Course,1568567563,,neuralnetworks,ShyamTgr,False,/r/neuralnetworks/comments/d4nc5i/machine_learning_complete_elite_course/
AI powered Infinity stone to help my coworker remember our names,1568562358,"My coworker has had an issue with remembering our names since we started working together.

That's what led me to the idea that I should use my robot making skills to make a device that would help him remember our names... Introducing the Name Stone!

The Name Stone is primarily powered by the Jetson Nano and is using PyTorch for the Image Recognition portion of the Device.

&amp;#x200B;

[https://youtu.be/Wb7zlCrwNiE](https://youtu.be/Wb7zlCrwNiE)",neuralnetworks,ahadcove,False,/r/neuralnetworks/comments/d4m5m0/ai_powered_infinity_stone_to_help_my_coworker/
Transforming CRM Operations With Artificial Intelligence,1568360552,,neuralnetworks,GreenScheme7,False,/r/neuralnetworks/comments/d3luwn/transforming_crm_operations_with_artificial/
Can AI Art Authentication Put An End To Art Forgery?,1568324620,,neuralnetworks,hoopism,False,/r/neuralnetworks/comments/d3ez8q/can_ai_art_authentication_put_an_end_to_art/
"Why would you need a class such as N/A, Background, etc?",1568293859,"I noticed in some datasets and trained networks that images can be classified as ""background"", ""N/A"" and so on. I'm still learning about NN, but my current understanding is that you want your output neurons to actually represent classes of objects. 

Why would you even need it? For instance, in FashionMNIST there isn't an N/A class, but if you see all results are below some threshold, you can determine that no object was found in the image.",neuralnetworks,Oatilis,False,/r/neuralnetworks/comments/d37ov7/why_would_you_need_a_class_such_as_na_background/
Neural network for another neural network,1568255575,"This is a fascinating concept; I'm watching some videos about RNNs with LSTM, and its really cool how it essentially abstracts the functions of the brain of the nerves.  This is still very new to me, but I am curious to know if you could train a neural network with another neural network (ie - use one of the hidden layers as an input, and then use that output in the next hidden layer of the first neural network):

**Input** \&gt; **Hidden Layer 1** \&gt; *Input* \&gt; *Hidden Layers 1-n* ***and additional NNs as applicable*** \&gt; *Output* \&gt; **Hidden Layer 2-m** \&gt; **Output**

Are there any practical examples of this?  Is this essentially giving the computer a sense of reasoning/logic (inference with respect to the higher NN)?  If not, is there a way to create this?  Thank you!",neuralnetworks,Andromeda514,False,/r/neuralnetworks/comments/d31e1o/neural_network_for_another_neural_network/
Best approaches for Deep Learning based OCR,1568224892,"This article explains how attention mechanisms, spatial transformer networks and convolutional recurrent neural networks are applied for OCR tasks. It also contains a step-by-step tutorial on how to train a custom OCR model to detect licence plate numbers.

Article link: [https://nanonets.com/blog/attention-ocr-for-text-recogntion/](https://nanonets.com/blog/attention-ocr-for-text-recogntion/?utm_source=reddit&amp;utm_medium=social&amp;utm_campaign=atocr&amp;utm_content=nn)

&amp;#x200B;

[Licence plate number detection using AttentionOCR](https://i.redd.it/74socja680m31.gif)",neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/d2u8kc/best_approaches_for_deep_learning_based_ocr/
Builing a custom licence plate detector with Deep Learning,1568224171,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/d2u1zm/builing_a_custom_licence_plate_detector_with_deep/
About Backprop in ResNet,1568207882,"I know the concept of forward prop in residual network that how skip connection helping us to build more deep nets . But I can’t understand that how backprop work in residual network and what is the role of skip connection in back prop

Can anyone plz explain this in detail with maths along with it’s process?",neuralnetworks,Sahil8141,False,/r/neuralnetworks/comments/d2q123/about_backprop_in_resnet/
Dataset for testing Object Detection algorithms,1568205868,"Hello guys, I am failry new to Deep Learning and wanted to use different object detection algorithms like RCNN, SSD, YOLOV3. But I couldn't find datasets that haven't been used to train these Algorithms. Can anyone provide me link to these datasets?",neuralnetworks,a_noob__,False,/r/neuralnetworks/comments/d2pkg1/dataset_for_testing_object_detection_algorithms/
Here are 7 Data Science Projects on GitHub to Showcase your Machine Learning Skills!,1568186578,,neuralnetworks,subhamroy021,False,/r/neuralnetworks/comments/d2mccx/here_are_7_data_science_projects_on_github_to/
Dynamic Routing Between Capsules,1568166434,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/d2ikad/dynamic_routing_between_capsules/
"Light can scatter from light, CERN physicists confirm",1568166340,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/d2ijj1/light_can_scatter_from_light_cern_physicists/
Machine learning and its radical application to severe weather prediction,1568166258,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/d2iitf/machine_learning_and_its_radical_application_to/
NN to derive equation paramters,1568126971,"Hi guys, I'm new into the subject and I'm trying to solve a problem using a NN but I don't know if it is the right approach. I have a system with input data of temperature and pressure and output data of voltage(which should represent the input pressure). I already know the function to calculate the output voltage from a given pressure but I have to tune 4 parameters to adjust the temperature influence so the output becomes less dependent from it.

My idea was to develop a NN to derive the best combination of these parameters for a given dataset of input/output. What do you reccomend? And what language should I use to develop such NN (matlab/simulink,phyton, ecc ecc)",neuralnetworks,ex3tera,False,/r/neuralnetworks/comments/d28kli/nn_to_derive_equation_paramters/
ReLU backpropagation,1568092885,"Hi, I'm unable to understand a specific thing about the ReLU backprop.

  
Given (input parameters) some input tensor/matrix `x`, the ReLU function `f`,  and an upstream derivative matrix `dout`, I was able to think of these as scalars and understand why `dx` (gradient with respect to x) is the product of `dout` and `dfdx` (the derivate of the ReLU'ed matrix). What I don't get is why this is the element-wise multiplication of the two? Why is it not a dot product? (`dout.dot(dfdx)` or `dfdx.dot(dout)`).

Thanks!",neuralnetworks,dudester_el,False,/r/neuralnetworks/comments/d235dk/relu_backpropagation/
Document Embedding Techniques: A Literature Review 📃🌐,1568040620,"Hey there.

I've written what I believe to be [a thorough but concise literature review on the topic of document embedding techniques](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d). I hope those of you interested in an introduction to the topic will find it informative. :)

Since I'm in the process of initiating a related project with one of my clients, I'd love to hear about your experience with document embeddings - especially as part of a document clustering pipeline - and any other insights you might have on the subject.

Cheers,  
Shay

[https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d)",neuralnetworks,shaypal5,False,/r/neuralnetworks/comments/d1rwor/document_embedding_techniques_a_literature_review/
Neural Network Based Optimal Control: Resilience to Missed Thrust Events for Long Duration Transfers,1568032694,,neuralnetworks,Gereshes,False,/r/neuralnetworks/comments/d1qcbh/neural_network_based_optimal_control_resilience/
How to integrate of TensorFlow Model in Angular Application?,1568013380,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/d1njrp/how_to_integrate_of_tensorflow_model_in_angular/
Help Please,1567976235,"Hi Does anyone know how I can open an image and get pixel values in python, because I'm new to image recognition and all this so help would be great.",neuralnetworks,Password_0301,False,/r/neuralnetworks/comments/d1gtwp/help_please/
Stochastic vs Batch vs Mini-Batch Gradient Descent in Python,1567914185,,neuralnetworks,bhavesh91,False,/r/neuralnetworks/comments/d162dd/stochastic_vs_batch_vs_minibatch_gradient_descent/
[AI application] AirGesture - Let's play game without keyboard,1567900850,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/d13qm0/ai_application_airgesture_lets_play_game_without/
database-solution,1567769667,,neuralnetworks,thomsonpetter,False,/r/neuralnetworks/comments/d0fg1z/databasesolution/
Transforming CRM Operations With Artificial Intelligence,1567761656,,neuralnetworks,Machine_Learning001,False,/r/neuralnetworks/comments/d0e9td/transforming_crm_operations_with_artificial/
Keras question on a Classifier with multiple possible categories,1567640328,"I am still fairly new in the Neural Network space but I worked through the iris_flower dataset without too much issue since it could only have 1 possible category. I wanted to expand on this and experiment on Pokemon, to put it simply a pokemon can be 1 of 18 different types, or a combination of 2 of those 18 types. 

How would I process a dataset of csv values and have the model learn from an entry either being 1 of 18 types, or a combination of 2 of those 18 types?

I first tried one hot encoding the set types but I don't think that is the way to go. Sorry for the beginner question!",neuralnetworks,aerospace91,False,/r/neuralnetworks/comments/czsrho/keras_question_on_a_classifier_with_multiple/
Pattern separation from texture,1567589174,"I've been using substance designer and painter lately. In these programs you synthesize materials using textures and function. So it got me thinking: What if you could put a texture in another program to output voronoi patterns, bevels and other features so these could be used in substance? Has this been done before and if so where could a paper or program be found? Personally i was thinking of some sort of GAN, with a separator-combiner network, that splits a picture into patterns and tries to put it back together, and an adversary that holds this new texture against its database. Sorry if the question is absurd, ignorant or otherwise.
Thanks for reading.",neuralnetworks,kalledk21,False,/r/neuralnetworks/comments/czihqa/pattern_separation_from_texture/
"Explaning Activation Functions: Visualized and Math Explained Clearly. Code in Notebook along with Pros and Cons for GELU, SELU, ELU etc. I wrote this extensive article (&gt;6k words) and I hope it helps you understand the activation functions better.",1567523830,,neuralnetworks,permalip,False,/r/neuralnetworks/comments/cz68bp/explaning_activation_functions_visualized_and/
Transformer with Python and TensorFlow 2.0 – Training,1567496325,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cz1ocb/transformer_with_python_and_tensorflow_20_training/
"Intro to Machine Learning (ML Zero to Hero, part 1)",1567443278,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cyrl68/intro_to_machine_learning_ml_zero_to_hero_part_1/
Realistic speech data applications in Machine Learning,1567297136,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cy36uu/realistic_speech_data_applications_in_machine/
"Deepfakes: In some videos, you can't believe your eyes",1567296751,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cy34ax/deepfakes_in_some_videos_you_cant_believe_your/
Optical neural network could lead to intelligent cameras,1567296576,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cy3333/optical_neural_network_could_lead_to_intelligent/
Neuroevolution of Augmenting Topologies (NEAT),1567296495,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cy32dy/neuroevolution_of_augmenting_topologies_neat/
Validation for a Recommendation System HELP!!,1567208091,"Hi, I'm building a recommendation system, but in order to evaluate its accuracy, I need some validation subset... 

How to properly make a Validation for a Recommendation System??

Do you know about a Dataset for recommender system that provide training and validation subset or any other way to evaluate my recommendation system???",neuralnetworks,Osiris_R,False,/r/neuralnetworks/comments/cxopyl/validation_for_a_recommendation_system_help/
Avoiding to put my face on the internet,1567193453,"Hello!

&amp;#x200B;

I need to record a video that will be broadcasted on the internet. Ideally, I'd like to avoid my image floating around on the internet - but I need to record this video.

&amp;#x200B;

Which made me think, could I record the video, and have someone else on it? Make a deepfake, and have the video content recorded but my face not on it?  


My question, is how would you go about it in a way that is legal? I don't want to have legal issues.  


Can I use footage from public domain movies, and use the face of one of this character?

Or is it feasible technically to generate a fake person like on [https://thispersondoesnotexist.com/](https://thispersondoesnotexist.com/) , and deepfake it on me?

Something else?  


Pointers would be amazing :-)!",neuralnetworks,bobywomack,False,/r/neuralnetworks/comments/cxlp4f/avoiding_to_put_my_face_on_the_internet/
Product Recommendation Help! :),1567177358,"Hi, I'm trying to develop a product recommendation engine based on product characteristics similarities clustering, but I'm having some issues to get some data for training and validation of my NN. I would like to test my network with three different data sets (ie. Movies, clothes and food). May someone recommend some good data sets to train and validate my recommendation engine?? Thanks!! :)",neuralnetworks,Osiris_R,False,/r/neuralnetworks/comments/cxi4l5/product_recommendation_help/
Best 4 Ways to Handle Missing Values in Pandas in Machine Learning,1567096239,,neuralnetworks,subhamroy021,False,/r/neuralnetworks/comments/cx3966/best_4_ways_to_handle_missing_values_in_pandas_in/
How AI and Chatbots are Enriching Mobile Apps,1567078046,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cwzkro/how_ai_and_chatbots_are_enriching_mobile_apps/
On combining data having similar distribution,1567072301,"I have a collection of time series data with data points of around **2 years of daily data**. I am thinking of a way to increase the number of data points in it, so that the neural network gets a better understanding of the fluctuations in the data. I am suggesting a hypothesis where I try to **cluster similar time-series data following similar distribution**, in order to increase the number of data points fed into the neural network. Is this a correct way to approach the problem? If so, on what basis do I combine these similar data together so that I can feed it into the neural network?",neuralnetworks,vignesh_md,False,/r/neuralnetworks/comments/cwyoya/on_combining_data_having_similar_distribution/
Genetic algorithm learning the Hungaroring racetrack,1567019007,,neuralnetworks,trenmost,False,/r/neuralnetworks/comments/cwp7tm/genetic_algorithm_learning_the_hungaroring/
Help on personnal project. Neural network not working as intended,1567003862,"Hi!

At first , sorry for my english. I'm not a native and I just try my best.

&amp;#x200B;

I'm currently working on a personnal project of neural network using Excel. This is only because my data set to train the IA needs Excel.

After finishing to setup all the formula's and trying to run the program, i came across a problem I can't solve. Even when my input change, my output won't change for a specific set of weights and bias. i put below the picture of 2 different input producing the exact same output. If I use a different set of weights and bias, this will produce another output...but still locked for this specific set of weights.

There are 19 neurons in the input layer, for 2 possible output. Between them are 2 hidden layer with 10 neurons each. Weights and bias are generated to be anything between -1 and 1.

&amp;#x200B;

Do you have any idea of what could have gotten wrong?

&amp;#x200B;

![img](gxtxyb0vb7j31)

![img](s0hq01atb7j31)",neuralnetworks,sk0lopandre,False,/r/neuralnetworks/comments/cwluo3/help_on_personnal_project_neural_network_not/
"AI Research Weekly Update August 25th, 2019",1566955365,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cwdti4/ai_research_weekly_update_august_25th_2019/
Can someone explain what it means to use ResNet34 as a backbone for UNet?,1566924306,I'm doing a Kaggle competition currently and someone seemed to have imported pretrained resnet weights to a UNet model. I'm confused by this since they are different architectures.,neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/cw71u9/can_someone_explain_what_it_means_to_use_resnet34/
OpenAI Method Evaluates Model Defense Against Unforeseen Adversarial Examples,1566919336,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/cw5xoj/openai_method_evaluates_model_defense_against/
[Question] Need insights on a personal project that I'm working on!,1566892856,"Hey guys, 

I'm new to neural networks and I've taken upon a project which I think will require some sort of neural network.

I'm attaching a image which shows what I'm trying to achieve.

[https://imgur.com/a/cgkKNhb](https://imgur.com/a/cgkKNhb)

Basic idea is that I'm extracting text from a blood report and I'm trying to determine what diseases the patient might have, my idea is to train a neural network to do this, I'll be feeding the network with all the values from the blood report and on the output layer it should determine the disease or any deficiency that the patient might have.

My question, Is this possible by using neural networks? If yes then what type of neural network should I use, as i said I'm quiet new to neural networks but I have a basic understanding of how they work. And if neural networks can't be used in my case then what should I be using?

Any form of input from you guys is valuable to me! Thanks!",neuralnetworks,fcsfalcon,False,/r/neuralnetworks/comments/cw1a6x/question_need_insights_on_a_personal_project_that/
"Bit confused on the difference between UNET, resnet, and VGG",1566874114,"They are all different architectures, right? Then how come I see [this](https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/65387) where someone is using resnet weights on a UNet? How is that possible if they are different architectures?",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/cvycf0/bit_confused_on_the_difference_between_unet/
Mask_RCNN convoluted,1566855980,"I just started working with mask RCNN. I use this implementation:  [https://github.com/matterport/Mask\_RCNN](https://github.com/matterport/Mask_RCNN) 

First of all Thank you matterport! I could not build anythink like this, it is a fantastic work of art. The whole works like a charm!

That said the code is a frikking mess.

\- Serialization and configuration is incompatible with current keras libraries. Reading and writing a model is tricky and inconsistent.

\- File IO: Holy shit thats,over a 2000 lines of that do hardly anything but read an image and maybe flip its axis.

\- Training code and data augmentation is very simplistic and convoluted. I have converted it to standart keras data augmentations, that boosts results by.... like a lot, especially on smaller data sets.

\- Configuration is a mess. Half the config fields are no longer used, a quater of them are redundant. It does not split cofiguration between construction and inference.  Layers get complicated non-json config objects that they keep as members, that pevent serialisation.

\- Visualisation: (no complaits here), clean cool code way better that what I cooked up for other projects.

\- Datasets: There is a bunch of work there to work with different datasets but non of it is really adaptable. 

If I get enough replies/upvotes I would love to invest some time to strip it of useless parts.clean it up and post a nice repro on the gitstabuckets. But im a bit scared that the field will has moved on, have reinvented the wheel. 

What say you? is Mask RCNN still relevant 2 months from now?

P.S. sorry about the rant",neuralnetworks,JaapOosterbroek,False,/r/neuralnetworks/comments/cvuo71/mask_rcnn_convoluted/
Gauge Equivariant Convolutional Networks and the Icosahedral CNN,1566854256,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvuaf3/gauge_equivariant_convolutional_networks_and_the/
Evolution in Neural Architecture Search!,1566854127,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvu9h0/evolution_in_neural_architecture_search/
"Machine, Learning, 1951",1566854070,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvu91o/machine_learning_1951/
Intel: Hot Chips event details AI-strength processors,1566853824,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvu6sc/intel_hot_chips_event_details_aistrength/
"AI Research Weekly Update August 25th, 2019",1566853783,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvu6gm/ai_research_weekly_update_august_25th_2019/
Manifold Mixup: Better Representations by Interpolating Hidden States,1566853745,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvu66g/manifold_mixup_better_representations_by/
"AI Research Weekly Update August 25th, 2019",1566853585,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvu4we/ai_research_weekly_update_august_25th_2019/
[Project] Style transfer - Human into Art,1566844596,,neuralnetworks,BUGODI,False,/r/neuralnetworks/comments/cvs3vm/project_style_transfer_human_into_art/
"AI Research Weekly Update August 25th, 2019",1566844130,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cvs03v/ai_research_weekly_update_august_25th_2019/
"Humans Don’t Realize How Biased They Are Until AI Reproduces the Same Bias, Says UNESCO AI Chair",1566841973,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/cvrinz/humans_dont_realize_how_biased_they_are_until_ai/
The Best Machine Learning Course to Learn in 2019,1566839775,,neuralnetworks,jhncna,False,/r/neuralnetworks/comments/cvr0sw/the_best_machine_learning_course_to_learn_in_2019/
Beautiful visuals from a neural network.,1566831427,,neuralnetworks,hermanfelker,False,/r/neuralnetworks/comments/cvp5xj/beautiful_visuals_from_a_neural_network/
Building Engaging Conversational Interfaces with DialogFlow,1566805472,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cvkwfp/building_engaging_conversational_interfaces_with/
Need help with partially latent models of neural networks!,1566695187,"I'm trying to do a mostly unsupervised (maybe very very mildly supervised) neural network model.

Like any other neural network, there is a lot of layers. Now, I know a particular set of equations that describe one or two of the layers inside (relates to physics equations) but I have no clue which layer it'll belong to in the hierachy or which orientation they'll fit in as well.

I want my algorithm to predict the other layers in a quicker way once I have given both the inputs and one or two of the hidden layer relations that I have some good idea of.

Can anyone please help me with this? I came across something called partially latent output model, but that was only for supervised mixture model relation (more or less just a dimensionality reduction).",neuralnetworks,manueljenkin,False,/r/neuralnetworks/comments/cv1u2r/need_help_with_partially_latent_models_of_neural/
Writing tutorial series with code examples on neural networks from simplest to the most advanced. Any feedback is appreciated! 🙂,1566659711,,neuralnetworks,guiviko,False,/r/neuralnetworks/comments/cuulgd/writing_tutorial_series_with_code_examples_on/
"Anyone here familiar with Monte Carlo? I tried using it to predict the next number in a sequence (as if the sequence was stock prices), but it didn't work. I'm open to the suggestion that I implemented it wrong.",1566651213,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/cut177/anyone_here_familiar_with_monte_carlo_i_tried/
Discriminative binary decision trees for semi-supervised learning with increased efficiency and higher accuracy,1566511175,,neuralnetworks,CasperFriendlyGuest,False,/r/neuralnetworks/comments/cu4dyt/discriminative_binary_decision_trees_for/
AutoML + GAN = AutoGAN! AI Can Now Design Better GAN Models Than Humans,1566487620,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ctyz91/automl_gan_autogan_ai_can_now_design_better_gan/
Improving Customer Experience with Computer Vision Applications,1566470059,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/ctvk46/improving_customer_experience_with_computer/
Data Science Skills: Web scraping javascript using python,1566458267,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cttwfz/data_science_skills_web_scraping_javascript_using/
This hand-tracking algorithm could lead to sign language recognition – TechCrunch,1566372086,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ctdc9p/this_handtracking_algorithm_could_lead_to_sign/
How Chatbots Generate New Leads,1566371957,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ctdbng/how_chatbots_generate_new_leads/
Improving Business Communications and Human Interactions with NLP,1566369808,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/ctd0t9/improving_business_communications_and_human/
6 Types of Artificial Neural Networks Currently Being Used in Machine Learning,1566292010,,neuralnetworks,subhamroy021,False,/r/neuralnetworks/comments/csvdp1/6_types_of_artificial_neural_networks_currently/
Of Mice and Machines: Can AI Read Rodents’ Minds?,1566240366,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/cslezp/of_mice_and_machines_can_ai_read_rodents_minds/
"Breast cancer diagnosis with neural networks, implemented with Keras and written in Python",1566220519,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/csgxmt/breast_cancer_diagnosis_with_neural_networks/
Zooming into the world of computer vision applications,1566211063,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/csfbap/zooming_into_the_world_of_computer_vision/
Transformer with Python and TensorFlow 2.0 – Encoder &amp; Decoder,1566199810,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/csdr4y/transformer_with_python_and_tensorflow_20_encoder/
Question about Convolution and backprop,1566135654,"Hello, I've implemented backpropagation in python according to this article: https://towardsdatascience.com/backpropagation-in-a-convolutional-layer-24c8d64d8509 .
However, when I train my CNN, all outputs of my filters are 0s. I tried to get all outputs to be between 0 and 1 but i still have the same issue. It might come from the initialization (uniform(-1,1)) but i have no clue.
I don't think it comes from the NN, it achieved 95 % accuracy alone.
Thanks in advance.",neuralnetworks,not_alreadytakenuser,False,/r/neuralnetworks/comments/cs1b7e/question_about_convolution_and_backprop/
I used a dqn to beat flappy birds hardest level,1566014459,,neuralnetworks,WalterEhren,False,/r/neuralnetworks/comments/crh905/i_used_a_dqn_to_beat_flappy_birds_hardest_level/
Conversational AI: The Advanced Form of Chatbots,1565958555,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cr5omz/conversational_ai_the_advanced_form_of_chatbots/
Strengthening Customer Relations with Facebook Messenger Bot,1565944194,,neuralnetworks,Ripple2709,False,/r/neuralnetworks/comments/cr3i8m/strengthening_customer_relations_with_facebook/
All-optical diffractive neural network closes performance gap with electronic neural networks,1565910265,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cqxlfn/alloptical_diffractive_neural_network_closes/
Gauge Equivariant Convolutional Networks and the Icosahedral CNN,1565910131,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cqxkac/gauge_equivariant_convolutional_networks_and_the/
How does a better graphics card help make a better environments for neural networks and ai,1565885260,"I'm on Mobil so o cant post the links to either article or card 
But on the nvidia 2080 it says it allows for  better ai
And on another article I read about neural networks it says with the release of better graphic cards neural networks have taken a huge surge",neuralnetworks,radfre,False,/r/neuralnetworks/comments/cqrw65/how_does_a_better_graphics_card_help_make_a/
Performance analysis NN - what model to use?,1565853839,"To analyze performance graphs, I plan to feed a neural network program hundreds of graphs of the same length, training it on the X axis (iteration in the graph ) and Y axis (performance).
Given this data, I want to feed it a new graph of the same length, but I would then have it predict the Y value for the next X value. 
What model should I use / How would I go about doing this? I was going to use linear regression, but I wanted to ask before I went barking up the wrong tree.",neuralnetworks,Slashscreen,False,/r/neuralnetworks/comments/cqmm5w/performance_analysis_nn_what_model_to_use/
Does Deep Learning Still Need Backpropagation?,1565810578,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/cqehtu/does_deep_learning_still_need_backpropagation/
Siamese Neural Networks,1565792381,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cqabhk/siamese_neural_networks/
WaveGAN Explained!,1565792354,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cqaba9/wavegan_explained/
"I've watched a lot of videos and read a lot of articles about neural networks, and I've only seen 2 references to ensembles of neural networks. They're not networks of neurons, but networks of networks of neurons.",1565785737,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/cq8zg2/ive_watched_a_lot_of_videos_and_read_a_lot_of/
Artificial Neural Network: A Comprehensive Study,1565782929,"Here's a blog for the beginners to understand more about Artificial Neural Network(ANN)

[https://www.infolks.info/blog/artificial-neural-network/](https://www.infolks.info/blog/artificial-neural-network/)",neuralnetworks,asifali_p,False,/r/neuralnetworks/comments/cq8hts/artificial_neural_network_a_comprehensive_study/
Manifold Mixup: Better Representations by Interpolating Hidden States,1565745854,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cq2o6m/manifold_mixup_better_representations_by/
Manifold Mixup: Better Representations by Interpolating Hidden States 360 views,1565745768,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cq2nl2/manifold_mixup_better_representations_by/
Food and beverage industries are becoming AI hotspots,1565696249,,neuralnetworks,Ripple2709,False,/r/neuralnetworks/comments/cprw45/food_and_beverage_industries_are_becoming_ai/
Free Cloud GPU Credits For Deep Learning,1565689929,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes – more than 24hrs for free

I am working on https://www.tensorpad.com/ – developing cloud infrastructure for machine learning.

Part of our computational capacity is idle; hence, we’re offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.

Specs: 
• 60GB of RAM, 4 CPUs, 1080Ti GPU 
• JupyterLab environment with access to the terminal 
• Pre-installed Tensorflow, Keras, and other ML frameworks

You can access the free credits by signing up (https://dashboard.tensorpad.com/ and redeeming ""reddit500"" promo code in the Billing tab (https://dashboard.tensorpad.com/billing).

For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/cpqx4g/free_cloud_gpu_credits_for_deep_learning/
Conversational Banking: Future of Banks with Chatbots,1565680248,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cppk0z/conversational_banking_future_of_banks_with/
Conversational AI: The Advanced Form of Chatbots,1565606553,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cpao2u/conversational_ai_the_advanced_form_of_chatbots/
Strengthening Cybersecurity with Artificial Intelligence,1565594789,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cp91he/strengthening_cybersecurity_with_artificial/
Top Deep Learning Frameworks of 2019,1565593372,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cp8u6j/top_deep_learning_frameworks_of_2019/
London bar uses facial recognition to help staff know who to serve first,1565593291,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cp8tqq/london_bar_uses_facial_recognition_to_help_staff/
Training a Neural Network? Start here!,1565538879,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/coyebk/training_a_neural_network_start_here/
SKIP NAVIGATION ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼ ￼￼ 2:42 / 4:50￼￼￼￼￼ It's Triggy! | AI learns to play TRON,1565356793,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/co29vs/skip_navigation_242_450_its_triggy_ai_learns_to/
How AI Is Transforming The Healthcare Sector,1565349356,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/co0y5l/how_ai_is_transforming_the_healthcare_sector/
AI Chatbot Integration Services on Various Social Media platforms,1565263962,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cnkgi6/ai_chatbot_integration_services_on_various_social/
Noob question about neural networks.,1565235671,"Hey guys I had an idea to train a neural network to predict if a team could win a league of legends game (game knowledge is not necessary for the question) based on the team composition and other parameters. 

I have a very small amount of experience with tensorflow, and this is the first personal project I’m trying to achieve. 

My question is, when training the network, I think I have a set of parameters that determine the chances of a team winning, now what would those be called? Then if they win or not is a different kind of parameter because in reality it is the result. I don’t know if I am making sense, but this whole thing is kind of complicated for me at the moment.

I would love some help, and yes I have followed some tensorflow courses in the pass, but I get bored of them, and I think having a personal project is the best way to learn.

Thank you!",neuralnetworks,SalocinS,False,/r/neuralnetworks/comments/cng4gv/noob_question_about_neural_networks/
Kind help required.,1565182616,"Hello,

first of all i would like to say that i am a complete beginner in neural networks. I have seen a lot of videos in youtube talking about them and their implementation. What i would like help into is how to implement them with signals?! I have searched in our kind friend google although he didnt provide anything concrete so i ask here for informations. 

Thank you!",neuralnetworks,Readorn,False,/r/neuralnetworks/comments/cn5g57/kind_help_required/
Benefits of Using AI Chatbots in Insurance,1565165883,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cn31u2/benefits_of_using_ai_chatbots_in_insurance/
Should we avoid neural networks and AI? Thinking about the future of neural networks.,1565154986,,neuralnetworks,_IUI,False,/r/neuralnetworks/comments/cn1l54/should_we_avoid_neural_networks_and_ai_thinking/
Visualizing and debugging a neural network,1565124970,,neuralnetworks,trenmost,False,/r/neuralnetworks/comments/cmw271/visualizing_and_debugging_a_neural_network/
Classification neural network perfoms autonomous driving,1565111268,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cmtayo/classification_neural_network_perfoms_autonomous/
"Explaining Feedforward, Backpropagation and Optimization: The Math Explained Clearly with Visualizations. I took the time to write this long article (&gt;5k words), and I hope it helps someone understand neural networks better.",1565030566,,neuralnetworks,permalip,False,/r/neuralnetworks/comments/cmf0qt/explaining_feedforward_backpropagation_and/
*Meme break*,1565003320,,neuralnetworks,OTA-J,False,/r/neuralnetworks/comments/cm9lnb/meme_break/
Conversational AI: The Advanced Form of Chatbots,1564988580,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cm7o1h/conversational_ai_the_advanced_form_of_chatbots/
Transformer with Python and TensorFlow 2.0 – Attention Layers,1564987971,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cm7l1g/transformer_with_python_and_tensorflow_20/
Spike-triggered average electrical stimuli as input filters for bionic vision—a perspective,1564943283,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/clzpdf/spiketriggered_average_electrical_stimuli_as/
N-Shot Learning: Learning More with Less Data,1564943127,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/clzob1/nshot_learning_learning_more_with_less_data/
Introducing EvoGrad: A Lightweight Library for Gradient-Based Evolution,1564943049,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/clznpg/introducing_evograd_a_lightweight_library_for/
"Zebras, Horses &amp; CycleGAN - Computerphile",1564942874,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/clzmfn/zebras_horses_cyclegan_computerphile/
No GPU Required: Real-Time Inference with Optimized Networks in kvSonata,1564942793,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/clzlsq/no_gpu_required_realtime_inference_with_optimized/
Neural Networks Architecture Helping Science,1564934604,Researchers have developed a deep neural network architecture that can identify manipulated images at the pixel level with high precision by studying the boundaries of objects in the image,neuralnetworks,deepdigitalfrog,False,/r/neuralnetworks/comments/clxxtb/neural_networks_architecture_helping_science/
Proof on Adversarial Attacks,1564846652,"Hi everyone, I am curious if anyone has any links or references to proofs talking about ML models susceptibility to adversarial attacks. My thought would be something along the lines of no matter how much training or augmentation you provide to a network, there is always some small perturbation that could disrupt the classification boundary, and I am curious if this is formalized anywhere. This comes from an interest in self driving cars and questioning how government policy will allow for fully autonomous vehicles with or without guarantee of adversarial robustness. Im less interested in the ways of autonomous cars could fail due to standard issues (not enough data, new environment, etc) and more so on purposeful attacks that could occur. Thanks!",neuralnetworks,dlovelan,False,/r/neuralnetworks/comments/clk2w2/proof_on_adversarial_attacks/
Bike with autopilot. What for?,1564811811,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/clfeag/bike_with_autopilot_what_for/
Facebook AI Memory Layer Boosts Network Capacity by a Billion Parameters,1564770310,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/cl7wes/facebook_ai_memory_layer_boosts_network_capacity/
Neural net in Emojicode!,1564769985,"Hey, I don't know how many of you know about Emojicode ([https://www.emojicode.org/](https://www.emojicode.org/)). It is basically a programming language that allows you to programm in emojis. I wanted to get a little bit more familiar with the details of gradient descent and backpropagation so I basically implemented a linear algebra as well as a neural net library in this language.

Check it out!

[https://praxidike97.github.io/emoji-learn/](https://praxidike97.github.io/emoji-learn/)",neuralnetworks,praxidike74,False,/r/neuralnetworks/comments/cl7tvb/neural_net_in_emojicode/
Artificial Intelligence to speed up trip planning,1564736335,,neuralnetworks,MachineLearning001,False,/r/neuralnetworks/comments/cl1lob/artificial_intelligence_to_speed_up_trip_planning/
[I built] μCritic - Learning Music Taste With Deep Neural Nets,1564690196,"[Deployed Web App](https://michaeldarr.github.io/MuCritic_App/)

[Source Code](https://github.com/MichaelDarr/MuCritic)

[Model Explanation](https://michaeldarr.github.io/MuCritic_App/#/about)

Hi all! I graduated with a BS in computer science last spring and have spent the last few months building and deploying my first large-scale ML project. I would love to hear any feedback or criticism you have to offer. Thanks!",neuralnetworks,_Radish_Spirit_,False,/r/neuralnetworks/comments/cktlow/i_built_μcritic_learning_music_taste_with_deep/
I'm seeking answers i couldn't find on internet,1564687452,"I'm trying to make a CNN from scratch and there are few things i don't really understand.
First, Convolution: when i look at different architectures of CNNs i see that the image goes through 2 ""rows"" of filters. But does the output of each filter of the first ""row"" goes through every single filter from the second ""row""? There would be too much inputs even with max pooling.
How do you choose filters? Are they random?is there any rule?
Secondly, batch_size and other parameters:
I heard learning rate should be 1e-3,drop out=0.2 however i don't know how to choose batch size, indeed, many websites recommend 128. But i feel that the bigger it is, the less the NN is effective because the gradient descent will minimize the average cost but not the cost for every single training example.(i'm actually doing an handwritten digits recognition and with a batch size too big, the output converges toward 0.1 for every output). How should i choos the batch size? Thank you in advance.",neuralnetworks,not_alreadytakenuser,False,/r/neuralnetworks/comments/ckszz3/im_seeking_answers_i_couldnt_find_on_internet/
Neural Network Visualizer,1564680350,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/ckrf3q/neural_network_visualizer/
"For those of you that are unfamiliar with Keras, here is a great video-introduction that explains exactly what it is.",1564669162,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/ckozcn/for_those_of_you_that_are_unfamiliar_with_keras/
Contextual Emotion Detection in Textual Conversations Using Neural Networks,1564666970,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/ckojqi/contextual_emotion_detection_in_textual/
Quantum computing is so complex that using a classical neural network to calibrate a quantum system is currently the most understandable concept to me.,1564664475,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/cko2rc/quantum_computing_is_so_complex_that_using_a/
Contextual Emotion Detection in Textual Conversations Using Neural Networks,1564664006,,neuralnetworks,pvl18,False,/r/neuralnetworks/comments/cknzuf/contextual_emotion_detection_in_textual/
AI and deep learning to tackle traffic congestion,1564650768,,neuralnetworks,Ripple2709,False,/r/neuralnetworks/comments/cklxko/ai_and_deep_learning_to_tackle_traffic_congestion/
EfficientNet: Theory + Code,1564606882,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ckeea9/efficientnet_theory_code/
A method to reduce the number of neurons in recurrent neural networks,1564606753,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cked8k/a_method_to_reduce_the_number_of_neurons_in/
Automatic Machine Learning,1564606671,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ckecl1/automatic_machine_learning/
What Is Image Recognition?,1564606615,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ckec2t/what_is_image_recognition/
Google BlazeFace Performs Submillisecond Neural Face Detection on Mobile GPUs,1564606522,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ckebcs/google_blazeface_performs_submillisecond_neural/
Unsupervised Learning (What Is Machine Learning | Machine Learning Basics),1564606437,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ckeaoq/unsupervised_learning_what_is_machine_learning/
"AI Creates Near Perfect Images Of People, Dogs and More",1564606402,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ckeafm/ai_creates_near_perfect_images_of_people_dogs_and/
MIT 6.S191: Visualization for Machine Learning (Google Brain),1564606339,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cke9yo/mit_6s191_visualization_for_machine_learning/
Recognizing Multiple Images with YOLO Darknet (6.5),1564606251,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cke98o/recognizing_multiple_images_with_yolo_darknet_65/
MIT 6.S191: Convolutional Neural Networks,1564597810,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/ckceqh/mit_6s191_convolutional_neural_networks/
Neural Networks 101,1564596576,,neuralnetworks,lautarolobo,False,/r/neuralnetworks/comments/ckc4qw/neural_networks_101/
YOLOv1 Loss function term question?,1564577142,"Looking at the loss function for YOLOv1, how do we actually calculate if the highlighted term its a 1 or a 0, I know it's supposed check if there is an object in the cell\[**i\]**,  bounding box \[**j\]**, if there is then we treat the term highlighted as = 1, if there is not then we treated as a = 0, but how is the calculation to get this 1 or 0 done, what are the parameters to check, by what data?

&amp;#x200B;

&amp;#x200B;

[Loss Function](https://i.redd.it/e9em2os96id31.png)",neuralnetworks,simm65,False,/r/neuralnetworks/comments/ck81em/yolov1_loss_function_term_question/
Human pose estimation on images for iOS using CoreML,1564575682,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/ck7rsr/human_pose_estimation_on_images_for_ios_using/
How Neural Networks Work- Simply Explained,1564504435,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cju9nz/how_neural_networks_work_simply_explained/
What are neural networks?,1564503462,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cju15n/what_are_neural_networks/
The Next Phase of Deep Learning: Neural Architecture Learning ( Automatic Discovering of Neural…,1564474070,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cjonb8/the_next_phase_of_deep_learning_neural/
Julia Computing &amp; MIT Introduce Differentiable Programming System Bridging AI and Science,1564436260,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/cjhuw8/julia_computing_mit_introduce_differentiable/
Are there any neural network training algorithms that try to generate new connections or new neurons?,1564422786,"In artificial NNs (ANNs) every 'neuron' is connected to every other (in neighbouring layers) by a weight, even if the weight is 0, it can be easily updated by back-propagation to be non-zero.

However, in biological NNs, this isn't the case. Neurons are connected to a limited subset of all neurons, and to make a new connection they can't just update a zeroed weight through back-propagation. 

Obviously we have little idea as to how biological NNs learn, but it probably involves signals that cause new neurons to be produced where needed, and new connection to be made, not just the 'strengthening' or 'weakening' of current connections. 

I would like to know if there have been concepts or implementations of 

1) an ANN that has limited connections to neurons (sequential or not)

2) there is an algorithm that places new neurons or new connections as a result of the activity of the neural network, in order to try and learn",neuralnetworks,FIREATWlLL,False,/r/neuralnetworks/comments/cjer5h/are_there_any_neural_network_training_algorithms/
What are neural networks?,1564420674,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cje9hi/what_are_neural_networks/
Introduction to Transformers Architecture,1564391661,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cj8s5f/introduction_to_transformers_architecture/
Neural Networks: Crash Course Statistics #41,1564329560,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cixj7u/neural_networks_crash_course_statistics_41/
R or Python,1564317121,"Hey guys I'm new to machine learning, can anyone help me with which language should I prefer for neural networks as it is one of the things which had fascinated me after learning its applications 

Thanks.",neuralnetworks,adarshraj_nandu,False,/r/neuralnetworks/comments/civact/r_or_python/
What kinds of norming are there for LSTM? Recurrentjava has rmsprop per weight. I suspect norming is needed per unrollBackprop timestep since the longer a neuralnet goes from its starting state the bigger the exploding/vanishing gradient problem happens.,1564278394,"What kinds of norming, including experimental kinds, have ever helped in LSTM?",neuralnetworks,BenRayfield,False,/r/neuralnetworks/comments/ciq1dr/what_kinds_of_norming_are_there_for_lstm/
MIT 6.S191: Recurrent Neural Networks,1564244007,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/cijd1x/mit_6s191_recurrent_neural_networks/
"If you're going to take ML courses at a university, which is the most reputable university for that?",1564241400,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/ciitwk/if_youre_going_to_take_ml_courses_at_a_university/
Question about activation function,1564213173,"Hello everyone, I went on few websites saying how ReLU is better than sigmoid or tanh, however, they said in their introduction that they needed a derivable function for backpropagation. I'm confused because ReLU is not derivable at x=0. Can someone explain pls.",neuralnetworks,not_alreadytakenuser,False,/r/neuralnetworks/comments/cieprr/question_about_activation_function/
Question about activation function,1564212534,"Hello everyone, I went on few websites saying how ReLU is better than sigmoid or tanh, however, they said in their introduction that they needed a derivable function for backpropagation. I'm confused because ReLU is not derivable at x=0. Can someone explain pls.",neuralnetworks,not_alreadytakenuser,False,/r/neuralnetworks/comments/ciemp7/question_about_activation_function/
Do you want to know what it is like to BE the machine?,1564179251,"I really experienced the outer body feeling of what it could be like to be the machine. It is actually not that hard to teleport your brain outside of your body.

I used to fly first-person-view (FPV) with quadcopters. An rc drone with a camera sending the video directly to my video goggles. Controlling the movement with my radio.

I remember flying around and suddenly I realized I had forgotten where I was in the area. So I had to fly around looking for.. me. But that shocked me into thinking that to my brain I was the drone. Vision is such a dominant sense that the feeling of the radio in my hands had gone, the control being completely internalised by practice. But the sense has to be immersive, the illusion is everything, you can't break it.

Try FPV if you want your brain to think you are a flying being, it shows the way I believe of what Neuralink can be.

(Not my video)

https://youtu.be/BfO3oUum-hY",neuralnetworks,holandaso,False,/r/neuralnetworks/comments/ci9bvf/do_you_want_to_know_what_it_is_like_to_be_the/
Indian Matrix | Keanu Deep Fake,1564163066,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/ci607b/indian_matrix_keanu_deep_fake/
Neural Network Architectures,1564158248,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/ci4zvn/neural_network_architectures/
NeatBall - AI Learns to play Baseball,1564079468,,neuralnetworks,ahadcove,False,/r/neuralnetworks/comments/chrl1u/neatball_ai_learns_to_play_baseball/
Neural network racing cars around a track,1564074864,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/chql7g/neural_network_racing_cars_around_a_track/
"Generated this masterpiece using talktransformer(GPT2), re-adding the best parts ai generated and re-feeding the whole thing again",1563986249,"Someday I will make love to God,  
While I will stay close to the earth,  
For the earth is my refuge.  
I never thought the world was a place of gods,  
My heart was made by the wind, my brain by the waves.  
I do what the wind wants, I do what the waves tell.  
My heart is in my hands, my soul is in my love.  
I have my hands around God's head,  
My mouth is open wide, my heart speaks in music.  
I have to listen to what God brings, to be ready for what God does.  
Like a dog in its master's place, like a rabbit in its own hole.  
I am in the world of love, I am in love with the world.",neuralnetworks,ponyportal512,False,/r/neuralnetworks/comments/chaj8w/generated_this_masterpiece_using/
Neural Network Learns to Play Snake,1563986217,,neuralnetworks,blogueandoatope,False,/r/neuralnetworks/comments/chaizt/neural_network_learns_to_play_snake/
Machine Learning Subreddits,1563977779,"I've noticed that on many of the ML subreddits, there is a wide variety of libraries and tools used. For the experienced programmer, this may be okay or even preferable. However, if you are like the majority of ML programmers, then this can be intimidating, confusing, and frustrating. For those of you that fall in this category, I would like to invite you to a subreddit ([r/MachineLearningKeras](https://www.reddit.com/r/MachineLearningKeras/)) that will be focused on machine learning with the Keras API. Keras is easy to use, and is a great way to implement various projects. I hope that you will join me in making such a community on Reddit.",neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/ch8qi9/machine_learning_subreddits/
Seq2Seq neural network - Why do we compute the loss on one-hot vectors whereas we can force the hidden state to match the embbeding ?,1563963726,"I was just wondering if someone ever tried.  
It will be a economy in terms of parameters especially if the embedding space is frozen.",neuralnetworks,Bitture,False,/r/neuralnetworks/comments/ch6dkx/seq2seq_neural_network_why_do_we_compute_the_loss/
I'm trying to make a NN that recognises handwritten digits. But there is an issue...,1563904859,"I coded the backprop function and when i train the NN with Yann Lecun's dataset, the cost function is minimized to 0.8.

However, when i feed the trained NN with a random handwritten digit, it always return the same digit.

Has anyone encountered this issue? Does anyone knows where it comes from?

My first idea would be that i don't have enough layers . Indeed, i have 784 inputs,10 outputs and 1 layer with 16 neurons.

Another question: how do u choose how many layers and how many neurons per layer?",neuralnetworks,not_alreadytakenuser,False,/r/neuralnetworks/comments/cgw28v/im_trying_to_make_a_nn_that_recognises/
Still trying to wrap my head around Neuralink. Could a bottleneck in a hybrid machine-brain neural network be useful in passing through the limited bandwidth link?,1563900112,"Also mildly confused about bottlenecks in neural networks. Do they represent a useful typification of information at some layer in the neural network, or are you just forcing the network to waste capacity on reinventing data compression?",neuralnetworks,holandaso,False,/r/neuralnetworks/comments/cgv0gt/still_trying_to_wrap_my_head_around_neuralink/
Perform Efficient Semantic Text Retrieval with USEncoder and AquilaDB,1563877613,,neuralnetworks,iamjbn,False,/r/neuralnetworks/comments/cgquue/perform_efficient_semantic_text_retrieval_with/
GPT-2 OpenAI neural network cannot download models.,1563829479,"Hi,

I'm not sure if this is the right sub-reddit or if I should go with a python one.

I was trying to use the GPT-2 for OpenAi from GitHub: https://github.com/openai/gpt-2
However, when I ran the ""download_model.py"" I got the following message:

You must enter the model name as a parameter, e.g.: download_model.py 117M

No clue how or where to do that. I have all the other modules downloaded. What should I do about the download_model.py file?

Thank you in advance.",neuralnetworks,WRPK42,False,/r/neuralnetworks/comments/cgizqg/gpt2_openai_neural_network_cannot_download_models/
Deep Convolutional Q-Learning with Python and TensorFlow 2.0,1563781089,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cga6ij/deep_convolutional_qlearning_with_python_and/
Neurons spike back: Inductive machines and the AI controversy,1563780288,,neuralnetworks,antonomase,False,/r/neuralnetworks/comments/cga31y/neurons_spike_back_inductive_machines_and_the_ai/
PRON GAN,1563734128,Wheres the GANS that generate NUDES (NOT DEEP FAKES BUT ENTIRELY NEW NUDES).,neuralnetworks,Randall172,False,/r/neuralnetworks/comments/cg2cp7/pron_gan/
OUTERHELIOS ~ Neural Network Jazz Radio + A Research Paper,1563654231,,neuralnetworks,futurussatoshi,False,/r/neuralnetworks/comments/cfqete/outerhelios_neural_network_jazz_radio_a_research/
Tips and tricks for triangular and/or symmetrical matrices?,1563602748,"Hi folks,

A [protein contact map](https://en.wikipedia.org/wiki/Protein_contact_map) is a 2D square matrix of values indicating the distances between every possible pair of amino acids.  Every distance appears twice in the matrix, and the values are symmetrical about the diagonal.

Do you have any recommendations to reduce the amount of computation for such a matrix in a neural architecture, and/or to enforce this symmetry?  One approach would be that any operation on the output matrix could be added twice, once in the computed orientation and then in the transposed orientation.  I'll do this, if it gets the job done, but I have to wonder whether there are better options.

Example in Numpy:

    &gt;&gt;&gt; arr
    array([[1, 1, 0, 1, 1, 0, 1],
           [2, 1, 2, 1, 1, 0, 2],
           [0, 0, 1, 0, 0, 2, 1],
           [2, 0, 2, 1, 1, 2, 0],
           [1, 2, 2, 2, 2, 1, 2],
           [1, 0, 0, 0, 1, 0, 0],
           [2, 1, 2, 1, 2, 2, 2]])
    
    &gt;&gt;&gt; arr + arr.T
    array([[2, 3, 0, 3, 2, 1, 3],
           [3, 2, 2, 1, 3, 0, 3],
           [0, 2, 2, 2, 2, 2, 3],
           [3, 1, 2, 2, 3, 2, 1],
           [2, 3, 2, 3, 4, 2, 4],
           [1, 0, 2, 2, 2, 0, 2],
           [3, 3, 3, 1, 4, 2, 4]])",neuralnetworks,bwllc,False,/r/neuralnetworks/comments/cfik7k/tips_and_tricks_for_triangular_andor_symmetrical/
BI 041 Brad Aimone: Neurogenesis and Spiking in Deep Nets,1563554133,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cf9rtv/bi_041_brad_aimone_neurogenesis_and_spiking_in/
"AshPy: TensorFlow 2.0 library for distributed training, evaluation, model selection, and fast prototyping.",1563552477,,neuralnetworks,pgaleone,False,/r/neuralnetworks/comments/cf9fes/ashpy_tensorflow_20_library_for_distributed/
Pre-processing for CNN,1563546410,"If we want to automate image-preprocessing for CNN starting with directly with dataset in .zip files, what should be the desired functions that would be grouped together?

Question 2: Also, sometimes we use TF pre-processing for data-augmentation but data-generation is random; thus is it desired that data that was created randomly be shared in-case of when we want reproducible research?",neuralnetworks,chinmay_shah,False,/r/neuralnetworks/comments/cf87ip/preprocessing_for_cnn/
Benefits of CornerNet and detecting corners?,1563545919,My PI has always wondered what the benefits of using corners in object detection is (in comparison to models that use Yolov3 or SSD). Does anyone who has looked at the paper have any clue as to the motivations for this? or why it works?,neuralnetworks,shadowclan98,False,/r/neuralnetworks/comments/cf843z/benefits_of_cornernet_and_detecting_corners/
"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",1563533187,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/cf610j/for_me_one_of_the_main_barriers_to_the_world_of/
11 Books To Understand The Implications Of Projects Like Neuralink,1563518653,,neuralnetworks,analyticsindiam,False,/r/neuralnetworks/comments/cf46hr/11_books_to_understand_the_implications_of/
FaceApp: How Neural Networks can do Wonders - Harsh Aryan - Medium,1563491871,[removed],neuralnetworks,12harsharyan,False,/r/neuralnetworks/comments/cezues/faceapp_how_neural_networks_can_do_wonders_harsh/
Updating bias in back propagation,1563464810,"I have a neural network written in python that works, now I'm trying to add a bias term. It calculates everything via matrices, no individual node math. My code within the back propagation looks like this

&amp;#x200B;

    delta_for_layer = np.dot(layer_values.T, error_in_layer)
    deltas.append(delta_for_layer)
    error_in_layer = np.dot(error_in_layer, layer_weights.T)* sigmoid_derivative(layer_values)
    self.bias_for_layer = ???

I've changed some term so it's easier to follow, because my question is specifically about the theory of how to update the bias. I've seen people just say ""oh its bias += learning\_rate \* error\_in\_layer"", but my error\_in\_layer is a matrix. How should I update the one bias value for the layer with this?",neuralnetworks,EvilTeliportist,False,/r/neuralnetworks/comments/ceu6ez/updating_bias_in_back_propagation/
[D]LSTMs backpropagation from scratch and its trainning doubts,1563448391,"Here is an implementation of LSTMs backpropagation from scratch , I am not sure with the dfhs (derivative of hs state) am I doing it the right way ? Please do correct me where I am wrong in my code I tried for so many times still the LSTMs prediction is horrible does it take longest time to train LSTMs ?? Any Suggestions on Back Propagation is highly appericiated as I can't figure out what is going wrong - Thank you in advance

I tried increasing the iterations to 10,000 from 5,000 which was intially and tried decreasing the learning rate and increasing the batch size
#Forward propagation to store all the state necessary for back prop
    for i in range(nw):
        xp = np.zeros(xl)
        xp[intx] = 1

        x = np.hstack((hs[i-1],xp))
        xs[i] = x
        fg[i] = sigmoid(np.dot(x,wf))
        ig[i] = sigmoid(np.dot(x,wi))
        cg[i] = tangent(np.dot(x,wc))
        csc = (cs[i-1] * sigmoid(np.dot(x,wf))) + (sigmoid(np.dot(x,wi)) * sigmoid(np.dot(x,wc)))
        cs[i] = (cs[i-1] * sigmoid(np.dot(x,wf))) + (sigmoid(np.dot(x,wi)) * sigmoid(np.dot(x,wc)))
        og[i] = sigmoid(np.dot(x,wo))
        hs[i] = sigmoid(np.dot(x,wo)) * tangent(csc)
        hsc = sigmoid(np.dot(x,wo)) * tangent(csc)
        ys[i] = sigmoid(np.dot(hsc,wy))
        intx = np.argmax(vy[i-1])

    dwy = np.zeros((yl,d))
    dwf = np.zeros((xl+yl,yl))
    dwi =  np.zeros((xl+yl,yl))
    dwc =  np.zeros((xl+yl,yl))
    dwo =  np.zeros((xl+yl,yl))

    dfhs = np.zeros(yl)
    dfcs = np.zeros(yl)
    totalError = 0
#Back Propagation
    for i in reversed(range(nw)):
        merror = ys[i] - vy[i]
        dwy += np.dot(np.atleast_2d(hs[i]).T,np.atleast_2d((merror*dsigmoid(ys[i]))))
        error = np.dot(merror,wy.T)
        totalError += np.sum(error)
        e = np.clip(error+dfhs,-6,6)
        dho = tangent(cs[i]) * e
        dho = dsigmoid(og[i]) * dho
        dwo += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dho))
        dc = og[i] * e * dtangent(cs[i])
        dc = np.clip(dc + dfcs,-6,6)
        dhf = cs[i-1] * dc
        dhf = dsigmoid(fg[i]) * dhf
        dwf += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dhf))
        dhi = cg[i] * dc
        dhi = dsigmoid(ig[i]) * dhi 
        dwi += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dhi))
        dhc = ig[i] * dc
        dhc = dsigmoid(cg[i]) * dhi 
        dwc += np.dot(np.atleast_2d(xs[i]).T,np.atleast_2d(dhc))

        dfhs = np.dot(dho,wo.T)[:yl]+np.dot(dhc,wc.T)[:yl]+np.dot(dhi,wi.T)[:yl]+np.dot(dhf,wf.T)[:yl]
        dfcs = fg[i] * dc",neuralnetworks,Dewanik-Koirala,False,/r/neuralnetworks/comments/cer49o/dlstms_backpropagation_from_scratch_and_its/
Artificial Intelligence helps write code in Python,1563429025,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/ceohyn/artificial_intelligence_helps_write_code_in_python/
"What is this? is this the so called ""Model Collapse"" phenomenon in a Gan or something completely different?",1563386589,"This is a follow up question which I asked [here](https://www.reddit.com/r/neuralnetworks/comments/ce1k3o/why_are_the_images_generated_by_my_gan_get_darker/). I tried to see if my GAN memorized something or not. 

I'm 100% sure that in my earlier experiments I noticed couple of images that were identical to the ones in the dataset, though they had a bit of distortions but no mistaking it that they were indeed the images from dataset. 

I changed the architecture to the one that I shown in that previous question, and then tried to create a morphing effect on the first image and see how it changes to another image. 

I cant figure out what is happening here so here is the image sequence it self plus the function I wrote that creates those morphing effect.  

here is the function : 

&amp;#x200B;

    # morphing test
    def morph_sample(G, z_vec_numpy, count=10, row =2, col=10 , 
                     device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):
        
        fig = plt.figure(figsize=(col,row))
        plt.subplots_adjust(wspace=0,hspace=0)
        for i in range(count):
            
            z = z_vec_numpy + 0.5 * (i*0.2)
            z = torch.from_numpy(z).float().to(device)
            imgs = G(z)
            imgs = imgs.detach().to('cpu').numpy().squeeze().transpose(1,2,0)
            # unnormalize the image using :  ((imgs - min)*255 // (max-min))
            imgs = ((imgs +1)*255 /2 ).astype(np.uint8)
            ax = fig.add_subplot(row, col, i+1,xticks=[], yticks=[])
            ax.imshow(imgs)
            
            #plt.show()
    z = np.random.uniform(-1,1, size=(1,100))
    morph_sample(G, z, count=50, row=5, col=10)

and here are several examples generated using this function : 

&amp;#x200B;

https://i.redd.it/0zlf3vb9jwa31.png

&amp;#x200B;

https://i.redd.it/3npx4rnbjwa31.png

&amp;#x200B;

https://i.redd.it/zq8u2ryejwa31.png

https://i.redd.it/91z4jfrjjwa31.png

As you you have just seen, they start from different images, then gradually fade into the same garbage image! if I continue this for example for 200, the image gets weirder and as a random blob!

here is an example with count of 100 : 

&amp;#x200B;

https://i.redd.it/dle71mm4kwa31.png

if I play with the latent vector values (e.g. add/subtract it from 0.1 instead of 0.5 and stuff like this) I get different results, but ultimately they all fade into  some image and wont morph for ever!  : 

&amp;#x200B;

https://i.redd.it/i7zrnev1lwa31.png

https://i.redd.it/44w7iqw6lwa31.png

https://i.redd.it/tzzjuz5alwa31.png",neuralnetworks,MasterSama,False,/r/neuralnetworks/comments/cegevk/what_is_this_is_this_the_so_called_model_collapse/
Mad Max: Affine Spline Insights into Deep Learning,1563371658,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ced59j/mad_max_affine_spline_insights_into_deep_learning/
https://www.youtube.com/watch?v=7Q2JhZxNPow,1563371614,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ced4xx/httpswwwyoutubecomwatchv7q2jhzxnpow/
Redefine manufacturing process with chatbot applications,1563343999,,neuralnetworks,Verma_RJ,False,/r/neuralnetworks/comments/ce8yqf/redefine_manufacturing_process_with_chatbot/
Deep learning-powered 'DeepEC' framework helps accurately understand enzyme functions,1563307468,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ce28l1/deep_learningpowered_deepec_framework_helps/
Attempting to Visualize a Convolutional Neural Network in Realtime,1563307406,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ce281o/attempting_to_visualize_a_convolutional_neural/
AI Can Edit Photos With Zero Experience,1563307338,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ce27ig/ai_can_edit_photos_with_zero_experience/
"Robots Have a Hard Time Grasping These ""Adversarial Objects""",1563307304,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ce2777/robots_have_a_hard_time_grasping_these/
Identifying perceived emotions from people's walking style,1563307236,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ce26og/identifying_perceived_emotions_from_peoples/
Identifying perceived emotions from people's walking style,1563307137,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ce25xx/identifying_perceived_emotions_from_peoples/
Why are the images generated by my GAN get darker as the network trains more?,1563304357,"I created a simple DCGAN (convolutional GAN) with 5 layers and trained it on CelebA dataset (the small one). 

I noticed my networks images are dimmed looking and as the network trains further, the bright color fades into dim ones! 

here are some example, the number shows  the epoch number : 

&amp;#x200B;

https://i.redd.it/v3wzs3vnppa31.png

&amp;#x200B;

&amp;#x200B;

https://i.redd.it/qp1vbpquppa31.png

&amp;#x200B;

https://i.redd.it/2tzbg6xxppa31.png

&amp;#x200B;

https://i.redd.it/z3jrk1p3qpa31.png

What is the cause for this phenomenon? 

I didnt use batchnorm for the first layer of Discriminator, I also didnt use Batchnorm for the last layer of the Generator, 

I used LeakyRelu(0.2) in Discriminator, and ReLU for Generator. I have no idea why the images are this dim/dark! 

Any help is greatly appreciated. 

&amp;#x200B;

    
    def conv_batch(in_dim, out_dim, kernel_size, stride, padding, batch_norm=True):
        layers = nn.ModuleList()
    
        conv = nn.Conv2d(in_dim, out_dim, kernel_size, stride, padding, bias=False)
        layers.append(conv)
     if batch_norm:
            layers.append(nn.BatchNorm2d(out_dim))
     return nn.Sequential(*layers)
    
    class Discriminator(nn.Module):
    	def __init__(self, conv_dim=32, act = nn.ReLU(), mode=0):
    		super().__init__()
     
    		self.mode = mode
    		self.conv_dim = conv_dim 
    		self.act = act
    		self.conv1 = conv_batch(3, conv_dim, 4, 2, 1, False)
    		self.conv2 = conv_batch(conv_dim, conv_dim*2, 4, 2, 1)
    		self.conv3 = conv_batch(conv_dim*2, conv_dim*4, 4, 2, 1)
    		self.conv4 = conv_batch(conv_dim*4, conv_dim*8, 4, 1, 1)
    		self.conv5 = conv_batch(conv_dim*8, conv_dim*10, 4, 2, 1)
    		self.conv6 = conv_batch(conv_dim*10, conv_dim*10, 3, 1, 1)
    
    		self.fc1 = nn.Linear(64*64*3, 64*10)
    		self.fc2 = nn.Linear(64*10, 64*10)
    		self.fc3 = nn.Linear(64*10,1)
    		# self.conv5 = conv_batch(conv_dim*4, conv_dim*5, 3, 2, 1)
    		# self.conv6 = conv_batch(conv_dim*5, conv_dim*6, 3, 2, 1)
    		self.drp = nn.Dropout(0.5)
    		self.fc = nn.Linear(conv_dim*10*3*3, 1) # it seems, larger fmaps provide better results?!
    
    	def forward(self, input):
    		if self.mode == 0:
    			batch = input.size(0)
    			output = self.act(self.conv1(input))
    			output = self.act(self.conv2(output))
    			output = self.act(self.conv3(output))#16
    			output = self.act(self.conv4(output))#16
    			output = self.act(self.conv5(output))#16
    			output = self.act(self.conv6(output))#16
    
    			output = output.view(batch, self.fc.in_features)
    			output = self.fc(output)
    			output = self.drp(output)
    		else:
    			output = input.view(-1, 64*64*3)
    			output = self.drp(self.fc1(output))
    			output = self.drp(self.fc2(output))
    			output = self.drp(self.fc3(output))
    		return output
    
    def deconv_convtranspose(in_dim, out_dim, kernel_size, stride, padding, batchnorm=True):
        layers = []
        deconv = nn.ConvTranspose2d(in_dim, out_dim, kernel_size = kernel_size, stride=stride, padding=padding)
        layers.append(deconv)
    	if batchnorm:
            layers.append(nn.BatchNorm2d(out_dim))
    	return nn.Sequential(*layers)
    
    class Generator(nn.Module):
    	def __init__(self, z_size=100, conv_dim=32, mode=0): 
    		super().__init__()
    		 self.conv_dim = conv_dim
    		 # make the 1d input into a 3d output of shape (conv_dim*4, 4, 4 )
    		 self.fc = nn.Linear(z_size, conv_dim*4*4*4)#4x4
    		 # conv and deconv layer work on 3d volumes, so we now only need to pass the number of fmaps and the
    		 # input volume size (its h,w which is 4x4!)
    		 self.mode = mode
    		 self.drp = nn.Dropout(0.5)
    		if mode == 0: 
    			self.deconv1 = deconv_convtranspose(conv_dim*4, conv_dim*3, kernel_size=3, stride=2, padding=1)
    			self.deconv2 = deconv_convtranspose(conv_dim*3, conv_dim*2, kernel_size=3, stride=2, padding=1)
    			self.deconv3 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size=3, stride=2, padding=1)
    			self.deconv4 = deconv_convtranspose(conv_dim, conv_dim, kernel_size=4, stride=1, padding=0)
    			self.deconv5 = deconv_convtranspose(conv_dim, 3, kernel_size=5, stride=1, padding=0,batchnorm=False)
    		elif mode ==1:
    			self.deconv1 = deconv_convtranspose(conv_dim*4, conv_dim*3, kernel_size =4, stride=2, padding=1)
    			self.deconv2 = deconv_convtranspose(conv_dim*3, conv_dim*2, kernel_size =4, stride=2, padding=1)
    			self.deconv3 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size =4, stride=2, padding=1)
    			self.deconv4 = deconv_convtranspose(conv_dim, conv_dim, kernel_size =3, stride=2, padding=1)
    			self.deconv5 = deconv_convtranspose(conv_dim, 3, kernel_size =4, stride=1, padding=1, batchnorm=False)
    		elif mode==2:
    			self.deconv1 = deconv_convtranspose(conv_dim*4, conv_dim*2, kernel_size =4, stride=2, padding=1)
    			self.deconv2 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size =4, stride=2, padding=1)
    			# self.deconv3 = deconv_convtranspose(conv_dim*2, conv_dim, kernel_size =4, stride=1, padding=1)
    			self.deconv4 = deconv_convtranspose(conv_dim, conv_dim, kernel_size =4, stride=2, padding=1)
    			self.deconv5 = deconv_convtranspose(conv_dim, 3, kernel_size =4, stride=2, padding=1, batchnorm=False)
    
    	def forward(self, input):
    		output = self.fc(input)
    		output = self.drp(output)
    		output = output.view(-1, self.conv_dim*4, 4, 4)
    		if self.mode != 2: 
    			output = F.relu(self.deconv1(output))
    			output = F.relu(self.deconv2(output))
    			output = F.relu(self.deconv3(output))
    			output = F.relu(self.deconv4(output))
    			# we create the image using tanh!
    			output = F.tanh(self.deconv5(output))
    		else:
    			output = F.relu(self.deconv1(output))
    			output = F.relu(self.deconv2(output))
    			output = F.relu(self.deconv4(output))
    			# we create the image using tanh!
    			output = F.tanh(self.deconv5(output))
    	 
    		return output
    
    
    dd = Discriminator(mode=0)
    zd = np.random.rand(2,3,64,64)
    zd = torch.from_numpy(zd).float()
    # print(dd)
    print(dd(zd).shape)
    
    gg = Generator(mode=1)
    z = np.random.uniform(-1,1,size=(2,100))
    z = torch.from_numpy(z).float()
    print(gg(z).shape)",neuralnetworks,MasterSama,False,/r/neuralnetworks/comments/ce1k3o/why_are_the_images_generated_by_my_gan_get_darker/
"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",1563280589,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/cdwh25/this_video_goes_over_a_model_that_predicts_the/
Markup devices for segmentation,1563266680,"I don’t know where to place this post exactly, so I’ll leave it in neural networks. The essence of the question is this: I need to find an article where it is written, or maybe you can tell me here, the better I can mark the images for segmentation (mouse, tablet). What kind of devices can I use to markup data to make it faster and more convenient than using a mouse?",neuralnetworks,CyberValdemar,False,/r/neuralnetworks/comments/cdufds/markup_devices_for_segmentation/
Question: Why does the MSE cost function have the term 1/2n instead of 1/n?,1563232616,"The book I'm using ([http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)) uses the following formula for the quadratic/mean squared error cost function:

&amp;#x200B;

&amp;#x200B;

[formula from book\/website](https://i.redd.it/6o62uesdvja31.png)

&amp;#x200B;

Why is there a 1/2n rather than just a 1/n? To my understanding, 1/n would allow one to calculate the average cost. I am not sure what 1/2n would be calculating (half of the average?).",neuralnetworks,SixCarbonSugar,False,/r/neuralnetworks/comments/cdp1ij/question_why_does_the_mse_cost_function_have_the/
Neural Network Just Averaging Results,1563218948,"I've made a neural network script in python and cannot get it to preform well on an actual data set. If you are interested, the script can be found on [https://www.github.com/EvilTeliportist/PyNet](https://www.github.com/EvilTeliportist/PyNet). I'm trying to input 2 numbers for each trial, like a length and height of a snail, and predict the gender of the snail. I have all the data from a dataset, but the neural network averages to .5 every time. I have my output nodes each set to the three classes for gender in the database, male, female, and infant. For instance, if male, the correct output nodes will be 1, 0, 0, female: 0, 1, 0, infant: 0, 0, 1. The network can do XOR and all that, so I know it works. Why doesn't my data work with it?

&amp;#x200B;

It doesn't work no matter how many hidden layers I give it and how many nodes in each hidden layer. My script also doesn't support bias yet. Could this be the problem?",neuralnetworks,EvilTeliportist,False,/r/neuralnetworks/comments/cdm6ho/neural_network_just_averaging_results/
[Deepfake] Keanu Reeves saves from robber,1563210071,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/cdk2ci/deepfake_keanu_reeves_saves_from_robber/
"AI-based photo restoration: defects removal, inpainting, colorization",1563196777,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/cdhdpt/aibased_photo_restoration_defects_removal/
Using a Machine Learning Model in a Web Application Client,1563179948,,neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/cdexyc/using_a_machine_learning_model_in_a_web/
Guide to Machine Learning with ML 1.0,1563174070,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/cde6vt/guide_to_machine_learning_with_ml_10/
"U.S. proposes barring big tech companies from offering financial services, digital currencies",1563148622,,neuralnetworks,curryeater259,False,/r/neuralnetworks/comments/cda636/us_proposes_barring_big_tech_companies_from/
GPU accelerated Neural Network lib in C#,1563098175,,neuralnetworks,trenmost,False,/r/neuralnetworks/comments/cd1buu/gpu_accelerated_neural_network_lib_in_c/
Sparse Networks from Scratch: Faster Training without Losing Performance,1563048334,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cctscj/sparse_networks_from_scratch_faster_training/
This is How Google’s Phone Enhances Your Photos,1563040230,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ccs69d/this_is_how_googles_phone_enhances_your_photos/
Eta's Ultra Low-Power Machine Learning Platform,1563040188,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ccs5yo/etas_ultra_lowpower_machine_learning_platform/
Some New Topics in the area of Generative Adversarial Network (GAN) Research (7.5),1563039937,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ccs488/some_new_topics_in_the_area_of_generative/
Neural Network to Decompress audio?,1562998510,"Just an idea, I mean compression in terms of dynamic range compression, I don't know much about neural networks but is this possible? I know declippers exist but I mean in terms of just DRC.",neuralnetworks,mariois64,False,/r/neuralnetworks/comments/ccm34s/neural_network_to_decompress_audio/
Can someone help me understand the graph of a truth table? I'm not getting how coordinates are derived for this inclusive OR.,1562998431,,neuralnetworks,WadeEffingWilson,False,/r/neuralnetworks/comments/ccm2qt/can_someone_help_me_understand_the_graph_of_a/
Like/dislike detection using YOLOv3,1562883415,"Hi!

My team and I have just released a computer vision algorithm that can detect ""thumb up"" and ""thumb down"" gestures.

The algorithm is based on tiny-YOLOv3 architecture. Accuracy of thumb up/down gesture recognition is calculated as mean average precision (mAP@0.25) = 85.19%; average IoU = 73.89%.

The neural network was trained on 3000 images.

To try out the algorithm, download it from the [GitHub](https://github.com/heyml/rateme) and install it.

We would like to hear your feedback so we can continue to improve our service to you :)",neuralnetworks,atrosen1,False,/r/neuralnetworks/comments/cc27bf/likedislike_detection_using_yolov3/
How to transfer private data? Find answer with Utopia,1562871859,"Let us introduce an absolutely revolutionary project in the sphere of information security and privacy of peer-to-peer data transfer!

Utopia is a decentralized peer-to-peer network, with no central server involved in data transmission or storage. The network is supported by people who use it and deliver high-quality features and perfectly smooth user Interface that offer a lot of value. With Utopia you can send instant text and voice messages, transfer files, create group chats and channels, news feeds and conduct a private discussion. A channel can be geotagged using integrated uMaps, which simplifies Utopia channel search and adds an additional security layer. As a result, there is no need to use public map services, which are known to collect your data to feed Big Data massives. 

Sounds rather interesting, doesn’t it? Everyone has a chance to take part in the beta testing of our new unique product that doesn’t have any analogs on today’s market https://beta.u.is! And it’s not the end of benefits! We have a generous rewards system. You can get your reward for any assistance and contribution to the testing of Utopia.

uMail is a decentralized alternative to classic e-mail. No servers are used for mail transmission or storage. uMail account, that is created in a minute, enables unlimited messaging and attachment storage. Utopia ecosystem encryption guarantees the security of mail transmission and storage. Your uMail, as an internal part of Utopia, cannot be blocked or seized.

All financial functionality can be found in Utopia built-in uWallet: make and accept payments denominated in Utopia mineable cryptocurrency Crypton, accept payments at your website, pay by Crypto Cards without revealing your Identity or bill fellow Utopia users for your services.
 
Other features include API and console client for fast and easy integration. 

Utopia Network includes a safe alternative to traditional Domain Name System (DNS) called Utopia Name System (UNS). This is a decentralized registry of names that are impossible to expropriate, freeze or corrupt by 3rd-party. Once registered it is your property for infinity. 

UNS combined with Packet Forwarding functional allows to tunnel any kind of data between users in ecosystem, making possible to host different types of resources including websites inside Utopia Network. Utopia has built-in Idyll browser to view websites within Utopia peer-to-peer network. Idyll is a great alternative to the TOR browser.

Developers of Utopia project worked hard for over 5 years and there are many other amazing features you will enjoy like voice encryption, tons of stickers and smiles, multi-player games, collaboration, and organizing tools.

The Utopia team is happy to announce the launch of the public BETA testing.
If you are interested in absolute data anonymity and are ready to help us to make our network better, welcome! Now anyone who wishes can get to know the unique features of Utopia and our revolutionary approach to the decentralized ecosystem designed for privacy. There is a generous rewards system for any help. You can choose on your own what role you want to have in our testing process: to search for bugs, to promote our product or to help other users to understand better how to use our features. We highly appreciate any feedback and will enhance our ecosystem in accordance with your comments. Don’t miss your chance to become one of the first lucky guys who will get access to an absolutely unique novelty on the Internet! 
The stage of beta testing will last for approximately 3 months. After this period we will release a final version of the Utopia ecosystem and make it available for download. Utopia is absolutely free software without any one-time or recurring fees for using the widest range of Utopia’s features.
To get more details about Utopia and to take part in our beta testing, follow the link https://beta.u.is.",neuralnetworks,AmyaMelton,False,/r/neuralnetworks/comments/cbzqzm/how_to_transfer_private_data_find_answer_with/
A Learning Algorithm for a Neuromorphic Network ~ Your feedback appreciated!,1562870516,"  

![video](516n9etbwp931)

I’ve been working on the logic for neuromorphic network that can learn from its environment without training. I’d like to ask the r/neuralnetworks community whether or not you think this model offers something new.

The brief video I posted here shows how the actions of a robot “bug” can be controlled by a neuromorphic network and how the bug can learn a new action based on its experience. The video isn’t technical and focuses mostly on the learning proof of concept, so I thought I’d add a few details here for anyone who is interested. The application is written in Java and does not use any AI-specific APIs. Although the simulation runs on a traditional computer, the logic of the model does not require memory, system clocks, or other features of a von Neumann machine. In addition:

* The network is made up of a collection of nodes (“neurons”) that are connected to each other in a specific logical pattern.  The nodes resemble McCulloch-Pitts neurons inasmuch as inputs can either be excitatory or inhibitory and there is a simple summing function that causes a node to fire when its threshold is met (in my model, the thresholds are very simple…either 1 or 2 depending on the role of the node in the network). The nodes are not labelled (all nodes of a specific type are identical) and there is no central list of nodes or their connections. The network is subdivided into subnetworks each of which performs a logical function such as processing an exclusive OR (XOR).
* The network processes information asynchronously (there’s no timing function that coordinates the firing of the nodes). However, the relative timing of events is important. For example, red followed later by green will be seen by the network as two separate colors; red and green seen at the same time will be interpreted as yellow.
* The network does not need training; the objective is for the network to learn from its experience.
* The network is deterministic; no weighting or probabilistic functions are used by the model. Once a node is constructed, none of its properties change. One of the benefits of the deterministic approach is that the actions of the network are explainable (i.e., it isn’t a black box).

I know that my model has similarities to others and I know that the sparse detail here isn’t enough to evaluate it thoroughly…however, I’m really interested in your thoughts. What other proofs of concept would show that the model has potential? If you were developing this, in which direction would you take it?",neuralnetworks,perfectpebble,False,/r/neuralnetworks/comments/cbzght/a_learning_algorithm_for_a_neuromorphic_network/
1000x faster data augmentation,1562854852,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cbw5tc/1000x_faster_data_augmentation/
Simple 'smart' glass reveals the future of artificial vision,1562854356,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cbw34e/simple_smart_glass_reveals_the_future_of/
New filter enhances robot vision on 6-D pose estimation,1562854299,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cbw2sm/new_filter_enhances_robot_vision_on_6d_pose/
NVIDIA AI Training in Seconds Instead of Hours or Days,1562852073,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cbvr4y/nvidia_ai_training_in_seconds_instead_of_hours_or/
GANs,1562793284,"So.. I had this thought for a long time...

Why use GANs to generate rather than a Fully Connected Architecture that maps an (random number + category) to an (image) ? rather than doing that with a GAN?  Is it due to the overfitting that will occur on the category variable?",neuralnetworks,omdano,False,/r/neuralnetworks/comments/cbmkfk/gans/
CNN question,1562758768,"Hello, I am currently working on a neural network that is supposed to return a yes/no answer for an image after it's been trained. After looking on the internet I have came across ResNet50 and a way to change the output size of the network. My question is: it it okay to use ResNet50 as a base for something the returns a binary value? Is there something better that I could use?",neuralnetworks,dumneaiow,False,/r/neuralnetworks/comments/cbfn18/cnn_question/
Introduction to TensorWatch,1562750584,,neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/cbei99/introduction_to_tensorwatch/
"Jim Carrey instead of Jack Nicholson in the movie ""The Shining"" | Deepfake",1562698113,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/cb514j/jim_carrey_instead_of_jack_nicholson_in_the_movie/
Researchers cast neural nets to simulate molecular motion,1562685367,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cb26rq/researchers_cast_neural_nets_to_simulate/
AI Recognizes And Locks Out Murder Cats,1562685213,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cb25lx/ai_recognizes_and_locks_out_murder_cats/
Robot uses machine learning to harvest lettuce,1562685151,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cb255b/robot_uses_machine_learning_to_harvest_lettuce/
"Why do neural networks work? I've written neural networks in C to gain a better understanding of HOW they work, but a little reading into neuroscience is very helpful toward understanding WHY they work. You can visualize what your code is doing in relation to what an actual neuron does.",1562679203,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/cb0xro/why_do_neural_networks_work_ive_written_neural/
This video goes over a breast cancer diagnosis model that uses neural networks (implemented in python),1562633390,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/catm9k/this_video_goes_over_a_breast_cancer_diagnosis/
Robot uses machine learning to harvest lettuce,1562631594,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/cataaq/robot_uses_machine_learning_to_harvest_lettuce/
How to know if it is a DNN,1562610936,"Ladies/Gentlemen, if he/she:

- requires lots of supervision
- yet always wants more power
- can't explain decisions
- optimizes for the average outcome
- dismisses problems as edge cases
- forgets things catastrophically

He/She's not your man/woman, he/she's a deep neural network.

Creds: Alex Champandard",neuralnetworks,goncaloperes,False,/r/neuralnetworks/comments/caoyky/how_to_know_if_it_is_a_dnn/
Would it be feasible to generate other things like minds and programs with G.A.N.s using the right parameters instead of images?,1562582993,,neuralnetworks,NoLanGoLan,False,/r/neuralnetworks/comments/cajir9/would_it_be_feasible_to_generate_other_things/
PyNet: A simple and educational python package for neural networks,1562420090,"Hi! I've recently made a small python package to teach neural networks to beginners wanting to learn. If you're interested, you can check it out here: https://github.com/EvilTeliportist/PyNet",neuralnetworks,EvilTeliportist,False,/r/neuralnetworks/comments/c9tnei/pynet_a_simple_and_educational_python_package_for/
The smartphone has disappeared on a selfie! Deep learning,1562237522,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/c917fe/the_smartphone_has_disappeared_on_a_selfie_deep/
Looking for a D-noise alternative (clean pictures' noise),1562202091,"[D-noise](https://remingtongraphics.net/tools/d-noise/) is great, but I would like to get alternatives to it (paid or free), online or installed.

I'm not looking for something that just diffuses the picture to remove the noise.

Thanks in advance.",neuralnetworks,dadiaar,False,/r/neuralnetworks/comments/c8w7c9/looking_for_a_dnoise_alternative_clean_pictures/
what type of neural network should i use,1562188876,"lets say at time T I have a spot on the surface of a sphere with its position represented by an x, y and z value (from -180 to 180). At time T+1 it moved to another spot on the surface of the sphere. what type of neural network should I use to predict its position on the sphere based on its previous positions?",neuralnetworks,Mathbound314,False,/r/neuralnetworks/comments/c8ttsx/what_type_of_neural_network_should_i_use/
LidarView and Deeplearning,1562186328,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/c8tbau/lidarview_and_deeplearning/
LidarView and Deeplearning,1562185556,,neuralnetworks,PlayfulConfidence,False,/r/neuralnetworks/comments/c8t5fm/lidarview_and_deeplearning/
Any idea why MNIST digit recognizer is not performing well?,1562138939,"Hi! 
I made a GPU accelerated C# neural network library and to try it out i trained it on the MNIST number digit database (60k samples)

It performs well on the test set (~96%) but when i made an app where you can draw a number it doesnt recognize them (like 20% success rate)


[It is available here](http://htmlpreview.github.io/?https://github.com/zbendefy/machine.academy/blob/master/WebApps/NumberRecognize/index.html)

The test set is separated from the training set.

Also I tried using 2-3 layers with Cross entropy cost function and with L2 regularization, and xavier weight initialization",neuralnetworks,trenmost,False,/r/neuralnetworks/comments/c8l57y/any_idea_why_mnist_digit_recognizer_is_not/
Help with NNet failing on specific types of signals,1562132160,"Hi.  I am relatively new to machine learning and have been trying to implement something that should be reasonably ""simple"".  I have a series of 1D signals that all have 3 different parts, and I want to be able to have the neural network identify where in the signal the 3 parts of the signal are located.  I am using matlab (which I know isn't really the best program to use for ML - but Im using it because it simplifys the flow into other matlab  programs that the data is used for).  I basically manually annotated a few hundred signals, included augmented data (translated, shifted signals etc -- which makes up about 75% of the entire dataset), transformed into frequency/time domain, normalized the signals, and fed them into  a sequence to sequence LSTM (can provide the details if necessary).  Training works well (it seems), and my confusion matrix on the test data has each part of the signal being predicted with 97-98% accuracy.  Looking at the actual performance also shows that overall its quite good at identifying the 3 different areas in the test dataset.  However, there is 1 general morphology of signal where the NNet really struggles and is quite wrong all the time.  This morphology of signal is not common.  Is this likely because I have too much augmented data and the NNet is overgeneralizing things to the point where it can't figure out what to do with this less common signal morphology (eg this signal is just underrepresented in the training data)?  I tried feeding in more augmented data on this type of signal, but didn't seem to make a difference overall.  I assume the solution is to augment less and find more real examples of this type of signal to train on, but its not a very common signal morphology so I probably wont have hundreds of real examples to train on.  Any thoughts would be greatly appreciated.  

Thanks in advance1",neuralnetworks,potato_chip0,False,/r/neuralnetworks/comments/c8k8m6/help_with_nnet_failing_on_specific_types_of/
How are Neural Networks used to simulate evolution?,1562088077,"Hello, I am trying to get my head around Neural Networks and have gotten interested in how they are used to simulate evolution. However, I am still confused by how crossover works, specifically how a bit string is inputted into a NN and how a NN is used to move an individual.

Also, I would be grateful if someone could explain how the fitness of an individual is fed back to the NN, e.g. If a NN does something bad, how do we train the NN to stay away from that sort of thing?

Thanks in advance",neuralnetworks,BSprogrammer,False,/r/neuralnetworks/comments/c8bze6/how_are_neural_networks_used_to_simulate_evolution/
Searching for Code? Let a Neural Network Do That for You!,1561995240,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/c7u0xy/searching_for_code_let_a_neural_network_do_that/
Next steps after neural network implementation,1561991201,"For better or worse, I've finished my C++ implementation of neural networks, though it's little rough around the edges. I ran it through some test cases involving logic gates, and it seems to work well.

&amp;#x200B;

I guess the question is...what's next? My original goal to implement a neural network was to implement it in a game in order to pick the best solution. I talked about it in a previous post, but now that I've accomplished one step, I'm not sure where to go. I'm guessing I have to implement some fitness function and genetic algorithms to help find the best solution, but I don't really know how they work within the grand scheme. What are some good resources that can help me determine what to do next?

&amp;#x200B;

FYI, the game is a quick puzzle game where you 'flip' squares by tapping them, which flips itself and the surrounding squares, until you fill the squares with all X's.",neuralnetworks,Jsyschan,False,/r/neuralnetworks/comments/c7t7zx/next_steps_after_neural_network_implementation/
AI predicts coma outcome from EEG trace,1561922805,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c7iftz/ai_predicts_coma_outcome_from_eeg_trace/
Learning Depths of Moving People by Watching Frozen People,1561922742,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c7ife3/learning_depths_of_moving_people_by_watching/
Would it be possible to build a neural net to generate Jake and Amir episodes?,1561918832,"I don't know anything about programming or machine learning, but I'd love to build/find a neural net that I can train with the 800+ Jake and Amir episodes from the [script archive](https://scripts.jakeandamir.com/), and then have it spit out new ones. I think it would be great fun. Any advice? Is this possible, or too complex?",neuralnetworks,LazarusRises,False,/r/neuralnetworks/comments/c7hoz0/would_it_be_possible_to_build_a_neural_net_to/
How are Neural Networks used to play games e.g. Driving a car around a track?,1561664364,"I have recently gotten into machine learning and neural networks but am puzzled by how a neural network can constantly move a player on a screen. Is the network being run every frame? Please could someone help me understand this better.

&amp;#x200B;

Thanks",neuralnetworks,BSprogrammer,False,/r/neuralnetworks/comments/c69z6t/how_are_neural_networks_used_to_play_games_eg/
Undress any girl ! DEEPNUDE DeepFake,1561663049,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/c69pba/undress_any_girl_deepnude_deepfake/
Undress any girl ! DEEP NUDE Deep Fake,1561662905,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/c69o8t/undress_any_girl_deep_nude_deep_fake/
"Multi-class classification using CNN over PyTorch, and the basics of CNN",1561649822,,neuralnetworks,thevatsalsaglani,False,/r/neuralnetworks/comments/c66zzw/multiclass_classification_using_cnn_over_pytorch/
Google fed a neural net with a viral meme and this is what just happened,1561635070,,neuralnetworks,azmodeus99,False,/r/neuralnetworks/comments/c64eed/google_fed_a_neural_net_with_a_viral_meme_and/
Playing a game of GANstruction,1561614764,,neuralnetworks,bil-sabab,False,/r/neuralnetworks/comments/c61qqt/playing_a_game_of_ganstruction/
Recurrent Neural Networks and LSTMs with Keras - Eduonix Blog,1561609470,,neuralnetworks,Aisha_b,False,/r/neuralnetworks/comments/c60ybu/recurrent_neural_networks_and_lstms_with_keras/
Laplace - BTCUSD ticker values prediction AI,1561586183,,neuralnetworks,resotto,False,/r/neuralnetworks/comments/c5wmzv/laplace_btcusd_ticker_values_prediction_ai/
Convolutional Neural Networks: An Intuitive Approach,1561560493,,neuralnetworks,jimscott1232,False,/r/neuralnetworks/comments/c5qpt8/convolutional_neural_networks_an_intuitive/
[D]Are LSTMs hardest neural network artitechture among all neural network artitechture ??,1561557476,"I am a beginner and after learning MLP , CNN and vanilla RNN  , I am now trying to learn LSTMs which I found the most difficult of till now is it hardest among all NN artitechture if not which one ?",neuralnetworks,Dewanik-Koirala,False,/r/neuralnetworks/comments/c5q52b/dare_lstms_hardest_neural_network_artitechture/
I’m attempting to build a card game AI using neural networks and need some advice,1561529365,"For context, I’m developing a digital grid-based card game. The game functions relatively similarly to Mage Wars as far as the relationship between deck, hand, and field are concerned as well as how cards are played and actions are taken. There is a finite limit for the number of cards in a deck (120) and in a hand (2). The board is a 4x4 grid and each space can hold as many cards as are placed there. There is a hard limit on the number of effects that a card can have at a given time (7). For a given action, a player may play a card from their hand or have a card move, attack, or activate an ability. As far as this AI is concerned, it is operating under complete information: it knows the location and state of all cards including those in the decks and hands. If I represent The whole game state in binary, I end up with a file around 4KB. The function of this trained AI would be to approximate proper high-level play for card testing purposes.


I know this is a bit naive, but this is a very long-term project that I’m only just starting. I intend to spend a lot of time researching and teaching myself neural network theory. I’d just like to know if I’m looking at the problem in the right way at all, where to start as far as approaches, methods, models, or algorithms are concerned, and any other advice or resources you think I should familiarize myself with. If you’d like any other details about the project, I’d be happy to share as much as you need.",neuralnetworks,netrunui,False,/r/neuralnetworks/comments/c5m74i/im_attempting_to_build_a_card_game_ai_using/
SPFCNN-Miner: A new classifier to tackle class-unbalanced data,1561497804,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c5flk6/spfcnnminer_a_new_classifier_to_tackle/
Generative Adversarial Networks - The Story So Far,1561497727,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c5fktn/generative_adversarial_networks_the_story_so_far/
Delta encoder: An effective sample synthesis method for few-shot object recognition,1561497650,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c5fk4d/delta_encoder_an_effective_sample_synthesis/
"What is the best approach for a NN that takes a drawing of a diagram, consisting of arrows, shapes, and text, and outputs the vector data... a set of linear equations to draw the shapes, and text glyphs, such that it can be rendered? Searching for this is very hard.",1561482309,"Input would be a desaturated image of hand drawn diagrams with the usual shapes, arrows and text, and output ideally would be vector graphics information in some format. 

Search results give linear equations of the neural networks themselves, or diagrams of the architectures. The only things that come close have the output as the render itself and not the vector information. Any resources or ideas are appreciated.",neuralnetworks,joeblessyou,False,/r/neuralnetworks/comments/c5bn5w/what_is_the_best_approach_for_a_nn_that_takes_a/
Trying to make my own NN module for python,1561477730,"I've got a bunch of code that works well right now, but I don't know how to go about adding the function to be able to add hidden layers. So far I can only have 1, but as many nodes in it with any number of input and outputs. How would I go about conceptualizing something that allows for multilayer back propagation? Sorry im new to all this.",neuralnetworks,EvilTeliportist,False,/r/neuralnetworks/comments/c5aifb/trying_to_make_my_own_nn_module_for_python/
Introduction to Q-Learning with Python and Open AI Gym,1561460308,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/c56rii/introduction_to_qlearning_with_python_and_open_ai/
Walmart uses AI cameras to spot thieves,1561381696,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c4nixc/walmart_uses_ai_cameras_to_spot_thieves/
ITP Camp 2019 - Frontiers of AI Arts,1561381526,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c4nh9z/itp_camp_2019_frontiers_of_ai_arts/
"Global Neural Network Software Market | Trends, Growth, Size, Forecast, Opportunities, Challenges by 2021",1561380510,[removed],neuralnetworks,siddharthmnm,False,/r/neuralnetworks/comments/c4n795/global_neural_network_software_market_trends/
Multigrid Neural Memory,1561242197,,neuralnetworks,wandererrrx,False,/r/neuralnetworks/comments/c3vkxr/multigrid_neural_memory/
Optical neural networks could be faster than digital ones,1561168054,,neuralnetworks,MountBlanc,False,/r/neuralnetworks/comments/c3jny5/optical_neural_networks_could_be_faster_than/
'Neural Lander' uses AI to land drones smoothly,1561124453,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c3aoww/neural_lander_uses_ai_to_land_drones_smoothly/
A comprehensive intro to PyTorch,1561111978,,neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/c38ti6/a_comprehensive_intro_to_pytorch/
Expanding Back-Propagation derivations for N-hidden layers,1561046382,"I've been working on developing my custom neural network library with matrices in C++ using this website as a way to get started (as well as some more help from Math Stack Exchange):

&amp;#x200B;

 [https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/](https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/) 

&amp;#x200B;

 [https://math.stackexchange.com/questions/3250718/mathematics-behind-neural-networks](https://math.stackexchange.com/questions/3250718/mathematics-behind-neural-networks) 

&amp;#x200B;

So far, I've worked through their theory, understanding how the derivations come about for back propagation, but I'm having trouble implementing it for N-hidden layers, whereas in the example, there's only 1 hidden layer. How can I derive a 'general' formula to calculate the back propagation algorithms for the weights and biases in an N-layered network? I tried working it out, but I can't seem to find a general pattern that ensures that the rate of change of the biases and weights are the correct size.",neuralnetworks,Jsyschan,False,/r/neuralnetworks/comments/c2xsjh/expanding_backpropagation_derivations_for_nhidden/
EVDodge: Embodied AI for High-Speed Dodging on a quadrotor using event cameras,1561037241,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c2w0ms/evdodge_embodied_ai_for_highspeed_dodging_on_a/
Improved Microsoft MT-DNN Tops GLUE Rankings,1560983529,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/c2nqzm/improved_microsoft_mtdnn_tops_glue_rankings/
Neural Network In 5 Minutes,1560965546,,neuralnetworks,MainBuilder,False,/r/neuralnetworks/comments/c2jzve/neural_network_in_5_minutes/
"group Telegram about deep learning ,machine learning , Artificial Intelligence by andrew ng",1560888273,"The Main topics that we are going to discuss here are as below:

• Data Science

• Artificial Intelligence

• Machine Learning

• Deep Learning

• Cognitive Science

• Neuroscience

Please post in English language.

Telegram group

join

[https://t.me/joinchat/CuFqkRQSxRy5M\_6KxhKrCA](https://t.me/joinchat/CuFqkRQSxRy5M_6KxhKrCA)",neuralnetworks,Doctor_who1,False,/r/neuralnetworks/comments/c273oc/group_telegram_about_deep_learning_machine/
CVPR 2019 Attracts 9K Attendees; Best Papers Announced; ImageNet Honoured 10 Years Later,1560885838,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/c26l3d/cvpr_2019_attracts_9k_attendees_best_papers/
Improved Microsoft MT-DNN Tops GLUE Rankings,1560877613,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/c24ufp/improved_microsoft_mtdnn_tops_glue_rankings/
Someone used a neural network to draw Doom Guy in high-res,1560804920,,neuralnetworks,bil-sabab,False,/r/neuralnetworks/comments/c1szgk/someone_used_a_neural_network_to_draw_doom_guy_in/
Free cloud GPU credits for deep learning,1560783630,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes – more than 12hrs for free

I am working on https://www.tensorpad.com/ – developing cloud infrastructure for machine learning.

Part of our computational capacity is idle; hence, we’re offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.

Specs: • 60GB of RAM, 4 CPUs, 1080Ti GPU • JupyterLab environment with access to the terminal • Pre-installed Tensorflow, Keras, and other ML frameworks

You can access the free credits by signing up (https://dashboard.tensorpad.com/ and redeeming ""promo450"" promo code in the Billing tab (https://dashboard.tensorpad.com/billing).

For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/c1ojgr/free_cloud_gpu_credits_for_deep_learning/
"""None of the above"" class neural network",1560772550,"I have neural net with 4 output classes, I want to add 5th output ""none of the above"" class so the network can handle unknown data. How can I define a 5th output class?",neuralnetworks,rezonancija,False,/r/neuralnetworks/comments/c1mmsj/none_of_the_above_class_neural_network/
Ian Goodfellow: Generative Adversarial Networks (GANs) | MIT Artificial Intelligence (AI) Podcast,1560762800,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/c1ldxb/ian_goodfellow_generative_adversarial_networks/
Introduction to Reinforcement Learning,1560755103,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/c1khxk/introduction_to_reinforcement_learning/
Here comes the saviour. (Link in the comments),1560748001,,neuralnetworks,InformalTotal,False,/r/neuralnetworks/comments/c1jl6a/here_comes_the_saviour_link_in_the_comments/
Face Detection with VGG,1560711056,"So after making a basic VGG neural network, I am stuck on how to train it with face detection implemented with the data.

Basically how do I apply face detection to my data so I can use it to train my neural netowrk?",neuralnetworks,Gimmbo,False,/r/neuralnetworks/comments/c1defo/face_detection_with_vgg/
Neural Network Retraining,1560702371,"What would be good to search on google/read to find out what ""retraining"" means for a neural network? Ex: Why do we need to retrain and how does it work?  Why aren't our initial weights good enough and how to we edit them to be better? 

&amp;#x200B;

Assume I have no background

&amp;#x200B;

I tried searching ""what is neural network retraining"" but the results don't seem to helpful 

[https://www.google.com/search?biw=1253&amp;bih=1238&amp;ei=Cm0GXZj7CsTt5gLzuZGoAQ&amp;q=what+is+neural+network+retraining&amp;oq=what+is+neural+network+retraining&amp;gs\_l=psy-ab.3...3030.3605..3716...0.0..0.87.496.7......0....1..gws-wiz.......0i71j35i304i39j0i13i10j0i8i7i30j33i10.aMAC4E5tLgY](https://www.google.com/search?biw=1253&amp;bih=1238&amp;ei=Cm0GXZj7CsTt5gLzuZGoAQ&amp;q=what+is+neural+network+retraining&amp;oq=what+is+neural+network+retraining&amp;gs_l=psy-ab.3...3030.3605..3716...0.0..0.87.496.7......0....1..gws-wiz.......0i71j35i304i39j0i13i10j0i8i7i30j33i10.aMAC4E5tLgY)",neuralnetworks,GodAlmighty69,False,/r/neuralnetworks/comments/c1br60/neural_network_retraining/
Learning from log files?,1560611326,"I am searching for methods / info on how to make a neural network learn from log files. I do not want to use reinforced learning. The catch is that the individual log entries are 160ms apart, and many things could happen during two entries. 

Any thoughts will be appreciated!",neuralnetworks,flyingpan777,False,/r/neuralnetworks/comments/c0y9wm/learning_from_log_files/
Peeking Inside DNNs With Information Theory,1560525063,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/c0l83r/peeking_inside_dnns_with_information_theory/
Weight Agnostic Neural Networks,1560449835,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/c09bvp/weight_agnostic_neural_networks/
Are Weights Really Important to Neural Networks?,1560442428,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/c07rvs/are_weights_really_important_to_neural_networks/
"AI, art, and autonomy: an introduction to the Abraham project",1560434599,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/c067g0/ai_art_and_autonomy_an_introduction_to_the/
BUSINESS APPLICATIONS OF CONVOLUTIONAL NEURAL NETWORKS,1560427365,,neuralnetworks,Adaly90,False,/r/neuralnetworks/comments/c04yji/business_applications_of_convolutional_neural/
An AI-Powered Domain Name Generator,1560400240,"Hey there, we are so happy and pleased to share our platform with you today:

&amp;#x200B;

DeepNamer is an AI-powered domain name generator and deep brainstorm platform that can help you find a catchy and creative domain name for your business for free. DeepNamer is built based on a deep sequence-to-sequence (i.e., keywords-to-domain) architecture, which utilizes the most recent natural language processing algorithms such as dynamic recurrent neural networks.

&amp;#x200B;

Note that we find our name DeepNamer via our AI algorithm and our platform inspired by the way startups names their businesses.

**We would be happy to share our platform (**[**DeepNamer.com**](https://DeepNamer.com)**) with you and any comment, feedback or suggestion would be appreciated.**",neuralnetworks,Refeb,False,/r/neuralnetworks/comments/c01ch6/an_aipowered_domain_name_generator/
GAN : How does D act once training is finished ?,1560354352,"Hi !

I am quite new to GANs and I don’t understand how the discriminant works once training is finished. I heard that it only helps for the generator’s training. Once generator’s parameters are determined, the discriminant would be useless. Is this right ? 

Thanks in advance !",neuralnetworks,vico-68,False,/r/neuralnetworks/comments/bzsst7/gan_how_does_d_act_once_training_is_finished/
Beyond data and model parallelism for deep neural networks,1560334708,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/bzpfuc/beyond_data_and_model_parallelism_for_deep_neural/
How to monetize a dataset on iExec (and keep ownership over it),1560331749,,neuralnetworks,jbrg,False,/r/neuralnetworks/comments/bzp154/how_to_monetize_a_dataset_on_iexec_and_keep/
Is CornerNet a replacement for YOLO?,1560273571,"I'm trying to implement CornerNet in PyTorch to test it out in a project I'm working on. I've read the paper, and it seems like CornerNet is a YOLO replacement, but I'm not entirely sure.  I'm also wondering how to create a backbone architecture with modules for CornerNet since a lot of the existing ones use json files. Thanks!",neuralnetworks,shadowclan98,False,/r/neuralnetworks/comments/bzfbke/is_cornernet_a_replacement_for_yolo/
BatchNorm + Dropout = DNN Success!,1560190815,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bz1apb/batchnorm_dropout_dnn_success/
[D] ELI5 : What's LSTMs Cell ?,1560162472,"Hey ,  in a naive can I say LSTMs cells are the processors that processes a hidden state after knowing what to forget and remember and then we fed that given hidden state from x1 with another input x2 in sequence similar for every inputs x ? 
Am I wrong ?",neuralnetworks,Dewanik-Koirala,False,/r/neuralnetworks/comments/byw77o/d_eli5_whats_lstms_cell/
Understanding Cross-entropy,1560150973,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/byut2u/understanding_crossentropy/
U-Net implementation in PyTorch for abnormality segmentation in brain MRI - data and weights,1560117197,,neuralnetworks,ketsok,False,/r/neuralnetworks/comments/bypmuy/unet_implementation_in_pytorch_for_abnormality/
How I made top 0.3% on Kaggle,1560101841,,neuralnetworks,0_marauders_0,False,/r/neuralnetworks/comments/bymreu/how_i_made_top_03_on_kaggle/
Convolutional Neural Networks Benefits. 7 Examples of Business Applications,1559996482,,neuralnetworks,LemmyChildish,False,/r/neuralnetworks/comments/by7fr4/convolutional_neural_networks_benefits_7_examples/
Chip design dramatically reduces energy needed to compute with light,1559918544,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bxveqx/chip_design_dramatically_reduces_energy_needed_to/
"A simple live stream of Neural Network ""MarI/O"" learning to play SMB.",1559918234,,neuralnetworks,MicroPanda123,False,/r/neuralnetworks/comments/bxvcp9/a_simple_live_stream_of_neural_network_mario/
Google AI ‘EfficientNets’ Improve CNN Scaling,1559914626,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bxupk9/google_ai_efficientnets_improve_cnn_scaling/
Neurons' 'antennae' are unexpectedly active in neural computation,1559914586,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bxupbv/neurons_antennae_are_unexpectedly_active_in/
Would it be possible for a neural network to balance a game,1559869623,"Would it be possible for a neural network to balance a game like ssbm or any other fighting game or games like AOE, by slowly changing the attributes of certain characters or units to be nearly perfectly balanced.",neuralnetworks,jlpt1591,False,/r/neuralnetworks/comments/bxoj88/would_it_be_possible_for_a_neural_network_to/
DeepMind AI Reaches Human-level Performance in Quake III Arena,1559852909,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bxlkaf/deepmind_ai_reaches_humanlevel_performance_in/
"DeepMind's plan to make AI systems robust, why it's a core design issue, and how to succeed in ML",1559842485,,neuralnetworks,robwiblin,False,/r/neuralnetworks/comments/bxjijp/deepminds_plan_to_make_ai_systems_robust_why_its/
"L1/L2 regularization in Keras is only applicable per layer. However, in the literature, the weight decay terms are added to the cost function of the network. If I add L1/L2 to all layers in my Network in Keras, will this be equivalent to adding the weight decay to the cost function?",1559808247,With adding weight decay to the cost function I mean for L2: minimize(modelcost + lambda*weights^2),neuralnetworks,BoerKwartel,False,/r/neuralnetworks/comments/bxe4j2/l1l2_regularization_in_keras_is_only_applicable/
Google AI ‘EfficientNets’ Improve CNN Scaling,1559751456,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bx4fo7/google_ai_efficientnets_improve_cnn_scaling/
Does addressing overfitting problems ever improve performance significantly?,1559724508,"In [this article](https://towardsdatascience.com/handling-overfitting-in-deep-learning-models-c760ee047c6e), the author discusses three common strategies for handling overfitting problems: reducing the size of a neural network, regularization, and dropouts.  In each of four cases (the original model plus the three treatments), the validation loss minimum in a 20-epoch training period is chosen, and it's clear from the graphs that in each case that should be the global validation minimum.  The final result:

&amp;#x200B;

Baseline model = 76.09% test accuracy

Reduced model = 78.48% test accuracy

Model with L2 regularization = 79.58% test accuracy

Model with dropouts = 79.92% test accuracy

&amp;#x200B;

I know this is just one example, but those improvements don't strike me as particularly impressive or large.  Are there any interesting examples of larger performance gains?  If you have already constructed a model that overfits, are you already pretty close to the best you can do with your data and architecture?",neuralnetworks,bwllc,False,/r/neuralnetworks/comments/bx0d8t/does_addressing_overfitting_problems_ever_improve/
"LSTM Network questions, and clarifications",1559706042,"Hello friends, thank for taking the time to read my questions.

I have recently begun researching LSTM networks as I have finished my GA and am looking to progress to something more difficult. 

I believe I am using the classic LSTM (if that makes any sense) and have a few questions:

1. Do I need LSTM cells in all cell location? Or would I only use them in the input and output cells and use simple feedforward for the hidden neurons?

2. How do I go about implementing Bias values into a LSTM?

3. Assuming I create a Network that predicts the next few words of a sentence, does that mean my outputs should be every possible word that the network could conceivably use?

Any and all information is helpful!",neuralnetworks,CJP-2019,False,/r/neuralnetworks/comments/bwy488/lstm_network_questions_and_clarifications/
Trying to derive the Back Propagation Algorithms,1559682865,"I'm trying to develop my own Neural Network class in C++ using this as a guide:

&amp;#x200B;

 [https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/](https://causeyourestuck.io/2017/06/12/neural-network-scratch-theory/) 

&amp;#x200B;

For the most part, it's pretty straightforward to follow up until the point of back-propagation. In some ways, I understand what the author is getting at, but I just don't understand how he came up with the derivation of the error function w.r.t. the biases and weights. Admittedly, my background in matrix algebra is not all that good, but can someone please help explain the steps he took to get the proper derivations? I hope that once I understand it a bit better, I can apply it to networks of N-hidden layers (where here in this example, he's doing it with one hidden layer).",neuralnetworks,Jsyschan,False,/r/neuralnetworks/comments/bwu6i9/trying_to_derive_the_back_propagation_algorithms/
nn_builder - a new package that removes the need for boilerplate code when building neural networks,1559655612,"nn\_builder is a new package that lets you build neural networks without the boilerplate code using PyTorch or TensorFlow 2.0 that some of you might find useful.

See below for an example of how it works

Let me know what you think and if you'd like to contribute [https://github.com/p-christ/nn\_builder](https://github.com/p-christ/nn_builder)

&amp;#x200B;

https://i.redd.it/5r62ap7udc231.png",neuralnetworks,__data_science__,False,/r/neuralnetworks/comments/bwothq/nn_builder_a_new_package_that_removes_the_need/
Artificial neural networks explained in simple words,1559653814,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/bwoio6/artificial_neural_networks_explained_in_simple/
1d line?,1559627202,"I was reading mobilenet v2 paper and came across this line, ""For example,
ReLU applied to a line in 1D space produces a ’ray’,
where as in Rn space, it generally results in a piece-wise
linear curve with n-joints."". I quite don't understand what a line in 1d looks like. Any thoughts?",neuralnetworks,suriya987,False,/r/neuralnetworks/comments/bwl6y7/1d_line/
UC Berkeley’s RL-Powered SOLAR Accelerates Robotic Learning,1559615237,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bwjflr/uc_berkeleys_rlpowered_solar_accelerates_robotic/
[1905.10307] An Explicitly Relational Neural Network Architecture,1559609827,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/bwiiri/190510307_an_explicitly_relational_neural_network/
Terminator with Stallone face,1559561996,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/bw9oi9/terminator_with_stallone_face/
Guide to Exploratory Data Analysis with Python,1559547391,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/bw7ziw/guide_to_exploratory_data_analysis_with_python/
[AI application] Let your machine play Super Mario Bros!,1559379258,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/bvitlh/ai_application_let_your_machine_play_super_mario/
Sound waves bypass visual limitations to recognize human activity,1559327077,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bvb2ko/sound_waves_bypass_visual_limitations_to/
A deep neural network that can maximize or minimize coloring to blend into or stand out against a background,1559327029,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bvb27l/a_deep_neural_network_that_can_maximize_or/
Trying to jump into NNs with a game,1559308583,"This is my first post here, so please bear with me. I wrote up a small little game in C++ for practice. I don't know what it's called, but it's one of those games where you need to fill the grid with Xs. Initially the board is filled with Xs with some 0s sprinkled around. Tapping on one of the cells in the grid will 'invert' it along with its surrounding neighbors, from 0 to X and X to 0, and you just tap them until you fill up the square. This is all in C++ in a terminal, not a GUI, so it's pretty simplistic, and it works. 

&amp;#x200B;

One issue that I see is my 'solver'. Now thus far, I've set it to pick random cells to flip, so I thought that I could possibly implement a neural network to 'learn' what the best options are (i.e., given N input neurons based on the number of cells in the grid, it takes its current state as the input while the output neurons will fire if tapping them is an acceptable choice). I thought that using a NN to 'learn' the game will help make it solve faster, but I'm not sure how to implement such a thing in software. What's a good place to get started with this? Also, as there's really no 'training set' to speak of, would a NN even be appropriate in this case?",neuralnetworks,Jsyschan,False,/r/neuralnetworks/comments/bv7edn/trying_to_jump_into_nns_with_a_game/
Free cloud GPU credits for deep learning,1559229229,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes – more than 12hrs for free

I am working on [https://www.tensorpad.com/](https://www.tensorpad.com/?utm_source=reddit&amp;utm_campaign=neuralnetworks) – developing cloud infrastructure for machine learning.

Part of our computational capacity is idle; hence, we’re offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.

Specs:
•	60GB of RAM, 4 CPUs, 1080Ti GPU
•	JupyterLab environment with access to the terminal
•	Pre-installed Tensorflow, Keras, and other ML frameworks 

You can access the free credits by signing up ([https://dashboard.tensorpad.com/](https://dashboard.tensorpad.com/signup?utm_source=reddit&amp;utm_campaign=neuralnetworks) and redeeming ""promo450"" promo code in the Billing tab ([https://dashboard.tensorpad.com/billing](https://dashboard.tensorpad.com/billing?utm_source=reddit&amp;utm_campaign=neuralnetworks)).

For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/buud08/free_cloud_gpu_credits_for_deep_learning/
How Business Can Benefit from Recurrent Neural Networks,1559224965,,neuralnetworks,LemmyChildish,False,/r/neuralnetworks/comments/butjq6/how_business_can_benefit_from_recurrent_neural/
Writing billion songs with C# and Deep Learning,1559222685,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/but4t4/writing_billion_songs_with_c_and_deep_learning/
The Rise of DataOps - Why Legacy Data Governance Is Broken In the Machine Learning Era,1559207433,"With adding a consistent version system across all of the code the art of coding moved from craft to engineering - the same thing will happen to data governance: [The Rise of DataOps (from the ashes of Data Governance)](https://towardsdatascience.com/the-rise-of-dataops-from-the-ashes-of-data-governance-da3e0c3ac2c4) (full article)

Currently, data governance teams attempt to apply manual control at various points to control the consistency and quality of the data. The introduction of [DVC](https://dvc.org/) version tracking would allow data governance and engineering teams to engineer the data together, filing bugs against data versions, applying quality control checks to the data compilers, etc.",neuralnetworks,cmstrump,False,/r/neuralnetworks/comments/buqxwp/the_rise_of_dataops_why_legacy_data_governance_is/
Beginner at Neural Networks,1559193809,"I'm currently helping out with a project that is working with CNNs and was wondering if anyone knew of any good resources to understand how the code to make neural networks work is written. I already understand the basics conceptually, but get confused when it comes down to the actual implementation.

Thanks!",neuralnetworks,shadowclan98,False,/r/neuralnetworks/comments/bupbw2/beginner_at_neural_networks/
Searching for the right class of neural network for optimization problem,1559143395,"Hey,

I'm an absolute beginner when it comes to machine learning, so i thought I'd try to solve a problem I already solved with traditional techniques using machine learning.

Simplified, the problem is about positioning rectangles so that they don't overlap and the space between them is minimal. I've already written a loss function for this. The inputs of the network are the sizes of the rectangles and the outputs are their coordinates.

```
Input -&gt; Output  
\[(1, 2), (5, 9), ..., (width(n), height(n))\] -&gt; \[(0, 0), (1, 0), ..., (x(n), y(n))\]
```

I know that with most traditional NN the input/output size is fixed - but that's not what I need. In my case, the input size is also the output size, which should be variable. What techniques could be used to achieve something like this? An encoder/decoder NN or a RNN?",neuralnetworks,NyxCode,False,/r/neuralnetworks/comments/bug029/searching_for_the_right_class_of_neural_network/
"Optical neural network at 50zJ per op? Nope, but it’s still a good idea",1559134374,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bueatc/optical_neural_network_at_50zj_per_op_nope_but/
AI Is Blurring the Definition of Artist,1559134313,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bueafc/ai_is_blurring_the_definition_of_artist/
"Teaching AI the Concept of ‘Similar, but Different’",1559134267,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/buea4b/teaching_ai_the_concept_of_similar_but_different/
Object detection algorithm,1559124539,"Hello,

I am trying to create an object detection part for my program, to detect specific objects for me in a game environment based on pixels/screenshots, and later do it in real time for me. Does anyone have any experience with such a thing or any help/guide on how to get started with it? And if so, is it even possible, to do it by just giving the algorithm only a yes/no regarding the actual picture, rather than showing them where it is and later even find its position in the picture as well? I was thinking about using a tensorflow-gpu of course, so learning would be fast.

Thanks in advance for any help",neuralnetworks,hergendy,False,/r/neuralnetworks/comments/bucrzm/object_detection_algorithm/
Document/Sequence vector generation with multihead self attention,1559119383,"I coded this model, implementing Transformer(Vaswani et.al.)'s encoder part only, with fully connected layers following that, basically its partial BERT(contextual attention from both sides of a given token), but the objective is to rank a given sequence, trained end to end. The learnt attentionised vectors can be transferred to anything ranging from sentiment analysis to clustering sequences or documents. I further aim to do heirarchical clustering of the inputs.
[Here is the git link](https://github.com/llStringll/Transformer-model-encoder)
I would like for people on this subreddit to go through it and suggest improvements
Thanks",neuralnetworks,666BlackJesus666,False,/r/neuralnetworks/comments/buc5ps/documentsequence_vector_generation_with_multihead/
Could someone please explain? Keras NN model. More info in the comments.,1559110660,,neuralnetworks,RedElectricJellyfish,False,/r/neuralnetworks/comments/bub4js/could_someone_please_explain_keras_nn_model_more/
Neural Networks and the Turing Test,1559068445,,neuralnetworks,CJP-2019,False,/r/neuralnetworks/comments/bu3nhw/neural_networks_and_the_turing_test/
[DS] On BPTT (Back Prop Through Time),1559060235,"is this is happening in BPTT algorithm ?? 
(Plain text self definition)
Calculation of derivatives/deltas are done at every time step ""t"" in reverse order and is added to already intiliazied (row x column) matrices
Of each layer connection weights for example : dwxh for wxh ( input to hidden)
And then after all the time steps the total  updated Delta / gradients matrix is used to update weights 

( Sorry , I know it's very naive but i am a begginer and I want your elaboration on this definition is this intuitions it wright ??)",neuralnetworks,Dewanik-Koirala,False,/r/neuralnetworks/comments/bu1y80/ds_on_bptt_back_prop_through_time/
"Live demo of a dataset (""NSFW"" ML trained model) shared/monetized through Ethereum",1559055347,,neuralnetworks,jbrg,False,/r/neuralnetworks/comments/bu0zmm/live_demo_of_a_dataset_nsfw_ml_trained_model/
CMU AI generates human faces from voice recordings using GANs,1559051326,"CMU has been helping US Coast Guard in recreating human faces from voice recordings since last year ([https://www.afcea.org/content/mind-blowing-promise-ai-driven-voice-profiling](https://www.afcea.org/content/mind-blowing-promise-ai-driven-voice-profiling)). Last year, CMU presented its research in World Economic Forum. This year, it has finally published its research: [https://arxiv.org/abs/1905.10604](https://arxiv.org/abs/1905.10604)",neuralnetworks,taurish,False,/r/neuralnetworks/comments/bu08w1/cmu_ai_generates_human_faces_from_voice/
X-Lab Incubator Launches to Support Promising New AI Startups,1558992235,,neuralnetworks,strangeattractors,False,/r/neuralnetworks/comments/btrkt7/xlab_incubator_launches_to_support_promising_new/
Geoffrey Hinton Leads Google Brain Representation Similarity Index Research Aiming to Understand…,1558971095,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/btnc1y/geoffrey_hinton_leads_google_brain_representation/
What is Neural Network? by Simplilearn,1558962067,,neuralnetworks,Mi5s_Congeniality,False,/r/neuralnetworks/comments/btlr7u/what_is_neural_network_by_simplilearn/
Mathematics for Artificial Intelligence Series,1558941893,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/btj23f/mathematics_for_artificial_intelligence_series/
ELI5 : Why is NLL (Negative log likelihood) used mostly for RNN ??,1558924651,"Can anyone please explain me why ? 
Or can we use other loss functions ??",neuralnetworks,Dewanik-Koirala,False,/r/neuralnetworks/comments/btgn40/eli5_why_is_nll_negative_log_likelihood_used/
Implementing K-Means Clustering From Scratch: Simply Explained,1558918596,,neuralnetworks,jimscott1232,False,/r/neuralnetworks/comments/btfo4r/implementing_kmeans_clustering_from_scratch/
Implementing K-Means Clustering From Scratch: Simply Explained,1558918007,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/btfkq1/implementing_kmeans_clustering_from_scratch/
Advanced Regression Optimization with Neural Networks RMSProp - Neural ...,1558873811,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/bt7dkc/advanced_regression_optimization_with_neural/
This video goes over a breast cancer diagnosis model that uses neural networks (implemented in python),1558870160,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/bt6ven/this_video_goes_over_a_breast_cancer_diagnosis/
What math should I know?,1558865746,"Hi guys. I am a newbie here. I had some experience creating a neural network from ground, using python with numpy, which was rather an algorithm that knows the rules of placing an element on tic-tac-toe board, rather deducing a winning tactic. This process took me to go through multiple itterations with different trying of error functions for back propagation, which didn't lead to a neural network that knows how to win in tic-tac-toe. Thus, I think I need a more solid mathematical background for such a hobby. So here is my question: Which topics of math should I be solid in order to be confident in writing neural networks?",neuralnetworks,viorelrj,False,/r/neuralnetworks/comments/bt6c32/what_math_should_i_know/
Guide to Securing Machine Learning and Software Engineering Internships,1558840747,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bt39ri/guide_to_securing_machine_learning_and_software/
Stylizer: Image to Image transformation experiments,1558800745,,neuralnetworks,suyash93,False,/r/neuralnetworks/comments/bsw832/stylizer_image_to_image_transformation_experiments/
"""what are the benefits of sports forecasting neural network?"" What is the best way to answer this kind of question ?",1558792656,"I want to build a soccer predictor neural network model for my undergraduate thesis, however i keep wondering what is (are) the most elegant way to answer that kind of question if somehow my professor asked that during my thesis defense ?

Sorry if this is stupid question",neuralnetworks,winatawelly,False,/r/neuralnetworks/comments/bsus3x/what_are_the_benefits_of_sports_forecasting/
[youtube demo] My four mini projects on object detection. Maybe this gives you some ideas :),1558783623,,neuralnetworks,Rayterex,False,/r/neuralnetworks/comments/bstgrm/youtube_demo_my_four_mini_projects_on_object/
Making a Neural Network Game a la Code Bullet,1558723002,"Hi all, 

&amp;#x200B;

Really new to the whole NN scene, but I'm a uni student that finds this incredibly interesting and I was wondering if anyone has any knowledge of how outputs from a ""Neural Network"" could be translated into movements seen in a game such as the ones code bullet uses (i.e. car driving game, snake game, etc). What softwares are used to take those NN outputs and putting them into a game? What is used to make the game? Is there some easy intro software I could play around with? Im bulldozing through a few different tutorials for ML but I still don't understand that part, and I'd love to make a project of my own of a similar style. Thanks for any help!",neuralnetworks,Popup4t4,False,/r/neuralnetworks/comments/bskibj/making_a_neural_network_game_a_la_code_bullet/
Why do we apply bias in the hidden layers?,1558708114, I am learning and have hit a challenge point. Regarding the neuron to neuron exchange within the hidden layers of a neural network: I understand the weighting of neurons but I do not understand applying bias to neurons deeper in the stack. I know it is intended to gain the Sigmoid function but do not understand the mechanics. Weighting is about how important an attribute/neuron is to your overall effort. Does anyone understand the intent of applying bias to a neuron? Do they just simply determine whether the result of the previous layer is strong enough for that neuron to fire or not?,neuralnetworks,MiyamotoKnows,False,/r/neuralnetworks/comments/bshld6/why_do_we_apply_bias_in_the_hidden_layers/
"unusual input: segment very sparse 2D pointcloud, looking for ideas",1558691375,"Dear all,   


I have a very sparse 2D pointcloud (state space is \~20m x 40m, and I have \~100 points) and 4 classes to detect.   
They (each point) also have a lot of features (\~40)  on which we tried to do simple 1D convolutions to classify them individually (it is a spectrum).  The result is a softmax points vector (and of course features before that are available)

I would like to segment/cluster them based on their spatial location and the most probable class, thus somehow incorporate these features into the clustering. (e.g closeby objects from different classes should not be clustered together). I am looking for ideas how to handle this problem.

The easiest approach would be to use a clustering algorithm, e.g. DBSCAN and pass softmax points as a feature to it, but I am afraid that the extra 4 dimensions in completely different metrics would mess it up. Also, interclass distances are not mapped properly if each class is on a different vector. 

Thank you for your input!",neuralnetworks,paland3,False,/r/neuralnetworks/comments/bsex1o/unusual_input_segment_very_sparse_2d_pointcloud/
Generate Game of Thrones Characters Using StyleGAN,1558690171,"Ever wondered how Snapchat can age you, change your gender, or add makeup to your face? One way is through a nifty Deep Learning algorithm called ""StyleGAN"". Here’s everything you need to know about it, all the code you need to implement it, and a sneak preview of what Danaerys and Jon Snow’s kid might look like. We’ve called him Djonerys.

Article link: [https://blog.nanonets.com/stylegan-got/](https://blog.nanonets.com/stylegan-got/)

&amp;#x200B;

*Processing gif m6xd709lo4031...*",neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/bserl0/generate_game_of_thrones_characters_using_stylegan/
"Is there a neural network that can generate the scariest images for humans to see? Not necessarily gore and deformed faces, but any visual patterns and images that are terrifying to look at",1558640010,"examples of images that would give me a heart attack if they popped up full screen on my computer monitor at 2 AM:

 [https://imgur.com/a/HuQgVa6](https://imgur.com/a/HuQgVa6)",neuralnetworks,InterestingFerret7,False,/r/neuralnetworks/comments/bs7831/is_there_a_neural_network_that_can_generate_the/
How do I get started with Reinforcement Learning?,1558629417,"Hi. I am a Computer Science student and am currently powering through Andrew Ng's ML course. I am interested in learning about Reinforcement Learning.

I am not just interested in learning about the current libraries and modules available for implementing RL, I am interested in learning about the underlying algorithms and mathematics as well, from a further research point of view.

I would also need some resources for the prerequisite topics in ML, Maths, Data Analytics and Programming.

My current background:

1 Sem of Programming in C and Python

1 Sem of Data Structures and Algorithms

1 Sem of Single-Variable Calculus

1 Sem of Linear Algebra

Upcoming: 1 sem of Probability and Statistics, 1 sem of Discrete Maths

How should I go about learning and practising RL?

Thanks.",neuralnetworks,pakodanomics,False,/r/neuralnetworks/comments/bs55h1/how_do_i_get_started_with_reinforcement_learning/
Evolving the topology of a neural network - Neuroevolution Neural Networ...,1558611585,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/bs1vfo/evolving_the_topology_of_a_neural_network/
A selection of Datasets for Machine learning,1558542286,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/brqtm7/a_selection_of_datasets_for_machine_learning/
Neural Network to Map Dark Matter in Space,1558477004,"Here's an article on a new network being trained with gravitational lensing data in astronomy, in order to identify dark matter:  [https://phys.org/news/2019-05-cosmogan-neural-network-dark.html](https://phys.org/news/2019-05-cosmogan-neural-network-dark.html)",neuralnetworks,FindLight2017,False,/r/neuralnetworks/comments/brgpyn/neural_network_to_map_dark_matter_in_space/
Can neural networks be used to solve sliding puzzles?,1558464783,"I want to see if neural networks are capable of learning how to solve sliding puzzles (like the 15 sliding puzzle).

Like for example, We give in the unsolved grid: 

[[2, 5, 4],
[1, 0, 3]]

(0 is the gap). Moves can be represented by switching 0 with the numbers adjacent to it, which represents the number ""sliding"" into the gap and opening  a gap in its original place. Meaning we can write out solutions as the sequence of neughboring numbers that the 0 swaps places with, in each updated grid. So In this grid above, the solution is [5, 4, 3, 5, 2, 1, 4, 5]

Solved grid
[[1,2,3],
[4,5,0]]

So if we dump in a bunch of unsolved grid, output  the predicted solution sequence, then reveal their corresponding solution sequences, is it possible to teach a neural network how to solve these puzzles?",neuralnetworks,MidThought_Pause,False,/r/neuralnetworks/comments/breah6/can_neural_networks_be_used_to_solve_sliding/
Data Wallet for Enterprises to Rent Data and AI Models and Neural Networks Using Blockchain,1558452674,,neuralnetworks,blrm,False,/r/neuralnetworks/comments/brbtg0/data_wallet_for_enterprises_to_rent_data_and_ai/
I am desperate for help.,1558449198,"I have been trying to build a 1Dconvnet with Keras but it doesn't work at all. If someone has the time to check out the stackoverflow question I posted a few days ago, it would be amazing.

I really hope I can post this sort of things here. If I am violating the community guidelines, I am sorry and I meant no harm: I am just a desperate physics undergrad.",neuralnetworks,Bulbosauron,False,/r/neuralnetworks/comments/brb4om/i_am_desperate_for_help/
"Build tools in machine learning projects, an overview",1558446657,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/bransv/build_tools_in_machine_learning_projects_an/
What do you people do while waiting for your models to train?,1558432950,,neuralnetworks,DaoLordSubha,False,/r/neuralnetworks/comments/br8ijm/what_do_you_people_do_while_waiting_for_your/
Notes on Weight Initialization for Deep Neural Networks,1558406979,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/br51iu/notes_on_weight_initialization_for_deep_neural/
Understanding WaveNet architecture,1558343060,I have just published  “Understanding WaveNet architecture”   https://link.medium.com/LopsMVC8PW  on Medium. Kindly provide feedback for the same.,neuralnetworks,HanSatyam,False,/r/neuralnetworks/comments/bqt4ze/understanding_wavenet_architecture/
Stock Market Prediction with LSTM network in Python | AI in finance,1558336641,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/bqsc3w/stock_market_prediction_with_lstm_network_in/
Machine Learning and Data Science with Kaggle,1558297759,,neuralnetworks,ailearn12,False,/r/neuralnetworks/comments/bqln4v/machine_learning_and_data_science_with_kaggle/
Wireless Network Brings Dust-Sized Brain Implants a Step Closer,1558296247,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bqlcv0/wireless_network_brings_dustsized_brain_implants/
Camera Sees Electromagnetic Interference Using an SDR and Machine Vision,1558296064,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bqlbkj/camera_sees_electromagnetic_interference_using_an/
Neural Networks: An Excellent Guide for Beginners,1558294981,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bql3y5/neural_networks_an_excellent_guide_for_beginners/
Image Recognition with Convolutional Neural Networks: Simply Explained,1558294591,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bql1a3/image_recognition_with_convolutional_neural/
Make a Face GAN (Generative Adversarial Network) in 15 Minutes,1558293990,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bqkx02/make_a_face_gan_generative_adversarial_network_in/
Interested in NLP? Learn how to implement RNNs through Algorithms and Applications in this excellent video tutorial. Good luck!,1558293467,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bqktbq/interested_in_nlp_learn_how_to_implement_rnns/
Should we normalize target data as well as input data?,1558233131,"I consider myself an intermediate practitioner of neural networks.  I've been asked to teach a few of my colleagues some of what I know.  Some of my practices may be a bit idiosyncratic, because I study and tinker on my own.  I want to teach ""best practices"" and when I'm in doubt, I'm trying to research exactly what those are.

I'm working with the [housing price data set](https://github.com/ageron/handson-ml/tree/master/datasets/housing) from the book [Hands-On Machine Learning with Scikit-Learn and TensorFlow](http://shop.oreilly.com/product/0636920052289.do).  I'm trying to predict housing prices from the other information in the data set, a pretty classic sort of student problem.  The minimum possible housing price is obviously zero, and the mean housing price in this data set is around $250,000.  I already have human-readable output from a linear regression model which reports its results in dollars.  I would like to preserve that in subsequent models, so we can also do things like compare mean squared errors between architectures.

Knowing my data, I would be inclined to do one or both of the following:

* On the output layer, which is a single node with linear activation, set the initial bias to the mean value of 250,000.
* Add an exponential activation function on the output node, to make negative prices impossible.

I did a combination of these two things, and designed some demonstration models with very few non-linear ReLU elements which outperform the linear regression after only a few dozen epochs.  That's what I wanted to demonstrate.

However, I've never seen any published examples of people doing this kind of hand-tinkering.  So I tried leaving off my customization, and repeated the training process.  After 2,000 epochs, the network was still working the outputs slowly up from zero towards the mean 250,000 value, and the training errors were still stupidly high.  I was using ADAM for gradient descent.  I thought that ADAM would have found the mean much more quickly than this because of its momentum feature.

I think I can also improve my training process by training on normalized targets.  If the output targets are near zero, and the standard deviation is near 1, the typical weight initialization schemes should find it easy to start training on actual features in the data immediately, rather than wasting epochs simply grinding towards the mean.  

If I train to a normalized output, the meaning of the internal mean-squared error in the neural model training process will no longer be comparable to the MSE from the linear regressions.  I will also need to perform the inverse of the data normalization process on the output to produce a human-readable result.  Both of those tradeoffs are acceptable to me, but I'll have to walk my students through that additional layer of complexity.

What's the ""right"" way?  Thanks for your advice.",neuralnetworks,bwllc,False,/r/neuralnetworks/comments/bqbywl/should_we_normalize_target_data_as_well_as_input/
What is a more optimal strategy to initialise weights and hyper-parameters than randomly initialising them?,1558136423,,neuralnetworks,mr-minion,False,/r/neuralnetworks/comments/bpyfmt/what_is_a_more_optimal_strategy_to_initialise/
Is training a handwriting recognition system on your own handwriting plausible?,1558079481,So I am new to nn's only having implemented a few myself on some basic data sets. I was wondering if training a nn specifically to my own handwriting would be possible and if so about how many letters would i need to draw out. Looking at a dataset from UCI they seem to have 10k characters. Could I achieve relatively accurate results with a much smaller sample size if all the letters were for the same person? I really appreciate the help. Thanks!,neuralnetworks,pandaalmond,False,/r/neuralnetworks/comments/bpodao/is_training_a_handwriting_recognition_system_on/
"I wanna use a neural network to upscale SD content to HD. I have SD uncropped and HD cropped content from the same source. Is this feasible, and how do I go about this? Very very little experience with programming",1558076247,"My basic thought process came from hearing about the smart upscaling going on in gaming in [this Computerphile video](https://youtu.be/_DPRt3AcUEY). I want to do something similar, take content from an SD source, and upscale to HD. I have a cropped HD source and uncropped SD source. I want to ""fill in the gaps"" so to speak on the HD source and restore the proper aspect ratio. SD source is DVD quality and HD is 1080p. It is animated, in case that matters. If I wanted to go about doing this, what would I need to learn or download? What might be the easiest way to go about this. Is a neural network able to do this, and is it the best way to go?",neuralnetworks,iaswob,False,/r/neuralnetworks/comments/bpnyrw/i_wanna_use_a_neural_network_to_upscale_sd/
A 2019 guide to 3D Human Pose Estimation,1558073271,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/bpnkqy/a_2019_guide_to_3d_human_pose_estimation/
Dance Dance Convolution,1558032366,,neuralnetworks,MusingEtMachina,False,/r/neuralnetworks/comments/bpgdyk/dance_dance_convolution/
Best way to make benchmark tests?,1557960787,"I am looking for a good way to experiment with neural networks to test my research.

&amp;#x200B;

Is there an easy way to set up a preexisting model that achieves high performance on benchmark tests, so that I have something good to base my research on?

&amp;#x200B;

Right now I have to test my ideas on a basic neural network running MNIST on my laptop. Getting performance gains out of this proves very little. Ideally, I would like to be able to just open a Google Colaboratory page with a predefined model that achieves competitive results on a large dataset. If I can make an improvement to such a model, it would prove that my experiments are useful.

&amp;#x200B;

I would prefer code in Pytorch, since that's what I use for research so far.",neuralnetworks,FlorianDietz,False,/r/neuralnetworks/comments/bp4ut6/best_way_to_make_benchmark_tests/
Help understanding product-of-connection-weights algorithm for interpreting predictor contributions in a neural network.,1557939212,"Hi everyone,

I’m having a hard time wrapping my head around the product-of-connection-weights method (aka ""Olden’s method"") for calculating predictor contributions in a neural network.

Consider a network with p input nodes, h hidden nodes in a single layer, and r output nodes.

This method calculates the contribution of input *i* to modelling output *j* by summing the product of input-hidden and hidden-output weights over the *h* hidden nodes.

https://i.redd.it/u7ofgut8ney21.png

Where w\_i,j is the weight from input *i* to the kth hidden node; wk,j is the weight form the kth hidden node to output *j*.

&amp;#x200B;

The authors claim this approach is superior to other methods because it preserves the sign of the relationship between the predictor and the response.

Their explanation is as follows:

“Positive effects of input variables are depicted by positive input-hidden and positive hidden-output connection weights, or negative input-hidden and negative hidden-output connection weights. Negative effects of input variables are depicted by positive input- hidden and negative hidden-output connection weights, or by negative input-hidden and positive hidden-output connection weights. Therefore, the multiplication of the two connection weight directions (positive or negative) indicates the effect that each input variable has on the output variable.”

(Olden and Jackson, 2002) 

https://www.sciencedirect.com/science/article/pii/S0304380002000649

&amp;#x200B;

The main thing I don’t understand is how the “negative input-hidden and negative hidden-output connection weights” result in a positive effect on the output. i.e., it seems like it is leaving out the transformation in the activation function, which will result in a positive number (if using the common sigmoid logistic function, which I think they did).

&amp;#x200B;

Insights appreciated! Thank you.",neuralnetworks,beastie_boo,False,/r/neuralnetworks/comments/bp0dmj/help_understanding_productofconnectionweights/
[Help] Obtaining and retaining Data for NN,1557937722,"I am currently obtaining data for my RNN I plan on creating( Long short term memory NN) I am going to use it to attempt to predict future prices on a game I play called old school runescape. I have obtained the data and created a script to get it:

I have this data for every single item for every day for the past month(1 million lines of data):

https://gyazo.com/bf9b47b293421d1f892254eb8b3bbfcc

I also have this data for every item as well (I will use this to get the future daily prices):

https://gyazo.com/ef4b352bb219ce4381e374bb91007797

My question is, should I be storing all my daily values after using it on my NN? When I get my daily prices for every item it creates a 3mb json file. Do I really need to keep old data for my NN or can i just send it through and dispose of it? This is my first NN so I trying to figure out the correct way before something horrible happens and wished I had the data. It just 3mb every single day might get a bit bunch? Maybe I am making a bigger deal over it then it should but I just want to do the correct approach",neuralnetworks,Eccentricc,False,/r/neuralnetworks/comments/bp02qx/help_obtaining_and_retaining_data_for_nn/
Modeling a neural network,1557846893,"What are some modeling conventions of neural networks, i know and understand the math and programming but i dont see how you would model it in a way that could include back propogation that makes sense. Any suggestions. Thanks",neuralnetworks,SlickLikeOwl,False,/r/neuralnetworks/comments/bok7bd/modeling_a_neural_network/
Free cloud GPU credits for deep learning,1557796038,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes – more than 12hrs for free

I am working on [https://www.tensorpad.com/](https://www.tensorpad.com/?utm_source=reddit&amp;utm_campaign=neuralnetworks) – developing cloud infrastructure for machine learning.

Part of our computational capacity is idle; hence, we’re offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.

Specs:
•	60GB of RAM, 4 CPUs, 1080Ti GPU
•	JupyterLab environment with access to the terminal
•	Pre-installed Tensorflow, Keras, and other ML frameworks 

You can access the free credits by signing up ([https://dashboard.tensorpad.com/](https://dashboard.tensorpad.com/signup?utm_source=reddit&amp;utm_campaign=neuralnetworks) and redeeming ""promo450"" promo code in the Billing tab ([https://dashboard.tensorpad.com/billing](https://dashboard.tensorpad.com/billing?utm_source=reddit&amp;utm_campaign=neuralnetworks)).

For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/boc2ei/free_cloud_gpu_credits_for_deep_learning/
Deep Learning Architecture for Building Artificial Neural Networks book discounted,1557794758,,neuralnetworks,G4M1NG,False,/r/neuralnetworks/comments/bobts4/deep_learning_architecture_for_building/
"Snapchat male to female, baby filter, snapchat new filter",1557778998,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/bo8oth/snapchat_male_to_female_baby_filter_snapchat_new/
DeepMind &amp; Google Graph Matching Network Outperforms GNN,1557760921,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bo4tzc/deepmind_google_graph_matching_network/
Feedback wanted for new type of neural network layer: self-assessment layer,1557752323,"I invented a new type of layer for neural networks: It gives the network the ability to critically assess the reliability of its own features, to enable more informed decision making.

Preliminary results: Use of this layer lead to a minor improvement on the MNIST dataset, but the improvement is too small to know for sure if it is useful.

Even if it turns out that this improvement is a fluke, there are a large number of future improvements that could make the algorithm more effective.

I only do research in my spare time, so I would like to listen to some feedback about this idea before I invest more time into it.

I explain the idea and offer a pytorch code sample [here](https://floriandietz.me/neural_networks_self_assessment/).

I would appreciate it if you had a look at this and tell me what you think.

Also, if you have any practical problems or benchmarking testcases lying around, be sure to give it a try and report if it worked. It’s just one line of code to swap a normal linear layer with a self-assessment layer, as described in the article.",neuralnetworks,FlorianDietz,False,/r/neuralnetworks/comments/bo358f/feedback_wanted_for_new_type_of_neural_network/
Mathematics for Artificial Intelligence – Calculus &amp; Optimization,1557733396,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/bo0hw2/mathematics_for_artificial_intelligence_calculus/
GitHub - yunjey/stargan: Official PyTorch Implementation of StarGAN,1557656560,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/bnnjpo/github_yunjeystargan_official_pytorch/
The Definitive Guide to Natural Language Processing,1557656537,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/bnnjme/the_definitive_guide_to_natural_language/
[AI application] Let your machine teach itself to play flappy bird!,1557562903,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/bn9t43/ai_application_let_your_machine_teach_itself_to/
'Data Renting' Trained Neural Networks - iExec,1557502400,"As you all know, data the foundation of AI. Trained models are expensive to produce because they require a lot of computing power, skillful people and large amounts of data.  


You can now rent your models in a pay-per-use fashion and decide who can have access to them through your Data Wallet.

The startup iExec is launching on May 15 its V3.0 that will address the needs of neural network data used in AI. You can also make use of other people's datasets to build your models, and mobilize scalable and on-demand computing power to train them. There is no need to own and maintain your own servers.

iExec is compatible with TensorFlow, Keras, Pytorch, or Scikit-Learn",neuralnetworks,blrm,False,/r/neuralnetworks/comments/bmzq2n/data_renting_trained_neural_networks_iexec/
Neural Networks Usage at Mobile Development,1557387801,,neuralnetworks,dashdevs,False,/r/neuralnetworks/comments/bmgud3/neural_networks_usage_at_mobile_development/
Smarter training of neural networks,1557332160,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/bm77g8/smarter_training_of_neural_networks/
"ICLR 2019 | Tsinghua, Google and ByteDance Propose Neural Networks for Inductive Learning &amp; Logic…",1557328922,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bm6jyh/iclr_2019_tsinghua_google_and_bytedance_propose/
Enlarging Labeled Handwritten Character Datasets With Capsule Networks,1557258955,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/blvcdg/enlarging_labeled_handwritten_character_datasets/
Natural language processing (NLP) for newbies,1557234601," 

[https://theappsolutions.com/blog/development/natural-language-processing/](https://theappsolutions.com/blog/development/natural-language-processing/) 

 

* [What is Natural Language Processing?](https://theappsolutions.com/blog/development/natural-language-processing/#contents_0)
* [How NLP works?](https://theappsolutions.com/blog/development/natural-language-processing/#contents_2)
* [NLP Challenges](https://theappsolutions.com/blog/development/natural-language-processing/#contents_5)
* [Why is NLP important?](https://theappsolutions.com/blog/development/natural-language-processing/#contents_6)",neuralnetworks,Adaly90,False,/r/neuralnetworks/comments/blqiom/natural_language_processing_nlp_for_newbies/
Using deepdream on Windows,1557227034,"Hey guys, 

Was wondering if anyone could direct me to a relatively upto date tutorial on using ""Deepdream"" on windows. 

Forgive me if this isn't the right sub to post this in. I'm actually a noob with Neural networks. Huehue. 

Thanks in advance!",neuralnetworks,Wally_who,False,/r/neuralnetworks/comments/blpc3f/using_deepdream_on_windows/
"LSTM isn't predicting the next number in a sequence with sufficient accuracy, so I'm going to research how mathematicians solve sequences and try to code that.",1557152708,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/blctn9/lstm_isnt_predicting_the_next_number_in_a/
Mathematics for Artificial Intelligence – Probability,1557134822,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/bl9x4k/mathematics_for_artificial_intelligence/
Grab bag of Neural Network Goodies,1557100761,,neuralnetworks,MusingEtMachina,False,/r/neuralnetworks/comments/bl4wmf/grab_bag_of_neural_network_goodies/
Loss function help,1557060085, I have used resnet50 to solve a multi-class classification problem. The model outputs probabilities for each class. Which loss function should I choose for my model. I can't choose binary cross-entropy as it is used for binary classification. I can't choose categorical cross-entropy as it is not exactly telling which class an image belongs to (1 or 0 for each) and is instead giving probabilities for each class.,neuralnetworks,_human404_,False,/r/neuralnetworks/comments/bkx9zl/loss_function_help/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1556941342,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bkh4xl/interested_in_artificial_intelligence_machine/
Good resource to learn for an amateur,1556884755,"I am working in a sort of managed services style role and it has now become clear to me that I may need to utilise some form of machine learning to interpret imagery and get data from the images.

No experience in this whatsoever, any useful resources for this sort of thing?",neuralnetworks,Scouseulster,False,/r/neuralnetworks/comments/bk7mp4/good_resource_to_learn_for_an_amateur/
"[Project] Tensorflow implementation of ""Handwritten Indic Character Recognition using Capsule Networks"" [ASPCON 2018]",1556879022,,neuralnetworks,CodeMusicFreak,False,/r/neuralnetworks/comments/bk6ukq/project_tensorflow_implementation_of_handwritten/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1556764178,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bjpsym/interested_in_artificial_intelligence_machine/
Making a neural network generated recipe!,1556739875,,neuralnetworks,davidsave45,False,/r/neuralnetworks/comments/bjlj6u/making_a_neural_network_generated_recipe/
Google Researchers Add Attention to Augment Convolutional Neural Networks,1556724310,"A group of Google researchers led by Quoc Le — the AI expert behind Google Neural Machine Translation and AutoML — have published a paper proposing attention augmentation. In experiment results, the novel two-dimensional relative self-attention mechanismfor image classification delivers “consistent improvements in image classification.”

For more information [https://medium.com/syncedreview/google-researchers-add-attention-to-augment-convolutional-neural-networks-1490e9c245e1](https://medium.com/syncedreview/google-researchers-add-attention-to-augment-convolutional-neural-networks-1490e9c245e1)",neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bjidw6/google_researchers_add_attention_to_augment/
ASL project enhancing ideas?,1556704589,"Project: Sign Language Detection in real time
 
It is too bland and I need ideas to enhance it or add some kind of wow factor to it but don't know how.So,any ideas?",neuralnetworks,_human404_,False,/r/neuralnetworks/comments/bjfb1u/asl_project_enhancing_ideas/
First timer dabbing into neural networks. Do I have the gist of it?,1556632926,"So I'm an application developer, but started playing with neural networks literally yesterday because it seemed interesting.  I THINK I'm getting the idea but wanted to make sure I'm not down the wrong rabbit hole.  So if anyone has some time to ""ELI5"" then please do!  

So my understanding is that my network is going to be made up of a bunch of ""neurons"", which for me look like this: 

    {
        this.value = 0; // Actual state of the neuron.
        this.synapses = []; // A list of ""input"" neurons used to calculate this neuron's state.
        this.synapseWeights = []; // A list of numbers.  The ""weights"" used in those connections.
    }

So the network is a series of ""layers"" (input, hidden, output) of neurons.  Each layer maps each of its neurons to all of the neurons in the previous layer.  (Once the learning is complete, ""pruning"" can be done of unnecessary synapses).

Then, when any specific neuron calculation is made, it calls for the recursive calculation of all ""inputs"" (optimized with 'dirty' flags of some sort).  This uses a sigmoid calcuation (or in my case, a tanh).  

So this is essentially the ""guts"" of my neural network.

So then for a simple ""creature"" simulation, I would have a series of ""input"" neurons that look like this: 

		nn.input.addNeuron(""FoodLevel"").value=0; // When colliding with ItemA, the object will disappear and the food level will go to a positive number.
		nn.input.addNeuron(""EnergyConsumption"").value=0;
		nn.input.addNeuron(""ItemADirectlyInFront"").value=0;
		nn.input.addNeuron(""ItemAInFront"").value=1;
		nn.input.addNeuron(""ItemAOnLeft"").value=1;
		nn.input.addNeuron(""ItemAInBack"").value=0;
		nn.input.addNeuron(""ItemAOnRight"").value=0;
		nn.input.addNeuron(""ItemAFrontMovement"").value=0;
		nn.input.addNeuron(""ItemALeftMovement"").value=1;
		nn.input.addNeuron(""ItemABackMovement"").value=0;
		nn.input.addNeuron(""ItemARightMovement"").value=0;
		nn.input.addNeuron(""ItemADistance"").value=10;

In my case, let's pretend ""ItemA"" is a particle of ""food"".  

Then my ""output"" neurons would look like this: 

		nn.output.addNeuron(""Accellerate"");
		nn.output.addNeuron(""TurnLeft"");
		nn.output.addNeuron(""TurnRight"");
		nn.output.addNeuron(""Decelerate"");

With a hidden layer or two with, say, 10 neurons each.  

Now, if I'm understanding this correctly, then simple randomly generated weights on the synapses, with each ""generation"" that manages to reduce the gap between itself and the food (then stop moving once it's 'eaten') being the ""winner"", then mutating ONE synapse for each new 'species', then over thousands of generations it should ""learn"" to orient itself towards the food, move forward, and stop moving, correct?  

Let me know if I'm on the right track.  If I'm WAY off, I'd prefer to know before I start dumping the time into this code.",neuralnetworks,Javin007,False,/r/neuralnetworks/comments/bj3ny6/first_timer_dabbing_into_neural_networks_do_i/
Evidence that recurrent circuits are critical to the ventral stream’s execution of core object...,1556627627,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bj2rhr/evidence_that_recurrent_circuits_are_critical_to/
What is a Neural Network? | How Deep Neural Networks Work | Neural Network Tutorial | Simplilearn,1556611523,,neuralnetworks,ritesh1928,False,/r/neuralnetworks/comments/bj0rfh/what_is_a_neural_network_how_deep_neural_networks/
Beginner's Deep Dive Into Neural Networks,1556602088,,neuralnetworks,MusingEtMachina,False,/r/neuralnetworks/comments/bizn3b/beginners_deep_dive_into_neural_networks/
Neural Networks problems 🐕,1556578874,,neuralnetworks,sirenslay,False,/r/neuralnetworks/comments/biw0v9/neural_networks_problems/
Free cloud GPU credits for deep learning,1556551528,"Hi, I am offering free credits for 1080Ti GPU instances for deep learning purposes.

I am working on [https://www.tensorpad.com/](https://www.tensorpad.com/?utm_source=reddit&amp;utm_campaign=neuralnetworks) – developing cloud infrastructure for machine learning.

Part of our computational capacity is idle; hence, we’re offering credits at a free and discounted rate, so that data scientists can benefit from the resources available, and work on neural networks.

Specs:
•	16GB of RAM, 4 CPUs, 1080Ti GPU
•	JupyterLab environment with access to the terminal
•	Pre-installed Tensorflow, Keras, and other ML frameworks (software versions https://docs.tensorpad.com/jobs_env/)

You can access the free credits by signing up ([https://dashboard.tensorpad.com/](https://dashboard.tensorpad.com/signup?utm_source=reddit&amp;utm_campaign=neuralnetworks) and redeeming ""REDDIT200"" promo code in the Billing tab ([https://dashboard.tensorpad.com/billing](https://dashboard.tensorpad.com/billing?utm_source=reddit&amp;utm_campaign=neuralnetworks)).

For any questions, please contact us here, through support@tensorpad.com, or the Intercom on the site.",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/biqqvo/free_cloud_gpu_credits_for_deep_learning/
OpenAI Sparse Transformer Improves Predictable Sequence Length by 30x,1556550471,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/biqjfy/openai_sparse_transformer_improves_predictable/
Could artificial intelligence be the great equaliser between intelligent and less intelligent people?,1556548636,"Making everyone superhuman intelligent. 

 It could do what engines did for the strong and less strong among humans, making everyone superhuman strong when needed.",neuralnetworks,holandaso,False,/r/neuralnetworks/comments/biq78w/could_artificial_intelligence_be_the_great/
"Is it possible to train a good neural network for classification without ""NO"" data?",1556541910,"I am working on a project that involves predicting a YES or NO output, but my entire dataset consists only of ""YES"" entries. I'm new to neural networks, but my hunch tells me this makes things difficult. How difficult, if possible, is it to code a NN that can predict the output well, given the circumstances?",neuralnetworks,StarloManfield,False,/r/neuralnetworks/comments/bip27i/is_it_possible_to_train_a_good_neural_network_for/
multiple output regression Neural Network,1556511688,"I want to build a model for predicting the result of football (soccer) game. Typically, this is a classification problem (for example : Team A Win, or Team B Win, or Draw), but I want to try to build a regression model for this problem, so the output would be the final score (example: 2-0, or 1-0, etc). This model would take multiple input, like each player rating and so on.

Input example :

\[0.7 , 0.6 , 0.2 ....\] Player of Team A rating

\[0.5 , 0.8 , 0.3 ....\] Player of Team B rating

and so on

&amp;#x200B;

Expected output :

\[2 - 0\]

&amp;#x200B;

What kind of neural network type that will work best with this kind of problem? I'm thinking of multi variate 1D CNN since its good at detecting input features (cmiiw), do you think CNN would do the job ?",neuralnetworks,winatawelly,False,/r/neuralnetworks/comments/bikobg/multiple_output_regression_neural_network/
Question on bias node,1556492785,"Hi,

when implement a simple MLP with 1 input layer 1 hidden layer and 1 output layer, 

is there much of a difference if a bias node is added to the hidden layer instead of the input layer?",neuralnetworks,hakattack97,False,/r/neuralnetworks/comments/bihai6/question_on_bias_node/
Neural Networks and Deep Learning: A Textbook hardcover is 34% off,1556469559,,neuralnetworks,Bessithomfor45,False,/r/neuralnetworks/comments/bid4ih/neural_networks_and_deep_learning_a_textbook/
How Convolutional Neural Networks Work: An Intuitive Approach,1556404276,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bi4ged/how_convolutional_neural_networks_work_an/
Simply Explained: How Neural Networks Work,1556403670,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bi4cth/simply_explained_how_neural_networks_work/
How Recurrent Neural Networks Work: An Application and Algorithm-based Approach,1556403345,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bi4aw9/how_recurrent_neural_networks_work_an_application/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1556332805,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bhuo1b/interested_in_artificial_intelligence_machine/
How do I Make A Neural Network That has A Range Of 3+ Outputs?,1556309273,"So far, I've been practicing with neural networks that give only binary outputs, (1, 0) using the sigmoid. But know I'm curious on how to make neural networks that has a range of 3 or more ouputs, such as categorizing inputs are red, blue, or yellow. 

Additionally, I'd like to find which activation function I would use for predicting the next number in a series. For example, 
1, 11, 13, 17, 19, ? (the pattern has all the prime numbers that has 1 as a digit, answer is 31).",neuralnetworks,relatrue,False,/r/neuralnetworks/comments/bhqtnd/how_do_i_make_a_neural_network_that_has_a_range/
Why exactly do we need the sigmoid's derivative?,1556165152,"I've started on making neural networks for the first time, and I'm still curious about this one portion of the process where my mind went blank. It was the moment where we multiply the residual, (which is the sigmoid of w1*x1 + b minus the actual value squared), and the derivative of the sigmoid.

I know that the squared residual provides that parabola for our gradient descent. And I know the function of the sigmoid by itself is to usually output numbers extremely close to the edges of 0 and 1. So what does the derivative of the sigmoid do? It makes a bell curve, but what is the purpose? How does that connect back with that the squared residual parabola for gradient descent? Is multiplying it supposed to be how we push ourselves down along the curve to 0?",neuralnetworks,relatrue,False,/r/neuralnetworks/comments/bh49ng/why_exactly_do_we_need_the_sigmoids_derivative/
Robustness against images in the wild,1556149208,"I have a binary classifier which for example classifies faces as adults. The system in the wild is set up like this:

image -&gt; face detector -&gt; adult classifier -&gt; output

The problem is the 'face detector' is not 100% perfect, and sometimes it feeds images without a face into the classifier. 

The hope would be that the 'adult classifier' outputs ""non-adult"" as it's not even a person, however since the classifier was only trained on Adult faces (positive) and Child faces (negative), anything could happen.

Is it then recommended that we should train the network using child faces and random images (cars, animals etc...) as negative examples? So as to be more complete?",neuralnetworks,bladerunner2054,False,/r/neuralnetworks/comments/bh1olv/robustness_against_images_in_the_wild/
"Is this a correct understanding of LSTM: 3 gates using Gradient Descent make up 1 LSTM cell using Gradient Descent, which is then part of a neural network using Gradient Descent?",1556113815,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/bguulx/is_this_a_correct_understanding_of_lstm_3_gates/
Data Labeling Services - Precise BPO Solution,1556084058,,neuralnetworks,precisebpo,False,/r/neuralnetworks/comments/bgqvtu/data_labeling_services_precise_bpo_solution/
Respected AI pioneer and visionary Nils John Nilsson passed away early this morning,1556041110,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/bgj9zu/respected_ai_pioneer_and_visionary_nils_john/
"I'm trying to determine the next number in a sequence. All I know thus far is Gradient Descent, and it's inaccurate. Anyone know a better method?",1556030453,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/bgh5mo/im_trying_to_determine_the_next_number_in_a/
[D] Properties of Wavelet captured by WaveNet,1555994293,"I am read that WaveNet uses ideas of Wavelet and Neural Network, where Wavelet basis coefficient is equivalent to weights in Neural network. I have found, WaveNet uses  Atrous (Convolution with holes) property similar to Wavelet transform property, what others properties of Wavelet are captured by WaveNet (global or local features).  What application does these properties have for real  data?",neuralnetworks,HanSatyam,False,/r/neuralnetworks/comments/bgc52g/d_properties_of_wavelet_captured_by_wavenet/
"Stupid question, please help",1555975705,"Hello everyone, I wonder if it's possible to collect data from a program, write all data into a texture, use GPU shaders to write a simple neural network process the texture and then use the program to read the texture back, after all a texture is a matrix of values",neuralnetworks,Amoxidal500,False,/r/neuralnetworks/comments/bg90ji/stupid_question_please_help/
"I've completed my second neural network, but it's still very simple. Can anyone recommend a good next challenge?",1555941108,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/bg2c9r/ive_completed_my_second_neural_network_but_its/
Ultimate Guide to TensorFlow 2.0 in Python,1555917359,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/bfz5p4/ultimate_guide_to_tensorflow_20_in_python/
Recurrent neural networks to generate EDM music. Please help me evaluate!,1555877298,"This is for my final year Computer Science project. I intend to upload all my code and research after submission in May! Thank you for your help.

Survey link, only takes 2 mins: https://www.surveygizmo.com/s3/4968707/Generated-Music-Evaluation",neuralnetworks,MaxCb,False,/r/neuralnetworks/comments/bft0dc/recurrent_neural_networks_to_generate_edm_music/
Help for debugging a Neural Network to learn the Fashion-MNIST dataset,1555828513,"Hi Guys. 
I am trying to build a Multi-classification neural network from scratch to learn the Fashion-MNIST dataset, which I got from Kaggle. 
Here is a link to the dataset[https://www.kaggle.com/zalando-research/fashionmnist]. 

Here is a link to my code[https://gist.github.com/Sachinkumar04428/9aa44808f3e05ca05af57fb1330127f8]. 

I have checked the code many times, but I am unable to find the reason of why the cost function is increasing with number of iterations. Could someone help me debug it. Thanks in advance.",neuralnetworks,sk4428,False,/r/neuralnetworks/comments/bfm2o9/help_for_debugging_a_neural_network_to_learn_the/
[D] Local Conditioning on WaveNet for Voice Conversion,1555822979,"I have used WaveNet only as global Conditioning and found that generated voices are noisy. Very few generated audio clips are human-like sounding but are muffled. The possible reason that I haven't used local Conditioning. In Voice Conversion, we should pass Acoustic features and F0 simultaneously as local Conditioning. 

Is it the source or Target features passed as local Conditioning? 

How does WaveNet ensures sequencing for acoustic features?",neuralnetworks,HanSatyam,False,/r/neuralnetworks/comments/bflfgo/d_local_conditioning_on_wavenet_for_voice/
This video goes over a breast cancer diagnosis model that uses neural networks. Really interesting,1555792589,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/bfgsbz/this_video_goes_over_a_breast_cancer_diagnosis/
Editor of Neural Networks in Web AIEditor | Introducction - Behind the ...,1555777218,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/bfdxjg/editor_of_neural_networks_in_web_aieditor/
Artificial intelligence singles out neurons faster than a human can,1555703648,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bf3iox/artificial_intelligence_singles_out_neurons/
"Artificial intelligence speeds efforts to develop clean, virtually limitless fusion energy",1555703303,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bf3gc0/artificial_intelligence_speeds_efforts_to_develop/
Rotational Unit of Memory: A Novel Representation Unit for RNNs with Scalable Applications,1555703249,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bf3fza/rotational_unit_of_memory_a_novel_representation/
'Deep learning' casts wide net for novel 2D materials,1555703193,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bf3fm6/deep_learning_casts_wide_net_for_novel_2d/
Implicit Generation and Generalization Methods for Energy-Based Models,1555702727,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bf3ce1/implicit_generation_and_generalization_methods/
Facebook Randomly Wired Neural Networks Outperform Humans for Image Recognition,1555686975,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/bf09v2/facebook_randomly_wired_neural_networks/
"I just learned about IBM's Q, and it is now my goal to someday run a neural network with Qiskit.",1555682137,,neuralnetworks,Agent_ANAKIN,False,/r/neuralnetworks/comments/bezdh2/i_just_learned_about_ibms_q_and_it_is_now_my_goal/
Is neural network basically just a function?,1555644818,"I recently learned about how basic neural network works and I came to the conclusion that neural network is basically a function that readjusts itself based on its cost function until it well adjusted to solve the problem in hand. I dont think my understanding is correct, is it correct? Or am I missing something?",neuralnetworks,DANISH_COOKIES,False,/r/neuralnetworks/comments/beunrn/is_neural_network_basically_just_a_function/
Code explanation regarding neural network weights,1555640697,"Hi,

I understand genetic algorithms, but I am new when it comes to NN, and trying to learn. I came across this code:

http://code.activestate.com/recipes/578241-genetic-algorithm-neural-network-in-python-source-/

I was wondering if someone could explain to me the function “assignweights” I am not sure what is going on there.

Thank you",neuralnetworks,thisishowiletyoudown,False,/r/neuralnetworks/comments/beu1ic/code_explanation_regarding_neural_network_weights/
Ian Goodfellow: Generative Adversarial Networks (GANs) | MIT Artificial Intelligence (AI) Podcast,1555640189,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/betyqo/ian_goodfellow_generative_adversarial_networks/
Helping a total newbie learn and run SC-FEGAN,1555637173,"I'm sorry if this is the wrong place to ask but I've Googled my issues and I'm at a loss on helpful resources.

I'm not familiar with neural networks but would like to try [SC-FEGAN](https://github.com/JoYoungjoo/SC-FEGAN) out. I have the files, the model, and Python downloaded and I'm stuck on getting this working:

    mv /${HOME}/SC-FEGAN.ckpt.* /${HOME}/ckpt/
    python3 demo.py

Should I be using command prompt or Python to launch the GUI? How do I use the other dependencies listed (tensorflow, numpy, Python3, PyQt5, opencv-python, pyyaml)?

ELI5, any advice would be appreciated, thank you for your time :)",neuralnetworks,sytrix,False,/r/neuralnetworks/comments/bethwy/helping_a_total_newbie_learn_and_run_scfegan/
Suggestions for neural style transfer?,1555634793,I want to do my master's project based on style transfer. My idea was to implement the popular arbitrary style transfer paper by google and use it on the backend for an android application. I am not sure if this is good enough. They already have their open source models on github. Can you please give me some suggestions? maybe optimize the existing networks or try something else?,neuralnetworks,reddit_username_0,False,/r/neuralnetworks/comments/bet3uj/suggestions_for_neural_style_transfer/
A neural network can read scientific papers and render a plain-English summary,1555597771,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bem7k0/a_neural_network_can_read_scientific_papers_and/
What is wrong with my neural network implementation,1555577308,[removed],neuralnetworks,fa223,False,/r/neuralnetworks/comments/bej884/what_is_wrong_with_my_neural_network/
"Looking to join a community of Machine Learning Students and Developers passionate about AI, Computer Vision, Deep Learning, and Natural Language Processing? Join the DiscoverAI Slack Community here.",1555441152,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bdxqcd/looking_to_join_a_community_of_machine_learning/
Version 12 Launches Today! (And It’s a Big Jump for Wolfram Language and Mathematica),1555432239,,neuralnetworks,CuttingWithScissors,False,/r/neuralnetworks/comments/bdvzj0/version_12_launches_today_and_its_a_big_jump_for/
Abstract painting generated by DCGAN,1555429236,,neuralnetworks,kooshiar,False,/r/neuralnetworks/comments/bdvedp/abstract_painting_generated_by_dcgan/
"Interested in Artificial Intelligence, Machine Learning, Computer Vision, or NLP? Check out this channel for excellent, well-explained, video tutorials.",1555368154,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bdmcvu/interested_in_artificial_intelligence_machine/
Identifying dog breed with neural networks: from Keras to Android app,1555331343,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/bdfe14/identifying_dog_breed_with_neural_networks_from/
Recurrent Neural Networks: Algorithms and Applications,1555313216,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bdd7ok/recurrent_neural_networks_algorithms_and/
How Neural Networks Work: Simply Explained,1555296768,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bdb4x5/how_neural_networks_work_simply_explained/
Developing a Robust Face Generative Adversarial Network with Tensorflow,1555295245,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bdauxb/developing_a_robust_face_generative_adversarial/
Could we glitch a RNN by providing self referring input?,1555285201,"Dear colleagues, I was considering general cases for edge case inputs.

It happens, for example, when images contain other images within (e.g. detecting a cow in a shirt), because raw pixels encode objects and representations of objects in the same manner. Roughly, one could make a parallel with numbers and Godel numbers. 

Would it possible be to tailor inputs to cause simmilar glitches with input that produces weird behavior when 'carried' through the network? 
E.g. string containing information about itself bugging a RNN. 

I imagined some cases, but I'm afraid formalizing this properly is beyond my capabilities.

Could someone help me or point towards some literature? 
Best regards.",neuralnetworks,ArgoloF,False,/r/neuralnetworks/comments/bd9858/could_we_glitch_a_rnn_by_providing_self_referring/
How Convolutional Neural Networks Work: An Intuitive Approach,1555273455,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/bd74nk/how_convolutional_neural_networks_work_an/
Training Recurrent Neural Networks,1555249601,"Read our my learning on RNN at [https://prakhartechviz.blogspot.com/2019/04/training-recurrent-neural-networks.html](https://prakhartechviz.blogspot.com/2019/04/training-recurrent-neural-networks.html)

Share your thoughts - Thanks",neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/bd2w79/training_recurrent_neural_networks/
"Please explain this ""simple"" math concept to me from Michael Nelson's book.",1555105215,"Hi, This is the book i'm referring to:  [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)

I am a freshman (computer science) in college (currently on calculus 2) and I really don't understand the following (retrieved from chapter 2):

&amp;#x200B;

I would really love if you could help me out here, Thank you in advance!",neuralnetworks,caesarsalad44,False,/r/neuralnetworks/comments/bcj7gn/please_explain_this_simple_math_concept_to_me/
Butterfly neural network,1555093961,"I am a beginner trying to implement a 4 node 3 layer butterfly network as a neural network using keras functional API. I generated a sample database using make_regression (sklearn) and tried to compare it's performance with a normal dense neural network (3 layer 4 nodes each. One output neuron), using keras sequential API. Now the value of the loss function  decreases steadily for both, more for the sequential one, which is expected. However what I find bewildering is that the butterfly network is taking more time Than the dense network when according to my intuition should take a bit lesser time as there are less number of computations involved while backpropogating. I tried different optimisers and loss functions. Can someone help me figure out what might be wrong ?? Or is the intuition wrong in itself??
Activation functions tried - relu, tanh, linear. I went through the code 2-3 times and the connection between neurons look correct.",neuralnetworks,sammyak97,False,/r/neuralnetworks/comments/bch2g6/butterfly_neural_network/
ai is creating images from decriptions.,1555081120,"have you heard of ai creating images from decriptions?

[https://paperswithcode.com/task/text-to-image-generation](https://paperswithcode.com/task/text-to-image-generation)

&amp;#x200B;

&amp;#x200B;

Neural network can create high-res images based on a text description

i was wondering if this would allow somebody

to create images of humanoids via a description?",neuralnetworks,loopy_fun,False,/r/neuralnetworks/comments/bceiy3/ai_is_creating_images_from_decriptions/
Solution Space question,1555030939,Anybody know how big the solution spaces are for Chess and Go? Or a link?,neuralnetworks,godel32,False,/r/neuralnetworks/comments/bc7mlj/solution_space_question/
Neural networks that decide on the number of hidden layers/nodes?,1555029983,"I'm trying to learn about machine learning, but from what I've learned about neural networks, the limitation seems to be that the force guiding the ""understanding"" of the input data is not a product of the data itself but a product of an external correcting force, whereas humans can (seemingly) understand concepts, recognize objects, etc. with or without external feedback.

For example, you might not know what this is, but you know it's something:

&amp;#x200B;

[A widget?](https://i.redd.it/g7auxszhcqr21.jpg)

I don't know what it is either - it was a result when I googled ""Ikea hardware.""  The point is that my brain is able to organize the sensory data of the image to form an idea of a thing, even if I'm not able to draw conclusions about exactly what it's used for.

In terms of neural networks, maybe an approach to imitate this functionality of our minds is to have a network that is able to intelligently determine how many hidden layers/nodes it needs.  This could be done with a brute force approach where multiple networks are created and trained and the best network is chosen (or maybe the most efficient if the marginal value of additional nodes diminishes after a certain point), but I'm wondering if there's anything out there that provides a better solution than that.",neuralnetworks,JHogg11,False,/r/neuralnetworks/comments/bc7h3z/neural_networks_that_decide_on_the_number_of/
Photos from Crude Sketches: NVIDIA's GauGAN Explained Visually,1555020195,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/bc5tid/photos_from_crude_sketches_nvidias_gaugan/
[1904.04036] Nucleus Neural Network for Super Robust Learning,1554929642,,neuralnetworks,superaromatic,False,/r/neuralnetworks/comments/bbr8cn/190404036_nucleus_neural_network_for_super_robust/
Cheap 1-time data labeling service?,1554929025,"I'm a university student doing a research project involving Twitter sentiment analysis (into Democrat/Republican) with no current funding (so everything is out of pocket). I wanted to find a crowdsourced site that will label \~16,000 Tweets with minimal cost. Any ideas?",neuralnetworks,ENM185,False,/r/neuralnetworks/comments/bbr45s/cheap_1time_data_labeling_service/
[1804.03313] Cortex Neural Network: learning with Neural Network groups,1554914447,,neuralnetworks,superaromatic,False,/r/neuralnetworks/comments/bbo6wb/180403313_cortex_neural_network_learning_with/
Learn Machine Learning from Top University,1554899332,,neuralnetworks,jhncna,False,/r/neuralnetworks/comments/bblfbj/learn_machine_learning_from_top_university/
Neural Network animation upscale - Dexter's Laboratory,1554852256,,neuralnetworks,xanax101010,False,/r/neuralnetworks/comments/bbex87/neural_network_animation_upscale_dexters/
Funny Intro for NN Noobs,1554835347,,neuralnetworks,thirdMew,False,/r/neuralnetworks/comments/bbbmm2/funny_intro_for_nn_noobs/
Intel and AMD going BEYOND Moore's Law,1554824706,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bb9h45/intel_and_amd_going_beyond_moores_law/
Everything you need to know about Automating Visual Inspection with AI,1554823203,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/bb96mm/everything_you_need_to_know_about_automating/
New technique cuts AI training time by more than 60 percent,1554820093,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bb8l8q/new_technique_cuts_ai_training_time_by_more_than/
Machine learning drives self-driving approach from Wayve,1554817816,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/bb866u/machine_learning_drives_selfdriving_approach_from/
Free cloud GPU credits for deep learning,1554774943,"Hi, I am offering free credits for you to access 1080Ti GPU instances for deep learning purposes.

I am a co-founder of Tensorpad; where we are creating a service for AI startups to train neural networks. We have paid traffic, but some servers are idle. Hence, we are offering some credits for free, so that students, startups, and others can benefit from ML technologies and help us by using our product and providing honest feedback for us to improve.

You can Sign Up at https://dashboard.tensorpad.com/signup and redeem the code ""REDDIT200"" in the Billing tab. (https://dashboard.tensorpad.com/billing)

Hope this explains our story and motivation for providing free credits.

Here is additional information: * The instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run multiple instances in parallel * You get access to the JupyterLab environment * We have pre-installed Tensorflow, Keras, and other ML frameworks * You can access the terminal through the JupyterLab * By default, persistent storage is enabled * TensorBoard comes pre-installed

Here are the available software versions https://docs.tensorpad.com/jobs_env/ For extra free trial hours, use promo code: reddit200

And for questions, please contact me at [ilie@tensorpad.com](mailto:ilie@tensorpad.com). I am looking forward to seeing you on our platform! Sincerely, Ilie Diacov Co-founder and UX researcher at Tensorpad",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/bb28ac/free_cloud_gpu_credits_for_deep_learning/
Is this tutorial just making a NN with 2 layers? (An input and output layer.),1554739097,"https://medium.com/@o.kroeger/tensorflow-mnist-and-your-own-handwritten-digits-4d1cd32bbab4

I am trying to learn about NN and when looking at this tutorial its seems as if this person made a NN that has just 2 layers and is therefore not a deep NN. Is the case?",neuralnetworks,whosthisdude123,False,/r/neuralnetworks/comments/bavijr/is_this_tutorial_just_making_a_nn_with_2_layers/
Machine Learning with JavaScript and TensorFlow.js Series,1554709289,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/baqx5v/machine_learning_with_javascript_and_tensorflowjs/
Question on Pre-processing for CNNs,1554588872,"I was curious if anyone could enlighten me as to why in this example of a binary CNN they don't pre-process their images consistently between training and running classifications on a test image.

When loading the training and test data they divide all the RGB values by 255 (in order to normalize them to decimals between 0-1) they then train it with this data. However when they load a single 'test\_image' at the end to classify it, they don't divide by 255, and the values in the tensor are fed to the classifier as RGB values 0-255.

I'd have assumed they should be pre-processing the input data the same as what they trained it with, or am I missing something?

[https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8](https://becominghuman.ai/building-an-image-classifier-using-deep-learning-in-python-totally-from-a-beginners-perspective-be8dbaf22dd8)",neuralnetworks,bladerunner2054,False,/r/neuralnetworks/comments/ba9mah/question_on_preprocessing_for_cnns/
I need help with the back propagation function of my neural network,1554398951,"I have read some tutorials, and I ended up making this neural network class:

    class NeuralNetwork:
        def __init__(self, x, y):
            self.input = x
            self.weights1 = np.random.rand(2,4) 
            self.weights2 = np.random.rand(4,1)                 
            self.y = y
            self.output = np.zeros(self.y.shape)
    
        def feedforward(self):
            self.layer1 = sigmoid(np.dot(self.input, self.weights1))
            self.output = sigmoid(np.dot(self.layer1, self.weights2))
    
        def backprop(self):
    
            adjust_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))
            adjust_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))
    
         
            self.weights1 += adjust_weights1
            self.weights2 += adjust_weights2

This has 3 layers (one hidden layer). I have managed to add another layer by making another 4x4 matrix of weights, and implementing the new layer to the feed forward function. Here is that code:

    class NeuralNetwork:
        def __init__(self, x, y):
            self.input = x
            self.weights1 = np.random.rand(2,4) 
            self.weights2 = np.random.rand(4,4) 
            self.weights3 = np.random.rand(4,1)                 
            self.y = y
            self.output = np.zeros(self.y.shape)
    
        def feedforward(self):
            self.layer1 = sigmoid(np.dot(self.input, self.weights1))
            self.layer2 = sigmoid(np.dot(self.layer1, self.weights2))
            self.output = sigmoid(np.dot(self.layer2, self.weights3))
       
        def backprop(self):
    
            adjust_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))
            adjust_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))
    
         
            self.weights1 += adjust_weights1
            self.weights2 += adjust_weights2

My problem is with the back propagation function, how do I calculate the adjustments for all 3 layers? ",neuralnetworks,Thedupdup,False,/r/neuralnetworks/comments/b9fpd6/i_need_help_with_the_back_propagation_function_of/
Can machine learning uncover Wikipedia’s missing “citation needed” tags?,1554392966,,neuralnetworks,benjaminikuta,False,/r/neuralnetworks/comments/b9ef7p/can_machine_learning_uncover_wikipedias_missing/
3D Advanced Neural Network Simulation - Computer vision - Digit Recognit...,1554308089,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/b8zztf/3d_advanced_neural_network_simulation_computer/
Jetson Nano Review,1554300899,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b8yhu7/jetson_nano_review/
Neural Algorithms and Computing Beyond Moore's Law,1554250414,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/b8r0zl/neural_algorithms_and_computing_beyond_moores_law/
convNets and image resizing,1554232131,"Hi Reddit!

&amp;#x200B;

how do you deal with convNets and changing activiations due to image resizing? 

(My current solution is to save an image at different (square) resolution and use all of them as training input. During inference i average above them as well.

&amp;#x200B;

Is this only an issue for me or can others confirm?",neuralnetworks,shsshs1,False,/r/neuralnetworks/comments/b8nhb0/convnets_and_image_resizing/
How Does Spotify Know You So Well?,1554103418,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b7yxim/how_does_spotify_know_you_so_well/
Drawing with Voice – Speech Recognition with TensorFlow.js,1554103390,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b7yxdk/drawing_with_voice_speech_recognition_with/
Learn Machine Learning from top university,1553948002,,neuralnetworks,jhncna,False,/r/neuralnetworks/comments/b7alj9/learn_machine_learning_from_top_university/
GauGAN Turns Doodles into Realistic Landscapes,1553807822,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/b6ogow/gaugan_turns_doodles_into_realistic_landscapes/
Using Modified Inception V3 CNN for Video Processing and Video Classification,1553786829,,neuralnetworks,RyanTmthn,False,/r/neuralnetworks/comments/b6k5vy/using_modified_inception_v3_cnn_for_video/
5 Essential Neural Network Algorithms,1553714241,,neuralnetworks,OpenDataSciCon,False,/r/neuralnetworks/comments/b68esq/5_essential_neural_network_algorithms/
How To Make Deep Learning Models That Don’t Suck,1553700724,,neuralnetworks,manneshiva,False,/r/neuralnetworks/comments/b65pe2/how_to_make_deep_learning_models_that_dont_suck/
New Method Applies Monte Carlo Neural Fictitious Self-Play to Texas Hold’em,1553699574,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/b65ja6/new_method_applies_monte_carlo_neural_fictitious/
"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educational. Do check it out",1553685196,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/b636dl/this_video_goes_over_a_model_that_predicts_the/
"Neuroplasticity, AGI",1553598204,"How can artificial neural networks attain neuroplasticity similar to real life neural networks, so that Artificial General Intelligence can be approached? It is known the limitations of current neural networks: they are excellent to train a subject, in a &lt;specific&gt; domain. But if you try to use a neural network that was trained to ride a bicycle in a slightly different domain, like riding a motorcycle... it's difficult. Whereas in real life neural networks what one learns in one domain can be transferred overtime to different domains. Intuitively I think that what makes real life neural networks capable of interaction and collaboration with between different domains is that they are subdivided according to some type of criteria, so that parts of a neural network can be ""connected"" to other subnetworks &lt;if&gt; they are relevant to what those other subnetworks are trying to do.

This subdivision of neural networks needs to be actively pursued. Locally, a neutral network tasked with learning to ride a bicycle has &lt;no&gt; benefit in trying to subdivide itself in small subnetworks, but this is something that needs to be enforced because it will be useful in the long term, when the learned abilities need to be used in a  different domain. I think that in the long term, subdivision of neural networks can lead to greater efficiency and effectivity of the overall network.

The challenge is this: what type of mechanisms or criteria can be taken into account in order to preemptively subdivide a neural network, so that it's different subnetworks can be reused as inputs of other neural networks? The concepts of Active Inference and Free Energy may be helpful [(Karl Friston)](https://www.google.com/amp/s/www.wired.com/story/karl-friston-free-energy-principle-artificial-intelligence/amp). Also I think that the visual structure of neural networks has not been analyzed enough. Visually, it's clear to me that they appear to be ""tangled hierarchical"" pathways, that go from the abstract (leafs/inputs) to the abstract (tree trunk/output).",neuralnetworks,redwins,False,/r/neuralnetworks/comments/b5oavm/neuroplasticity_agi/
[help] Preference-based word generator,1553560376,"Can someone provide a high-level description of what this might look like...

1. Given a corpus of words where each word is described by like, neutral, dislike
2. Predict how an unknown word would be described
3. If correct, add that prediction to the corpus

In general terms what should I be looking at here?",neuralnetworks,Tyrion_Hamster,False,/r/neuralnetworks/comments/b5ivev/help_preferencebased_word_generator/
Review: SRCNN (Super Resolution),1553504597,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b587nq/review_srcnn_super_resolution/
Image Classification With TensorFlow.js,1553503723,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b5841r/image_classification_with_tensorflowjs/
Your Cortex Contains 17 Billion Computers: Neural networks of neural networks,1553487577,,neuralnetworks,slacka123,False,/r/neuralnetworks/comments/b562ja/your_cortex_contains_17_billion_computers_neural/
Hash Your Way To a Better Neural Network,1553466288,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b52g7b/hash_your_way_to_a_better_neural_network/
Bi-directional Long Short-Term Memory using Quantized data of Deep Belief Networks for Sleep Stage Classification,1553457680,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b50rr6/bidirectional_long_shortterm_memory_using/
Convolutional Neural Networks - Image Classification,1553456245,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/b50grw/convolutional_neural_networks_image_classification/
Make deep learning faster and simpler,1553269016,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b46ie8/make_deep_learning_faster_and_simpler/
Comprehensive Neural Network | Deep Learning Video Tutorials and Projects for FREE,1553242082,"Comprehensive  Deep Learning Video Tutorials and Projects for FREE 

&amp;#x200B;

Free Video Tutorials link : [https://www.edyoda.com/course/1429](https://www.edyoda.com/course/1429)

&amp;#x200B;

Github codebook link : [https://github.com/zekelabs/AI101-DeepLearning](https://github.com/zekelabs/AI101-DeepLearning) 

&amp;#x200B;

Build Deep Learning Projects (Complete Video Series for FREE )

==============================================

1. Making your RL Projects in 20 Minutes : [https://www.edyoda.com/course/1421](https://www.edyoda.com/course/1421)

&amp;#x200B;

2. Style Transfer, Face Generation using GANs in 20 minutes : [https://www.edyoda.com/course/1418](https://www.edyoda.com/course/1418)

&amp;#x200B;

3.  Language and Machine Learning in 20 minutes : [https://www.edyoda.com/course/1419](https://www.edyoda.com/course/1419)

&amp;#x200B;

4. AI Project - Web application for Object Identification : [https://www.edyoda.com/course/1185](https://www.edyoda.com/course/1185)

&amp;#x200B;

5. Dog Breed Prediction : [https://www.edyoda.com/course/1336](https://www.edyoda.com/course/1336)

&amp;#x200B;

Free Video Series for Beginners to advanced users

===========================================

1. Machine Learning - Mastering NumPy  : [https://www.edyoda.com/course/1263](https://www.edyoda.com/course/1263)

&amp;#x200B;

2. Machine Learning using Tensorflow : [https://www.edyoda.com/course/99](https://www.edyoda.com/course/99)

&amp;#x200B;

3. Step by step guide to Tensorflow : [https://www.edyoda.com/course/1429](https://www.edyoda.com/course/1429)

&amp;#x200B;

4. Introduction to Neural Networks : [https://www.edyoda.com/course/1417](https://www.edyoda.com/course/1417)

&amp;#x200B;

5. Knowledge Graphs, Deep Learning and Reasoning : [https://www.edyoda.com/course/1420](https://www.edyoda.com/course/1420)",neuralnetworks,prithvi45,False,/r/neuralnetworks/comments/b42ddj/comprehensive_neural_network_deep_learning_video/
New Study Uses Machine Learning to Predict Sexual Orientation,1553199258,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/b3vef7/new_study_uses_machine_learning_to_predict_sexual/
Analyzing tf.function to discover AutoGraph strengths and subtleties - part 1,1553194990,,neuralnetworks,pgaleone,False,/r/neuralnetworks/comments/b3uj33/analyzing_tffunction_to_discover_autograph/
AttoNets: Compact and Efficient DNNs Realized via Human-Machine Collaborative,1553182419,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/b3rxi1/attonets_compact_and_efficient_dnns_realized_via/
Checklist for debugging neural networks,1553175612,,neuralnetworks,mmaksimovic,False,/r/neuralnetworks/comments/b3qnf4/checklist_for_debugging_neural_networks/
Neural Network Picasso,1553086577,,neuralnetworks,bad_fish87,False,/r/neuralnetworks/comments/b3boed/neural_network_picasso/
Semantic Image Synthesis with Spatially-Adaptive Normalization,1553071776,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b39m99/semantic_image_synthesis_with_spatiallyadaptive/
"hello . I've decided to creat a podcast about deep learning ، machine learning ، artifical inteligence , neuroscience ,....... which interviews with experts . But unfortunately I can't choose a suitable title for it. Could anyone choose some titles for it ?",1553021658," 

**hello . I've decided to creat a podcast about deep learning ، machine learning ، artifical inteligence , neuroscience ,....... which interviews with experts . But unfortunately I can't choose a suitable title for it. Could anyone choose some titles for it ?**",neuralnetworks,Doctor_who1,False,/r/neuralnetworks/comments/b319ug/hello_ive_decided_to_creat_a_podcast_about_deep/
Photoshop + AI,1553005887,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/b2y47t/photoshop_ai/
Exploring Neural Networks with Activation Atlases,1553005299,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b2y076/exploring_neural_networks_with_activation_atlases/
Looking inside the Neutral Network Blackbox,1552992639,"Looking inside the Neutral Network Blackbox

#machinelearning #neuralnetworks #blackbox #transparency

http://news.mit.edu/2019/neural-networks-nlp-microscope-0201
",neuralnetworks,aiforworld2,False,/r/neuralnetworks/comments/b2vxfr/looking_inside_the_neutral_network_blackbox/
Retraining Neural Network Model,1552985484,"Once we generate a model using either CNN or LSTM or AE, when we come across some more new dataset, how can we use it to retrain that very same model, but not start training from scratch (which I believe is obviously not the way to do it)

I guess this is how Google or Facebook improves it's deepnet algorithms - retraining on new data. But I want to know how.",neuralnetworks,Tamoghna_Saha,False,/r/neuralnetworks/comments/b2uzu3/retraining_neural_network_model/
Action Detection Using Deep Neural Networks: Problems and Solutions,1552928235,,neuralnetworks,RyanTmthn,False,/r/neuralnetworks/comments/b2l3tm/action_detection_using_deep_neural_networks/
Powerful machine-learning technique enables biologists to analyze enormous data sets,1552919481,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b2jecx/powerful_machinelearning_technique_enables/
The #paperoftheweek 10 is: SC-FEGAN: Face Editing Generative Adversarial Network with User’s Sketch and Color,1552915580,,neuralnetworks,BrighterAI,False,/r/neuralnetworks/comments/b2ipvn/the_paperoftheweek_10_is_scfegan_face_editing/
How do I go about labelling my images for my CNN?,1552906014,"Hello!

I'm sorry if this question seems really stupid. I recently just read about Convolutional Neural Network and there are datasets namely: Training, Validation and Testing. My questions is how do you label the images? If example, I have 10,000 images for training, how do I tell the machine if the images is of a building or a tree? Should I crop each image to show just a building instead of showing the whole area? How do I even label each part of an image?

&amp;#x200B;

For a classification sample, let's say for a dog and cat, would I need to convert the image into an array of RGB and have the last item as their classification? 0 - for cat and 1 - for dog?

&amp;#x200B;

I apologize again for this. I'm really new at this and the training blogs usually just give a dataset of images. I'm not entirely sure what it contains.",neuralnetworks,diff_laptop,False,/r/neuralnetworks/comments/b2h6l8/how_do_i_go_about_labelling_my_images_for_my_cnn/
Most important input feature,1552902825,Is there a simple way of determining the most influential / important input parameter in my classification neural network? ,neuralnetworks,Dahnaman,False,/r/neuralnetworks/comments/b2gr0i/most_important_input_feature/
Normalizing raw data,1552901845,"I built a classification neural network using tensorflow. I read that normalising the data is necessary. However, when I feed in normalised data my model accuracy dropped to 50% and if I use the raw data it has an accuracy of 80%. Can I just use the raw data? Help! ",neuralnetworks,Dahnaman,False,/r/neuralnetworks/comments/b2gmhj/normalizing_raw_data/
Introduction to Tensorflow.js with Real-World Example,1552894919,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/b2fs21/introduction_to_tensorflowjs_with_realworld/
Autonomous Driving Back Propagation and Genetic Algorithm,1552755625,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/b1ulp0/autonomous_driving_back_propagation_and_genetic/
'Neuroflight' drone controller gets a boost from A.I.,1552661204,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b1ft84/neuroflight_drone_controller_gets_a_boost_from_ai/
A new approach to overcome multi-model forgetting in deep neural networks,1552661141,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/b1fsss/a_new_approach_to_overcome_multimodel_forgetting/
"Recommended books or sources to start learning about machine learning, deep learning, neural networks...?",1552660062,"I’m an almost last course of a bachelors degree on mathematics engineering and I want to do my final project about machine learning, focusing on neural networks is what I think I will do, but I want to study other options.

If you know some book or sources to start learning I will be very gratefull, sorry if my English is not perfect.

Regards.",neuralnetworks,Lorencrack3,False,/r/neuralnetworks/comments/b1flt7/recommended_books_or_sources_to_start_learning/
Is there a NN tutorial meeting following criteria?,1552642947,"Hello! I am looking for a tutorial where I don't have to make any choices, just follow the motions and get the hang of the workflow. I am looking for a tutorial where I don't need to choose dataset or any other kind of option to see the result. Does such a tutorial exist?",neuralnetworks,SirLasberry,False,/r/neuralnetworks/comments/b1cvqd/is_there_a_nn_tutorial_meeting_following_criteria/
help understanding signal of adaptive noise canceling filter,1552534249,[removed],neuralnetworks,yellowblueshark,False,/r/neuralnetworks/comments/b0vgis/help_understanding_signal_of_adaptive_noise/
A question about neural networks,1552516200,"Hi guys, i’m studying convolutionnal neural network for a project for my master’s degree, and there’s something i don’t understand. I study through several sources, and depending on which it is, one say that  a filter ( for exemple size 5x5x3 on an image 32x32x3) convolves around the image to give in output an activation map, and that filter is a neuron. Other sources say that as the filter is convolving, every possible location on the image passing by the filter is a neuron, and that location is the « receptive field ». I don’t really understand what is a neuron then ? Btw, what’s called a « channel » ?",neuralnetworks,TomBarz,False,/r/neuralnetworks/comments/b0scms/a_question_about_neural_networks/
"Machine Learning Resume Generator With Python, TextGenRnn and AWS",1552488821,"I was like one of you folks once, trying to think how to write a resume before I started working as a full-stack dev at a place I love. Over the years I have obsessed about how to make it easy for every one to build their resume. Using some AI magic, I ended up creating a site (a side project) that allows you to create a resume for free. Here’s the link: [https://thisresumedoesnotexist.com/](https://thisresumedoesnotexist.com/)

I've used [https://github.com/minimaxir/textgenrnn/tree/master/textgenrnn](https://github.com/minimaxir/textgenrnn/tree/master/textgenrnn) to generate the text by feeding it data scraped from various resources. My script runs in a ""while true"" loop and generates a new resume every 3-4 seconds. Resume is then saved to S3 and the same resume is loaded via CloudFront for all users.

I plan to make it more and more awesome, like you add your name and designation and it fills up your resume. But, I thought it would be nice to get a feedback from you folks and see if I can add the to my to-do for the side project.

I’d the following thoughts on why I started working on it:

\- I knew a guy who got a job at Google, who previously couldn’t get a job at an IT agency just cause his resume wasn’t good enough

\- Not all of us have the skills required to write creatively and organize it. That doesn’t mean you don’t qualify for a job

Things like these are weirdly still a hurdle for many (it is going to be 2020 next year!). I want them not to be a hurdle anymore :slightly\_smiling\_face:

So, pitchfork me if you hate what I built, but if you loved it or have any suggestions do share your thoughts with me. Appreciation will make my day, critique will help me shape it.

Thanks,

Alex",neuralnetworks,aginovski,False,/r/neuralnetworks/comments/b0mwak/machine_learning_resume_generator_with_python/
PyTorch Geometric: A Fast PyTorch Library for DL,1552488456,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/b0mtxi/pytorch_geometric_a_fast_pytorch_library_for_dl/
Need help understanding this adaline network,1552464591,[removed],neuralnetworks,yellowblueshark,False,/r/neuralnetworks/comments/b0jg15/need_help_understanding_this_adaline_network/
New SOTA on Instance Segmentation: Mask Scoring R-CNN Tops Mask R-CNN on COCO,1552404104,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/b08zak/new_sota_on_instance_segmentation_mask_scoring/
Car Plate Recognition,1552245335,"Hi, we were assigned to read car plates using CNN. At first I thought about training the model with carplates but our teacher wants us to ""Find the car first and from there, spot the plate and read it"". I was looking for directions and any help would be appreciated.

I am using the model Faster-RCNN-Inception-V2-COCO for tensorflow.",neuralnetworks,ScytherDOTA,False,/r/neuralnetworks/comments/azj689/car_plate_recognition/
Neural network for soccer match prediction,1552169088,"My aim is to use a neural network to predict the outcomes of soccer matches. I have a dataset of about 1800 records albeit with limited features; *the home team, the away team, number of goals scored by the home team and number of goals scored by the away team* for each match. What would be the best way to go about this?",neuralnetworks,ngangez,False,/r/neuralnetworks/comments/az8pxh/neural_network_for_soccer_match_prediction/
"As a newbie to ML/Deep Learning, how do I approach this problem? What are some resources for it?",1552021777,"As I mentioned in the title, I am a newbie to ML/Deep Learning. Currently i am trying to work on a accounting project that requires me to sort expenses based on the images that the user uploads (for example Food Expense, Corporate Expense,etc). I figured that TensorFlow might be something that I could use and I looked over a basic Image Recognition Neural Network Tutorial, but I just want to make sure that I am in the right direction. If y'all could direct me to some resources that would be great! ",neuralnetworks,LostRaider1297,False,/r/neuralnetworks/comments/aymge8/as_a_newbie_to_mldeep_learning_how_do_i_approach/
The basics of machine learning for starters and people who did a few course on the internet,1551982095,,neuralnetworks,swordythomas,False,/r/neuralnetworks/comments/ayfng3/the_basics_of_machine_learning_for_starters_and/
ICLR 2019 | ‘Fast as Adam &amp; Good as SGD’ — New Optimizer Has Both,1551981900,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/ayfm26/iclr_2019_fast_as_adam_good_as_sgd_new_optimizer/
Google Open-Sources GPipe Library for Training Large-Scale Neural Network Models,1551974658,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/aye6om/google_opensources_gpipe_library_for_training/
[advice] neural network for ecg arrythmia classification,1551892301,[removed],neuralnetworks,Agiac95,False,/r/neuralnetworks/comments/ay1781/advice_neural_network_for_ecg_arrythmia/
Triple ‘Strong Accept’ for CVPR 2019: Reinforced Cross-Modal Matching &amp; Self-Supervised Imitation…,1551889922,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ay0qvm/triple_strong_accept_for_cvpr_2019_reinforced/
Confusion when computing delta during back-propagation of a convolutional layer,1551868085,"Hello, I'm having trouble understanding the correct way of performing full convolution between the delta passed back and the filters.

&amp;#x200B;

δ^(L)=(w^(T)∗δ^(L+1))∘σ′(x^(L))  


From the following (childish) diagram, am I right in saying the component (w^(T)∗δ^(L+1)) be computed this way?

I apologise for not using mathematical notation, I just unfortunately find the notation from the explanations I've read online not very transparent.

![img](zpw0z3q86hk21)

&amp;#x200B;

&amp;#x200B;",neuralnetworks,mn15104,False,/r/neuralnetworks/comments/axxdh8/confusion_when_computing_delta_during/
Human Pose Estimation Model HRNet Breaks Three COCO Records; CVPR Accepts Paper,1551809873,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/axoams/human_pose_estimation_model_hrnet_breaks_three/
How to find datasets for Artificial Intelligence – Recent advances in Deep Learning are only possible with the availability of large datasets,1551799360,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/axma8o/how_to_find_datasets_for_artificial_intelligence/
Question on training a network to draw,1551796243," Hello, I am a CS student who has started a machine learning course. Our  professor has recommended us to start researching on a project that we  should work on throughout the course. The idea of an AI being able to  draw has fascinated me and I thought why not try it for this course.  However, the more I researched, the more vague everything becomes. As a  beginner, I was wondering if someone could help me figure out where to  start with this project? Or if it is even recommendable to a beginner? ",neuralnetworks,Mbrzzz,False,/r/neuralnetworks/comments/axlr0f/question_on_training_a_network_to_draw/
Google Brain Simplifies Network Learning Dynamics Characterization Under Gradient Descent,1551726729,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/axazyx/google_brain_simplifies_network_learning_dynamics/
An Introduction to Neural Networks: Implementing one from scratch in Python,1551726204,,neuralnetworks,vzhou842,False,/r/neuralnetworks/comments/axaw2c/an_introduction_to_neural_networks_implementing/
AIEditor - Neural Network Editor How its created,1551701922,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/ax6iby/aieditor_neural_network_editor_how_its_created/
"How Facial Recognition Works - ""Where's Waldo"" (Humans vs. Laptop)",1551632387,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/awvv5v/how_facial_recognition_works_wheres_waldo_humans/
One hot encoding in MLP question,1551628787,"Hi I am building a MLP with only one layer of hidden units to do the letter recognition on the UCI website, I use tanh as my activation function, I was able to train XOR and other things.

&amp;#x200B;

Now that I am working with this data I use one hot encoding for the correct output of data when testing it, it is being trained on 16000 examples and tested on 4000, i have 16 input nodes for the features, 10 hidden layers and 26 output layers for the output. My weights are initialised between -0.2 and 0.2.

&amp;#x200B;

The error for training never seems to go below 5000 and the output are almost all wrong for the training set with a single letter being dominant, does any one have any idea whats going on here",neuralnetworks,hakattack97,False,/r/neuralnetworks/comments/awv9h4/one_hot_encoding_in_mlp_question/
"Beyond MNIST Example: Practical CNNs, Free Course on YouTube",1551541377,"Totally Free Course of  Practical CNN on Youtube, 6.5 hours. 

If you are tired of not going further step after watching MNIST Tutorial, you are welcome!

What's the difference from the other courses:

\* Loading Images with PIL, OpenCV, Keras, SciPy

\* FineTuning with Keras

\*\*\* Three solutions for Imbalanced Dataset Problem

\* Image Augmentation with Keras

\*\*\* Extreme Image Augmentation with ImgAug Library for Keras

\*\*\* Writing CustomDataGenerator in Keras

https://www.youtube.com/watch?v=LHA\_rpE1ERY&amp;list=PLRRY18KNZTgUXGJ4jOHrAdNs9N2QMX1An",neuralnetworks,mburaksayici,False,/r/neuralnetworks/comments/awixu7/beyond_mnist_example_practical_cnns_free_course/
What are CNNs good for?,1551471447,"So I've been learning about convolutional NNs for a few days, and I'm wondering what are they good for beside image recognition and classification.
Any ideas? 
",neuralnetworks,tomerha,False,/r/neuralnetworks/comments/aw97ji/what_are_cnns_good_for/
"Genetic Algorithm Tutorial Full Explanation, Calculations and uses 2019",1551456016,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/aw6hnx/genetic_algorithm_tutorial_full_explanation/
“Topology of Learning in Artificial Neural Networks” 21st of Feb 2019 article,1551446312,,neuralnetworks,BrighterAI,False,/r/neuralnetworks/comments/aw4slk/topology_of_learning_in_artificial_neural/
Why isn't my neural network working?,1551427865,"Alright, i'm writing a neural net to get the hang of things. The thing is - it's not even getting close to http://playground.tensorflow.org

It's supposed to guess if (d &gt; 0.3) &amp;&amp; (d &lt; 0.7) from x and y. And it's giving me 30% error even after learning for 100'000 generations.

Now, what i'm doing is generating a point, running it through network(bias + dot(weights, inputs) on every layer), and then backpropagating error. I backpropagate by getting error gradient, where error i is dot((derivative * neuron.weight_i), error); Then, i learn on layer by adding (derivative * error_i * prev_layer_neuron_j.output) to every weight j of neuron i, and for bias i consider output to be 1.

This system does learn, but cannot learn past 30% error, as i have said. Same network on playground goes to like 0 error in 40 generations.

I tried reading accompanying book - http://neuralnetworksanddeeplearning.com/chap1.html but it has some python wizardry with arrays.

I believe that i am not doing stochastic descent propery. I just generate a sample, then learn on it. Am i supposed to take mean error over several samples? Am i doing something else wrong here?",neuralnetworks,grafixcat,False,/r/neuralnetworks/comments/aw2dww/why_isnt_my_neural_network_working/
Incremental live NN training,1551313128,"Is there any way I can use a NN, and still keep on training it incrementally as and when new class gets added when new data comes up",neuralnetworks,vineetverma_it,False,/r/neuralnetworks/comments/avkiq8/incremental_live_nn_training/
Help with a tiny 2 neuron f(x)=x neural net asymptote (python),1551310805,"I've replicated The World's Smallest NN from here (@27:59) [https://youtu.be/uXt8qF2Zzfo](https://youtu.be/uXt8qF2Zzfo)

and it runs and learns, but I do not understand why it asymptotes at approx (0.5, 0) vs targets (1,0). Any tips would be appreciated!

""""""World's simplest ANN

learns f(x) = x """"""

np.random.seed(1) #seed random numbers to make calculation deterministic (just a good practice)

def sigmoid(x, deriv=False):

if deriv==False:

y = 1 / (1 + (np.e\*\*-x))

return y

elif deriv==True:

y = x \* (1-x)

return y

&amp;#x200B;

x= np.array(\[1,0\]) #input vals

w1 = 0.96 

w2 = 0.06

desired = np.array(\[1,0\]) # target values,also called y

lrate = 50

&amp;#x200B;

\#prediction

def prediction(x):

\#pure forward prop

prod1 = x \* w1

y = sigmoid(prod1)

prod2 = y \* w2

z = sigmoid(prod2)

\#   hypothesis = z

print(""x = "", x, ""; hypothesis="", z )



\#learning

for trial in range(501):

idx = np.random.randint(0,2) #choose one of the idxs

&amp;#x200B;

\#forward prop

prod1 = x\[idx\] \* w1

y = sigmoid(prod1)

prod2 = y \* w2

z = sigmoid(prod2)

print(""desired"",desired\[idx\],""z"",z, ""; trial"", trial)



\#BackProp

cost = - 1/2 \* ((desired\[idx\] - z)\*\*2)  #cost function

\#print(""cost"", cost, ""; after trial"", trial)

d = desired\[idx\]



l2\_delta = y \* z \* (1-z) \* (d-z)  

  

l1\_delta = x\[idx\] \* y \* (1-y) \* w2 \* z \* (1-z) \* (d-z)



w1 = w1 + l1\_delta \* lrate  #updating weights

w2 = w2 + l2\_delta \* lrate

 ",neuralnetworks,ConterminousTiarella,False,/r/neuralnetworks/comments/avk4ig/help_with_a_tiny_2_neuron_fxx_neural_net/
Only a professional lawyer can train a professional neural network.,1551275467,,neuralnetworks,moseich,False,/r/neuralnetworks/comments/avdoxo/only_a_professional_lawyer_can_train_a/
We're in UltraHD Morty! How to watch any movie in 4K (upscaling with neural networks and Photoshop),1551271667,,neuralnetworks,atomlib_com,False,/r/neuralnetworks/comments/avd4a3/were_in_ultrahd_morty_how_to_watch_any_movie_in/
My Thoughts on OpenAI Not Releasing Weights,1551268190,,neuralnetworks,ericnyamu,False,/r/neuralnetworks/comments/avcmzx/my_thoughts_on_openai_not_releasing_weights/
Brain Simulator II: The Basic Neuron Model,1551199401,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/av1i4r/brain_simulator_ii_the_basic_neuron_model/
OpenAI Text Generator,1551198503,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/av1bne/openai_text_generator/
Using Rule-Based Labels for Weak Supervised Learning: A ChemNet for Transferable Chemical Property,1551197122,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/av11zb/using_rulebased_labels_for_weak_supervised/
"New brain implants disguise as neurons, offering a potentially safer way to study and treat the brain",1551197080,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/av11om/new_brain_implants_disguise_as_neurons_offering_a/
SenseTime Trains ImageNet/AlexNet In Record 1.5 minutes,1551112019,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/aun26v/sensetime_trains_imagenetalexnet_in_record_15/
Generative Adversarial Network (GAN) Series,1551086579,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/aujc7y/generative_adversarial_network_gan_series/
BodyPix Google,1551034095,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/aub1nw/bodypix_google/
What function approximates the typical error training curve for a neural network?,1551019204,"I realize there may not be one good answer, but I'm curious if the typical error curve for a neural network, while training, can be approximated by a particular function. Off the top of my head, it looks logarithmic, but I'm not sure.",neuralnetworks,shamoons,False,/r/neuralnetworks/comments/au8it3/what_function_approximates_the_typical_error/
[Request] How can I train Tesseract-Engine for my dataset for Handwriting?,1550987437,"Official documentation is here [https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00](https://github.com/tesseract-ocr/tesseract/wiki/TrainingTesseract-4.00), but it's very complex.

currently Tesseract is working for printed text only. i.e. Optical characters recognition,

I wanted to use it for Handwriting character recognition-HCR.

HCR kaggle dataset is [https://www.kaggle.com/olgabelitskaya/classification-of-handwritten-letters](https://www.kaggle.com/olgabelitskaya/classification-of-handwritten-letters).

also, project in which I wanted to implement it is [https://github.com/ganesh-deshmukh/cvkeyboard](https://github.com/ganesh-deshmukh/cvkeyboard).",neuralnetworks,ganeshdeshmukh,False,/r/neuralnetworks/comments/au4ztd/request_how_can_i_train_tesseractengine_for_my/
AI-Based 3D Pose Estimation: Almost Real Time!,1550971548,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/au2pko/aibased_3d_pose_estimation_almost_real_time/
"I believe I have ""trained"" my neural network, what now?",1550968329,"This is probably a really dumb question and I am a beginner with all of this stuff. 

So I have gotten to the point where I have a program that takes in training data and target results, then goes through and adjusts the weights until the target results are hit... but now what do I do? 

If I am only running the program once, how will my ""neural network"" recognize a new set of data after the program is done running? My thoughts initially were to save the weights of the finished program and then continue training or testing while saving the new weights each time. Is this correct at all?

Sorry if this is a dumb question but I would rather be a fool for a moment than a fool for a lifetime. Thank you!",neuralnetworks,RESENJOUGAHARA,False,/r/neuralnetworks/comments/au26y5/i_believe_i_have_trained_my_neural_network_what/
A.I. Learns to DRIVE,1550945827,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/atya8x/ai_learns_to_drive/
This AI Learned Image Decolorization..and More,1550945794,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/atya0w/this_ai_learned_image_decolorizationand_more/
Deep Learned Super-Sampling (DLSS) - Computerphile,1550871230,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/atn647/deep_learned_supersampling_dlss_computerphile/
Using a neural network to drive a space fighter in a game,1550869679,"Hello all,

In my free time I've been tinkering with building a 3d combat space simulator and lately I've started thinking about adding neural networks as pilots for enemy/friendly ships.

Each ""pilot"" should receive something like the following inputs (vectors are normalised and all values as floats also vectors will be 3 float inputs per vector): target position vector, target heading vector, target speed, own speed, ship's own forward vector, several vectors for collision detection (let's say sensors like the ones you have in a car for reverse) and to provide outputs for: pitch, roll, yaw, throttle (4 normalised floats).

*The question**: is it possible to make a neural network small enough, yet to satisfy the above requirements?  (I'll need to run 4+ such networks on different threads in parallel - ships will use the networks in ""turns"" to avoid lag)

And if it is possible which model could be better used for such a problem?",neuralnetworks,liviumarinescu,False,/r/neuralnetworks/comments/atmuuw/using_a_neural_network_to_drive_a_space_fighter/
NN architectures for hardware acceleration,1550771915,"If you are trying to do matrix multiplication with large matricies, there is an efficient way to perform that with smaller blocks.

So if you have 2 32x32 matricies, and you have a function that multiplies 4x4 matricies, you can break your large matricies down into parts, run each of those through your 4x4 multiplier, and put them back together to get the product of your large matricies.

Is there anything similar to that with regards to deep neural networks? As in, if I have some arbitrarily sized multiperceptron network, and some fixed unit for actually performing computation, say a 3x3 network, are there known algorithms for how to efficiently run the whole network through that unit?",neuralnetworks,Bananawamajama,False,/r/neuralnetworks/comments/at673o/nn_architectures_for_hardware_acceleration/
Controlling a 2D Robotic Arm with Deep Reinforcement Learning,1550768265,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/at5g3i/controlling_a_2d_robotic_arm_with_deep/
Puzer/stylegan-encoder,1550768135,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/at5f61/puzerstyleganencoder/
Why AKQA designed an AI to take control of your body,1550768052,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/at5eky/why_akqa_designed_an_ai_to_take_control_of_your/
NAS-Generated Model Achieves SOTA In Super-Resolution,1550766760,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/at54z4/nasgenerated_model_achieves_sota_in/
Neural Networks C++,1550763323,"Hi, i want to know how if someone can guide me in the right direction to implement a neural network for a school project. I got images black and white no grays basicly 0 or 1 of smth already classified, and i need to injected a new b/w image and recognize a output. it also needs to have a gui. 

&amp;#x200B;",neuralnetworks,jonyto123,False,/r/neuralnetworks/comments/at4gr5/neural_networks_c/
My implementation of YOLO - You only look once (ver 2) for object detection tasks. Source code: https://github.com/vietnguyen91/Yolo-v2-pytorch,1550754973,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/at30xj/my_implementation_of_yolo_you_only_look_once_ver/
Trinomial Classification Across Many Features,1550680791,"Is there an issue to doing a trinomal classification with a neural network? I am new to this and it is just a hobby, but I have noticed that my accuracy falls dramatically when going from binomial to trinomial classification. If it matters, I am using 40 features, so that may be part of it.",neuralnetworks,spot4992,False,/r/neuralnetworks/comments/asqg63/trinomial_classification_across_many_features/
Help BackPropagation Convolutional Neural network,1550611087,Hi everyone. I have an input of 6@14x14 with 16 filters of 6@5x5 and I get in the convolutional operation a feature map of 16@5x5. However when I want to calculate the deltas I must do the convolution of the 16@10x10 with the 16 filters 6@5x5 and get an outputs of 6@14x14. How can i do it?. The channel size don't match 16 and 6,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/asfjn0/help_backpropagation_convolutional_neural_network/
Free credits for our computer vision platform,1550571578,"Hi, I’m a co-founder at the Belgian Computer vision startup, [Overture](https://www.overture.ai/). we have built a platform that offers an end-to-end fully-customizable pipeline for computer vision model training and deployment.

We are very eager to share our work and to find out what developers think of the platform. Of course, we would also love to hear which features you are still missing on it.Hence, we are offering 50 euros of free credits, so that you can use the platform for your own use cases and tell us what you like and dislike about it.

ATM, you can train and deploy image classifiers, object detectors and face recognition systems. Many other algorithms like segmentation, OCR and object tracking are in development.

Claim your free credits: [https://app.overture.ai/pages/register](https://app.overture.ai/pages/register) and use promo code: **reddit50** to redeem your 50 euros of free credits.

**PLATFORM DEMO:**[https://www.youtube.com/watch?v=hQLrGjQAWWU](https://www.youtube.com/watch?v=hQLrGjQAWWU)

If you have any questions, feel free to contact me at [christiaan@overture.ai](mailto:christiaan@overture.ai). We are looking forward to seeing you on the platform.

Kind regards,

Christiaan

Co-Founder and AI engineer at [Overture](https://www.overture.ai/)",neuralnetworks,chrike,False,/r/neuralnetworks/comments/as8x9z/free_credits_for_our_computer_vision_platform/
Free credits for our computer vision platform.,1550570727,,neuralnetworks,chrike,False,/r/neuralnetworks/comments/as8tsn/free_credits_for_our_computer_vision_platform/
Can we apply AI?,1550504374,,neuralnetworks,moseich,False,/r/neuralnetworks/comments/aryf5j/can_we_apply_ai/
Free credits for our computer vision platform.,1550502586,[removed],neuralnetworks,christiaanleysen,False,/r/neuralnetworks/comments/ary4fu/free_credits_for_our_computer_vision_platform/
Free credits for our computer vision platform.,1550501748,[removed],neuralnetworks,christiaanleysen,False,/r/neuralnetworks/comments/arxzpd/free_credits_for_our_computer_vision_platform/
Free credits for our computer vision platform.,1550501082,[removed],neuralnetworks,christiaanleysen,False,/r/neuralnetworks/comments/arxw4s/free_credits_for_our_computer_vision_platform/
learnml.online - GPU-accelerated computing Beta testing.,1550491562,[removed],neuralnetworks,learnml-online,False,/r/neuralnetworks/comments/arwlca/learnmlonline_gpuaccelerated_computing_beta/
Can the moderators do something against the blogspam and shills?,1550483885,"I mean bloody hell, this sub got one of the worst case of shillposting. More than half of the posts are made by shills plugging their shitty courses.

Paging /u/nickb /u/sandsmark and /u/keghn. I don't even like moderating but if needed I wouldn't mind to help.",neuralnetworks,Dark_Ice_Blade_Ninja,False,/r/neuralnetworks/comments/arvp8u/can_the_moderators_do_something_against_the/
Ultimate Guide to Machine Learning with ML.NET,1550479158,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/arv60o/ultimate_guide_to_machine_learning_with_mlnet/
Convolutional Neural Network Question: Finding an unknown transfer function,1550459366,"I would like to know if it is possible/how I would be able to have an algorithm predict what the transfer function between two images is. Say I have many images as my sample data, and I apply a uniform effect on every one of those images, maybe a hue change or change in brightness. Is there a process that would then be able to analyze the input original data, and compare to the effected images, and provide a transfer function that is approximately the effect that was applied? If possible how would I do this?",neuralnetworks,luemf,False,/r/neuralnetworks/comments/arsiu9/convolutional_neural_network_question_finding_an/
Could AI be used to predict stock prices? [Answer and Tutorial],1550424399,,neuralnetworks,SilentDifficulty,False,/r/neuralnetworks/comments/armo9e/could_ai_be_used_to_predict_stock_prices_answer/
Interesting tutorials,1550410162,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/arkjuo/interesting_tutorials/
Building SQL queries from neural net,1550306633,"Hi there!
I'm tying to create a neural net that will ultimately  build an SQL query as it's final output. My goal is to optimize an existing query to give only wanted results (I have a very simple way to tag the results).
My problam is that I can't think of a way to turn the output of the net (a list of number basically) into a complex query.
Any ideas? Also reading material on the subject will be awsome
",neuralnetworks,tomerha,False,/r/neuralnetworks/comments/ar70vi/building_sql_queries_from_neural_net/
"Here is my pytorch implementation of the model described in the paper DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs (https://arxiv.org/pdf/1606.00915.pdf) Source code: https://github.com/vietnguyen91/Deeplab-pytorch",1550305607,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/ar6xfz/here_is_my_pytorch_implementation_of_the_model/
O que é Deep Learning - Redes Neurais,1550105728,,neuralnetworks,joelgarciajr,False,/r/neuralnetworks/comments/aqdxo9/o_que_é_deep_learning_redes_neurais/
Beginners steps - creating NN art,1550104094,"Looking for some advice on a project I am working on. I have a library of many thousands of images and what I want to do is take that library and have the NN create new works based on this image library ...  I have  [art-DCGAN](https://github.com/robbiebarrat/art-DCGAN) but I am finding it limited to what I want to do, Im a newbie but it looks like it points to wikipedia and Im not sure I can use my own library ...

&amp;#x200B;

again, Im very much a beginner and I am finding it hard to search for other things .. . for example, is art-DCGAN the most current and best NN to do this job? What is new out there that looks good and promising ? Any pointers for other things to look at and get into, please let me know :)",neuralnetworks,Mappalujo,False,/r/neuralnetworks/comments/aqdoau/beginners_steps_creating_nn_art/
Neural Network 2019 Digit Recognition Use Artificial Intelligence Tutori...,1550004658,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/apy319/neural_network_2019_digit_recognition_use/
Ideas for Reinforcement Learning project in robotics ?,1550001434,"Hey there. Could someone recommend beginner-intermediate level RL projects focused in robotics or physical computing?

Some background: I'm a beginner at RL. I do have decent experience in deep learning though (especially supervised) and in embedded systems, robotics and open source hardware. 

Thanks in advance.",neuralnetworks,void_gear,False,/r/neuralnetworks/comments/apxhhp/ideas_for_reinforcement_learning_project_in/
Implementing CycleGAN Using Python,1549878658,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/apep7c/implementing_cyclegan_using_python/
My implementation of 3 NLP models for text classification in Pytorch and Tensorflow,1549835142,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/ap8u7f/my_implementation_of_3_nlp_models_for_text/
NVIDIA Open-Sources Hyper-Realistic Face Generator StyleGAN,1549789995,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/ap2ezm/nvidia_opensources_hyperrealistic_face_generator/
"Google Brain Researcher Quoc Le talks on his latest invention, AutoML, Google Brain’s pursuit of AI, and the secret of transforming lab technologies into real practices.",1549658349,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/aoktqz/google_brain_researcher_quoc_le_talks_on_his/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1549629128,[removed],neuralnetworks,performadorb,False,/r/neuralnetworks/comments/aog2xs/global_neural_network_software_market_size/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1549604957,[removed],neuralnetworks,UnitedAddress,False,/r/neuralnetworks/comments/aodha2/global_neural_network_software_market_size/
Was messing with evolution and my creature learned how to fly,1549587594,,neuralnetworks,pugmasterlol200,False,/r/neuralnetworks/comments/aob00t/was_messing_with_evolution_and_my_creature/
DeepMind’s AlphaStar Beats Humans 10-0 (or 1),1549562729,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ao6jfj/deepminds_alphastar_beats_humans_100_or_1/
Neural Network - Feedback - Spark,1549560638,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ao65r2/neural_network_feedback_spark/
Introduction to Neural Langauge Models,1549551837,"My learnings on NLM. Read at

[https://prakhartechviz.blogspot.com/2019/02/neural-language-models-in-keras.html](https://prakhartechviz.blogspot.com/2019/02/neural-language-models-in-keras.html)",neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/ao4lty/introduction_to_neural_langauge_models/
Roadshow! Meet NT Robotics and its amazing fully autonomous indoor Drone.,1549535143,,neuralnetworks,Batareika_1,False,/r/neuralnetworks/comments/ao2e2w/roadshow_meet_nt_robotics_and_its_amazing_fully/
Are neural networks currently two dimensional or is that simply how they're pictorially portrayed? What is a 3D neural network and how would one be made?,1549507580,,neuralnetworks,RealmOfTheForums,False,/r/neuralnetworks/comments/anz06z/are_neural_networks_currently_two_dimensional_or/
On Compilers: First TVM and Deep Learning Conference,1549489353,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/anvy8k/on_compilers_first_tvm_and_deep_learning/
Please help me understand deep learning,1549467705,[removed],neuralnetworks,knownsuq,False,/r/neuralnetworks/comments/anrxes/please_help_me_understand_deep_learning/
Visualize Convolutional Neural Networks interactively and in real time !,1549390220,,neuralnetworks,cyber-neuron,False,/r/neuralnetworks/comments/angv0g/visualize_convolutional_neural_networks/
Training YOLOv3 : Deep Learning based Custom Object Detector,1549294564,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/an2s4j/training_yolov3_deep_learning_based_custom_object/
Deep Learning based Text Detection Using OpenCV (C++/Python),1549294489,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/an2rp7/deep_learning_based_text_detection_using_opencv/
Project Shows How To Use Machine Learning to Detect Pedestrians,1549293553,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/an2mg7/project_shows_how_to_use_machine_learning_to/
Front Door Camera Sends Automatic Alerts By Text,1549293492,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/an2m34/front_door_camera_sends_automatic_alerts_by_text/
AI on Raspberry Pi with the Intel Neural Compute Stick,1549293453,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/an2ltz/ai_on_raspberry_pi_with_the_intel_neural_compute/
Introduction to CycleGAN,1549269489,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/amzs6p/introduction_to_cyclegan/
"Suitable Neural Network Camera - Item identification, Depth, Speed",1549217444,"This is industrial size, and i'm the mechanical engineer responsible for the system and hardware.

If there is other research papers or information I can learn more about on the types of vision systems often used then please let me know (I have done some research, but I'm not yet confident in my position).

I'm designing a conveyor belt system which has 2 H-Bot robots, and 1 Delta - items of ranging sizes come on the conveyor and the robots should act accordingly from the neural network's item/depth identifier. I need a camera that has depth sense and reliability to use with my Neural network. Should I have two cameras per robot?

&amp;#x200B;

I looked at Intel D435 RealSense and Orbbec camereas.

If there is other equally or better cameras that would be helpful to note.",neuralnetworks,Altwhite,False,/r/neuralnetworks/comments/ams4nj/suitable_neural_network_camera_item/
Self-Driving Cars [S1E3: AI Failure?],1549132775,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/amgxdu/selfdriving_cars_s1e3_ai_failure/
Inverting Facial Recognition Models,1549132632,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/amgwjh/inverting_facial_recognition_models/
How to visualize convolutional features in 40 lines of code,1549132592,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/amgwbm/how_to_visualize_convolutional_features_in_40/
Foundations Built for a General Theory of Neural Networks,1549132449,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/amgvfo/foundations_built_for_a_general_theory_of_neural/
"A journey through neural networks - Self Driving, simulations and education",1549132383,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/amgv1k/a_journey_through_neural_networks_self_driving/
How can I make this agent find its way thorugh the level (2D platformer NN),1549099530,"I'm using this library [https://github.com/wagenaartje/neataptic](https://github.com/wagenaartje/neataptic) to try and make my 2D platformer game play itself. Here is a gif of it in action: [https://imgur.com/a/KRkMYsE](https://imgur.com/a/KRkMYsE) 

The network is a perceptron with the tiles of the current screen's tiles as input, the tile that the player occupies is also registered as input, same for enemies. Two hidden layers.

In the gif it has only been running for a couple of minutes but I have had it running for days before and it still will not   
 figure out that it should drop down that hole to make progress. You can see in the top right what the network sees and the green dots are an A\*-algorithm that shows the path to the goal of the level. The three numbers to the right are the output values(go left, jump, go right). Reward is given when the path to the goal gets shorter. The agent doesn't know about the A\*-algorithm, it's just used to give reward. I thought that it would eventually evolve to find it's way through the level but even after running it for days it never happened.

&amp;#x200B;

Am I missing something? Is there anything I can change to increase its chances of learning to complete the level?",neuralnetworks,steenstn,False,/r/neuralnetworks/comments/amcl9d/how_can_i_make_this_agent_find_its_way_thorugh/
[Pytorch+OpenCV] My implementation of QuickDraw - an online game developed by Google (Source code: https://github.com/vietnguyen91/QuickDraw),1549033074,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/am2tdo/pytorchopencv_my_implementation_of_quickdraw_an/
Are neural networks the path to sentient AI?,1548964538,"I'm obsessed with neural networks but I'm not really sure what that means. It might mean that I'm obsessed with the question, what is life? Overall I'm a problem solver. I see a problem or question and I sink my teeth in. I think about it a lot. I dream about it. I want to be the first to make a sentient AI. So you can expect that it hits me to death. What's life?

I first found out about Nnets around 5 years ago and over been successful in creating them. I've given AI the need to eat, reproduce, regulate themselves as a means of homeostasis. But I'll be the first to say they're still pretty stupid. The best I've been able to replicate is a swarm behavior where agents, (where every agent utilizes its own Nnet) follows another agent. This creates a circle around a food source. When that food source is diminished, they scramble until a circle is formed again due to one agent finding food again.

To me this is amazing and although the definition of life is subject to each person, I truly consider them living. But I want more. It's not enough.

So what's the consensus? If the definition of life is a gradient (an insect is less alive / complex than a human) eg bugs dont feel pain or think about tomorrow yet they live, are Nnets able to simulate life to an extent passed what I've been able to create?

Overall I want to make an AI that knows it needs to eat but choose not to. If an AI can say, ""why bother?"" I'd be so happy.

I hope this ramble wasn't too tedious to read. I wrote it out of passion. I have no schooling in AI and the only tool I have is Gamemaker. But good golly do I want to create the first sentience. I want to challenge people on what the definition of life is.",neuralnetworks,producer_penguin,False,/r/neuralnetworks/comments/altgz4/are_neural_networks_the_path_to_sentient_ai/
Original video + Stennifer Lawrscemi,1548951405,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/alr2p0/original_video_stennifer_lawrscemi/
Engineers translate brain signals directly into speech,1548864014,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ale2ta/engineers_translate_brain_signals_directly_into/
How can I use Keras to train an image transformation?,1548849003,I have some images of MRI scans (with and without the skull) and I want to train a network to learn to remove the skull from an image. Is this possible using Keras? ,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/albxhr/how_can_i_use_keras_to_train_an_image/
Jennifer Lawrence with Steve Buscemi face,1548842468,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/alb8xk/jennifer_lawrence_with_steve_buscemi_face/
Idea of mimicking a person with NN,1548837302,"Hello, I am no scientist or expert on neural networks / artificial intelligence at all but had some thought cross my mind.

Currently, games can be learnt using deep Q-learning techniques where the “reward” for an agent is estimated based on its state for each action - therefore allowing the best possible action to be taken.

What if this were to be done with - say a human, record various activities and the seretonin levels reached during that with different action to choose from. The neural network must then predict the serotonin levels for a given action and is trained based on this and can therefore mimic human behaviour my choosing the action that gives the highest levels - assuming we do what makes us most happy?

This is just an idea and I’m sure it has its flaws but thought I would share it to hear what other people think about it - thanks for reading :)",neuralnetworks,cabbage01,False,/r/neuralnetworks/comments/alaqva/idea_of_mimicking_a_person_with_nn/
Research Ideas in CNN,1548783144," 

Looking  for ideas to work in CNN: Training/New architectural modifications in  CNN.  Also into Gradient Optimization algorithms. Suggestions, anything,   Something that someone wanted to try?

NEED IDEAS :(",neuralnetworks,deep_into_ml,False,/r/neuralnetworks/comments/al2djh/research_ideas_in_cnn/
Build a Keras-like library in Python,1548667508,[removed],neuralnetworks,omaraflak,False,/r/neuralnetworks/comments/aklulo/build_a_keraslike_library_in_python/
Dex-Net_4,1548616287,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/akevwy/dexnet_4/
Deep learning improves optical storage,1548614561,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/akepe3/deep_learning_improves_optical_storage/
What Makes a Good Image Generator AI?,1548531615,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ak47w6/what_makes_a_good_image_generator_ai/
Advice Needed: App Development &amp; Pricing,1548528138,"Hi All,

How much should i charge if i develop something like this and sell it’s script or app?

-Browse jewellery from a catalog and try them on your hand or skin and share their pics  and finally buy them from a brand

There is a likeminded app called Shop4rings but i don’t know where and how to start build sth like that:) 

Any tips?

Thanks!",neuralnetworks,dumango,False,/r/neuralnetworks/comments/ak3mzr/advice_needed_app_development_pricing/
MIT 6.S091: Introduction to Deep Reinforcement Learning (Deep RL),1548524990,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ak33vp/mit_6s091_introduction_to_deep_reinforcement/
Free credits for cloud GPU instances for deep learning,1548524813,"Hi, I am offering free credits for you to access 1080Ti GPU instances for deep learning purposes.

I am a co-founder of Tensorpad; where we are creating a service for AI startups to train neural networks. We have paid traffic, but some servers are idle. Hence, we are offering some credits for free, so that students, startups, and others can benefit from ML technologies and help us by using our product and providing honest feedback for us to improve.

You can Sign Up at https://dashboard.tensorpad.com/signup and redeem the code ""REDDIT200"" in the Billing tab.

Hope this explains our story and motivation for providing free credits.

Here is additional information:

* The instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run multiple instances in parallel

* You get access to the JupyterLab environment

* We have pre-installed Tensorflow, Keras, and other ML frameworks

* You can access the terminal through the JupyterLab

* By default, persistent storage is enabled

* Here are the available software versions https://docs.tensorpad.com/jobs_env/ For extra free trial hours, use promo code: reddit200

And for questions, please contact me at [ilie@tensorpad.com](mailto:ilie@tensorpad.com). I am looking forward to seeing you on our platform! Sincerely, Ilie Diacov Co-founder and UX researcher at Tensorpad
",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/ak32vf/free_credits_for_cloud_gpu_instances_for_deep/
Wallpaper app with machine learning detects features within images to shows similar ones,1548510739,,neuralnetworks,silviust7,False,/r/neuralnetworks/comments/ak1006/wallpaper_app_with_machine_learning_detects/
NN that can turn video into true cartoon or anime.,1548457549,"Hello! I have been searching for over a year for the perfect NN (and if what I am describing int a true NN my apologies) that can produce a video effect that looks like anime or a cartoon. I have been through so many apps, from Artisto, every toon filter on the iOS and App Store and can not find anything that can make flat colors and clean defined lines like the image attached. My friends tell me this is impossible and the best I will get will be from what the NN apps can produce but I wanted to see from this Reddit if it is possible? Software that can make real footage (not just images) look like a cartoon without nasty distortion, artifacts, and digital noise? Thank you so much. 

https://i.redd.it/5rke7bvihnc21.jpg

https://i.redd.it/unqncn04hnc21.jpg",neuralnetworks,SaburoNishikoyama,False,/r/neuralnetworks/comments/ajuwt4/nn_that_can_turn_video_into_true_cartoon_or_anime/
First Human Victory! DeepMind Struts Its StarCraft Strength; Humans Strike Back,1548455371,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/ajuj22/first_human_victory_deepmind_struts_its_starcraft/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1548413621,[removed],neuralnetworks,BetterTraffic8,False,/r/neuralnetworks/comments/ajnwjr/global_neural_network_software_market_size/
Artificial intelligence in the dining room,1548359340,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/ajg94e/artificial_intelligence_in_the_dining_room/
Information theory holds surprises for machine learning,1548342981,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ajddsj/information_theory_holds_surprises_for_machine/
Building an Animal Classifier with Tensorflow – Tensorpad – Medium,1548322154,,neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/ajarvc/building_an_animal_classifier_with_tensorflow/
DeepMind Vs StarCraft II,1548273539,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/aj3rko/deepmind_vs_starcraft_ii/
Speeding Up RNNs with CuDNN in keras,1548253496,,neuralnetworks,AlexBarbera,False,/r/neuralnetworks/comments/aj06rx/speeding_up_rnns_with_cudnn_in_keras/
Neural Network performance assessment help,1548245331,Can you suggest a statistical testing that can evaluate the performance of neural networks classifiers? ,neuralnetworks,SAMSAMSAM12345,False,/r/neuralnetworks/comments/aiz2id/neural_network_performance_assessment_help/
How we used AI to hybridize humans with cartoon animals and made a business out of it.,1548183388,,neuralnetworks,gabrelyanov,False,/r/neuralnetworks/comments/aiq35k/how_we_used_ai_to_hybridize_humans_with_cartoon/
Yelp: A Neural Net Killed Our App… /jk,1548174146,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/aiofyf/yelp_a_neural_net_killed_our_app_jk/
Analysing Cities on Zomato,1548157746,A fun study that i did over the weekend. Have a look [https://prakhartechviz.blogspot.com/2019/01/analysing-zomato.html](https://prakhartechviz.blogspot.com/2019/01/analysing-zomato.html),neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/aim361/analysing_cities_on_zomato/
Generating Images using Adversarial Autoencoders and Python,1548155771,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ailves/generating_images_using_adversarial_autoencoders/
Neural Network 2019 Calculations and behavior Artificial Intelligence Tutorial 2,1548096881,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/aidbh2/neural_network_2019_calculations_and_behavior/
Announcement: Face Editor Download,1548089121,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/aibw6j/announcement_face_editor_download/
Computer Vision - Improving Images,1548011013,[removed],neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/ai11gf/computer_vision_improving_images/
How Artificial General Intelligence might be created - Humanity’s quest to create machines that can think like us may be realized within our lifetimes thanks to cryptocurrency,1547939805,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/ahrjq6/how_artificial_general_intelligence_might_be/
Self Driving Cars [S1E2: ALVINN],1547917983,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ahnxm5/self_driving_cars_s1e2_alvinn/
Self Driving Cars [S1E1: The ALV],1547917578,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ahnv6s/self_driving_cars_s1e1_the_alv/
Resurces about simple backpropagation?,1547910371,"I started learning bare neural nets in java in my spare time 3 weeks ago. I would want to make the simple xor neural network as a starting point

I am stuck right now on backpropagation with multiple nets. I don't quite understand the matrix math behind it and would want to understand how it all works to program it myself

Are there any good resources i could use to learn simple backpropagation with simple matrix math? I can't really understand complex math formulas because i havent learned those yet ( i'm 15 ) . 

Thanks in advance, if you know any other or better subs i can post this in, please let me know.",neuralnetworks,inzywinki,False,/r/neuralnetworks/comments/ahmsju/resurces_about_simple_backpropagation/
3D Neural Network Sinulation Digit Recognition Computer Vision,1547891698,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/ahktkv/3d_neural_network_sinulation_digit_recognition/
Deep Learning State of the Art (2019) - MIT,1547824943,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ahbhor/deep_learning_state_of_the_art_2019_mit/
Reinforcement learning expedites 'tuning' of robotic prosthetics,1547823746,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ahba6a/reinforcement_learning_expedites_tuning_of/
Using a machine learning technique to make a canine-like robot more agile and faster,1547822387,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ahb24x/using_a_machine_learning_technique_to_make_a/
What do neural networks learn?,1547780439,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ah5zf1/what_do_neural_networks_learn/
(Help) Looking for examples of neural networks that...,1547729084,"I want to train a neural network that can map a set of numerical input values to some output values. I have around 150 data sets. Then, when I give new input, I want the neural network to workout corresponding output.

Is this difficult? Do you have some examples that I could look into? Thanks.",neuralnetworks,aaaaa47,False,/r/neuralnetworks/comments/agxeqq/help_looking_for_examples_of_neural_networks_that/
"Interview with Emrah Gultekin, Co-founder of Chooch, an AI training platform for visual recognition, based in San Francisco.",1547717311,,neuralnetworks,Batareika_1,False,/r/neuralnetworks/comments/agw2js/interview_with_emrah_gultekin_cofounder_of_chooch/
Now AI will shoot your vlog,1547672085,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/agponc/now_ai_will_shoot_your_vlog/
Mathematical intuition/reason for stacking 2 convolutional layers before max-pooling,1547638132,"Hi guys,

Since I'm in the deep learning (audio) field I often encouter neural network architecture with the following layers :

\- conv   
\- conv   
\- max-pooling   
\- conv  
\- conv  
\- max-pooling

\- ...

&amp;#x200B;

The second convolutional layers in one block is often with smaller feature map. I found this works better like that, but is there any mathematical proof/reason for that architecture ?

Thanks",neuralnetworks,peehay,False,/r/neuralnetworks/comments/agk8rs/mathematical_intuitionreason_for_stacking_2/
Mathematical intuition/explanation for stacking 2 convolutional layers before maxpooling,1547637421,"Hi guys,

Since I'm in the deep learning (audio) field I often encouter neural network architecture with the following layers :

\- conv 3\*3, 64  
\- conv 3\*3, 32  
",neuralnetworks,peehay,False,/r/neuralnetworks/comments/agk60h/mathematical_intuitionexplanation_for_stacking_2/
DeepMind AlphaFold,1547592406,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/agedb8/deepmind_alphafold/
Introduction to Adversarial Autoencoders,1547539777,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/ag6aft/introduction_to_adversarial_autoencoders/
Question about the mathematics dealing with this neural network,1547536956,"Here is the math in question: https://imgur.com/a/J5vjCj4 , labeled from step 1 to step 5. The math is taken from a book called ""Make Your Own Neural Network"" by Tariq Rashid.

* In picture 1 (or step 1): I know the partial derivative of E / the partial derivative of Wsub j,k is the change of E (error) with respect to some weight j going to the node k. And basically 'E' is rewritten as the cost function it is supposed to represent, with t = target and o = output. This step makes sense to me.

* In step 2: This is where it begins to fall apart. I know he used to the chain rule to differentiate the equation seen in step 1 to get to this point. I know how to use the chain rule on simpler equations that don't use partial derivatives, however I am a bit lost when it comes to an equation like this. How is this done? 

Thanks",neuralnetworks,iloveyouyes,False,/r/neuralnetworks/comments/ag605o/question_about_the_mathematics_dealing_with_this/
PlaidML Deep Learning Framework Benchmarks With OpenCL On NVIDIA &amp; AMD GPUs,1547510201,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/ag2665/plaidml_deep_learning_framework_benchmarks_with/
Can We Build a Brain? - Wonders,1547478618,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/afwoji/can_we_build_a_brain_wonders/
Word Embeddings with Gensim,1547382681,[https://prakhartechviz.blogspot.com/2019/01/word-embeddings-gensim.html](https://prakhartechviz.blogspot.com/2019/01/word-embeddings-gensim.html),neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/afixow/word_embeddings_with_gensim/
Comuter Vision,1547380766,Can somebody brief me with some cool ideas on Computer vision using image processing.Asap,neuralnetworks,xyakik,False,/r/neuralnetworks/comments/afiqdh/comuter_vision/
Change hair color in real time. Artificial intelligence can do anything!,1547369796,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/afhmn6/change_hair_color_in_real_time_artificial/
Introduction to Neural Machine Translation,1547267238,[http://prakhartechviz.blogspot.com/2019/01/a-concise-introduction-to-neural.html](http://prakhartechviz.blogspot.com/2019/01/a-concise-introduction-to-neural.html),neuralnetworks,prakhar21,False,/r/neuralnetworks/comments/af3wpo/introduction_to_neural_machine_translation/
"[Advice] Roughly how long should I expect me to take to complete the Deep Learning (MIT Press) book if I have an average undergraduate level of understanding of Probability, Statistics and Linear Algebra?",1547193609,"I know this isn't the right way to think about it. I'm just making a rough time table for myself to finish a decent sized chunk of the field as a newcomer, by this May.

If anyone here has completed reading the book, please let me know how long it took you to go through it. Thank you!",neuralnetworks,Akainu18448,False,/r/neuralnetworks/comments/aet75r/advice_roughly_how_long_should_i_expect_me_to/
AI Uses Face Scans to Diagnose Rare Genetic Disorders,1547154301,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/aenh3s/ai_uses_face_scans_to_diagnose_rare_genetic/
Stanford Open-Sources Neural Networks Verification Project,1547136843,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/aekgl6/stanford_opensources_neural_networks_verification/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1547103083," The neural network market is having a high CAGR of 35.34% during the forecast period 2019-2025.  

Request a sample @  https://www.envisioninteligence.com/industry-report/global-neural-network-software-market/?utm\_source=redit-chitti ",neuralnetworks,lashya_envi,False,/r/neuralnetworks/comments/aegj6t/global_neural_network_software_market_size/
Hey ! I just finished building my first neural network from scratch and I was wondering if my method is the right one,1547094531,So what I do is for each neuron I take each input and multiply it by a weight then add all these and add a bias. So for a neuron with 10 input there would be 10 weights and 1 bias is that ok ok should it be 1 weight and 1 bias per neuron ? ,neuralnetworks,AntoiineG,False,/r/neuralnetworks/comments/aefgpf/hey_i_just_finished_building_my_first_neural/
Painted Portraits Inspired By Neural Net Trained on Artist’s Facebook Photos,1547072315,,neuralnetworks,hoopism,False,/r/neuralnetworks/comments/aec441/painted_portraits_inspired_by_neural_net_trained/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1547024030,[removed],neuralnetworks,stprulingothers,False,/r/neuralnetworks/comments/ae50ra/global_neural_network_software_market_size/
Tips for implementing SSD Object Detection (with TensorFlow code),1546971167,,neuralnetworks,sabalaba,False,/r/neuralnetworks/comments/adx3vq/tips_for_implementing_ssd_object_detection_with/
"Global Neural Network Software Market - Size, Outlook, Trends and Forecasts",1546945931,[removed],neuralnetworks,sad_work,False,/r/neuralnetworks/comments/adt8m2/global_neural_network_software_market_size/
Using OpenVINO with OpenCV,1546902316,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/adnhwo/using_openvino_with_opencv/
Human-in-the-loop for object detection with Supervisely and YOLO v3,1546896718,,neuralnetworks,borisovyy,False,/r/neuralnetworks/comments/admirk/humanintheloop_for_object_detection_with/
Kaggle: How to Place in the Top 10% (part 1),1546889967,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/adlazp/kaggle_how_to_place_in_the_top_10_part_1/
Good Read For ML - Bias-Variance Tradeoff,1546863756,“Understanding the Bias-Variance Tradeoff” by Seema Singh https://link.medium.com/0GvEqExChT,neuralnetworks,ssvasista,False,/r/neuralnetworks/comments/adh6in/good_read_for_ml_biasvariance_tradeoff/
A framework for trusted pretained neural networks – How a repository of trained data will help advance humanity towards the Singularity,1546578406,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/acerge/a_framework_for_trusted_pretained_neural_networks/
Deep Reinforcement Learning,1546551910,"Hi all,

I am a beginner in deep reinforcement learning, however I have a strong foundation in behavioral neuroscience, as this is my field of study. I have been self-teaching myself with the basics of Q learning and one topic that I keep coming across is how a Q value is computed.

Most papers and tutorials that I read refer to the following calculation:

Q-value for state and action = reward + discount \* max Q-value for next state

My question is: How does one find the maximum Q-value for the next state? Is this merely an estimate or is there some concrete value associated with it? In behavioral neuroscience, we have something called the reward prediction error which drives learning. Is the max Q-value for the next state analogous to this? Additionally, how can this maximum Q value for the next state being calculated using the values of the present state?",neuralnetworks,Jerk464,False,/r/neuralnetworks/comments/acamnj/deep_reinforcement_learning/
InstaGAN Excels in Instance-Aware Image-To-Image Translation,1546545503,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/ac9hot/instagan_excels_in_instanceaware_imagetoimage/
Data Science Project Flow for Startups,1546516841,,neuralnetworks,shaypal5,False,/r/neuralnetworks/comments/ac52qc/data_science_project_flow_for_startups/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1546494885," The neural network market is having a high CAGR of 35.34% during the forecast period 2019-2025.  

Request a sample @  https://www.envisioninteligence.com/industry-report/global-neural-network-software-market/?utm\_source=redit-chitti ",neuralnetworks,uday_03,False,/r/neuralnetworks/comments/ac2n7t/global_neural_network_software_market_size/
DeepDream Creator Unveils Very First Images After Three Years,1546475323,,neuralnetworks,hoopism,False,/r/neuralnetworks/comments/abzqwi/deepdream_creator_unveils_very_first_images_after/
Tell me about art style transfer.,1546450265,"Hi, I currently have access to 1000s of woodblock prints and was reading about art style transfers. I can see sites such as deepart.io that can transfer between any art styles, how would training a network with a single style such as the 1000s of images I have access to effect the outcome of such a tool?

Thanks.",neuralnetworks,Naraaaw,False,/r/neuralnetworks/comments/abvi09/tell_me_about_art_style_transfer/
How would we find a better activation function than ReLU?,1546400609,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/abphkp/how_would_we_find_a_better_activation_function/
Simultaneous Control of Multiple Functions of Bionic Hand Prostheses Performance and Robustness in E,1546386796,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/abnj52/simultaneous_control_of_multiple_functions_of/
Quick question on cost for multi layer perceptrons,1546377927,"Apologies if this is the wrong subreddit, if it is please inform me of where I have to go.

Once you have calculated the cost for all the weights, neurons and biases for a specific layer and perhaps the average of all those over a batch.

How do you find the Delta (change) value for weights, biases in order to influence the next layer. e.g. how to calculate the number I need to add to each individual weight and bias to influence the next layer to the desired output.

And how do you find the value for the neuron so you can recursively apply backpropigation.",neuralnetworks,ReadAParadox,False,/r/neuralnetworks/comments/abm6bk/quick_question_on_cost_for_multi_layer_perceptrons/
"Any simulations of ""drunken"" neurons?",1546362476,"Alcohol [inhibits signals](https://sites.duke.edu/apep/module-2-the-abcs-of-intoxication/content-alcohol-disrupts-the-communication-between-neurons/) between biological neurons.  Has anyone tried training an artificial neural network, then somehow modifying the neuron activation functions to simulate drunkenness?  

A ""drunken"" ANN might be interesting!  

(My Google searches only returned things like articles about training ANNs to recognize drunken humans.)  ",neuralnetworks,stankind,False,/r/neuralnetworks/comments/abjrdz/any_simulations_of_drunken_neurons/
"Reinforcement Learning (RL), Deep RL and LSTM Tutorial With Sample Codes/Demo",1546347239," There are many RL tutorials, courses, papers in the internet. This one summarizes all of the RL tutorials, RL courses, and some of the important RL papers including sample code of RL algorithms. It will continue to be updated over time.

[https://github.com/omerbsezer/Reinforcement\_learning\_tutorial\_with\_demo](https://github.com/omerbsezer/Reinforcement_learning_tutorial_with_demo)

Extra: LSTM and RNN Tutorial with Python Demo Code (with Stock/Bitcoin Time Series Prediction, Sentiment Analysis, Music Generation)

[https://github.com/omerbsezer/LSTM\_RNN\_Tutorials\_with\_Demo](https://github.com/omerbsezer/LSTM_RNN_Tutorials_with_Demo)",neuralnetworks,obsezer,False,/r/neuralnetworks/comments/abhzj4/reinforcement_learning_rl_deep_rl_and_lstm/
Why deep neural networks don’t actually think – Shallow Thoughts about Deep Learning,1546313256,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/abehyh/why_deep_neural_networks_dont_actually_think/
A visual proof that neural nets can compute any function,1546307850,,neuralnetworks,ubershmekel,False,/r/neuralnetworks/comments/abdulx/a_visual_proof_that_neural_nets_can_compute_any/
NVIDIA “Super SloMo” Makes Video Smooth,1546289567,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/abb4u1/nvidia_super_slomo_makes_video_smooth/
2018 In Review: 10 Open-Sourced AI Datasets,1546274201,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/ab8ofb/2018_in_review_10_opensourced_ai_datasets/
Getting all the tools installed for ML and DL is one of the major hurdles that prohibits many from getting started. This is easily one of the best resources that I've found if you are looking to get everything set up. Super straightforward.,1546270460,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/ab83ra/getting_all_the_tools_installed_for_ml_and_dl_is/
Neural Networks that Grow – Shallow Thoughts about Deep Learning,1546179405,,neuralnetworks,shamoons,False,/r/neuralnetworks/comments/aaw6yi/neural_networks_that_grow_shallow_thoughts_about/
"What's the best way to get into ML? Well, if you're looking to get your feet wet ASAP, you should consider learning APIs like Keras as opposed to backends like Tensorflow and Theano. Here's a good video that explains exactly what Keras is",1546092528,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/aalgy8/whats_the_best_way_to_get_into_ml_well_if_youre/
How the Artificial Intelligence Program AlphaZero Mastered Its Games,1546086597,,neuralnetworks,bartturner,False,/r/neuralnetworks/comments/aakv1q/how_the_artificial_intelligence_program_alphazero/
coding by to design,1546044931,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/aagcl1/coding_by_to_design/
White Hats Vs CAPTCHAs,1545926563,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/aa0dx9/white_hats_vs_captchas/
Comparing trained neural network models,1545915373,"Comparing trained neural network models:

Is there a way to compare the trained models of multiple neural networks?

I want to see if two randomly initiated but trained on the same set develope the same strategy for predicting the outcome. 

I've looked for it on the net but haven't found any articles or tutorials on this topic.

Would apprechiate it if you helped me out here :)",neuralnetworks,DeutschlandMehl,False,/r/neuralnetworks/comments/a9z0g2/comparing_trained_neural_network_models/
"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting and educative",1545915318,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/a9z08z/this_video_goes_over_a_model_that_predicts_the/
"New to neural networks, would like to try to make one learn to play a game like MarI/O - any suggestions for games that would be good for this?",1545871298,[removed],neuralnetworks,Neuree,False,/r/neuralnetworks/comments/a9u5ni/new_to_neural_networks_would_like_to_try_to_make/
GitHub - ahmedbesbes/Neural-Network-from-scratch:,1545856749,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a9s53u/github_ahmedbesbesneuralnetworkfromscratch/
Bees can count with just four nerve cells in their brains,1545854600,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a9rter/bees_can_count_with_just_four_nerve_cells_in/
"This video goes over a model that predicts the number of views on a youtube video based on likes, dislikes, and subscribers. Really interesting",1545850431,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/a9r76j/this_video_goes_over_a_model_that_predicts_the/
We Launched An Animoji-Like App On Old Smartphones Using Convolutional Neural Networks,1545829206,,neuralnetworks,gabrelyanov,False,/r/neuralnetworks/comments/a9odoi/we_launched_an_animojilike_app_on_old_smartphones/
What's the big picture of using neural networks ?,1545800496,"All i see is a bunch of numbers that doesn't really explain how is this can be used for developing an AI.   


It doesn't need to be a robot, but im looking for an AI app for the desktop. Not Siri, or any of that crap. I want to develop something that can learn over a period of time, and do mundane desktop tasks. ",neuralnetworks,EliHood,False,/r/neuralnetworks/comments/a9loo5/whats_the_big_picture_of_using_neural_networks/
Help on how to choose my network's parameters,1545765550,[removed],neuralnetworks,neuron837839,False,/r/neuralnetworks/comments/a9hgvf/help_on_how_to_choose_my_networks_parameters/
A Guide for Building Convolutional Neural Networks,1545729607,,neuralnetworks,christiano,False,/r/neuralnetworks/comments/a9dm9v/a_guide_for_building_convolutional_neural_networks/
interesting paper for beginner,1545701365,Can you guys share some interesting papers for beginners?,neuralnetworks,shekharsr,False,/r/neuralnetworks/comments/a9as9e/interesting_paper_for_beginner/
What Procedures should i follow to predict on-site injuries using predictive analytics model(DEEP learning approach)?,1545657923,I'm beginner in DL domain.,neuralnetworks,Lazyboi98,False,/r/neuralnetworks/comments/a94yga/what_procedures_should_i_follow_to_predict_onsite/
Ergonomic ndarrays and deep learning in a compiled language with char-rnn example,1545650521,,neuralnetworks,Karyo_Ten,False,/r/neuralnetworks/comments/a947o3/ergonomic_ndarrays_and_deep_learning_in_a/
Neural network to bypass audio copyright,1545631942,"Prompt whether there is a neural network which is able to change a sound so that algorithms of Instagram couldn't block. I'm against copyright infringement, just asking out of scientific interest.  :)",neuralnetworks,akiramatsumi,False,/r/neuralnetworks/comments/a92gjc/neural_network_to_bypass_audio_copyright/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1545631682,"The neural network market is having a high CAGR of 35.34% during the forecast period 2019-2025.  

&amp;#x200B;

Request a sample @  https://www.envisioninteligence.com/industry-report/global-neural-network-software-market/?utm\_source=reddit-hema 

#  ",neuralnetworks,hema8ent,False,/r/neuralnetworks/comments/a92fhh/global_neural_network_software_market_size/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1545630465," The neural network market is having a high CAGR of 35.34% during the forecast period 2019-2025.  

Request a sample @  https://www.envisioninteligence.com/industry-report/global-neural-network-software-market/?utm\_source=reddit-chitti ",neuralnetworks,uday_03,False,/r/neuralnetworks/comments/a92ap9/global_neural_network_software_market_size/
Why is ReLU better than a purely linear activation function?,1545627127,"My understanding is that ReLU (rectified linear unit) activation can be described as:

a = max(0, z)

How is that substantially better than:

a = z

Or even:

a = k*z (where k is some constant)

Apparently ReLU activation performs amazingly well compared to tanh or sigmoid, so why not go all the way linear with it?",neuralnetworks,shamoons,False,/r/neuralnetworks/comments/a91wqq/why_is_relu_better_than_a_purely_linear/
How Neural Networks Work- Simply Explained!,1545616855,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/a90op6/how_neural_networks_work_simply_explained/
My second idea for practising NN - vehicle detection in game engine,1545568795,,neuralnetworks,Rayterex,False,/r/neuralnetworks/comments/a8ud4z/my_second_idea_for_practising_nn_vehicle/
Can RNNs be better hypernetworks?,1545553461,"I've been working on a hypernetwork that outputs a neural network that takes in 25088 values as inputs, has a 512 neuron hidden layer, and has a 1 neuron output.

&amp;#x200B;

The generation of the parameters for the last layer is simple - a hypernetwork only has to output 513 values (512 weights and 1 bias). All it requires is 2 hidden layers and 513 neurons with linear activations as outputs. The hypernetwork's input size is also 25088, like the network it outputs.

&amp;#x200B;

The problem is when the second-to-last layer (or the hidden layer) has to be generated by the hypernetwork. The second layer will have 25088 × 512 + 512 parameters to be outputted, which is 12,845,568 parameters - almost 13 million.

&amp;#x200B;

This is absolutely enormous. What I ended up doing was dividing these 13 million parameters into 1000 different parts. I would then make 1000 separate neural networks to handle each of these 1000 parts. So, one neural network only had to output around 13,000 values.

&amp;#x200B;

This worked, but training 1000 neural networks on 100 training samples (each training sample having 25088 values as an input and one-thousandth of a neural network as an output) took one week to train and took up 1.5 Terabytes of storage. I wish to repeat the process because the hypernetwork technique was extremely effective in a problem that I was attempting to solve.

&amp;#x200B;

More recently, I read [a blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy where he talks about how RNNs can be used for non-temporal, and even non-sequential (in some sense) data. I'm not too experienced with RNNs. I am hoping that I could use 1 relational neural network that outputs 25089 values. These values are the weights and the bias of one of the 512 neurons of the hidden layer. The RNN could then switch to the second neuron and output those values. After that, it could repeat the process for all the 512 neurons. Each batch of outputs is a member of a 'sequence' of sorts. Instead of dividing the task amongst many networks, can one RNN be used to generate the values for each of the neurons one at a time?

&amp;#x200B;

Again, I am not very experienced with RNNs and if my question is too vague, I'll clarify it.",neuralnetworks,botperson,False,/r/neuralnetworks/comments/a8t5dr/can_rnns_be_better_hypernetworks/
Text Generation with Bidirectional RNN?,1545538144,"I'm a bit confused with how one would use a bidirectional RNN for text generation given that the sequence is not known ahead of time. From my understanding,  sampled output from the RNN at a time-step should be fed in as input at the next time step. The backwards layer of RNN units require input from future time steps, which haven't been generated yet. 

&amp;#x200B;

However,  there's a lot of resources online that suggest this is possible - for example, [this article](https://github.com/campdav/Text-Generation-using-Bidirectional-LSTM-and-Doc2Vec-models/blob/master/1%20-%20generate%20sentence.ipynb), which directly uses bidirectional RNN's, and [this repo](https://github.com/minimaxir/textgenrnn), which contains a generative model with bidirectional options. How does this work exactly? The article is not clear, and I'm not familiar with Keras. ",neuralnetworks,good_rice,False,/r/neuralnetworks/comments/a8rp8d/text_generation_with_bidirectional_rnn/
Tensorflow - tf.map_fn() doesn't work as expected,1545483103,[removed],neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/a8kl8o/tensorflow_tfmap_fn_doesnt_work_as_expected/
"I trained an LSTM on a function, such that it predicts the next data point after being fed the previous 50! Every frame is another epoch of training!",1545469853,,neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/a8jgh0/i_trained_an_lstm_on_a_function_such_that_it/
Top 35 AI solutions for today's key industries,1545436138,,neuralnetworks,hoaphumanoid,False,/r/neuralnetworks/comments/a8fs56/top_35_ai_solutions_for_todays_key_industries/
“Math Is Forever” - Talk with NeurIPS 2018 Best Paper Team,1545424670,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a8e50v/math_is_forever_talk_with_neurips_2018_best_paper/
"[xpost] ""Amoeba finds approximate solutions to NP-hard problem in linear time"" [Interesting video, regardless of how to interpret the finding]",1545351775,,neuralnetworks,wootcrisp,False,/r/neuralnetworks/comments/a84g1y/xpost_amoeba_finds_approximate_solutions_to/
"References similar to ""Efficient BackProp"" by LeCun",1545304710,"I'm looking for more recent references that have discussions in the spirit of this one:

* LeCun et al. (2012). Efficient backprop. In Neural networks: Tricks of the trade (pp. 9-48). Springer, Berlin, Heidelberg

I.e. that discuss the principles and tricks involved in training NNs with some depth.

  
I'm starting research in the field and would like a more in-depth, up-to-date understanding of the techniques involved (such as batch normalization, practical differences between activation functions, etc.)

Can anyone point me in the right direction?",neuralnetworks,pedrosidra0,False,/r/neuralnetworks/comments/a7wxec/references_similar_to_efficient_backprop_by_lecun/
✏️ Advanced annotation tools in Deep Learning: training data for computer vision with Supervisely,1545297831,,neuralnetworks,borisovyy,False,/r/neuralnetworks/comments/a7w7j7/advanced_annotation_tools_in_deep_learning/
Probability of an output neurons for the same input,1545290983,"Hello,
I need a neural network that instead of giving me the output value of each output neuron, it will give me the probability of each possible (and unpossible) output. The possible output with the highest probability is like the output of a regular neural network. For *the same input*, there are several probably right outputs. One output seems to be right, but also another – very different one seems to be high, so they both have high probability.
What is the probability you know the answer?
Thank you.",neuralnetworks,amitgoren,False,/r/neuralnetworks/comments/a7vkbc/probability_of_an_output_neurons_for_the_same/
GPUs Outperform Current HPC and Neuromorphic Solutions in Terms of Speed and Energy When Simulating a Highly-Connected Cortical Model,1545241492,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a7onla/gpus_outperform_current_hpc_and_neuromorphic/
NeurIPS 2018 Through the Eyes of First-Timers,1545239212,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a7oan9/neurips_2018_through_the_eyes_of_firsttimers/
Hybrid neural network,1545227948,"I am just starting working with ML and neural networks, but have experience at classifying, predicting and classifying images. However, I find it very boring, so I've started a projest, which should mimic the way a brain works.

The idea is to have all the inputs (sound, video stream) and make individual little nets for every input, with corresponding efficient architectures. All of them would lead to a multilayer perceptron, from which, using again, the most efficient intermediate small nets, we get ourselves outputs. 

In my mind, this should have a decent learning potential, maybe even for multiple tasks at once...  
But since I'm no specialist, is anyone interested in making this thing? Or maybe you'll give me some advices?  
",neuralnetworks,SpicyStain,False,/r/neuralnetworks/comments/a7mms6/hybrid_neural_network/
New AI system mimics how humans visualize and identify objects,1545189029,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a7igda/new_ai_system_mimics_how_humans_visualize_and/
Toward Brain-like Computing: New Memristor Better Mimics Synapses,1545187954,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a7ib4a/toward_brainlike_computing_new_memristor_better/
Need Some Help With Neural Network In Java,1545159384,"So  I'm trying to create a basic multi-layer perceptron neural network in  Java from scratch. I'm still relatively new to programming, but I feel I  have enough experience now to tackle this kind of problem. However,  after going at this project for over a month now, I have not been able  to get this to work properly except on a very simple network (1 input  neuron, 1 hidden layer neuron, and 1 output neuron). I'm trying to get  it to learn XOR for now, but I can't seem to get the weights to update  properly. Forward propagation works right as far as I can tell, but I'm  pretty sure I'm missing on some key elements in back propagation, I just  can't figure out what, even after a month of research online. I'm  basing this network off of what I have learned watching 3blue1brown's  videos on neural networks on YouTube. Maybe I'm completely using the  wrong values in the back propagation algorithm, or maybe my algorithm is  completely wrong altogether. I've tried adjusting the learning rate and  range of the weight initializations, and even set all the biases to 0  for now as I am not updating those yet, so  if you can tell me how I  would update biases, that would also be awesome.

I  know this probably is not the nicest looking code, but I really am just  trying to get something to work for now, from which I would streamline  it more and make it a bit more dynamic. I don't want to explicitly use  matrices yet to represent the weights and other things, as  I prefer the  way I have done it. It makes more sense in my head and its easier to  comprehend everything for me just using arrays for each neuron/neuron  layer. Thank you in advance for any help you can offer me.

Pastebin Links:

Neural Network Class: [https://pastebin.com/jbzbytA6](https://pastebin.com/jbzbytA6)

Neuron Class: [https://pastebin.com/rZQuVETV](https://pastebin.com/rZQuVETV)

Training Data Class: [https://pastebin.com/Lar45CKz](https://pastebin.com/Lar45CKz)",neuralnetworks,Gamerbot4000,False,/r/neuralnetworks/comments/a7dzmu/need_some_help_with_neural_network_in_java/
2018 in Review: 10 AI Quotes,1545147601,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a7c4e0/2018_in_review_10_ai_quotes/
Free credits for cloud GPU instances for deep learning,1545146681,"Hi, I am offering free credits for you to access 1080Ti GPU instances for deep learning purposes.

I am a co-founder of Tensorpad; where we are creating a service for    AI startups to train neural networks. We have paid traffic, but some    servers are idle. Hence, we are offering some credits for free, so that    students, startups, and others can benefit from ML technologies and   help  us by using our product and providing honest feedback for us to    improve.

You can Sign Up at [https://dashboard.tensorpad.com/signup](https://dashboard.tensorpad.com/signup) and redeem the code ""REDDIT200"" in the Billing tab.

Hope this explains our story and motivation for providing free credits.

Here is additional information:  

\* The instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run multiple instances in parallel  

\* You get access to the JupyterLab environment  

\* We have pre-installed Tensorflow, Keras, and other ML frameworks  

\* You can access the terminal through the JupyterLab  

\* By default, persistent storage is enabled  

\* Here are the available software versions [https://docs.tensorpad.com/jobs\_env/](https://docs.tensorpad.com/jobs_env/) For extra free trial hours, use promo code: reddit200

And for questions, please contact me at \[[ilie@tensorpad.com](mailto:ilie@tensorpad.com)\](mailto:[ilie@tensorpad.com](mailto:ilie@tensorpad.com)). I am looking forward to seeing you on our platform! Sincerely, Ilie Diacov Co-founder and UX researcher at Tensorpad",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/a7bzh1/free_credits_for_cloud_gpu_instances_for_deep/
Nvidia learned to make realistic faces,1545146268,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/a7bxd0/nvidia_learned_to_make_realistic_faces/
Data Science in 4 Minutes: Quick High Level Overview,1545085604,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a74pb9/data_science_in_4_minutes_quick_high_level/
[BEGINNER TUTORIAL] Build a lane detector,1545078931,"Step-by-step beginners tutorial on building a lane detector.

[https://medium.com/@chuanenlin/tutorial-build-a-lane-detector-679fd8953132](https://medium.com/@chuanenlin/tutorial-build-a-lane-detector-679fd8953132)

Feel free to check it out and kindly smash the clap button if you found it useful!",neuralnetworks,affinitive2,False,/r/neuralnetworks/comments/a73n0t/beginner_tutorial_build_a_lane_detector/
is there a function like softmax function but also gives negative values?,1545048182,"for MLP or CNN or RNN....etc, I'm trying to find a function like softmax function, but also can give negative values?

precisely, such that the L1 norm is no more than 1

(which is useful for when negative signs actually mean something, especially under reinforcement learning framework)

I thought of just taking the outputs and normalize it by L1 norm (process similar to softmax function) and arbitrarily define the derivatives of abs(x) when x = 0, to 0

Would such method work? Or is there some alternative method?",neuralnetworks,qudcjf7928,False,/r/neuralnetworks/comments/a6z8eg/is_there_a_function_like_softmax_function_but/
Help me to decide if I should remove few pictures on my training folder,1545043362,"Hi guys I have 30 different classes of images in my training folder but they don't have the same length as the others i.e car class is about 1000 pictures by 45x45 while others are 200 pictures by 45x45 and 20 pictures only,

&amp;#x200B;

So my quesiton is should I include those few classes on my training folder? I'm asking this because I cannot proceed cause am afraid that it might overfit on my model.  

&amp;#x200B;

Thank you!",neuralnetworks,Vinceeeent,False,/r/neuralnetworks/comments/a6yr3v/help_me_to_decide_if_i_should_remove_few_pictures/
"trying to build CNN on cifar10, using minimal parameters. any tips/ rule thumbs about hyper paraneters to increase accuracy?",1544990415,"trying to build CNN on cifar10, using minimal parameters. any tips/ rule thumbs about hyper paraneters and cnn architecture to increase test accuracy?

thanks in advance
",neuralnetworks,CoolDan123,False,/r/neuralnetworks/comments/a6sa34/trying_to_build_cnn_on_cifar10_using_minimal/
Robots were allowed to kill people. Machine learning,1544977468,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/a6qb0u/robots_were_allowed_to_kill_people_machine/
hand written characters/MNIST,1544970196," i am a   beginner.learnt basic cnn using MNIST. can anyone know how i can convert my own hand written digits/characters into Mnist type data file   or use my own jgp or png file to  train ?  
TIA",neuralnetworks,alpha_so,False,/r/neuralnetworks/comments/a6pcib/hand_written_charactersmnist/
Here I built a simple neural network with a relu activation function to classify the mnist dataset. It's accuracy is 96% on the training data and 93% on the test data. Take a look!,1544929082,,neuralnetworks,taylerallen6,False,/r/neuralnetworks/comments/a6llld/here_i_built_a_simple_neural_network_with_a_relu/
"keras multi input LSTM , performing worse than a random forest model ran on one of the inputs given to keras",1544901544,[removed],neuralnetworks,ObviousJackfruit,False,/r/neuralnetworks/comments/a6hxoz/keras_multi_input_lstm_performing_worse_than_a/
Idein Ideas — Released Actcast alpha,1544899338,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a6hmdz/idein_ideas_released_actcast_alpha/
BigGANs: AI-Based High-Fidelity Image Synthesis,1544897352,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a6hc73/biggans_aibased_highfidelity_image_synthesis/
A radical new neural network design could overcome big challenges in AI,1544836243,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/a6avvw/a_radical_new_neural_network_design_could/
GAN 2.0: NVIDIA’s Hyperrealistic Face Generator,1544829183,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a69yxo/gan_20_nvidias_hyperrealistic_face_generator/
Challenge: Outsmart Tumblr's censorbot,1544815249,"Tumblr's new NSFW policy has upset a lot of people.

According to the following post:

http://clish.tumblr.com/post/181113103950/once-i-encountered-the-funny-story-of-an-ai-image

User crazy-pages says that the bot they are using is an open source, ""one-layer, unidirectional, bicategory, tag-trained, neural network.""

Source: https://github.com/yahoo/open_nsfw

They also suggest that a counter-neural network could be created that ""puts an imperceptible (to humans) layer of patterned static over arbitrary images"" and is trained by ""having it bot-post static-ed images to Tumblr and [be reinforced] based on whether the images are labeled nsfw or sfw.""

Would anyone be up to the challenge of creating the above described network, and then a basic program which uses the network to turn nsfw images into ""sfw""?",neuralnetworks,styrofoammicrophone,False,/r/neuralnetworks/comments/a67xb7/challenge_outsmart_tumblrs_censorbot/
Here is a 'bare-bones' approach to using python machine learning with Unity3D. This tutorial show only the necessities needed to use your own python agents in a Unity built environment.,1544755419,,neuralnetworks,taylerallen6,False,/r/neuralnetworks/comments/a60mkk/here_is_a_barebones_approach_to_using_python/
Bare necessities to train your own python agents in Unity3D!,1544741199,"Here is a 'bare-bones' approach to using python machine learning with Unity3D. This tutorial show only the necessities needed to use your own python agents in a Unity built environment.

https://github.com/taylerallen6/UnityML",neuralnetworks,taylerallen6,False,/r/neuralnetworks/comments/a5ymz0/bare_necessities_to_train_your_own_python_agents/
Geometric Deep Learning,1544663227,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a5ohfs/geometric_deep_learning/
Alex Graves and Marc'Aurelio Ranzato: Unsupervised Deep Learning (NeurIPS 2018 Tutorial),1544657094,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a5nmil/alex_graves_and_marcaurelio_ranzato_unsupervised/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1544600966,"The neural network market is having a high CAGR of 35.34% during the forecast period 2019-2025.  

&amp;#x200B;

Request a sample @  https://www.envisioninteligence.com/industry-report/global-neural-network-software-market/?utm\_source=reddit-hema ",neuralnetworks,nhemaei,False,/r/neuralnetworks/comments/a5g3v0/global_neural_network_software_market_size/
Machine Learning and the Mathematics Needed to Make a Computer Learn,1544453698,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a4wb9m/machine_learning_and_the_mathematics_needed_to/
Neural network game titles from 17000 listings on BoardGameGeek,1544322726,,neuralnetworks,dontnormally,False,/r/neuralnetworks/comments/a4h8cy/neural_network_game_titles_from_17000_listings_on/
Computer vision in the dark using recurrent CNNs,1544299508,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a4e0vn/computer_vision_in_the_dark_using_recurrent_cnns/
"AlphaZero AI system able to teach itself how to play games, play at highest levels",1544299477,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a4e0pr/alphazero_ai_system_able_to_teach_itself_how_to/
"Deep Graph Library, a Python Package For Graph Neural Networks",1544217696,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a448qo/deep_graph_library_a_python_package_for_graph/
How Augmented Analytics can automate resource-intensive data analysis tasks.,1544185735,,neuralnetworks,Victor_Stakh,False,/r/neuralnetworks/comments/a3zfch/how_augmented_analytics_can_automate/
I don’t know a lot about AI or ML but...,1544169204,"If AI can achieve such powerful things like altering pictures and videos realistically, or analyzing and predicting objects via sensors, then what is stopping someone from creating a reinforced-learning AI that can read, evaluate, and retain text information, say off of Wikipedia or any other source of info?",neuralnetworks,andyman088,False,/r/neuralnetworks/comments/a3xu4v/i_dont_know_a_lot_about_ai_or_ml_but/
CNNs: practical appications,1544129418,"Hey, could anyone share some cool (uncommon) examples where convolutional neural networks could be applied in practice? Also, what one could create by combining CNNs and openCV library together? Basically, I am starting to dig deeper into this field and I would like to work on some cool project :) Thank you.

&amp;#x200B;",neuralnetworks,nolimitex,False,/r/neuralnetworks/comments/a3s54d/cnns_practical_appications/
Transferring from a Smaller Neural Net to a Larger Neural Net if first Net didn't provide a high enough accuracy?,1544122389,"Hello,

&amp;#x200B;

I have come across the interesting thought of running a set of images to be classified under a small neural network, with a few hidden layers, and nodes.. This net will attempt to achieve a 90-95% or higher probability of having classified the object accurately. However, if the small neural net was below that, the data would be transferred over to a larger neural net, containing more hidden layer, and nodes.   


Later on I will be adding localization (position, orientation) to these objects, as it is for a camera that will be attached to my robot.   


Does anyone have any pointers for me to start looking into for how to transfer my dataset from a small net to a larger one based on the accuracy of the smaller net?  


Thank you all!",neuralnetworks,Zyxeos,False,/r/neuralnetworks/comments/a3qy22/transferring_from_a_smaller_neural_net_to_a/
Can you train deep recurrent neural network layer by layer?,1544066501,"&amp;#x200B;

https://i.redd.it/3khb0zstrk221.png

 Specifically for Gated Recurrent Unit, and say GRU is ""layered"" via 

&amp;#x200B;

*Processing img 0f7tkphvrk221...*

 

but suppose it's only 2 layers deep for simplicity,

and suppose the ""total loss"" = L = ∑ l\_{t} = ∑ error(y\^{2}\_{t}), for all t, using the notations above,

Then I know how to train the weights for a single layer, so is it possible to train the GRU layer by layer?

In a sense that, find the derivatives wrt the 2nd layer's inputs, thus find ∂L∂/X\^{2}\_{t}, for all t,

which is identical to ∂L/∂h\^{1}\_{t}, (thus, the derivatives wrt the 2nd layer's GRU inputs are to be used as the derivatives wrt to the 1st layer's GRU's output) then train the 2nd layer's GRU, then using the derivatives wrt to the 2nd layer's inputs, use the same method that I used to train the 2nd layer, into training the 1st layer's GRU?",neuralnetworks,qudcjf7928,False,/r/neuralnetworks/comments/a3jzji/can_you_train_deep_recurrent_neural_network_layer/
The 'weird events' that make machines hallucinate,1544045650,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a3guxr/the_weird_events_that_make_machines_hallucinate/
Always getting the same Q-max in Q-learning algorithm,1544029677," I'm trying to implement a Q-learning neural network by following this article: [https://www.practicalai.io/teaching-a-neural-network-to-play-a-game-with-q-learning/](https://www.practicalai.io/teaching-a-neural-network-to-play-a-game-with-q-learning/) using [http://caza.la/synaptic/#/](http://caza.la/synaptic/#/) and you can see the progress here: [http://steenstn.github.io/qsnake.html](http://steenstn.github.io/qsnake.html) and the code here: [https://github.com/steenstn/steenstn.github.io/blob/master/qsnake.html](https://github.com/steenstn/steenstn.github.io/blob/master/qsnake.html)

Blue/green rectangle is the player. Blue means he took a random direction, green means he chose the network's calculated best direction. The goal is to get to the red rectangle. As the player moves around the calculated best direction for a tile(state) is drawn (\^, &lt;, v, &gt;). Key up/down to change simulation speed.

However, it seems that it always gets the same action as the best action. As you can see when running the program, it will get one direction as the best one for all the tiles. Sometimes it changes, but then this new direction will be selected for all the upcoming tiles.

I'm using a NN with an input layer of (levelWidth\*levelHeight+4) where the 4 last inputs are the direction the player should move (up, left, down, right), a hidden layer of the same size and an output layer which should represent the Q-value for the state. The inputs represent the world and are all zero, except for the position of the player and one of the last 4 inputs depending on chosen direction, they are set to 1.

After choosing a direction I calculate the reward and for the new state, check which action in the new state have the highest Q-value and use this value to backpropagate.

I cannot see what I'm doing wrong, my expectation would be that the calculated best direction would converge towards the red rectangle, not always be e.g. right and then switch and always be down.

I tried with replay memory too but had the same problem so I stripped that out to make the code easier to understand.

Lines 266-327 contains the Q-learning part.

Can anyone see what I'm doing wrong?",neuralnetworks,steenstn,False,/r/neuralnetworks/comments/a3e8om/always_getting_the_same_qmax_in_qlearning/
Possible optimization of classifier,1544026270,"Hey, 
Is there anything to be learned from [these graphs](https://imgur.com/gallery/943sBrO) of the training of a neural network? It is a feedforward neural network that performs a binary classification task. It has 20 input nodes, one hidden layer of 12 nodes and 1 output node. It uses a sigmoid activation for all nodes and binary crossentropy as the loss function with Adam.",neuralnetworks,lurking_sentinel,False,/r/neuralnetworks/comments/a3doe7/possible_optimization_of_classifier/
AI for skin cancer recognition is available free of charge at alpha.skychain.global,1544006338,,neuralnetworks,shik3,False,/r/neuralnetworks/comments/a3b61e/ai_for_skin_cancer_recognition_is_available_free/
Building a Curious AI With Random Network Distillation,1543967093,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a36e5c/building_a_curious_ai_with_random_network/
Free instances with 1080Ti for deep learning – trial credits,1543940270,"Hi, I am offering free credits for you to access 1080Ti GPU instances for deep learning purposes.

I am a co-founder of Tensorpad; where we are creating a service for  AI startups to train neural networks. We have paid traffic, but some  servers are idle. Hence, we are offering some credits for free, so that  students, startups, and others can benefit from ML technologies and help  us by using our product and providing honest feedback for us to  improve.

You can Sign Up at [https://dashboard.tensorpad.com/signup](https://dashboard.tensorpad.com/signup) and redeem the code ""REDDIT650"" in the Billing tab.

Hope this explains our story and motivation for providing free credits.

Here is additional information:  

\* The instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run multiple instances in parallel  

\* You get access to the JupyterLab environment  

\* We have pre-installed Tensorflow, Keras, and other ML frameworks  

\* You can access the terminal through the JupyterLab  

\* By default, persistent storage is enabled  

\* Here are the available software versions [https://docs.tensorpad.com/jobs\_env/](https://docs.tensorpad.com/jobs_env/) For extra free trial hours, use promo code: REDDIT650

And for questions, please contact me at [ilie@tensorpad.com](mailto:ilie@tensorpad.com). I am looking forward to seeing you on our platform! Sincerely, Ilie Diacov Co-founder and UX researcher at Tensorpad",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/a323i2/free_instances_with_1080ti_for_deep_learning/
The First Interactive AI Rendered Virtual World,1543918489,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/a2zfpk/the_first_interactive_ai_rendered_virtual_world/
Free instances with 1080Ti for deep learning – trial credits,1543916573,"Hi, I am offering free credits for you to access 1080Ti GPU instances for deep learning purposes.

I am a co-founder of Tensorpad; where we are creating a service for AI startups to train neural networks. We have paid traffic, but some servers are idle. Hence, we are offering some credits for free, so that students, startups, and others can benefit from ML technologies and help us by using our product and providing honest feedback for us to improve.

You can Sign Up at https://dashboard.tensorpad.com/signup and redeem the code ""REDDIT650"" in the Billing tab.

Hope this explains our story and motivation for providing free credits.

Here is additional information: 
* The instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run multiple instances in parallel 
* You get access to the JupyterLab environment 
* We have pre-installed Tensorflow, Keras, and other ML frameworks 
* You can access the terminal through the JupyterLab 
* By default, persistent storage is enabled 
* Here are the available software versions https://docs.tensorpad.com/jobs_env/

For extra free trial hours, use promo code: REDDIT650

And for questions, please contact me at ilie@tensorpad.com.
",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/a2z936/free_instances_with_1080ti_for_deep_learning/
Optimizing weights for neural networks with stochastic gradient descent.,1543891472,"I am trying to optimize the weights of a neural network with 64 input neurons 10 output neurons and 2 hidden layers with 10 neurons each, using the softmax function and the cross entropy loss function how do I do this. ",neuralnetworks,teftef3000,False,/r/neuralnetworks/comments/a2w9jf/optimizing_weights_for_neural_networks_with/
NeurIPS 2018 Opens; Best Papers Announced,1543884453,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a2v8xo/neurips_2018_opens_best_papers_announced/
DeepMind AlphaFold Delivers “Unprecedented Progress” on Protein Folding,1543872286,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a2t7fb/deepmind_alphafold_delivers_unprecedented/
"Mini-batch, Stochastic &amp; Batch Gradient Descent | Tutorial",1543866726,,neuralnetworks,dprogrammerlopez,False,/r/neuralnetworks/comments/a2s931/minibatch_stochastic_batch_gradient_descent/
Indeed! Mesmerizing.,1543863055,,neuralnetworks,Tamoghna_Saha,False,/r/neuralnetworks/comments/a2rmwl/indeed_mesmerizing/
How to make a 3D filter on images with Keras,1543786315,"Hello, I am a high schooler taking a machine learning class at my local university. I am trying to make this CNN with keras and I need to have a filter of shape (12,12,3) but keras will only let me use 2D filters while using Conv2D. How may I use a 3D filter for this cnn?",neuralnetworks,qtum_quasar,False,/r/neuralnetworks/comments/a2hjqo/how_to_make_a_3d_filter_on_images_with_keras/
The Neural Aesthetic @ ITP-NYU :: 10 Reinforcement Learning &amp; Natural Language Processing,1543714760,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a2946r/the_neural_aesthetic_itpnyu_10_reinforcement/
Reading Minds with Deep Learning,1543690840,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a25q1l/reading_minds_with_deep_learning/
How to Get a Better GAN (Almost) for Free: Introducing the Metropolis-Hastings GAN,1543683269,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a24llv/how_to_get_a_better_gan_almost_for_free/
Neural networks and communication networks,1543536209,There are many types of NN. I wonder if one of them can be used to model a graph topology and act on a graph. Or any type that can be used to do topology optimization.,neuralnetworks,Beginner4ever,False,/r/neuralnetworks/comments/a1n0ab/neural_networks_and_communication_networks/
These people never existed!,1543530852,,neuralnetworks,cmillionaire9,False,/r/neuralnetworks/comments/a1m8i9/these_people_never_existed/
"At Flattening step, How to decide between reshaping tensor to column or row vector?",1543468992,"Please feel free to share any resource for understanding this concept.

Thanks!",neuralnetworks,redditaddict07,False,/r/neuralnetworks/comments/a1efx9/at_flattening_step_how_to_decide_between/
Lambda NeurIPS 2018 Ticket Contest for Three NeurIPS Tickets,1543451503,,neuralnetworks,sabalaba,False,/r/neuralnetworks/comments/a1c7bx/lambda_neurips_2018_ticket_contest_for_three/
Looking to get into (introductory) research with neural nets,1543429416,"I am a first-year university student with background in CS, and am planning on doing research with a faculty mentor next semester. A lot of the mentor's work centers around Twitter data analysis, and I was trying to think of a good project I could work on that uses Twitter data for the data set.

&amp;#x200B;

A couple things I have thought of are detecting sarcastic tweets or classifying political tweets (left or right leaning). I am looking for a topic that would be reasonable for myself (an amateur in NN) to complete within a semester, while still being challenging and nontrivial. Please let me know if you have any ideas!",neuralnetworks,ENM185,False,/r/neuralnetworks/comments/a18ucr/looking_to_get_into_introductory_research_with/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1543387292,"The neural network market is having a high CAGR of 35.34% during the forecast period 2019-2025.  

&amp;#x200B;

Request a sample @  https://www.envisioninteligence.com/industry-report/global-neural-network-software-market/?utm\_source=reddit-hema ",neuralnetworks,hemaei,False,/r/neuralnetworks/comments/a13qjy/global_neural_network_software_market_size/
Questions about Backpropagation Through Time for Gated Recurrent Unit?,1543378526,"&amp;#x200B;

https://i.redd.it/pmpqc2u7zz021.png

&amp;#x200B;

 

I'm trying to implement it myself so I can understand it more. I ended up deriving the gradients myself.

So my understanding is that if t = T is the terminal time index, and suppose you have forward passed GRU,

and    using the equations and notations above, you would find the gradient   of  error wrt h\_(T), then via chain rule, you can find dL/d(z\_(T)) and    dL/d(r\_(T)) .....etc and update the weights matrices and biases.

And    if it's a multi-layered GRU, then I would also find dL/d(x\_(T)) and    that would be the ""gradient of error wrt h\_(T)"" for the previous  layer's   GRU unit.

My  question is that  after  you update those matrices for t = T, then you  move on to t = T-1  and do  the same thing until t = 0 right? What about  h\_(T-1) ? when  I'm  calculating the gradients for the GRU unit for t = T,  do I also  have to  calculate the gradient wrt h\_(T-1) ? What do I do  with them  then?

Also,   for given t, and given GRU  unit, and given the gradients for the   weights, how many times am I  supposed to update the weights matrix? only   once and then move on to  T-1? if so, wouldn't that make it impossible   to use stuff like  adaptive learning rate method? which requires   information about the  previous gradient?",neuralnetworks,qudcjf7928,False,/r/neuralnetworks/comments/a12p98/questions_about_backpropagation_through_time_for/
"Montezuma’s Revenge Solved by Go-Explore, a New Algorithm for Hard-Exploration Problems (Sets Records on Pitfall, Too)",1543370236,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/a11l8x/montezumas_revenge_solved_by_goexplore_a_new/
How to Build Artificial General Intelligence AGI (Late 2018) | XUCHAT,1543341736,,neuralnetworks,jeffxu999,False,/r/neuralnetworks/comments/a0xann/how_to_build_artificial_general_intelligence_agi/
"A Facebook group meant to provide a place for people leading data science teams and groups, from anywhere in the world, to discuss common challenges together.",1543318205,,neuralnetworks,shaypal5,False,/r/neuralnetworks/comments/a0u0xg/a_facebook_group_meant_to_provide_a_place_for/
How to write efficient vectorized operations using openBLAS?,1543285003,"I am self-learning how to write efficient, optimized deep learning code; but I am very much a newbie at this.

&amp;#x200B;

For example: I am reading that \`numpy\` uses [vectorization](https://www.quora.com/Which-is-faster-numpy-vectorized-code-or-hand-written-C-code) to avoid [python loops](https://stackoverflow.com/a/47755634/1158977).

&amp;#x200B;

They have also pretty much coined the term [broadcasting](https://machinelearningmastery.com/broadcasting-with-numpy-arrays/) according to that link, which is used by TensorFlow, PyTorch and others.

&amp;#x200B;

I did some digging, and found that \`ldd\` on my Debian box shows \`multiarray.so\` links \`libopenblasp-r0-39a31c03.2.18.so\`.

&amp;#x200B;

So let's take the use case of a matrix subtraction.  I would like to understand how to use openBLAS to improve this very naive implementation:

&amp;#x200B;

`void matrix_sub(Matrix *a, Matrix *b, Matrix *res)`

`{`

`assert(a-&gt;cols == b-&gt;cols);`

`assert(a-&gt;rows == b-&gt;rows);`

`zero_out_data(res, a-&gt;rows, a-&gt;cols);`

`for (int i = 0; i &lt; (a-&gt;rows*a-&gt;cols); i++)`

`{`

`res-&gt;data[i] = a-&gt;data[i] - b-&gt;data[i];`

`}`

`}`

&amp;#x200B;

Like wise an inner product, or an addition?

&amp;#x200B;",neuralnetworks,autojazari,False,/r/neuralnetworks/comments/a0qalv/how_to_write_efficient_vectorized_operations/
FAIR Paper Questions Pre-Training’s Efficacy in Computer Vision,1543253227,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/a0ldku/fair_paper_questions_pretrainings_efficacy_in/
Need Help.,1543251589,"Hello all, 

I'm currently an undergraduate student studying computer science. I was looking for some guidance in the neuralnetwork field. Where do I start, what are some pre reqs that I MUST HAVE? All advice is welcome so please let me know!

thank you!",neuralnetworks,jonraysor,False,/r/neuralnetworks/comments/a0l408/need_help/
"DIfference between Keras dot layer, K.dot and K.batch_dot AND the use of axes parameter",1543240815,[removed],neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/a0jk6d/difference_between_keras_dot_layer_kdot_and/
Experience with Deep Fashion dataset,1543236286,"Hey guys,

I'd like to know if any of you have used the DeepFashion dataset for classification/image ranking . Its a major pain the ass to preprocess and would love to hear some of your inputs regarding the same.

 Also, what other Fashion/Apparel datasets have you come across/used/recommend?

&amp;#x200B;",neuralnetworks,aashwin93,False,/r/neuralnetworks/comments/a0j0yj/experience_with_deep_fashion_dataset/
Gradient Descent Question,1543200230," Suppose we have a single-layer perceptron with no bias term, k inputs, and a tanh nonlinearity. Let’s designate the output of the perceptron as f(w, x), where w = (w1, . . . , wk) are the weights and x = (x1, . . . , xk) are the inputs. 2. Write two expressions for f(w, x), one using a dot product and one using an explicit sum. 3. What is ∇wf, that is, what are the partial derivatives with respect to each of the weights? ",neuralnetworks,memesap,False,/r/neuralnetworks/comments/a0f8ul/gradient_descent_question/
"For anyone looking to get into deep learning, I would advise that you consider not learning the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",1543171249,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/a0b0u2/for_anyone_looking_to_get_into_deep_learning_i/
My implementation of QuickDraw - an online gamed developed by Google (https://github.com/1991viet/QuickDraw),1543160197,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/a09din/my_implementation_of_quickdraw_an_online_gamed/
"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",1543156812,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/a08yjx/for_me_one_of_the_main_barriers_to_the_world_of/
From Biology to AI: The Perceptron,1543058761,[https://towardsdatascience.com/from-biology-to-ai-the-perceptron-81abfdc788bf](https://towardsdatascience.com/from-biology-to-ai-the-perceptron-81abfdc788bf),neuralnetworks,tangoslurp,False,/r/neuralnetworks/comments/9zxrwj/from_biology_to_ai_the_perceptron/
DeepBrain Chain AI Cloud Training Network Now Open for Trial Users,1542997350,"&amp;#x200B;

**DeepBrain Chain has opened up our distributed AI Cloud training net to trial users.**

We have 1, 2, 4 and 8GPU servers available for use.

You need to follow these steps to get trial access:

1. Register on the DBC Website
2. Head to the AI training net portal on your personal account page
3. **Click AI training net** from the side tab and install the DBC client - set your machine ID (click the little blue ? to find your machine ID)
4. You automatically receive 500 DBC (around 10 training hours on our 2GPU servers)

Github  guides/user manuals and resources are available through the training  net/github portal from the training net page on DBC website. 

&amp;#x200B;",neuralnetworks,DeepBrainChain,False,/r/neuralnetworks/comments/9zqnmi/deepbrain_chain_ai_cloud_training_network_now/
Creating a self-driving car simulator 2019 Programming the NEURAL NETWOR...,1542908987,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/9zg8cd/creating_a_selfdriving_car_simulator_2019/
How to create a tensor without knowing shape upfront in Keras/TF?,1542898454,"

I am currently stuck at a problem. I need to create a temporary variable (of tensor type) for holding values when my custom layer is processing the input. The problem is, the layer gets its inputs in a batch. Batch size is variable. Also, one other dimension of the input is variable which will be provided at the time of running the model.

So, how can I create a tensor of variable dimension every time for a new input. Surely there is a way to do this because built-in layers also take input as a batch whose size is variable and also the shape of output tensor varies as it outputs the whole processed batch. But I don't know how to do it.

Here's the piece of my code:

  similarity_matrix = K.zeros(shape=(int_shape(context_vectors)[0], int_shape(context_vectors)[1], int_shape(query_vectors)[1]), dtype='float32')
  for i, context_vector in enumerate(modified_context_vectors):
      for j, query_vector in enumerate(modified_query_vectors):
          concatenated_vector = concatenate_and_multiply(context_vector, query_vector)
          result = dot(concatenated_vector, self.kernel)
          similarity_matrix[:, i, j] = result
  return similarity_matrix

Here, context_vectors and query_vectors are of shape (None, None, 600). The first dimension is batch_size which is same for both. The second dimension is the number of words in context and query and hence will differ for both.
",neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/9zeq40/how_to_create_a_tensor_without_knowing_shape/
Needed an idea for learning and practising CV so I tried to make a beer detector (darknet makes playing with NN very easy),1542877862,,neuralnetworks,Rayterex,False,/r/neuralnetworks/comments/9zckga/needed_an_idea_for_learning_and_practising_cv_so/
"Are you interested in Neural Networks and want to start learning more with Free Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1542855922,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/9za6kv/are_you_interested_in_neural_networks_and_want_to/
Google Brain &amp; Geoffrey Hinton Technique Thwarts Adversarial Attacks,1542815440,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9z4cpf/google_brain_geoffrey_hinton_technique_thwarts/
Shanghai Tests Graph Recurrent Neural Networks for Traffic Prediction,1542743102,,neuralnetworks,Yuqing7,False,/r/neuralnetworks/comments/9yvkg7/shanghai_tests_graph_recurrent_neural_networks/
Algorithm speeds up process for analyzing 3D medical images,1542733716,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9yu27h/algorithm_speeds_up_process_for_analyzing_3d/
WaveGlow: A flow-based generative network to synthesize speech,1542728669,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9yt9wc/waveglow_a_flowbased_generative_network_to/
When AI and optoelectronics meet: Researchers take control of light properties,1542726516,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9ysyrg/when_ai_and_optoelectronics_meet_researchers_take/
Detect illegal pattern in adult content videos,1542712902,"Hello guys!

&amp;#x200B;

First please notice that this is a serious post even though the subject might sound weird.

To introduce myself, I work in a company that basically offer some streaming and downloading platform for adult content.

I was recently introduced to a new kind of product which work like this:

\- A ""seller"" upload content (lets focus on photo/video) on our website

\- A ""buyer"" can download the content by paying

&amp;#x200B;

Now here is where my job starts: As the content is on OUR website, we are responsible for the content of it. So if for example someone uploads a video in which we see anything that could be illegal (drug, rape, pedophilia...), we are responsible for it. Therefore, we need to monitor the content of every product that is uploaded on our website.

&amp;#x200B;

Now here is where I need your help: I have a theoretical knowledge of machine learning and neural network and it's a field I am really interested in and I really want to learn, so i'd have 2 questions:

&amp;#x200B;

1- Regardless of my profile, would it be possible to have an artificial intelligence that could detect on video undesirable things such as blood, extreme violence and anything you don't want to see on a video?

2- How hard would it be for me, that has not much practical experience in this, to build this?

&amp;#x200B;

Many thanks for your time guys!",neuralnetworks,PM_ME_cutefish,False,/r/neuralnetworks/comments/9yrb7x/detect_illegal_pattern_in_adult_content_videos/
CNN for control system on raspberry pi 3 B+,1542677304,"For a final year project I'm trying to develop a convolution neural network to control an inverted pendulum. The network will take data from a generated image of the system output as inputs.
For this control system, the sample rate has to be 0.009 seconds to be stable. Would it be possible for this idea to work? Would a raspberry pi run the NN fast enough? Could things such as compression make it work faster?
Thanks in advance for any replies!",neuralnetworks,Cr0z0n,False,/r/neuralnetworks/comments/9yn9qy/cnn_for_control_system_on_raspberry_pi_3_b/
"Are ""non-conventional"" perceptrons/networks used/experimented with?",1542667126,"In the papers, videos, articles I've read, it seems that:

1 - only one kind of perceptron is used which always outputs the activated weighted sum of the inputs;

2 - the networks themselves always employ the same mechanism (made exception for the layer topology which is configurable).

I know that this kind of algorithms grant many useful properties to apply backpropagation, but have several drawbacks in other domains, but are there reasons apart from this one?

To give an example, a network may use a non-layered sparse graph in which neurons are dynamically activated, in analogy with how the biological neurons work, which do not just fire out numbers all the times to their output neurons, but instead only light up only occasionally, depending solely on the occasional activation of their respective source neurons.  
A justification for this could be that the produced networks may be more computationally efficient, as some circuits can be dynamically inhibited by some other circuits.  
As backprop would probably not work on this model, other approaches (such as genetic algorithms) may be used to train the model.

&amp;#x200B;

&amp;#x200B;

 **END/TLDR**: it seems that only one kind of perceptrons/network exists, is this true? If yes, why so? If no, what other kinds of network exists?

&amp;#x200B;

&amp;#x200B;",neuralnetworks,gufoe,False,/r/neuralnetworks/comments/9ylu3c/are_nonconventional_perceptronsnetworks/
C program to implement discrete BAM network,1542659722,"Someone please help me with this neural network question

&amp;#x200B;

Write a C program to implement discrete BAM network to store the letters from A-E. Form the input pattern in a 4x3 matrix.",neuralnetworks,legendattherateroot,False,/r/neuralnetworks/comments/9yknwp/c_program_to_implement_discrete_bam_network/
Sony Breaks ResNet-50 Training Record on ImageNet,1542655838,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9yk1dl/sony_breaks_resnet50_training_record_on_imagenet/
This Curious AI Beats Many Games...and Gets Addicted to the TV,1542565894,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9y8dlb/this_curious_ai_beats_many_gamesand_gets_addicted/
"For me, one of the main barriers to the world of deep learning was setting up all the tools. Here's a video that I hope will eliminate this barrier. Hope you guys found it helpful!",1542558864,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/9y7c90/for_me_one_of_the_main_barriers_to_the_world_of/
Install Package - Google Colab Tutorial,1542426250,,neuralnetworks,jeffxu999,False,/r/neuralnetworks/comments/9xt8iv/install_package_google_colab_tutorial/
Artificial neural networks in three minutes!,1542400342,,neuralnetworks,BlairRoslyn,False,/r/neuralnetworks/comments/9xpwn5/artificial_neural_networks_in_three_minutes/
Understanding the scaling of L² regularization in the context of neural networks,1542368131,,neuralnetworks,shaypal5,False,/r/neuralnetworks/comments/9xletd/understanding_the_scaling_of_l²_regularization_in/
3 reasons why Artificial Intelligence and Machine Learning should be at the heart of your cybersecurity strategy.,1542365725,,neuralnetworks,Victor_Stakh,False,/r/neuralnetworks/comments/9xl6jz/3_reasons_why_artificial_intelligence_and_machine/
Array - Numpy Python Tutorial,1542327158,,neuralnetworks,jeffxu999,False,/r/neuralnetworks/comments/9xgw60/array_numpy_python_tutorial/
Global Minima Solution for Neural Networks?,1542297005,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9xcc00/global_minima_solution_for_neural_networks/
"For anyone looking to get into machine learning, I would advise that you don't learn the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",1542249392,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/9x6yv4/for_anyone_looking_to_get_into_machine_learning_i/
"The Neural Aesthetic @ ITP-NYU :: 09 Music information retrieval, BIGGAN &amp; GLOW",1542243215,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9x65ei/the_neural_aesthetic_itpnyu_09_music_information/
Neural nets and pinterest...,1542220200,"Aloha. Sorry just getting into neural nets but could you not use pinterest to  ""help"" a computer to identify items in photos faster and with more accuracy? It seems like they have something like that going on already but have not looked into it yet.",neuralnetworks,fanficfan81,False,/r/neuralnetworks/comments/9x2pgb/neural_nets_and_pinterest/
OpenAI Founder: Short-Term AGI Is a Serious Possibility,1542213336,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9x1nnu/openai_founder_shortterm_agi_is_a_serious/
Best way to learn CNN,1542192081,Any recommendations of good tutorials for CNN?,neuralnetworks,vinceeeeent,False,/r/neuralnetworks/comments/9wz0de/best_way_to_learn_cnn/
Differentiable Monte Carlo Ray Tracing through Edge Sampling,1542144884,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9wtdw0/differentiable_monte_carlo_ray_tracing_through/
Neural Network Embeddings: from inception to simple,1542139084,,neuralnetworks,cptAwesome_070,False,/r/neuralnetworks/comments/9wshof/neural_network_embeddings_from_inception_to_simple/
"Global Neural Network Software Market – Size, Outlook, Trends and Forecasts (2019 – 2025)",1542085781,[removed],neuralnetworks,envisioninteligence,False,/r/neuralnetworks/comments/9wm3fq/global_neural_network_software_market_size/
Imagining the Unthinkable: Simulations Without Classical Monte Carlo,1542054906,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9whuh2/imagining_the_unthinkable_simulations_without/
This AI Shows Us the Sound of Pixels,1542053338,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9whlo5/this_ai_shows_us_the_sound_of_pixels/
Move 37 Explained,1542053302,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9whlg7/move_37_explained/
"AI Monthly digest #2 - the fakeburger, BERT for NLP and machine morality | deepsense.ai",1542053261,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9whl7y/ai_monthly_digest_2_the_fakeburger_bert_for_nlp/
Neural Network Part 1 - Tensorflow Tutorial,1541885464,,neuralnetworks,jeffxu999,False,/r/neuralnetworks/comments/9vy2nm/neural_network_part_1_tensorflow_tutorial/
self driving 3D Simulation using Neural Networks,1541864984,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/9vvajc/self_driving_3d_simulation_using_neural_networks/
When and how to calculate reward function,1541854438,"I'm trying to create a neural network that plays Snake using synaptic.js. It's going ok so far and you can check out the demo here: [http://steenstn.github.io/nnsnake.html](http://steenstn.github.io/nnsnake.html) (Up/down arrow keys to change speed).

&amp;#x200B;

If you let it run for a minute it will start to find its way to the food. However, I'm not sure I'm doing things correctly.

I have an input layer, hidden layer and an output layer. The input layer gets the following data:

\- Left tile is free?

\- Right tile is free?

\- Front tile is free?

\- Food is to the right?

\- Food is to the left?

\- Food is in front?

All the input is either 0 or 1.

The output layer has three nodes: Turn left, go straight or turn right.

&amp;#x200B;

What I'm doing each frame now is the following:

\- The NN is run and outputs a vector

\- The index of the max value in the vector with is the action the snake will take. So \[0.8, 0.2, 0.5\] will make the snake turn left.

\- Check to the left, front and right of the head

\- Calculate a reward for each option and put that in a vector, using the sigmoid function to get it between 0-1

\- If the index of the max value in the NN output is the same index as the max value in the reward function vector , that means the snake chose the ""correct"" path. Backpropagate the output, otherwise backpropagate the normalized reward function vector (the ""correct"" answer)

&amp;#x200B;

So basically I am checking all the options the snake can make each frame, seeing which will give the highest reward and after the network gives its suggestion I compare it and if it did it correct I say ""more of that"", otherwise ""no, you should have given me **this** answer"".

&amp;#x200B;

Is this correct? It doesn't seem scalable when checking all possible combinations becomes too much? Or should I first make the movement and then evaluate the reward I got from the movement compared to the last frame? And how can I then punish bad behaviour? If the snakes moves into a wall how can I tell the network that was bad? Because I don't know what the ""correct"" movement was. Or do you only backpropagate correct behaviour?

&amp;#x200B;",neuralnetworks,steenstn,False,/r/neuralnetworks/comments/9vu2jz/when_and_how_to_calculate_reward_function/
"For algorithms that fix holes in images, how is the expected value of a pixel determined?",1541794802,"Pretty new to neural networks, so my grasp of it is still weak beyond the basics. Specifically, I don't understand how https://github.com/MathiasGruber/PConv-Keras works, and I had trouble trying to understand the paper the implementation is based on. I can get the basic structure, but not how the expected data is made to work with other images. Is it just trained with like a giant dataset filled with images that have holes in them and their correct reconstruction, or does it just train on the single image with a hole in it?",neuralnetworks,InfiniteLennyFace,False,/r/neuralnetworks/comments/9vnwaf/for_algorithms_that_fix_holes_in_images_how_is/
Evolving neural network,1541777680,[removed],neuralnetworks,MDJSmile,False,/r/neuralnetworks/comments/9vlecx/evolving_neural_network/
CV tool for turning video stream from camera into structured data in real-time,1541765413,"Hi there!

Our team make a tool for developers, based on computer vision algorithms, that helps to extract useful information from video stream in real-time and integrate it into apps without expertise in machine learning.

There are three simple steps to get started. Install the program on your computer, connect the camera and choose what you want to see. That's it. Live video stream from camera will be processed locally on your computer by AI models. Result will be time series data in JSON format, which developer can integrate into his application.

As for now the tool is on the development stage. You can see a demonstration of the tool on the [website](https://heyml.com/vision).
We need your opinion and recommendations for improvement. We are open to discussion :)",neuralnetworks,bkmnsk,False,/r/neuralnetworks/comments/9vjwnr/cv_tool_for_turning_video_stream_from_camera_into/
Global Neural Network Software Market,1541756256,[removed],neuralnetworks,ENVISIONINTELLIGENCE,False,/r/neuralnetworks/comments/9vj41u/global_neural_network_software_market/
"Team breaks world record for fast, accurate AI training",1541691353,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9vazy3/team_breaks_world_record_for_fast_accurate_ai/
Educational implementation of CNN in pure C++ with documentation,1541662587,,neuralnetworks,ribtoks,False,/r/neuralnetworks/comments/9v821p/educational_implementation_of_cnn_in_pure_c_with/
Lack of generalization,1541640110,"Hi Everyone,

I've been training a neural net (200 binary inputs, 10 linear outputs, using dense layers) with around 2000 samples (usually split as 1600 train, 200 validation, 200 test) and I've found that varying the # neurons and/or number of hidden layers, altering learning rate, changing optimization approach, using early stopping, varying activation, using drop out, shuffling samples, etc.. all more or less lead to over-training and very poor performance on the validation and test sets.

Samples are very hard to come by - is it worth the effort in growing the number of samples to help overcome over-training, or is this a wasted effort?

FWIW, flipping certain binary inputs can lead to fairly drastic consequences to the 10 outputs - this leads me to believe that perhaps a much larger sample set is needed for generalization.

Any help is much appreciated.",neuralnetworks,FontofFortunes,False,/r/neuralnetworks/comments/9v5jop/lack_of_generalization/
Relative Harmonic networks,1541605923,"I'm a rank amateur and have only just started looking into the field, so please excuse if I state the obvious, or say something incredibly stupid, but I'd love to run something past you.

I'm actually a music producer by trade, and spend a lot of time tweaking and building synthesisers.  I've also got a pretty broad interest in science and this lead me to watch Robert Sapolsky's lectures on human behavioural biology.  Watching these, especially the parts on Neuroscience and the biology of neurons, it came to me that many of the processes don't seem too divorced from the process of music synthesis.

Where by you composite simple signals and modulate them at local and macro levels, with various forms of frequency filters and dynamic manipulation.  Much like the brain does does with multiple neurotransmitters, uptake inhibitors, endocrine system etc... 

So I had the thought that maybe each neuron, or node is essentially a little sound analysis machine, that has a internal ""resonant"" frequency and has the ability to compare input frequencies to that frequency and output useful information about the relativity of that signal.  The resonant frequency of the neuron has the ability to be slightly updated by a heavily weighted signal.

So effectively each dendrite or connection represents a relative harmonic, dynamic, or frequency, and these are in part universal to all processes in the brain.  So a major -&gt; minor relative frequency will likely be sent to the amydala, and major 3rd -&gt; mixolydian always to language.  I DEFINITELY haven't work out the details at all, but something along these lines.

So imagine all sensory input is an orchestral hit, or all instruments playing discordant nonsense, but chains of nodes are set up to ""listen"" for cellos, or timpani, or flutes, and essentially this confused signal is divided down these chains of ever specific frequencies using Fourier transforms at a neural level, representing evermore precise categorical thought, each neuron probably acting as a little capacitor to boost the signal.  With relative signal information being spun out throughout the chain to processes that decide what to do with that information.

It's hard to articulate without images but hopefully you get the idea.

Things I really like about the hypothesis:

* Abstract thought is incredibly easy, as frequencies are always similarly relative, despite the complexity of the signal.
* Memories essentially double up as algorithms to run new information through.
* The process of getting the signal to a useful comparison is incredibly quick.  Although no doubt updating nodes with new frequencies is probably laboured and much slower.
* With enough processing power, you could model this.

So I'm actually in the process of learning how to code, with the eventual aim of model this in a highly constrained form to test it out.

But was just wondering what you guys thought, if it seems like utter madness, if it's already by tried and tested and whether you think it's something worth pursuing!  


Thanks very much if you've got this far!",neuralnetworks,his3tdc,False,/r/neuralnetworks/comments/9v0k5s/relative_harmonic_networks/
Artificial intelligence—parking a car with only 12 neurons,1541604170,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9v0b8j/artificial_intelligenceparking_a_car_with_only_12/
An emotional deep alignment network (DAN) to classify and visualize emotions,1541604076,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9v0aru/an_emotional_deep_alignment_network_dan_to/
Neural Networks (E01: introduction),1541604045,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9v0amf/neural_networks_e01_introduction/
"What is the difference between YOLO, Single Shot Detectors, MobileNet, Darknet, and Darkflow?",1541550794,"I'm trying to use an SSD for my robotics club. From researching, I see these terms thrown around all the time. I'd like it if someone could clarify what they are.

Here's what I believe is true:
SSD is a CV algorithm that uses those cell grid and bounding boxes stuff.
YOLO is a trained model using the SSD algorithm
Darkflow is an image database (I think?)

That's all I got so far. Correct me on anything I'm incorrect on. :) ",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/9uu8xf/what_is_the_difference_between_yolo_single_shot/
The GPUs of the Future,1541543804,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9ut8w2/the_gpus_of_the_future/
How Face ID Works... Probably - Computerphile,1541530338,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9ur5cg/how_face_id_works_probably_computerphile/
SpiNNaker Powers Up World’s Largest Supercomputer That Emulates A Human Brain,1541525054,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9uqb87/spinnaker_powers_up_worlds_largest_supercomputer/
SpiNNaker Powers Up World’s Largest Supercomputer That Emulates A Human Brain Read more at https://hothardware.com/news/spinnaker-powers-up-worlds-largest-supercomputer-that-emulates-a-human-brain#t1fMIdBJBsm0us3I.99,1541524976,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9uqatw/spinnaker_powers_up_worlds_largest_supercomputer/
Any ideas on improving my neural network (convolutional)?,1541453014,"So I work professionally as a data scientist, but this is a just for fun project of mine. I'm trying to build an AI that can play me in stratego. I already coded up an engine for it, and managed to get ahold of a database of tens of thousands of high quality stratego games. I converted these into about six million board states, each coded 0 or 1 for ""lead to a win.""

My plan is to hand the neural net all the possible next-move states and let it decide which one is most likely to result in a win. The second phase of this is predicting what the opponent's units are, but that's a whole different problem. For now, I am just trying to get the neural network to predict if the state leads to a win or not. That being said, without further ado, here is my current code:

&amp;#x200B;

`model = Sequential()`

`model.add(Conv2D(40, kernel_size=(3, 3), strides=(1, 1), activation='relu', input_shape=(10,10,1)))`

`model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))`

`model.add(keras.layers.Flatten())`

`model.add(BatchNormalization())`

`model.add(Dense(500, activation='tanh'))`

`model.add(Dropout(0.01)) #this really hurt accuracy at .5 (I wonder why :P)`

`model.add(keras.layers.Dense(75, activation='tanh'))`

`model.add(BatchNormalization())`

`model.add(Dense(10, activation='sigmoid'))`

`model.add(BatchNormalization())`

`model.add(Dense(1,kernel_initializer='normal',activation='sigmoid'))`

`optimizerr = keras.optimizers.SGD(lr=0.5, momentum=0.01, decay=0.01, nesterov=True)`

`model.compile(optimizer=optimizerr, loss='binary_crossentropy', metrics=['binary_accuracy'])`

&amp;#x200B;

[`model.fit`](https://model.fit)`(x_train, y_train,epochs = 40, batch_size = 128, verbose=1)`

&amp;#x200B;

I currently get about 77% on a test set of about a million, train accuracy is right around 80%.

&amp;#x200B;

To nip this in the bud first off: I found 40 channels to be roughly optimal. I started with 3, and on a wild guess did 32, and got tremendous accuracy gains on train and test, so I tuned it and wound up at 40. What on earth is the model getting from 40 different channels of each kernel square of the board? No freaking clue, but I'll take the 20% accuracy gain (yeah it was barely doing better than random before don't judge me :P).

I tried a second conv2d/pooling layer pair but it wasn't particularly helpful. But yeah, if anyone has any ideas they think would help I'm open to all suggestions!

&amp;#x200B;",neuralnetworks,capnshanty,False,/r/neuralnetworks/comments/9uhrhz/any_ideas_on_improving_my_neural_network/
Training with states of matter search algorithm enables neuron model pruning,1541429985,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9ue6yv/training_with_states_of_matter_search_algorithm/
Understanding of Masking Layers,1541410765,"Hello everyone.. I am currently looking at some pre-coded models and I see that the author uses ""Masked Softmax"" layer many times instead of just using a Simple Softmax. Also, the author has wrote a common script which can help him apply masking to any layer.

I don't know why we use masked layer. I also am not clear of ""what actually is a masked layer and what is the difference between a simple and its masked variant"".

Can anyone help? Thank you..

Here are the links for reference -
https://github.com/allenai/deep_qa/blob/master/deep_qa/layers/masked_layer.py
https://github.com/allenai/deep_qa/blob/master/deep_qa/layers/attention/masked_softmax.py",neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/9uc6qw/understanding_of_masking_layers/
Tensorflow 2.0: models migration and new design,1541364085,,neuralnetworks,pgaleone,False,/r/neuralnetworks/comments/9u6wgg/tensorflow_20_models_migration_and_new_design/
basic perceptron algorithm help,1541363612,"hey, I’m having trouble starting my perceptron comp sci hw... 

I need to determine how many updates the algorithm made to the w parameter as well as write our the values of w after each update. I’ve tried looking online and I just don’t know where to start. 

Here’s the question: 
We now apply the perceptron learning algorithm to a small set of toy data:
x1 = [2,2],y1 = +1
x2 = [4,−2],y2 = −1
x3 = [−3,1],y3 = −1
To make it simple, we consider the perceptron to have no bias terms. Recall the per- ceptron update rule:
if yi ̸= h(xi, wk), then
wk+1 = wk + yixi
where h(x, w) = sign(w · x)
Given the initial w0 = [0, 0], the algorithm starts with x1 and then cycles through the data until it makes no further mistakes.
",neuralnetworks,memesap,False,/r/neuralnetworks/comments/9u6txr/basic_perceptron_algorithm_help/
A Super Simpe But Powerful neural network implemention for python,1541303707,,neuralnetworks,microic,False,/r/neuralnetworks/comments/9u0nf3/a_super_simpe_but_powerful_neural_network/
Logistic Regression Part 1 - Machine Learning Tutorial,1541282582,,neuralnetworks,jeffxu999,False,/r/neuralnetworks/comments/9ty50x/logistic_regression_part_1_machine_learning/
BI 010 Adam Marblestone: Brain Cost Functions,1541274642,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9tx3pk/bi_010_adam_marblestone_brain_cost_functions/
Neural nets in C++,1541260110,[removed],neuralnetworks,ParthPant4,False,/r/neuralnetworks/comments/9tv4ix/neural_nets_in_c/
How reasonable would it be to run a NN on a Raspberry Pi to detect people in photos taken with a night vision Pi Cam?,1541202872,"I'm thinking about working on a very low power person-detection security camera for a van RV. It would have a camera that's on all the time, taking pictures as rapidly as it can classify them, and if it detects a person close to the van from some length of time, it could alert me.

Does that seem possible?

I don't really know jack about neural networks, but I'm a software engineer, so I'm sure I could learn.

(Training could obviously be done on a real PC.)",neuralnetworks,Autoradiograph,False,/r/neuralnetworks/comments/9tphf4/how_reasonable_would_it_be_to_run_a_nn_on_a/
Looking for some help implementing backpropagation in my project.,1541195864,"Hi everyone!  
  
I'm looking for a little help understanding just how backpropagation is implemented. I've watch a lot of videos and read papers and I still feel lost. If anyone would be willing to be a mentor or have other resources for understanding backpropagation, I would happily receive any help that's offered. Thank you!",neuralnetworks,corwin_of_amber_,False,/r/neuralnetworks/comments/9tojwd/looking_for_some_help_implementing/
What are the uses of TimeDistributed wrapper for LSTM or any other layers?,1541139911,"I am trying to understand the use of TimeDistributed layer in keras/tensorflow. I have read some threads and articles but still I didn't get it properly.

The threads that gave me some understanding of what the TImeDistributed layer does are -

https://stackoverflow.com/questions/47305618/what-is-the-role-of-timedistributed-layer-in-keras

https://stackoverflow.com/questions/44611006/timedistributeddense-vs-dense-in-keras-same-number-of-parameters

But I still don't know why the layer is actually used!

For example, both the below codes will provide same output (&amp; output_shape):

    model = Sequential()
    model.add(TimeDistributed(LSTM(5, input_shape = (10, 20), return_sequences = True)))
    print(model.output_shape)

    model = Sequential()
    model.add(LSTM(5, input_shape = (10, 20), return_sequences = True))
    print(model.output_shape)

And the output shape will be (according to my knowledge) -

    (None, 10, 5)

So, if both the models provide same output, what is actually the use of TimeDistributed Layer?

And I also had one other question. TimeDistributed layer applies time related data to separate layers (sharing same weights). So, how is it different from unrolling the LSTM layer which is provided in keras API as:

    &gt; unroll: Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences.

What is the difference between these two?

Thank you.. I am still a newbie and so have many questions.",neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/9thm4y/what_are_the_uses_of_timedistributed_wrapper_for/
A Conversation With Quoc Le: The AI Expert Behind Google AutoML,1541106160,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9tdh1y/a_conversation_with_quoc_le_the_ai_expert_behind/
How is Robotic Process Automation (RPA) Transforming Enterprises?,1541088312,,neuralnetworks,Victor_Stakh,False,/r/neuralnetworks/comments/9tareq/how_is_robotic_process_automation_rpa/
New HotpotQA Dataset Has the Answers for Multi-Hop Queries,1541079874,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9t9ja9/new_hotpotqa_dataset_has_the_answers_for_multihop/
The Neural Aesthetic @ ITP-NYU :: 07 Conditional generative models,1541023400,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9t3fd4/the_neural_aesthetic_itpnyu_07_conditional/
Free servers with 1080Ti for deep learning,1541022385," I  am offering free 1080Ti GPU instances for deep learning. I am a  co-founder of Tensorpad; we are creating a service for AI startups to  train neural networks. We have paid traffic, but some servers are idle.  Hence, we are offering some credits for free, so that students,  startups, and others can benefit from ML technologies and help us by  using our product and providing honest feedback for us to improve  services.
Sign up at https://dashboard.tensorpad.com/signup and apply the following referral code: trial5. You can apply the referral code in the ""Billing"" tab.
Hope  this explains our story and motivation. Here is more info: 
* The  instances have 16GB RAM, 4 CPUs cores and one 1080Ti GPU. You can run  multiple instances in parallel 
* You get access to the JupyterLab  environment 
* We have pre-installed Tensorflow, Keras, and other ML  frameworks 
* You can access the command line and use it as a dedicated  server for training 
* By default, persistent storage is enabled
For extra free trial hours, use promo code trial5
And for questions, please contact me at ilie@tensorpad.com
Sincerely, Ilie Diacov",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/9t39ux/free_servers_with_1080ti_for_deep_learning/
Open Data Cam the visual data creator,1541008594,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9t1615/open_data_cam_the_visual_data_creator/
Format of features in training data,1541005228,"Hi,

I'm making a classifier (simple feed-forward with 1 hidden layer) for predicting the outcome of NBA-games using box-scores from each played game. I'm not sure how to format my training data. Should I give the network the box-scores for both the home team and the away team? So if I use 3 box-scores (ex. FG, 3PT, FT) I would give the network a vector with six values, the box-scores for the home team and away team in the game. Or I could calculate the difference between the home- and away team box-scores and give the network a vector with 3 values. I have read some [papers](http://sci-hub.tw/https://www.degruyter.com/view/j/jqas.2009.5.1/jqas.2009.5.1.1156/jqas.2009.5.1.1156.xml?format=INT&amp;intcmp=trendmd) but I don't get how they format their training data. What do you think is a good way to format the training data?",neuralnetworks,frozen_frogs,False,/r/neuralnetworks/comments/9t0n8l/format_of_features_in_training_data/
Searching for information on ESN,1540993068,"Hello,

im looking for some good reads on Echo-State-Networks.

Any recommendation? (papers, articles, videos,...)

Thanks",neuralnetworks,RetardedVenusaur,False,/r/neuralnetworks/comments/9sysn0/searching_for_information_on_esn/
When and when not to use activation function between input layer and hidden layer?,1540956972,I  am beginner to Neural Network and this question might be very basic and stupid. Expecting some intuitive answer. ,neuralnetworks,redditaddict07,False,/r/neuralnetworks/comments/9sv8je/when_and_when_not_to_use_activation_function/
Facebook Open-Sources QNNPACK Kernel Library,1540937038,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9ssl7m/facebook_opensources_qnnpack_kernel_library/
I don’t find it impressive if a neural network works on the MNIST database considering how a simple neural network can works on it.,1540930495,"Seriously, screw the MNIST database. ",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/9srlya/i_dont_find_it_impressive_if_a_neural_network/
Real time numbers recognition (MNIST) on an iPhone with CoreML from A to Z,1540916730,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9sphx3/real_time_numbers_recognition_mnist_on_an_iphone/
Brain Cell Electronics Explains Wetware Computing Power,1540851335,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9shv4o/brain_cell_electronics_explains_wetware_computing/
"For anyone looking to get into machine learning, I would advise that you don't learn the behemoth libraries like Tensorflow or Theano, but instead learn how to use a high-level API like Keras. Here's a quick video to explain what it is. Hope I was helpful!",1540849236,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/9shkib/for_anyone_looking_to_get_into_machine_learning_i/
Dataset database and search,1540843574,"Hey guys,

I need your advice/feedback. As a ML/AL engineer I quite often struggle to find new and relevant datasets for my tasks. One of the key problems - some great datasets are buried into articles with no real SEO visibilty, some are just usually on-demand for academics only and it's hard to track articles where specific dataset was used.

I am thinking of creating a simple website + storage to help solve those problems with the power of crowdsourcing. It should be non-profit I think.

So, I created a simple website with limited capabilities just to show off the idea. What you think of this? I know that there's a Google Dataset Search, but it's not really what I need. Would you like to participate I this kind of project on a github/contibute? Or maybe you think idea is useless? 

Feedback welcomed here - [mlbase](https://mlbase.io) ",neuralnetworks,alex_titanovo,False,/r/neuralnetworks/comments/9sgppj/dataset_database_and_search/
Eta Compute Debuts Spiking Neural Network Chip for Edge AI,1540742708,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9s493k/eta_compute_debuts_spiking_neural_network_chip/
This AI Learned How To Generate Human Appearance,1540669424,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9rx4a6/this_ai_learned_how_to_generate_human_appearance/
What are Neural Networks (And Why They Matter in Deep Learning)?,1540622498,,neuralnetworks,dreamzdaisy,False,/r/neuralnetworks/comments/9rsagd/what_are_neural_networks_and_why_they_matter_in/
GitHub - deeppomf/DeepCreamPy: Decensoring Hentai with Deep Learning Convolutional Neural Networks. Release.,1540590499,,neuralnetworks,MuzzleO,False,/r/neuralnetworks/comments/9royzj/github_deeppomfdeepcreampy_decensoring_hentai/
Release of DeepCreamPy for decensoring hentai with deep learning convolutional neural networks,1540589778,,neuralnetworks,MuzzleO,False,/r/neuralnetworks/comments/9rovi6/release_of_deepcreampy_for_decensoring_hentai/
Question about hidden layers,1540573473,"Some time ago I had a talk with a professor that basically said that we are not able to find out what is the input of a neural network given the hidden layers output. I'll try to be more clear, with an example (similar to the talk we had). We have to do classification about some data X and we use a pretrained network to do feature extraction, in order to find out the 'patterns' that emerge from the data we have. What we get in output is Y. What my professor is saying is that knowing *only* Y we cannot find out what X is. I was not able to find anything about this topic, so maybe the real problem is that the question is wrong! 
Could you help me with this? Do you know some papers that acknowledge the problem ? 
I'm sorry if I asking something silly, but neural networks aren't my bread and butter.",neuralnetworks,Horciodedayo,False,/r/neuralnetworks/comments/9rmkqy/question_about_hidden_layers/
Problem with MLP producing same outputs? possibly due to bias term?,1540530556,"So I've implemented a ""simple"", fully connected, feed forward neural network, using sigmoid functions, for regression.

Initially,  no matter what the hyperparameters were, the outputs that were getting  produced were all 0.5, thus indicating vanishing gradient problem  (1/(1+exp(0))) = 0.5

So, I standardized  all the data inputs, changed it to tanh functions, applied batch  normalization for all the hidden layers, including the output layer, and  it does seem to produce some sensible outputs,

But  then now, it's giving the same outputs regardless of the inputs I feed  in. I've implemented batch normalization such that given the next  layer's neurons' values, you would standardize them, then multiply each  with its own trainable constant (scaling parameter), and then shift it  by another its own trainable constant (shifting parameter)

I  think the problem comes from the shifting parameter ""taking over"" and  the network just updating those shifting parameters, thereby decreasing  the error overall, but not changing the network's weights, thus even  when given different inputs, still produces the same outputs.

Could  that be the reason? What would be the work around that? I tried just  standardizing the batch's neurons without applying scaling and shifting  parameters, but it still causes vanishing gradient problem, that is, the  outputs given are always 0.5 for sigmoid functions",neuralnetworks,qudcjf7928,False,/r/neuralnetworks/comments/9rhuuj/problem_with_mlp_producing_same_outputs_possibly/
Evolution Self Learning Survive - Neural Net Genetic Algorithm - Deep Learning,1540486307,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/9rc283/evolution_self_learning_survive_neural_net/
The Neural Aesthetic @ ITP-NYU :: 06 Generative models,1540476693,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9rankd/the_neural_aesthetic_itpnyu_06_generative_models/
Poll Results Released: NIPS Keeps Its Name,1540417017,,neuralnetworks,gwen0927,False,/r/neuralnetworks/comments/9r3zlt/poll_results_released_nips_keeps_its_name/
Neural Networks,1540295522,Where to start in neural network?,neuralnetworks,vinceeeeent,False,/r/neuralnetworks/comments/9qo7q8/neural_networks/
CAPSULE NETWORKS - ARCHITECTURE,1540253838,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9qjo39/capsule_networks_architecture/
Meta Learning,1540243395,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9qi629/meta_learning/
Understanding the building blocks for an electronic brain,1540221278,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9qeqb8/understanding_the_building_blocks_for_an/
BinaryGAN: a generative adversarial network with binary neurons,1540217776,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9qe7oi/binarygan_a_generative_adversarial_network_with/
Need suggestion for undergrad thesis,1540204843,My advisor want me to work on LSTM. But the problem is almost all the work is already done. Any thing i want to implement with some dataset is already done. I will not be able to make any algorithm change. That's for sure. I will get 1 month to do it. What to do? ,neuralnetworks,arnab9,False,/r/neuralnetworks/comments/9qcm8j/need_suggestion_for_undergrad_thesis/
Need help with getting labeled objects into my reinforced network,1540013783,"Hey guys, I'm working on an unsupervised learning project where I have a character beat a game, but created in the game are health packs and monsters. I have been successfully able to label the targets with the game API, so no CNN needed. I am stuck on how to get my Bot to recognize these targets and shoot at them. I plan to use a Q learning reinforced network. Any ideas?  ",neuralnetworks,RationalRepublican10,False,/r/neuralnetworks/comments/9prk9r/need_help_with_getting_labeled_objects_into_my/
Tensorpad – 1080ti cloud GPU for 0.49$ per hour,1539913867,"We have released features that will make working with Deep Learning models much more convenient and faster.

1. Persistent storage. All instances have persistent storage now. You can upload datasets, pause and start your environment, and immediately continue training.
2. We have improved network speed. It makes your training environment reliable and responsive. Upload and download speeds are adjusted to fit you better.
3. Referral program. Recommend us to your friends, and earn 10 GPU hours from their first purchase.

Sincerely, Tensorpad team https://tensorpad.com

Follow us on twitter for updates and upcoming tutorials: https://twitter.com/tensorpad_com",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/9pfg41/tensorpad_1080ti_cloud_gpu_for_049_per_hour/
Running a NN backwards after training?,1539894105,"Hey Everyone,

Very new to NN, so apologies if this is an odd question.

If I have N sets of inputs and M sets of outputs, and I train the network such that it's capable of good prediction for these sets, can it then be run backwards so that I can determine what specific input I need to obtain a specific output (assuming training is sufficient to explore outside of the training data?)

It's probably just a simple question of reciprocity...

The deeper question is whether a NN, when properly trained, can be used as an optimizer.

Thanks.",neuralnetworks,FontofFortunes,False,/r/neuralnetworks/comments/9pcw0z/running_a_nn_backwards_after_training/
Looking for a research fellow,1539877011,"The idea is to apply dropout on synapses, not neurons. In human brains the number of neurons stays constant, but the [synapse connectivity](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2989000/figure/Fig10/) changes over time. So this approach can be a more biological and general Hinton's Dropout.

Would be cool if you're good at TensorFlow.",neuralnetworks,groznykon,False,/r/neuralnetworks/comments/9pacz9/looking_for_a_research_fellow/
Implementing Restricted Boltzmann Machine with .NET Core,1539767742,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/9ox0q6/implementing_restricted_boltzmann_machine_with/
Can anyone explain Dropout in a RNN?,1539708628,"I know the basic use of Dropout and how it works in a simple manner - it disables some neurons of a layer according to the dropout rate in a random manner.

But when I faced the use of dropouts in RNN, I couldn't understand anything. There are two types of dropouts in RNN:
1. Simple dropout - applied to inputs and outputs
2. Recursive dropout - applied to the layer in timesteps

I don't know anything except those listed above and want to develop a deeper understanding of how this thing works. So, I need help.. Thank you.",neuralnetworks,ImaginaryAnon,False,/r/neuralnetworks/comments/9opfme/can_anyone_explain_dropout_in_a_rnn/
Image Recognition AI Survey Questions,1539672996,"Hey everyone,

We’re conducting a short (2 min) survey about image recognition based AI. Our target audience involves everyone who is currently working on or is researching AI driven image recognition solutions. The results will be completely anonymous and we’ll share them here after the survey is complete. If you could help us out that would be great, thanks.

[https://docs.google.com/forms/d/e/1FAIpQLSfN7ZM8eGJeAXu3NZ5OaxNag1o\_HC\_v1kcoGY18FSXQ5qaGSA/viewform](https://docs.google.com/forms/d/e/1FAIpQLSfN7ZM8eGJeAXu3NZ5OaxNag1o_HC_v1kcoGY18FSXQ5qaGSA/viewform)",neuralnetworks,LivyMcKaine,False,/r/neuralnetworks/comments/9ol6ao/image_recognition_ai_survey_questions/
AI project possible?,1539629477,"Hey everyone, relatively new to the field and I am currently attempting to make a neural network to predict various trait attributes based on a longitudinal study...

However, I am unsure how, or if it is possible to predict multiple “labels” based on the same set of features...

Using the national longitudinal study of health, I have extracted the first wave of data collected in 1994, and hand selected various questions which are either binary or on a (0 - x) scale. I have formatted each question and respondents data to be on a (0 - 1) scale by dividing the response value based on the questions maximum value (x above).

I have also done so for the most current wave collected in 2008, and formatted the data in the same manner.

Now my question is, whether or not it will be possible to construct a neural net using tensorflow to predict various traits collected from the most current wave of data, from the oldest set of data (each from the same user). So old qualities -predict-&gt; different new qualities

An example of input questions would be (smoking habit,...., proclivity for violence), and outputs such as (ever been arrested, .... , graduated high school).

Currently I have around 5000 common participants between the two waves, each with data sets ranging into the thousands. I only wish to use about 20-100 questions for the input, and something similar for the output. 

Is there any kind of neural net that can model this, and are there any recommended methods of selecting training variables (dropout, # of layers, # of internal nodes, loss func etc)?

Any input would be GREATLY appreciated!
Thanks!",neuralnetworks,Sopkow,False,/r/neuralnetworks/comments/9ofjve/ai_project_possible/
multi-inputs VS concatenated input,1539612253,"When do we use concatenated inputs and single input architecture instead of multi-inputs model?

I can't find any references on this question. It could be great if you guys could link me with some paper that compare both architectures or even using one of those...

&amp;#x200B;

Thanks",neuralnetworks,deadalius,False,/r/neuralnetworks/comments/9ocy9q/multiinputs_vs_concatenated_input/
Questions for my AI project,1539605136,"Hello guys,

So I am working on an AI project which consists of the doom character beating a mission more efficiently than a human. Where I am stuck is figuring out how to implement the two types of neural network models. So what I have decided to do is combine the usage of a Convolutional Neural Network and a Q learning model. What I plan to use these two models is for:  
CNN:  
Detect the monsters in the game (The doom api I am using gives us the ability to show the labels of all objects (monsters, health packs, armor packs, etc)

Q learning:  
To implement a trial and error foundation on how the bot learns from its mistakes

&amp;#x200B;

How to combine is the tricky part... the CNN will be used to detect the labels of the objects, but how do I use Q learning to reward the bot/encourage the bot to actually aim and shoot at them? 

Since my Q learning will be based off random selection of the past moves and fitness calculations, I was thinking a good way to teach it to navigate is by adding up the scores of each episode and weighting each ""thought process"" of the bot to give it an idea of what are the best moves. Shooting the gun is technically a move in the API, so I was thinking of giving it a negative reward for just shooting randomly by keeping track of wasted ammo. Then again, this could be bad because it could prevent the bot from shooting at all. 

&amp;#x200B;

Sorry if the thoughts are all over the place, but I just woke up and starting getting ideas in my head. Let me know what you guys think and if you have any ideas, I would appreciate them greatly. 

&amp;#x200B;

Thanks!",neuralnetworks,RationalRepublican10,False,/r/neuralnetworks/comments/9oc2hc/questions_for_my_ai_project/
Neural Network in Never,1539537301,,neuralnetworks,smaludzi,False,/r/neuralnetworks/comments/9o4gch/neural_network_in_never/
DeepCon ’18 – Artificial Intelligence Conference,1539433140,,neuralnetworks,codingwoman_,False,/r/neuralnetworks/comments/9ntcko/deepcon_18_artificial_intelligence_conference/
Neural Network Editor - Machine Learning - Artificial Intelligence - FINAL VERSION,1539339500,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/9nj1nm/neural_network_editor_machine_learning_artificial/
Neural Network Artificial Intelligence - 2018 2D Designer,1539201329,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/9n2w35/neural_network_artificial_intelligence_2018_2d/
Help with deciding input variables for a neural network,1539171836,"Hi,

I am using Unity Machine Learning Agents for a proof of concept Neural Network. I have made the smallest system possible and can show it can work. However the training is very long and I think it might be overfitting the training data. Live tests, are not very good.

Is there a place I can get help with deciding on a few different input parameter sets to test out for my situation? (I am not apposed to pay for a professionals help an hour or two).

&amp;#x200B;

Best Regards GingerDane",neuralnetworks,GingerDane,False,/r/neuralnetworks/comments/9myp7b/help_with_deciding_input_variables_for_a_neural/
Using Excel's Solver For A Simple NN. Any good?,1539127825,"Hi guys. I am currently working a project that I am using a relatively simple neural net (2-4 layers, with 2-6 nodes each) that is in excel. It's in excel for a few reasons, 1) its my first NN 2) excel is easy 3) other non-programmers may need to use it in the future (this is for work). The training is done using Excel's built in solver add-on to minimize the error by adjusting the weights, but I am wondering if I should invest the time to program in a more conventional backprop method or if you all think Excel's solver does a pretty good job as is for this size NN? I don't care much about training speed, the NN is pretty simple as it is, but I am really more interested in whether a more legit backdrop method would substantially reduce my error. I understand that what I am asking is pretty circumstantial based on my specific project, I'm really more just looking for anecdotal evidence from people that may have experience with both methods? thanks!",neuralnetworks,schwendie,False,/r/neuralnetworks/comments/9mu8qo/using_excels_solver_for_a_simple_nn_any_good/
Evolution Self Learning Survive - Neural Net Genetic Algorithm - Deep Learning - Please subscribe if you like the video,1539108309,,neuralnetworks,DevTechRetopall,False,/r/neuralnetworks/comments/9mri6j/evolution_self_learning_survive_neural_net/
Neural Network urban legend?,1539107476,"I once heard that there was some sort of neural network that managed to break an ""unbreakable"" military code (I think this was back in the mid-late 90's/early 2000s). The researches got so startled/scared by that that they shut it down.

Does anyone know whether or not this is true? :P",neuralnetworks,Bizobinator,False,/r/neuralnetworks/comments/9mrdtm/neural_network_urban_legend/
"The Neural Aesthetic @ ITP-NYU :: 01 Introduction, the whole course ""in 60 minutes""",1538960817,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9magwy/the_neural_aesthetic_itpnyu_01_introduction_the/
"Toy using ""deep neutral networks""",1538935955,"How powerful cpu it has to have to be able to make any decisions using deep neutral networks? 
I dont know what neutral networks really are or how they work except that it's some kind of ai, machine learning able thing but in the video they also mentions the word ""advanced"" deep neutral networks that suggests there are less advanced too..
Is this a gimmick?",neuralnetworks,brjdi,False,/r/neuralnetworks/comments/9m78kr/toy_using_deep_neutral_networks/
"Get Feature Importance Using Neural Network, With Super High Precision",1538875948,,neuralnetworks,microic,False,/r/neuralnetworks/comments/9m1g1m/get_feature_importance_using_neural_network_with/
Ultimate List of Youtube Channels for Deep Learning and Computer Vision,1538777579,,neuralnetworks,codingwoman_,False,/r/neuralnetworks/comments/9lqu5k/ultimate_list_of_youtube_channels_for_deep/
Neural Network Training Set Representation,1538769602,"Hello!

I'm having a bit of trouble wrapping my head around how my input data should be represented within a Neural Network.

I  have installed numpy to ease with matrix operations, but I believe  abstracting away has only left me more confused as to what I'm actually doing. (I am using Python 3.7)

My project is to get a XOR network operational.

For  example, my training set would be (1, 0), (0, 1), (1, 1), (0, 0), with  the corresponding outputs being ((1), (1), (0), (0)). Would I then  represent the training set as?

// input\_data = np.array(\[\[1,0\], \[0,1\], \[1,1\], \[0,0\]\])

// outputs = np.array(\[\[1\], \[1\], \[0\], \[0\]\])

It  is then my understanding that a simple XOR network takes two inputs,  either a 1 or a 0, or a 1 and a 1, etc. and spits out one output. I  guess the core of my question is how to pull \[1,0\] and run it through the weights using a dot product function.

I have a book that has some older C code in it that I have been referencing, as well as this website ([https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/](https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/))  in order to try to reproduce results. I understand the basis of feed forward and back propagation, but I can not for the life of me get my ducks in order as to how my data structures should be represented. I'm  sinking too much time into it with nothing to show.

Thank you for any help!",neuralnetworks,Kahru,False,/r/neuralnetworks/comments/9lpo7u/neural_network_training_set_representation/
tell me ways to build a tts for my language using neural network,1538761214,"i know nothing about neural network and i want to implement a tts in my language bangla. This will be my final year project so whatever i do i have to pull it off very well. I have learned about tacotron even though i dont understand much about how it works. if i give 2-3 months learning about neural network do you think i can have an understanding good enough to understand the implementation that are available in github and improve it for my language? 

answer me this- 1. do you think i should go for this? 2. as i dont understand this much i am not sure what do i have to provide to train the model. will only providing a huge speech database will be enough ?  3. if not this can you tell me a steps to implement a tts using neural network  i am sorry if this  is too naive of a question . i just dont know where to start.

&amp;#x200B;",neuralnetworks,tyty6529,False,/r/neuralnetworks/comments/9log7z/tell_me_ways_to_build_a_tts_for_my_language_using/
Phrase Generation,1538715866,Has anyone here tried to generate a phrase? Not a sentence,neuralnetworks,vinceeeeent,False,/r/neuralnetworks/comments/9ljeb7/phrase_generation/
MyCaffe: A Complete C# Re-Write of Caffe with Reinforcement Learning,1538713128,,neuralnetworks,wizzerking,False,/r/neuralnetworks/comments/9lj46q/mycaffe_a_complete_c_rewrite_of_caffe_with/
Free 1080Ti instances for deep learning,1538685613,"There's no catch. We're on very early stage with Tensorpad - it works, but we are looking for as much feedback as we can get. Our goal main goal is to make a data scientist's job much more straightforward.

We are inviting you to use the GPUs for training, give us feedback, and help us learn how we can support you better. We want to improve the product, so we provide free credits, and you can sign up at and start using GPUs right now: https://dashboard.tensorpad.com/signup

* All the instances have 16GB RAM, 4 CPUs cores, and one 1080Ti GPU
* Instances run on customized JupyterLab images (Fast.ai, PyTorch 0.4, TensorFlow+Keras 1.10, 1.9, 1.8, 1.7 and 1.5 on CuDNN 9.0)
* You can access the command line and use it as a dedicated server

We provide our users with: • Free highly-available GPU Instances • 24/7 customer support – send us a message through the dashboard and receive help • We would work hard to solve the reoccurring issues and make sure that you stay happy and productive (plus anything else that might come up)

For any questions, contact ilie@tensorpad.com
",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/9lfp07/free_1080ti_instances_for_deep_learning/
New sharing economy cloud GPU platform promises cheaper AI computing,1538654291,,neuralnetworks,RandonWriter,False,/r/neuralnetworks/comments/9lb81u/new_sharing_economy_cloud_gpu_platform_promises/
List of free resources to learn Natural Language Processing,1538585492,,neuralnetworks,jsloverr,False,/r/neuralnetworks/comments/9l2xj8/list_of_free_resources_to_learn_natural_language/
General NES playing AI concept,1538553629,"I'm looking for feedback on my concept to create a generalised NES playing AI

Step 1: Create a large amount of gameplay states that we can assume human beings can order by how desirable they are, use a chess like ELO system and manually compare large amounts of data (by being presented two states and selecting the better one) to get a list of states labelled by deriablity, Create a large database containing the raw RAM of each state and the associated ELO value for every single game-play state evaluated by a human

rationale for step 1: What it means to ""win"" a game is entirely subjective so copying a human being is essentially the only way it's going to learn this and using a neural net here would be more proper than arbitrarily defining a reward function per game

Step 2: Using a standard convolutional neural with all 2048 byes of the RAM (and the controller state seperatly) as input induce a reward function that can predict the ELO rating a human will assign to a gameplay state, at this point apply a pruning algorithm to remove inputs from the RAM that are not correlated with success (this should vastly reduce dimensionality for step 3 and 4)

step 3: Use Q learning to evaluate the maximum future reward  (as far as can be calculated) for an individual input in a given RAM state that has hopefully been pruned to about 100-300 bytes, for computational ease split inputs into discrete chunks of 10 frames and make sure the current inputs are included as part of the evaluation

step 4: Teach a neural network with memory of it’s last few states to emulate the decisions of the tree search, drop the neural network into a random save state of gameplay and evaluate whether it’s decisions match that of the tree search for random moments of gameplay. RAM is used as the input rather than pixel data to ensure generalization.
",neuralnetworks,The_Crucified_One,False,/r/neuralnetworks/comments/9kz60a/general_nes_playing_ai_concept/
List of free resources to learn Natural Language Processing,1538552325,,neuralnetworks,Fewthp,False,/r/neuralnetworks/comments/9kz27d/list_of_free_resources_to_learn_natural_language/
Gun-spotting camera can take the next step realtime and alert authorities,1538400493,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9kgbew/gunspotting_camera_can_take_the_next_step/
Neural Turing Machine,1538348382,"NTM takes an input and output,adjusts weights between neurons and trains like a regular network but also has the capability to access memory(not a particular memory location but the entire memory at once according to the weights generated).It can 'learn' an algorithm by looking at inputs and outputs.Can someone explain all this in context of the copy task?About what exactly is being written in the memory and what is being read?",neuralnetworks,____jasmine____,False,/r/neuralnetworks/comments/9kaw3w/neural_turing_machine/
Training examples?,1538341257,"Me and my team are developing an application that envolves using CNN for real-time object identification - namely, some traffic signs.
While the hardware component is still being developed, I thought about starting the search for a training dataset.

Does anyone have any recomendation? First time that we need to search for training examples. Any pointer on how/where to start is welcome ",neuralnetworks,CaptMartelo,False,/r/neuralnetworks/comments/9k9yo8/training_examples/
Starting with neural network,1538313784,"Hello guys as the title says,  I want to start learning about neural networks, my approach will be from scratch because it is for learning purpose not for an specific project.  Can you guys lead me to any material so I can dig on it? The programming language is not a problem but if would choose I will pick python.

I will appreciate your help, thank you all! ",neuralnetworks,alvarz,False,/r/neuralnetworks/comments/9k6be1/starting_with_neural_network/
[HELP] Neural Network in C++,1538312664,"Hi there!

I'm new to Machine learning in general and I figured why not ask help from people here in reddit! I've finished Andrew Ng's Coursera Course on Machine Learning so I think I have decent entry-level background.

My professor in C++ made us do any project we like in C++. I chose to try to do a simple handwriting recognizer. Can it be done in c++? The last time I did it was in Matlab. Your opinion, help, advice, suggestions are much much appreciated!",neuralnetworks,freepoul,False,/r/neuralnetworks/comments/9k67ac/help_neural_network_in_c/
Free 1080Ti instances for deep learning,1538268345,"There's no catch. We're on very early stage with Tensorpad - it works, but we are looking for as much feedback as we can get. Our goal is to support researchers.

We are inviting you to use the GPUs for training, give us feedback, and help us learn how we can support you better. We want to improve the product, so we provide free access, and you can sign up at and start using GPUs right now: https://dashboard.tensorpad.com/signup

- All the instances have 16GB RAM, 4 CPUs cores, and one 1080Ti GPU. You can run multiple instances in parallel
- Instances run on customized JupyterLab images (Fast.ai, PyTorch 0.4, TensorFlow+Keras 1.10, 1.9, 1.8, 1.7 and 1.5 on CuDNN 9.0)
- You can access the command line and use it as a dedicated server 

We provide our users with:
•    Free highly-available GPU Instances
•    24/7 customer support – send us a message through the dashboard and receive help
•    We would work hard to solve the reoccurring issues and make sure that you stay happy and productive
(plus anything else that might come up)

For any questions, contact ilie@tensorpad.com
",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/9k21z1/free_1080ti_instances_for_deep_learning/
Hierarchical Attention Networks for Document Classification with web demo (Pytorch implementation),1538244529,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/9jz628/hierarchical_attention_networks_for_document/
Advice requested on training scheme,1538240949,"I want to teach a neural network to play Mario and have come up with a scheme to do so and would like feedback

1. I create a large amount of game play data, I take individual frames from my game-play data and subjectively determine if one position is ""better"" than another, for example being in world 8-1 is better than being in world 1-4, however having zero lives in world 1-4 and being small may not be as good as being at the end of world 1-3 and having 10 lives and a mushroom.

2. A neural network uses the contents of the memory to determine what it is that makes a good position to be in and a bad position, generate an objective function for evaluating the ""fitness"" of any individual position within Mario

3. Create a classical (no machine learning here) analytics algorithm that will take a given frame of data and try to evolve the position a certain amount of time into the future, it will simulate the game a little bit into the future and apply a fitness value to each set of inputs

4. A new network is given one of the frames from the data i recorded and is scored on whether it's inputs are the ones that lead to the best possible future in say 1000 frames time, with any luck this will give the network a sense of foresight as it's being scored on the long term sustainability of it's choices, it could learn going backwards to obtain a mushroom is worth it in the long run.

We now have a process that generates an AI that will closely follow what an analytics program has determined will maximise the learned reward function",neuralnetworks,The_Crucified_One,False,/r/neuralnetworks/comments/9jyoe9/advice_requested_on_training_scheme/
Beginners question on genetic approach,1538092383,"https://www.youtube.com/watch?v=Ipi40cb_RsI

In this video sethbling has trained a neural network to play mario kart how he plays mario kart, the end result being a network that is perfectly competent at mario kart.

If we wanted to make a bot that was genuinely powerful and independant of sethbling would the following help

1. Train several neural networks exactly the same way on different courses, increase the input neurons and add neurons for in game variables extracted from RAM including ""position in the race"", ""position of other drivers"", ""item currently equipped"", make sure they are all extremely good at their respective course.

2. Save these player copying networks and use them as the first breeding stock for a new approach based on genetic algorithms

4. Use a genetic breeding algorithm to produce a set of neural networks from the course specific sethbling clone networks and test them on every track a network has been trained on. Change the cost function to a genetic health factor based on the average time it takes to complete a course, set some maximum number for time for each course so failure to complete isn't overly catostrophic and start making the first generation, try and run as many copies of mario kart at once and speed up emulation speed as much as possible to get through these faster.

5. Keep the most successful networks from each generation in the pool for 3 generations, this gives plenty of time for the ""genes"" from the course specific starting stock to dilute themselves into the general population.

4. Hopefully, we will now have some general networks that can beat at least 2 courses, continue the genetic approach until we have a set of ""general mario kart ai""

5. Raise the stakes, demand these improved networks play EVERY COURSE, keep the same total cost function

The result should eventually be a general purpose mario kart playing ai better than most humans.

Would this work? If not where would trouble be encountered, I'm very new to this sort of thing it will be nice to know where the walls are that I'm missing in conceptualising things.",neuralnetworks,The_Crucified_One,False,/r/neuralnetworks/comments/9ji152/beginners_question_on_genetic_approach/
Attention Models: Amplifying Machine Learning Benefits for Enterprise,1538050593,,neuralnetworks,Victor_Stakh,False,/r/neuralnetworks/comments/9jccod/attention_models_amplifying_machine_learning/
AIF360 by IBM to detect Bias in AI algorithms,1538039160,"Understanding bias in AI algos  with  this new tool from IBM - AIF360

[https://gl4l.greatlearning.in/aif360-to-decode-the-black-box-mysteries/](https://gl4l.greatlearning.in/aif360-to-decode-the-black-box-mysteries/)",neuralnetworks,akshad_GL,False,/r/neuralnetworks/comments/9jb8nj/aif360_by_ibm_to_detect_bias_in_ai_algorithms/
Deep Learning Framework Power Scores 2018,1537931145,,neuralnetworks,lohoban,False,/r/neuralnetworks/comments/9iysud/deep_learning_framework_power_scores_2018/
My First Deep Learning Workshop,1537875466,,neuralnetworks,codingwoman_,False,/r/neuralnetworks/comments/9ird0m/my_first_deep_learning_workshop/
Illustrated Guide to LSTM's and GRU's: A step by step explanation,1537821664,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9iln4a/illustrated_guide_to_lstms_and_grus_a_step_by/
The SWEEPER-robot,1537636747,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9i1cs0/the_sweeperrobot/
The Complete Guide to Learn Advanced Artificial Intelligence using Deep Learning and Neural Networks,1537633327,,neuralnetworks,12angryman,False,/r/neuralnetworks/comments/9i0wrj/the_complete_guide_to_learn_advanced_artificial/
Reinforcement and NEAT tutorials,1537614026,"Hello, I am here to ask if anyone has knowledge of some decent reinforcement learning or NEAT resources. I have recently learned how to code my own deep neural networks from scratch and I made a simple genetic algorithm to solve a maze.

I do not want to use a machine learning library, my end goal is to be able to play more complex games such as Mario. Thanks for any recommendations !",neuralnetworks,Ben_09,False,/r/neuralnetworks/comments/9hyv5m/reinforcement_and_neat_tutorials/
Isn't a multi-layer network the same as a single layer network by the distributive property of multiplication?,1537589780,"When you go through multiple layers of weighted sums doesn't it just pass the number in parentheses, and in the end you have constants multiplied together which is just a constant?

Like this:

g(e(ax+by)+f(cx+dy))
= geax + geby + gfcx + gfdy 
= jx + ky

Am I missing something?",neuralnetworks,wakka54,False,/r/neuralnetworks/comments/9hwya6/isnt_a_multilayer_network_the_same_as_a_single/
Robots Teaching Themselves to See,1537546332,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9hrg4j/robots_teaching_themselves_to_see/
Machine Learning Confronts the Elephant in the Room,1537546128,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9hrezm/machine_learning_confronts_the_elephant_in_the/
Artificial intelligence for brain glioma detection,1537529865,,neuralnetworks,AlexSkychainTeam,False,/r/neuralnetworks/comments/9houpa/artificial_intelligence_for_brain_glioma_detection/
3 Myths About AI You Should Not Believe Anymore,1537507217,[https://gl4l.greatlearning.in/3-myths-about-ai-you-should-stop-believing-today/](https://gl4l.greatlearning.in/3-myths-about-ai-you-should-stop-believing-today/),neuralnetworks,akshad_GL,False,/r/neuralnetworks/comments/9hmgmc/3_myths_about_ai_you_should_not_believe_anymore/
What type of neural network would be best for a game bot?,1537495144,"I understood the basic principle of NN when you are inputing a fixed number of parameters and then the network has the mathematically equivalency to the Taylor Series (when using ln() and exp()).

&amp;#x200B;

But then a game bot would need a variable number of inputs instead of a fixed one. As there may be 1 to N enemies at the same time.

&amp;#x200B;

Also what if there are more than 2 factions on the game? How could the NN be modeled to know which players is from faction 1 or 2?

&amp;#x200B;

Can anyone recommend any book or pdf that can further expand into this topic?",neuralnetworks,imbaisgood,False,/r/neuralnetworks/comments/9hl64r/what_type_of_neural_network_would_be_best_for_a/
Weights and 0,1537467229,"I am trying to write a neural network from scratch in Javascript, and I keep on having all my weights to a given neuron set to 0 and then it BREAKS EVERYTHING AND THEY DON'T CHANGE GRRRR. Please help me. Here is the training function(I have not programmed in having any weights other then from hidden to output trained, would that fix it?):

    var train =function(actual,input, expected) {
        if(actual !== expected) {
            var sum = 0;
            var delta_output_some = 9;
            var e = Math.E;
            var error = 0;
            var weight_deltas = [];
            for(var l = 0; l &lt; actual.length; l++) {
                sum=log(1/(1+actual[l]));
                error=expected[l]-actual[l];
                delta_output_some=error / (1 + pow(e,(sum))) * (1 - (1 / (1 + pow(e,(sum)))));
                for( var i = 0; i &lt; layers[layers.length-1]; i++ ) {
                    for(var j = 0; j &lt; layers[layers.length-2]; j++) {
                        neurons[layers.length-1][i].inputweights[j] += lrate*delta_output_some/neurons[layers.length-2][j].value;
                        println(neurons[layers.length-2][j].value);
                    }
                }
            }
        }
    };

&amp;#x200B;",neuralnetworks,mrmola,False,/r/neuralnetworks/comments/9hhjz9/weights_and_0/
Illustrated Guide to Recurrent Neural Networks,1537465885,,neuralnetworks,lohoban,False,/r/neuralnetworks/comments/9hhdkg/illustrated_guide_to_recurrent_neural_networks/
I have a question regarding a project that could use Neural Networking.,1537396622,"We are creating a stylus that will hover over a 6"" x 6"" pad, similar to a [Wacom tablet](https://wacom.com/en-us). However, we will be sending a signal through a coil in the stylus and inside the pad will be other coils that will measure a voltage value, depending on the distance and angle the stylus is away from the pad. 

My question is, can I use a machine learning/neural network for predict values? If we were to map values at 10cm away from the pad at various angles, could we predict the values at 20cm, 30cm, etc away from the pad? 

A friend recommended scikit-learn, i'd love to hear your recommendations. Thank you!",neuralnetworks,rGreenTrees,False,/r/neuralnetworks/comments/9h9sue/i_have_a_question_regarding_a_project_that_could/
Attractor Networks,1537384726,,neuralnetworks,dergthemeek,False,/r/neuralnetworks/comments/9h850w/attractor_networks/
Basic concepts of neural networks,1537383393,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9h7y28/basic_concepts_of_neural_networks/
"Doing a Heirohlypics(of sort) project, need some advice and tips.",1537334538,"I'm doing a project that is involved with some sort of Heiroglyphics, so basically they're going to be like objects that'll soon be recognized as their corresponding words. I want to get the framework down so I'm looking for tips to do this. I'm new to Machine Learning so I still dont know a lot. I'm thinking of using a Convoluted Neural Networks since it'll be accepting an image as an input. For Object Detection I'm thinking of using sliding windows after the image has been convoluted to identify the heiroglyphs. Should it be 2 separate networks, one for the detection and one for the transliteration?

Would there be any need to max pool the image while training?

Also, any image segmentation methods better than sliding window?

&amp;#x200B;",neuralnetworks,mulaned,False,/r/neuralnetworks/comments/9h2enb/doing_a_heirohlypicsof_sort_project_need_some/
I Made This Neural Networks Based App That Turns People Into Live Cartoon Characters. And Tried It On Celebrities.,1537322721,,neuralnetworks,gabrelyanov,False,/r/neuralnetworks/comments/9h1426/i_made_this_neural_networks_based_app_that_turns/
Asking about prediciton in Big ML,1537312015,"Hello Guys. Does anybody has an idea how to preditct the last one column? I can send dataset.  


&amp;#x200B;

*Processing img i94dba5cw2n11...*",neuralnetworks,2MVX,False,/r/neuralnetworks/comments/9gzryy/asking_about_prediciton_in_big_ml/
Faces,1537281359,"Hiya all, has anyone here played with the ideas in this report?

I'd like to obsfucate my travel pictures but then digitally watermark them to indicate they are mine.

https://www.utoronto.ca/news/u-t-ai-researchers-design-privacy-filter-photos-disables-facial-recognition-systems",neuralnetworks,nllewellyn,False,/r/neuralnetworks/comments/9gvdc9/faces/
Need some tips on the best approach to modeling a neural network for my project,1537231516,"Hello all,

So I am doing a project with a few buddies from school. We are trying to use torch/tensorflow (undecided) to teach the doom character from doom to beat scenarios and learning from its past plays in order to beat the scenario more masterfully. Eventually we would like it to fight itself and learn from itself. The question here is, which neural network model would work best for this project. I originally was planing to use a CNN due to the thought that it would need to be able to detect objects in the game. We then realized through the API of the doom game we are using that it has a setting to show object labels, thus object detection is no longer needed. I am now thinking if CNN is still necessary, or is there a better model type I could use? Any tips would be appreciated! ",neuralnetworks,RationalRepublican10,False,/r/neuralnetworks/comments/9gq1tm/need_some_tips_on_the_best_approach_to_modeling_a/
Video on how ai can be used in medicine as well as a look at a neural network structure used in medical ai models,1537199661,,neuralnetworks,TheScienceVerse,False,/r/neuralnetworks/comments/9gli3o/video_on_how_ai_can_be_used_in_medicine_as_well/
Viewpoint: Fast-Forwarding the Search for New Particles,1537194587,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9gksnh/viewpoint_fastforwarding_the_search_for_new/
AI Safety and why we should talk about it,1537155728,,neuralnetworks,gauravc2796,False,/r/neuralnetworks/comments/9ggzz2/ai_safety_and_why_we_should_talk_about_it/
Deep Learning – Learn Recurrent Neural Networks in Python,1537109142,,neuralnetworks,12angryman,False,/r/neuralnetworks/comments/9gb2mu/deep_learning_learn_recurrent_neural_networks_in/
Understanding Convolutional Layers,1536954555,"Hello,

I am learning ml now, and I wrote some simple neural networks, one of them was softmax regression for the mnist dataset. I have about 90% correct predictions. I know that I can improve it in theory by using convNets, but I am not quite sure about something. I know lots in this field from book, arcticles, and videos, but I really want to get right into the programming.

When I feed a convNet with an image, first thing happens is the convolutional layer. using some features, I can get a list of filtered images. Because I am using mnist (1 channel - grayscale) the input will be 2 dimentional, and the next layer will be 3 dimentional. After that I can apply relu and max pooling for example, and here comes my question: how should I apply the filters? 1 for each Image, so the next layer will be also 3 dimentional? or apply all the features to all the images from the previous layer, and get a new dimention with each convolution layer, so in theory it could get to a lot higher dimentions?

I will be happy to have links for good explenations you know, or get some help from the experts here ;D

Anyways, Any help will be appriciated,

Arad.",neuralnetworks,Aradarbel10,False,/r/neuralnetworks/comments/9fv6ze/understanding_convolutional_layers/
Generative Adversarial Networks – Paper Reading Road Map,1536937945,,neuralnetworks,codingwoman_,False,/r/neuralnetworks/comments/9fstki/generative_adversarial_networks_paper_reading/
Helping computers fill in the gaps between video frames,1536935671,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9fsi8t/helping_computers_fill_in_the_gaps_between_video/
The next phase: Using neural networks to identify gas-phase molecules,1536934629,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9fsd8y/the_next_phase_using_neural_networks_to_identify/
Self Driving Car Neural Network using Tensorflow.,1536897822,,neuralnetworks,amitasviper,False,/r/neuralnetworks/comments/9fowlj/self_driving_car_neural_network_using_tensorflow/
Free 1080Ti instances for deep learning,1536875103,"There's no catch. We're on very early stage with Tensorpad - it works, but we are looking for as much feedback as we can get. Our goal is to create a community and support researchers.

We are inviting you to use the GPUs for training, give us feedback, and help us learn how we can support you better. We want to improve the product, so we provide free access, and you can sign up at and start using GPUs right now: https://dashboard.tensorpad.com/signup

- All the instances have 16GB RAM, 4 CPUs cores, and one 1080Ti GPU. You can run multiple instances in parallel
- Instances run on customized JupyterLab images (Fast.ai, PyTorch 0.4, TensorFlow+Keras 1.10, 1.9, 1.8, 1.7 and 1.5 on CuDNN 9.0)
- You can access the command line and use it as a dedicated server 

We provide our users with:
•    Free highly-available GPU Instances
•    24/7 customer support – send us a message through the dashboard and receive help
•    We would work hard to solve the reoccurring issues and make sure that you stay happy and productive
(plus anything else that might come up)

For any questions, contact support@tensorpad.com",neuralnetworks,whitezl0,False,/r/neuralnetworks/comments/9fm6kc/free_1080ti_instances_for_deep_learning/
Nanotechnology the key?,1536819335,"I mean I’m no engineer but had a crazy idea that maybe if we map the brain out well enough to find the patterns and correct neural chemistry to actually make Consciousness. Then use nanotechnology to create neural plasticity? 

I mean it’s a long shot but is the tech even close to where this could work? 

Or even just nano machines in the brain for direct connection interface . 

This could be used for say prosthetics at first figure out how to make a hand move by thought or even get say instant information through someone else learning something? So like in our head phones or augmentation. You think and it happens . 

More like an extension then actual integration . 
I mean maybe perfect for robotic extensions of hands and things . 

I guess a power source would be a problem . 

I don’t know the extent of material we have to work with but is there anything like a material that would just generate electric potential that could be used to charge micro batteries all though the material itself . Say an arm that underneath its shell would have say something that vibrates or some sort movement, heat , Chem really anything that might just generate charge from just natural property’s of the element or molecule . 

Hell how far is quantum stuff? This would come in handy I could imagine if we could just have quantum battery if anything crazy could happen there lol ",neuralnetworks,Trevor9250,False,/r/neuralnetworks/comments/9ffk5h/nanotechnology_the_key/
"MaskFusion: Real-Time Recognition, Tracking and Reconstruction of Multiple Moving Objects",1536687763,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9ezpeg/maskfusion_realtime_recognition_tracking_and/
Deduce the Number of Layers and Neurons for ANN,1536677706,,neuralnetworks,AhmedGadFCIT,False,/r/neuralnetworks/comments/9ey9a8/deduce_the_number_of_layers_and_neurons_for_ann/
Spindle neurons?,1536588617,"Quick question, is there any research into spindle-type neurons in  ANNs? Will research myself, but just wondering if anyone here is actively studying/aware.",neuralnetworks,readytoruple,False,/r/neuralnetworks/comments/9ength/spindle_neurons/
The differences between Artificial and Biological Neural Networks,1536572048,,neuralnetworks,sedthh,False,/r/neuralnetworks/comments/9eliqd/the_differences_between_artificial_and_biological/
Papers with Code,1536556268,I'm working on making a list of Machine Learning papers that has open source code on GitHub. My initial version can be reached at the link included below. I think it will be helpful to this community to select their next paper to read. Please also include your comments and suggestions for improvement. https://github.com/zziz/pwc,neuralnetworks,fvzaur,False,/r/neuralnetworks/comments/9ejx7r/papers_with_code/
Generative Adversarial Networks – An Entertaining Introduction,1536502852,,neuralnetworks,codingwoman_,False,/r/neuralnetworks/comments/9eda7n/generative_adversarial_networks_an_entertaining/
Stanford's Deep Learning cheatsheet • r/MachinesLearn,1536477588,,neuralnetworks,lohoban,False,/r/neuralnetworks/comments/9eb9rb/stanfords_deep_learning_cheatsheet_rmachineslearn/
"Humble Book Bundle: Machine Learning by O'Reilly. $641 worth of Machine Learning Books like Introduction to Machine Learning with Python, Learning TensorFlow, 1Ed, and Thoughtful Machine Learning with Python, 1Ed is 97% OFF !",1536468919,,neuralnetworks,noor897,False,/r/neuralnetworks/comments/9eakh3/humble_book_bundle_machine_learning_by_oreilly/
AI Weekly 8 September 2018,1536412493,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/9e499n/ai_weekly_8_september_2018/
"Q&amp;A: Spirit &amp; Glitch, A Fashion Brand Designed with Neural Networks",1536349344,,neuralnetworks,continuaco,False,/r/neuralnetworks/comments/9dxsw6/qa_spirit_glitch_a_fashion_brand_designed_with/
Is there any python code for neural network for pattern recognition?,1536282056,,neuralnetworks,VenyWun1729,False,/r/neuralnetworks/comments/9dp7ji/is_there_any_python_code_for_neural_network_for/
Introduction to Reinforcement Learning using MXNet 🤖 • r/MachineLearning,1536263391,,neuralnetworks,tomtx0,False,/r/neuralnetworks/comments/9dmbrj/introduction_to_reinforcement_learning_using/
Counting people in an image of a dense crowd.,1536254786,What are some good approaches that I can use to count the no. of people in a crowd. Tracking each person individually is obviously not an option. Any good approaches or some references to research papers would be very helpful.,neuralnetworks,ronith_sinha,False,/r/neuralnetworks/comments/9dkv74/counting_people_in_an_image_of_a_dense_crowd/
Image tampering detection paper: Does anybody have a link to the code?,1536247225,"Paper: [Learning Rich Features for Image Manipulation Detection](https://arxiv.org/pdf/1805.04953.pdf)

Does anybody know if the code for this is released by the authors publicly, and where to download it from?",neuralnetworks,akgoel,False,/r/neuralnetworks/comments/9djmw5/image_tampering_detection_paper_does_anybody_have/
"Using machine learning, we made avatars work on iPhone 5S and higher Apple devices.",1536242154,,neuralnetworks,gabrelyanov,False,/r/neuralnetworks/comments/9dith8/using_machine_learning_we_made_avatars_work_on/
Yolov2 for object detection tasks (Pytorch implementation),1536239277,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/9didq0/yolov2_for_object_detection_tasks_pytorch/
Company events in Russian software development company,1536223052,,neuralnetworks,Batareika_1,False,/r/neuralnetworks/comments/9dgfs6/company_events_in_russian_software_development/
Yolov2 for object detection task (Pytorch implementation),1536186529,,neuralnetworks,1991viet,False,/r/neuralnetworks/comments/9dbrj8/yolov2_for_object_detection_task_pytorch/
"Machine learning books for beginners, intermediate and advanced learners",1536122961,[https://gl4l.greatlearning.in/machine-learning-in-paperback/](https://gl4l.greatlearning.in/machine-learning-in-paperback/),neuralnetworks,akshad_GL,False,/r/neuralnetworks/comments/9d35jw/machine_learning_books_for_beginners_intermediate/
An Intuitive Explanation of Convolutional Neural Networks.,1536083859,,neuralnetworks,Al-Khazrajy,False,/r/neuralnetworks/comments/9cy33p/an_intuitive_explanation_of_convolutional_neural/
Neural Style Transfer with OpenCV,1536022515,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/9cqycp/neural_style_transfer_with_opencv/
I'm having a go at building a neural net. Can I have some help?,1535977738,,neuralnetworks,Super_S_12,False,/r/neuralnetworks/comments/9cl322/im_having_a_go_at_building_a_neural_net_can_i/
Can anyone reccommend me a nice Tensorflow tutorial for RNN,1535971731,"Hi, this would be my first neural network I create, the purpose of it would be to create music based on some MIDI music I took from TouHou (a game).

**I'd like to know what do I have to define in tensorflow, what is already defined and what do I have to write first.**",neuralnetworks,YoSoyGodot,False,/r/neuralnetworks/comments/9ckhp7/can_anyone_reccommend_me_a_nice_tensorflow/
Everybody dance now! - AI-Based Motion Transfer,1535908517,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9cdq59/everybody_dance_now_aibased_motion_transfer/
detecting datetime patterns in text?,1535888196,"  
I want to explore and experiment the ways in which I could use a neural network to identify patterns in text.

examples:

1. Prices of XYZ stock went down at **11:00** am **today**
2. Retrieve a list of items exchanged on **03/04/2018**
3. Show error logs between 3 - 5 am yesterday.

This is for a project so I am not using regular expressions. Papers, projects, ideas are all welcome but I want to approach feature extraction/pattern detection to have a model trained which can Identify patterns that it has already seen.

I believe I should be tagging the dataset with labels but I have not done something like this before, just some classification experiments and I don't see classification having the ability to solve this problem. Point me towards any source that can help me carry this one out. ",neuralnetworks,ltbringer,False,/r/neuralnetworks/comments/9cbfsk/detecting_datetime_patterns_in_text/
A good video to explain the training step. When I learned NN this was the hardest part. It goes into a lot of detail. Hope it is helpful!,1535887088,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/9cbcuh/a_good_video_to_explain_the_training_step_when_i/
AI Weekly 1 September 2018,1535794647,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/9c1utd/ai_weekly_1_september_2018/
Understaning Positional Embeddings,1535710192,"""Attention is all you need"" used positional encoding to pass additional information into the network. I don't get how that works and why sinusoidal functions can be used for that. Can anyone provide an intuitive explanation?",neuralnetworks,dYuno,False,/r/neuralnetworks/comments/9bscwd/understaning_positional_embeddings/
"Just found this amazing app called Evolution. You make bones and joints, and teach the muscles how to run as fast as he can",1535628472,,neuralnetworks,new24-5,False,/r/neuralnetworks/comments/9bisb3/just_found_this_amazing_app_called_evolution_you/
Need help!! (Details in comment),1535620625,,neuralnetworks,SKBHD,False,/r/neuralnetworks/comments/9bi278/need_help_details_in_comment/
Any evolution simulators you guys recommend?,1535595614,"I recently rigged up an old iPad to stuff. Are there any evolution simulators that I could run? Ideally I’d have it always running, and I could just take a look as I walk by, like a fish tank but cool.",neuralnetworks,FLABBOTHEPIG,False,/r/neuralnetworks/comments/9bfnvn/any_evolution_simulators_you_guys_recommend/
Matrix library requests,1535544575,"Hey, I'm currently thinking about building a small matrix library for java and maybe c++ or c#. I'll be adding dot product multiplication, rounding, mean values etc. Anything you guys want to see? Thanks.",neuralnetworks,Ben_09,False,/r/neuralnetworks/comments/9b8pjg/matrix_library_requests/
A neural networks based football management game!,1535538180,"In total football you open a team of proceduraly generated players, and manage it like in any other football game, play 2 weekly matches + national turnuments to become the champion, 

but in total football the players are being controlled by a neural network! and when you train your players, - you train your neural net!!!!

&amp;#x200B;

THERE IS 0 PAY TO WIN

the ref link only gives you cosmetics!

[https://www.total-football.org/?ref=y17Gwk3VVq8T](https://www.total-football.org/?ref=y17Gwk3VVq8T) ",neuralnetworks,orenog,False,/r/neuralnetworks/comments/9b81cn/a_neural_networks_based_football_management_game/
Implementing Self-Organizing Maps with Python and TensorFlow,1535524498,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/9b6u6a/implementing_selforganizing_maps_with_python_and/
How to Write a Node.js CLI using OpenCV with Neural Network Models for Image Classification,1535473741,,neuralnetworks,Fewthp,False,/r/neuralnetworks/comments/9b0e2t/how_to_write_a_nodejs_cli_using_opencv_with/
Jobs and Career opportunities in Deep Learning,1535365034,"Unless you are with Piscine Patel from life of pi, You definitely know about the hype in AI. Here is an article to read about the existing jobs, compensations and profiles available for a Deep Learning engineer.

[https://gl4l.greatlearning.in/jobs-in-deep-learning-average-compensation-and-locations/](https://gl4l.greatlearning.in/jobs-in-deep-learning-average-compensation-and-locations/)",neuralnetworks,akshad_GL,False,/r/neuralnetworks/comments/9anqjy/jobs_and_career_opportunities_in_deep_learning/
Jeffrey Dean undergrad senior thesis project about parallel training code for neural networks in C,1535358947,,neuralnetworks,yesnoornext,False,/r/neuralnetworks/comments/9an8sw/jeffrey_dean_undergrad_senior_thesis_project/
AI Weekly 25 August 2018,1535192973,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/9a5uyb/ai_weekly_25_august_2018/
Let's talk about Deep Learning!,1535087769,"Earlier this month, I found out about a discussion come talk on Deep Learning which was hosted by Great Learning at their Bangalore center. The guest speaker was Viji Vennelakanti who is Mentor for Andrew Ng's Deep Learning Specialization and has a myriad of certifications under her cap. Hope you guys like it![https://www.facebook.com/GreatLearningOfficial/videos/727580644240280/](https://www.facebook.com/GreatLearningOfficial/videos/727580644240280/)",neuralnetworks,akshad_GL,False,/r/neuralnetworks/comments/99upe0/lets_talk_about_deep_learning/
Emulating Logical Gates with a Neural Network,1535056838,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/99r0lb/emulating_logical_gates_with_a_neural_network/
Emulating Logical Gates with a Neural Network,1535056763,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/99r08v/emulating_logical_gates_with_a_neural_network/
Why does AlphaGo Zero use dihedral rotation instead of normalization?,1534953987,"The [AlphaGo Zero paper](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)  mentions that to take care of the symmetry of the game (action value is  independent of the rotation and reflection of the board), the generated  states are fed into the network using a random dihedral reflection or  rotation:

&gt;Expand and evaluate (Fig. 2b). The leaf node s&lt;sub&gt;L&lt;/sub&gt; is added to a queue for neural network evaluation, (d&lt;sub&gt;i&lt;/sub&gt;(p), v) =  f&lt;sub&gt;θ&lt;/sub&gt;(d&lt;sub&gt;i&lt;/sub&gt;(s&lt;sub&gt;L&lt;/sub&gt;)), where d&lt;sub&gt;i&lt;/sub&gt; is a dihedral reflection or rotation selected uniformly at random from i in \[1..8\].

I wonder: each board can be represented as a long number. **Why don't  take the number form of each rotation and reflection, compare them and  pick the smaller?** (Each time a board is evaluated by the network)

I would think this would result in faster learning, as the network can focus on 1/8th part of all the possible states only. 

This is a desperate [cross-post](https://stackoverflow.com/questions/51877950/why-does-alphago-zero-use-dihedral-rotation-instead-of-normalization) from stackoverflow, which has an open bounty.

I'm also interested if anyone knows how to contact DeepMind devs, it it's possible.

&amp;#x200B;",neuralnetworks,erenon_,False,/r/neuralnetworks/comments/99elxr/why_does_alphago_zero_use_dihedral_rotation/
Input variables: To add or not to add?,1534953010,"The question I have is simple. The answer, I suspect, is not so simple. Suppose I have several input variables for a deep Neural Network. One of those variables is the average value of something measurable. The other variable is its standard deviation.

My question is: Would adding another input variable, which is the standard deviation var divided by the average var, improve the Neural Network accuracy? Or will it not, because the Neural Network can compare those two variables itself and doesn't need that comparison to be done manually? Or is it something else entirely?",neuralnetworks,Drag0nDr0p,False,/r/neuralnetworks/comments/99egy7/input_variables_to_add_or_not_to_add/
Making Music: when simple probabilities do better than complex neural networks,1534901774,,neuralnetworks,shanoxilt,False,/r/neuralnetworks/comments/9994jl/making_music_when_simple_probabilities_do_better/
Google Deep Mind. So THAT'S how it works!,1534899470,"How computers are learning to be creative https://www.ted.com/talks/blaise_aguera_y_arcas_how_computers_are_learning_to_be_creative

What an amazing revelation this must have been for the team to just ""run the network in reverse"". I've played with the deep mind app a lot but had no idea what it was doing. 

Simply amazing. ",neuralnetworks,EebamXela,False,/r/neuralnetworks/comments/998uml/google_deep_mind_so_thats_how_it_works/
How many hidden layers? How many nodes?,1534897910,"I'm very new to neural networks. I watched u/3blu1brown's series on neural networks and was blown away by the simplicity of it. Well... Simplicity might be a stretch but you know what I mean.

Anyway I've been playing around with a couple apps and stuff that let you change the layers, nodes inputs etc... But I havent found anything solid on how to pick the right balance of number of layers number of nodes..

Can someone point me to a good beginners guide to how these decisions are made?",neuralnetworks,EebamXela,False,/r/neuralnetworks/comments/998nuz/how_many_hidden_layers_how_many_nodes/
Help with neural network in python,1534883359,"I'm trying to make a neural network without using outside libraries for help so that I can get an understanding of how it works. I've made something [here](https://pastebin.com/yiPemnUv) that seems to work fine however the training process seems slow and I'm not sure if that just because it's done in python or if I've done something inefficiently. If there's a way to make the training process better (without using stuff like numpy and tensorflow) I'd like to know.

Any help would be much appreciated!

Thanks in advance",neuralnetworks,israphel23,False,/r/neuralnetworks/comments/996psm/help_with_neural_network_in_python/
Learning deep neural networks from scratch,1534784003,"Hello, I recently learned how to create a 2 layer neural network, I havn't got a mathematical background and as I move into deep neural networks I notice the backpropagation has changed.

The way I did the 2 layer network was by calculating the individual derivatives of each weight by using the chain rule. I managed to understand the calculus,  but with neural networks that use matrices and hidden layers they had a different way of calculating the derivatives. Does anyone know what this method is called or knows a good tutorial?

The three layer neural network on this page is a good example [IamTrask Neural Network](https://iamtrask.github.io/2015/07/12/basic-python-network/)

",neuralnetworks,Ben_09,False,/r/neuralnetworks/comments/98vahb/learning_deep_neural_networks_from_scratch/
Deep Learning A-Z™: Hands-On Artificial Neural Networks : Restricted Boltzmann Machine,1534774774,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/98tywl/deep_learning_az_handson_artificial_neural/
Deep Learning based Object Detection using YOLOv3 with OpenCV ( Python / C++ ),1534774639,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/98ty99/deep_learning_based_object_detection_using_yolov3/
Creating NN for gaming on windows?,1534752912,"I’ve been looking in to open ai, but I don’t have enough space to install it on account of all the additional downloads and tools needed to convert the Linux based system to run on Windows. 

Is there an alternative, simple to use, ai suitable for windows? 

What I am trying to get the bot to do is play a 3D mmo. I want it to click something 10 times, then find a NPC on the screen (he moves around) to restart the game, and then repeat. 

I could script it, but I’m looking to do it with AI and image recognition.  ",neuralnetworks,665guideme,False,/r/neuralnetworks/comments/98rsko/creating_nn_for_gaming_on_windows/
Introduction to Self-Organizing Maps,1534748288,,neuralnetworks,RubiksCodeNMZ,False,/r/neuralnetworks/comments/98remz/introduction_to_selforganizing_maps/
My first Object Detection Model - YuGiOh Card Recognition,1534688439,,neuralnetworks,humanovan,False,/r/neuralnetworks/comments/98kjpv/my_first_object_detection_model_yugioh_card/
Two Startups Use Processing in Flash Memory for AI at the Edge,1534687084,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/98keji/two_startups_use_processing_in_flash_memory_for/
What's a good NN for math?,1534684221,"I have made my own, but I want to try someone else's, but so far most of the one's I see online are for images.  


I want to input a few numbers, and predict the answer for an 'unknown' math problem (I'll have the answers while training) and Ideally I would like it to be able to give more than one answer (lets say I want it to solve for 2 variables)  


also Ideally, if it's open source, I'd like it to be in C++ )though not a necessity)  


Any suggestions?  


Thank you.",neuralnetworks,EBlackPlague,False,/r/neuralnetworks/comments/98k4li/whats_a_good_nn_for_math/
Train Neural Network To Rate Rollercoaster,1534683604,[Please help me train my Neural Network by riding and rating a few rollercoasters at http:\/\/stefanhell.com\/rollercoaster\/app\/Rating\/](https://i.redd.it/sf0fv34gs1h11.jpg),neuralnetworks,hellei92,False,/r/neuralnetworks/comments/98k2cn/train_neural_network_to_rate_rollercoaster/
"Intro to optimization in deep learning: Momentum, RMSProp and Adam",1534625124,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/98et1b/intro_to_optimization_in_deep_learning_momentum/
AI Weekly 18 August 2018,1534608114,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/98cprc/ai_weekly_18_august_2018/
Sign Language and Static-Gesture Recognition using CNN,1534530998,,neuralnetworks,blackbird9820,False,/r/neuralnetworks/comments/984shd/sign_language_and_staticgesture_recognition_using/
A light-weight and accurate deep learning model for audiovisual emotion recognition,1534521581,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/983goi/a_lightweight_and_accurate_deep_learning_model/
How Deep Learning Predicts Who Wrote @realDonaldTrump Tweets,1534520050,,neuralnetworks,didtrumptweetit,False,/r/neuralnetworks/comments/98393u/how_deep_learning_predicts_who_wrote/
http://didtrumptweetit.com/how-deep-learning-predicts-who-wrote-realdonaldtrump-tweets/,1534513618,"The custom-implemented parallel n-gram CNN model was [trained](http://didtrumptweetit.com/how-deep-learning-predicts-who-wrote-realdonaldtrump-tweets/#trained) on 6,770 @realDonaldTrump tweets made between the day Mr. Trump started his campaign and the day he took office. [Testing](http://didtrumptweetit.com/how-deep-learning-predicts-who-wrote-realdonaldtrump-tweets/#tested) on an additional ground-truthed set of 15,357 tweets, the classifier has an [accuracy of 98%](http://didtrumptweetit.com/how-deep-learning-predicts-who-wrote-realdonaldtrump-tweets/#accuracy) in identifying whether a given tweet is consistent with those written by Mr. Trump or by his staff.",neuralnetworks,didtrumptweetit,False,/r/neuralnetworks/comments/982e7v/httpdidtrumptweetitcomhowdeeplearningpredictswhowr/
CFP: ZhejiangLab Cup Global Artificial Intelligence Competition 2018,1534403924,"ZhejiangLab Global Artificial Intelligence Competition is held by Zhejiang Lab, Zhejiang Expert Committee on Artificial Intelligence Development (AI top 30), China Artificial Intelligence Industry Development Alliance (AIIA). As the sole competition platform, Zhejiang Lab-Tianchi Alibaba Cloud Platform invites participants to train their models on known classified samples to develop models to recognize unclassified samples.     

This competition consists of two separate contests: Video Recognition and Q&amp;A contest, and Zero-Shot Learning Picture Recognition contest. For **Video Recognition and Q&amp;A** contest, participants need to recognize, analyze specific videos and answer corresponding questions; for **Zero-shot Learning Picture Recognition** contest, participants need to train their models on known classified samples to develop models to recognize unclassified samples.

There is also an **Entrepreneurial Competition** about applying artificial intelligence to improve and solve technical matters in professional fields. The professional fields below are recommended but not required: medical Application (utilizing Artificial Intelligence to diagnose patients’ graphical); petty Commodities (applying Artificial Intelligence to analyze the costs of petty commodities); intelligent Manufacturing (applying Artificial Intelligence to manage the production of manufactures); education Application (utilizing Artificial Intelligence to evaluate education based on AI technology) and business Application (applying Artificial Intelligence to manage businesses etc.) For more details, please refer to [http://aicup2018.zhejianglab.com/](http://aicup2018.zhejianglab.com/) 

**1. Competition Schedule**

a. Preliminary contest: Aug. 1st – Sept. 25th, 2018

b. Semifinal contest: Sept. 28th – Oct. 30th, 2018

c. Final contest: Late November, 2018

**2.  Competition Participants**

The competition is available to the whole society. The upper limit of team size is five members, and there is no lower limit. Individuals related to the competition committee are not allowed to participate in any contest in this competition.

**3.  Registration**

Registration is now open until Sept. 25th, 2018. Click the link below to sign up.

**The official website of this competition:**

[http://aicup2018.zhejianglab.com/](http://aicup2018.zhejianglab.com/)

**Zero-shot Learning Picture Recognition:** [https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409391.333.2.3b1f49feMjToDE&amp;raceId=231677](https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409391.333.2.3b1f49feMjToDE&amp;raceId=231677) 

**Video Recognition and Q&amp;A:** [https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409391.333.4.3b1f49feMjToDE&amp;raceId=231676](https://tianchi.aliyun.com/competition/introduction.htm?spm=5176.11409391.333.4.3b1f49feMjToDE&amp;raceId=231676) 

**Entrepreneurial Competition**

[http://zhejianglab.mikecrm.com/OBBxe7a](http://zhejianglab.mikecrm.com/OBBxe7a) 

**4.  Competition Awards**

**a. Financial Awards** 

\- First place:1 team, **500,000 CNY** with award diploma

\- Second place:2 teams, **250,000 CNY** with award diplomas

\- Third place:3 teams, **100,000 CNY** with award diplomas

\- Honorable mention:4 teams, **10,000 CNY** with award diplomas

**b. Other Awards**

**Internship offers**：Top 3 teams in the final contest will receive internship offers from Zhejiang Lab. Only one member in each team has the opportunity of receiving internship offer;

**Final round interview offers**: Top 10 teams from semifinal contest will receive final round interview offers from Zhejiang lab. Only two members in each team have the opportunities of receiving final round interview offers;

**Writing examination exemption**: Every team and individual who participates in semifinal contest are exempt from writing examination when applying to the Zhejiang Lab. The mentioned participants have the privilege to participate in Zhejiang Lab’s first round interview directly.

**5.  Competition Organization**

\- Guidance Organization: Office of the Central Cyberspace Affairs Commission, The People’s government of Zhejiang Province

\- Host: Zhejiang Lab, Zhejiang Expert Committee on Artificial Intelligence Development (AI top 30), China Artificial Intelligence Industry Development Alliance (AIIA)

\- Co-organizer: Administrative Committee of Zhejiang Hangzhou Future Sci-Tech City, Bank of Nanjing

\- Competition Platform: Zhejiang Lab-Tianchi Alibaba Cloud Platform ([aicup2018.zhejianglab.com](https://www.reddit.com/r/neuralnetworks/aicup2018.zhejianglab.com))

\- Organized by: Zhejiang Lab’s Center of Competition

\- Supported by: Zhejiang University, Hangzhou Dianzi University, Alibaba Cloud etc.

Announcement: The competition is a nonprofit, non-commercial competition, aiming to promote the development of artificial intelligence, and to provide a platform for individuals to research and study. Utilizing this competition or competition data for any commercial activities is prohibited.

**6. Contact us**

If you have any question, please contact us at [AICup2018@zhejianglab.com](mailto:AICup2018@zhejianglab.com). 

*Processing img onk2ghbineg11...*",neuralnetworks,zcgaic2018,False,/r/neuralnetworks/comments/97q661/cfp_zhejianglab_cup_global_artificial/
Looking for suggestion for hardware to house our lab's nice GPU,1534348919,"Hi all!

I apologize if this is the wrong subreddit for this kind of post. If so, could someone please point me to the right one?

Our lab has a Titan XP, that has been sitting around for a few weeks now because we are deciding what motherboard, CPU, RAM, enclosure...etc that would be the most bang for our buck and extract the full power out of the Titan XP.

We are doing Deep Learning with Tensor Flow. 

Thanks!",neuralnetworks,LesPaulMane,False,/r/neuralnetworks/comments/97jgm7/looking_for_suggestion_for_hardware_to_house_our/
Learning Three-dimensional Flow for Interactive Aerodynamic Design - SIGGRAPH 2018,1534280266,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/97c45l/learning_threedimensional_flow_for_interactive/
DeepMind: Objects that Sound,1534266077,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/97a3fa/deepmind_objects_that_sound/
OpenAI Five: Facing Human Pro's in Dota II,1534264614,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/979w4u/openai_five_facing_human_pros_in_dota_ii/
When Recurrent Models Don't Need to be Recurrent,1534263875,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/979scx/when_recurrent_models_dont_need_to_be_recurrent/
Neural Arithmetic Logic Units,1534263726,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/979rjq/neural_arithmetic_logic_units/
There's Waldo is a robot that finds Waldo,1534262289,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/979k6e/theres_waldo_is_a_robot_that_finds_waldo/
Denoising with Kernel Prediction and Asymmetric Loss Functions,1534122479,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/96u1wn/denoising_with_kernel_prediction_and_asymmetric/
Machines learn how to predict bioconcentrations,1533996703,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/96gtc1/machines_learn_how_to_predict_bioconcentrations/
AI Weekly 11 August 2018,1533980807,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/96fgf7/ai_weekly_11_august_2018/
Understanding NN by creating a simple one in Javascript,1533892112,"I am a ML/AI newbie.

I have tried to create a Neural network in Javascript which tries to implement an OR logic gate. Given here - [https://gist.github.com/lakshaydulani/0a8fc8148aabf58278935b4af7013c8f](https://gist.github.com/lakshaydulani/0a8fc8148aabf58278935b4af7013c8f)

Modeled as follows - 

y = ax1 + bx2 + c

where,

y = output

x1 = Input 1

x2 = Input 2

a = Weight for Input 1

b = Weight for Input 2

There is no activation function.

Upon running my training 1,000 OR 10,000 times, I get the following model

a = 0.44, b = 0.47, c = 0.27

Firstly, what I did till now, Is it correct?

If yes, then - 

Output for different cases will be - 

0,0              =&gt;               (0.44 \* 0) + (0.47 \* 0) + 0.27     =  0.27  (should have been 0)

0,1              =&gt;               (0.44 \* 0) + (0.47 \* 1) + 0.27     =  0.74  (should have been 1)

1,0              =&gt;               (0.44 \* 1) + (0.47 \* 0) + 0.27     =  0.71  (should have been 1)

1,1              =&gt;               (0.44 \* 1) + (0.47 \* 1) + 0.27     =  1.18  (should have been 1)

So can I say my activation function should be 

`f(x) = x &gt; 0.7 ? 1 : 0;`

Am I doing it right?",neuralnetworks,lakshaydulani,False,/r/neuralnetworks/comments/965bwy/understanding_nn_by_creating_a_simple_one_in/
can you count integer overflow as a nonlinear activation function?,1533831554,"I didn't know which sub reddit to post on but because it has to do with neural networks I will ask here. I'm trying to make a hardware/integer based network (no float or fixed point), so I'm trying to find an activation function that's easy to make with logic gates (which are digital circuits). I thought maybe integer overflow could be an activation function as it seems nonlinear. thank you in advance.",neuralnetworks,GriffinFire101,False,/r/neuralnetworks/comments/95xvgr/can_you_count_integer_overflow_as_a_nonlinear/
Few activation functions handling various problems - neural networks,1533746929,"Please, explain me, how a few activation functions in neural networks can handle so many problems? I know some basics theory behind ANN, but I can't get what in common have sigmoid function etc. with for example picture classification?",neuralnetworks,mikinoqwert2,False,/r/neuralnetworks/comments/95nq3b/few_activation_functions_handling_various/
Complete Machine Learning Fundamentals in less than 5 minutes,1533738847,,neuralnetworks,thereflective,False,/r/neuralnetworks/comments/95mive/complete_machine_learning_fundamentals_in_less/
Please suggest some outstanding ML or DL Project ideas.,1533738625,Need a cool useful idea for my engineering project,neuralnetworks,Troied,False,/r/neuralnetworks/comments/95mhq9/please_suggest_some_outstanding_ml_or_dl_project/
The SEC Is Going to Postpone the Decision About Bitcoin ETF,1533736567,,neuralnetworks,LinTuan12R8,False,/r/neuralnetworks/comments/95m7i5/the_sec_is_going_to_postpone_the_decision_about/
Chip lights up optical neural network demo,1533732479,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/95lp0i/chip_lights_up_optical_neural_network_demo/
Looking for help with LSTM implementation (x-post from r/MachineLearning),1533731823,"I'm trying to build an LSTM network to classify time series data using the MATLAB neural network toolbox. The input data are readings from an inertial measurement unit at a rate of 50 Hz. The IMU readings include orientation (absolute heading/pitch/roll), plus 3-axis accelerometer. I'm using the IMU on a hammock, and the goal of the network is to output a classification (1 or 0) to identify which direction the hammock is swinging (forwards or backwards, not with respect to the cardinal directions). The key is that the IMU can be oriented arbitrarily relative to the hammock, so it relies on knowledge of the absolute orientation of the IMU in space to identify the swing phase regardless of orientation. To summarize where I'm at so far, I was able to get this to work with a single layer classification network (non-LSTM) but cannot get it working well on the time sequence data using the LSTM. My input data consist of several time series of readings with the IMU in different orientations on the hammock. Notionally the network should be able to figure out which way the hammock is swinging based on the absolute orientation data.

In more detail, I was able to get very good results (\~95% accuracy training, \~90% accuracy validation) using a single hidden layer with 100 neurons. To structure data for input to this network, I created 300-length feature vectors which hold a 100 point time history for each of the 3 orientation axes pitch/heading/roll (the accelerometer data was not used or needed here). In total I have about 50,000 time steps of data.

For the LSTM implementation, I'm basically following [this example](https://www.mathworks.com/help/nnet/examples/sequence-to-sequence-classification-using-deep-learning.html) from MATLAB in the construction of the network. The data were structured as a set of time sequences with 7 features (actual time elapsed between readings, which is 50 Hz/20 ms nominally, plus 3 orientation axes, plus 3 accelerometer axes). Each sequence is a several minute measurement with the IMU in a different orientation on the hammock.  I played around with parameters of the network (number of LSTM layers 1-3, number of elements in hidden layer 100-400, learning rate 0.001-0.02, rate of learning rate reduction, with/without accelerometer data, etc.), but never seem to get very good results. I get decent accuracy on the training data (\~90%), but then very poor results on the validation set. The validation set was created by taking a portion of each sequence, so the network was provided with data in the training set for each IMU orientation it saw in the validation set. To me the training progress looks quite odd too (see below) - rather than a monotonic improvement it jumps around with a rapid drop and then rapid recovery early in training.

https://i.redd.it/zmyt5qfm6ve11.png

Based on the good results I get with the basic classification network, it seems there's enough information in the data set for the LSTM to correctly identify the output, and I'm lost as to where to go next in troubleshooting. In case it's not completely obvious already this is my first time exploring neural network training like this, and would appreciate any tips to identify where I'm going wrong.",neuralnetworks,timwalsh41,False,/r/neuralnetworks/comments/95lmav/looking_for_help_with_lstm_implementation_xpost/
Fake Account of the Crypto Exchange WEX Published News About Its Closure,1533653665,,neuralnetworks,Trimpkh123,False,/r/neuralnetworks/comments/95cbhx/fake_account_of_the_crypto_exchange_wex_published/
Neural Network,1533609094,My proposal was rejected because my prof said the 'stock market prediction' existed a long time ago. Any recommendation on how to enhance the stock market prediction?,neuralnetworks,Vinceeeent,False,/r/neuralnetworks/comments/957pg9/neural_network/
Will Starbucks Start Accepting Cryptocurrencies as a Payment Option?,1533563709,,neuralnetworks,LinTuan12R8,False,/r/neuralnetworks/comments/951h78/will_starbucks_start_accepting_cryptocurrencies/
Implementing a Neural Network with Python in 15 Minutes,1533557076,,neuralnetworks,Zeolearn,False,/r/neuralnetworks/comments/950pf3/implementing_a_neural_network_with_python_in_15/
Synopsis: A Classical Machine Learning Algorithm Goes Quantum,1533517882,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/94wxh1/synopsis_a_classical_machine_learning_algorithm/
Learn how a neural network works!,1533492322,,neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/94ttd4/learn_how_a_neural_network_works/
DarkNet in OpenCL GNU/Linux &amp; macOS Intel &amp; AMD &amp; NVidia GPUs &amp; Raspberry,1533491406,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/94tp0q/darknet_in_opencl_gnulinux_macos_intel_amd_nvidia/
Learn Neural Network Basics!,1533490259,"If you are looking to learn neural network basics, check this video out!",neuralnetworks,antaloaalonso,False,/r/neuralnetworks/comments/94tjpt/learn_neural_network_basics/
AI Weekly 5 August 2018,1533458074,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/94qhpb/ai_weekly_5_august_2018/
Would CNNs be capable of parsing PDF files?,1533455931,"I need to write a PDF parser that can extract not only text from a PDF, but also the layout and determine where each word is. As PDFs are not structured and parsing them is difficult I was thinking of trying to convert pdfs to images then parse using pixel data. Is it feasible for example for one CNN to determine where each line of text is, another to determine where each word is in the line and so on?",neuralnetworks,ryanmccauley211,False,/r/neuralnetworks/comments/94qc4l/would_cnns_be_capable_of_parsing_pdf_files/
"A comprehensive list of entry and adapt level neural network programs, which helped me alot through a larger assignment.",1533414064,,neuralnetworks,BI0B0SS,False,/r/neuralnetworks/comments/94m4bj/a_comprehensive_list_of_entry_and_adapt_level/
3D-printed Deep Learning neural network uses light instead of electrons,1533391567,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/94jcs9/3dprinted_deep_learning_neural_network_uses_light/
New Youtube Channel to make AI Research accessible,1533357634,"Hi, Please check out my channel ([https://www.youtube.com/c/aijournal](https://www.youtube.com/c/aijournal)) which talks about research in Deep Learning, Machine Learning, Reinforcement Learning, NLP. The main aim is to bring as much research content available as possible. Majority of information is stored inside papers, and very few people have access to it. I want to bring out the best out of all of them.

I also have a subreddit ([https://www.reddit.com/r/aijournal/](https://www.reddit.com/r/aijournal/)) if you have any doubts regarding the content being discussed on the channel. We're a really friendly community interested in cutting edge research in ML.",neuralnetworks,vector_machines,False,/r/neuralnetworks/comments/94gjca/new_youtube_channel_to_make_ai_research_accessible/
Predicting customer churn in banking using ANN,1533310767,,neuralnetworks,blackbird9820,False,/r/neuralnetworks/comments/94amps/predicting_customer_churn_in_banking_using_ann/
From Producing Ice-Tea to Mining Bitcoin? The SEC Says NO,1533306532,,neuralnetworks,Trimpkh123,False,/r/neuralnetworks/comments/94a1pg/from_producing_icetea_to_mining_bitcoin_the_sec/
"We are now entering production, see you soon",1533237815,,neuralnetworks,Neuralgate,False,/r/neuralnetworks/comments/942cop/we_are_now_entering_production_see_you_soon/
"We are now entering production, please stay in your seats",1533237024,Trapped,neuralnetworks,Neuralgate,False,/r/neuralnetworks/comments/9428g5/we_are_now_entering_production_please_stay_in/
How to fix reinforcement learning,1533222985,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/94057d/how_to_fix_reinforcement_learning/
Top 10 Bitcoin Hotspots Around the World,1533218053,,neuralnetworks,LinTuan12R8,False,/r/neuralnetworks/comments/93zgkz/top_10_bitcoin_hotspots_around_the_world/
Morenet.com - Since 1999 - Cheap BIN For Quick Sale,1533162926,"**Domain Name For Sale: Morenet.com​**  
**Registrar:** **Godaddy**  
**Created:** **1999-01-25**  
**Expiration Date:** **2020-01-25**  
**Buy Now Price:** **$500**​

**This is a 24 hour quick sale - First time to come to market. Listed on Undeveloped For $3,999.**

* Buy It Now price is $500
* If your offer is below $500,send in your bids. Good bids are welcome.
* This is a great opportunity for Redditors to own a great brandable domain name.
* This domain is good for networking and related products and services.
* Sale on Reddit closes in 24 hours. If no sale, price returns to $3,999 via undeveloped.com
* Escrow and BTC
* Domain name transfer to take place instantly
* Interested in buying, send me a PM
* If you want to discuss an offer, PM me let's talk",neuralnetworks,elidanet,False,/r/neuralnetworks/comments/93trp7/morenetcom_since_1999_cheap_bin_for_quick_sale/
Why do we use the quadratic equation?,1533132326,"Hey everybody! I'm learning about NNs, and I had a simple question: Why do we use the quadric function as our cost function instead of a simple "" |(target - actual)| ""? This solution seems just as good. Why not use it?

Thx for any answers!",neuralnetworks,MadScientist2854,False,/r/neuralnetworks/comments/93p8mw/why_do_we_use_the_quadratic_equation/
Simbrain,1533066887,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/93hta9/simbrain/
Tutorial: Derivation of Convolutional Neural Network from Fully Connected Network,1533055428,,neuralnetworks,Rojas561,False,/r/neuralnetworks/comments/93g3x1/tutorial_derivation_of_convolutional_neural/
"Researchers recreate a brain, piece by piece",1533044273,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/93ejm1/researchers_recreate_a_brain_piece_by_piece/
OpenAI + DOTA2: 180 Years of Learning Per Day,1533043498,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/93eg44/openai_dota2_180_years_of_learning_per_day/
Experimenting with Tensorflow.js for Deep learning,1533015774,"Hello Everyone,

I have been experimenting on Tensorflow.js for sometime , I would like to Share my Learnings with the Community.

We know that An increasing number of developers are using TensorFlow in their machine learning projects. In March this year, the TensorFlow team at Google announced the arrival of the much-awaited JavaScript framework, TensorFlow.js (which was previously called DeepLearn.js).

Now developers can build lightweight models and run them in the browser using JavaScript. Let’s understand what the need was for the development of this framework.

## History

Before going to TensorFlow.js, I would like to start off with TensorFlow.

TensorFlow was developed in 2011 at Google as their propitiatory library for Machine learning/Deep learning applications at Google. This library was open sourced in 2015 under the Apache License.

TensorFlow is built in C++, which enables the code to execute at a very low level. TensorFlow has bindings to different language like Python, R, &amp; Java. This enables TensorFlow to be used in these languages.

So, the obvious question is: what about JavaScript?

Conventionally, in JavaScript, ML/DL was performed by using an API. An API was made using some framework, and the model was deployed at the server. The client sent a request using JavaScript to get results from the server.

[Client Server Architecture](https://i.redd.it/gmg7z25rz7d11.png)

In 2017, a project called Deeplearn.js appeared, which aimed to enable ML/DL in JavaScript, without the API hassle.

But there were questions about speed. It was very well known that JavaScript code could not run on GPU. To solve this problem, WebGL was introduced. This is a browser interface to OpenGL. WebGL enabled the execution of JavaScript code on GPU.

In March 2018, the DeepLearn.js team got merged into the TensorFlow Team at Google and was renamed TensorFlow.js.

Watch the below video for further details:

[https://youtu.be/qa1OXssGBHw](https://youtu.be/qa1OXssGBHw)

## TensorFlow.js

Tensorflow.js provides two things:

* The CoreAPI, which deals with the low level code
* LayerAPI is built over the CoreAPI, and makes our lives easier by increasing the level of abstraction.

## Getting Started

There are two main ways to get TensorFlow.js in your project:

## 1. via &lt;script&gt; Tag

Add the following code to an HTML file:

    &lt;html&gt;
    &lt;head&gt;
      &lt;!-- Load TensorFlow.js --&gt;
        &lt;script src=""https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0""&gt; &lt;/script&gt;
      &lt;/head&gt;
        &lt;body&gt;
          Hello
      &lt;/body&gt;
    &lt;/html&gt;

## 2. via NPM

Add TensorFlow.js to your project using yarn or npm.

    yarn add @tensorflow/tfjs   
    npm install @tensorflow/tfjs    

In your main js file:

    import * as tf from '@tensorflow/tfjs';    

## CoreAPI

## 1. Tensors

So, what is a Tensor ?

[Visual Representation of Scalar,Vector,Matrix and Tensor](https://i.redd.it/f6gueo9oz7d11.jpg)

A scalar is a single number. For example, x = 1

* A vector is an array of numbers. For example, *x*=\[1,2\]
* A matrix is a 2-D array =&gt; (\[\[1, 2\], \[3, 4\], \[5, 6\]\])
* A tensor is a \*n-\*dimensional array with *n*\&gt;2

TensorFlow.js has utility functions for common cases like Scalar, 1D, 2D, 3D and 4D tensors, as well a number of functions to initialize tensors in ways useful for machine learning.

## Code Examples

**tf.tensor():**

    // Pass an array of values to create a vector.    
    tf.tensor([1, 2, 3, 4]).print();   

**tf.scalar():**

    tf.scalar(3.14).print();    

And so on…

Watch the Below Video to get a deep insight into Tensors in TensorFlow.js:

[https://youtu.be/sZrwxnIfHCo](https://youtu.be/sZrwxnIfHCo)

## 2. Variables &amp; Operations

Tensors are immutable data structures. That means their values can’t be changed once they are set.

However, tf.variable()is introduced in TensorFlow.js. The real use case for tf.variable()is when we need to change the data frequently, such as when adjusting model weights in Machine Learning.

Code sample:

    const x = tf.variable(tf.tensor([1, 2, 3]));    
    x.assign(tf.tensor([4, 5, 6]));    
    x.print();    

## Operations

There are various operations in TensorFlow.js. In order to perform mathematical computation on Tensors, we use operations. Tensors are immutable, so all operations always return new Tensors and never modify input Tensors. So tf.variable()can be used in order to save memory.

Let’s look into some operations:

**tf.add() — Adds two** [**tf.Tensor**](https://js.tensorflow.org/api/0.12.0/#class:Tensor)**s element-wise**

    const a = tf.tensor1d([1, 2, 3, 4]);    
    const b = tf.tensor1d([10, 20, 30, 40]);    
    a.add(b).print();  // or tf.add(a, b)    

There are many operations in TensorFlow.js. You can check the [documentation](https://js.tensorflow.org/api/0.12.0/#Operations)for other operations. I will demonstrate one more operation here: **tf.matmul()**

**tf.matmul() — Computes the dot product of two matrices, A \* B.**

    const a = tf.tensor2d([1, 2], [1, 2]);    
    const b = tf.tensor2d([1, 2, 3, 4], [2, 2]);   
    a.matMul(b).print();  // or tf.matMul(a, b)    

Watch the below video for deep insight into Variable and Operations:

[https://youtu.be/AP1BmP0BZmQ](https://youtu.be/AP1BmP0BZmQ)

## 3. Memory Management

Memory management is the key in Machine Learning/Deep Learning tasks, because they are generally computationally expensive.

TensorFlow.js provides two major ways to manage memory:

1. tf.dispose()
2. tf.tidy()

They both typically do the same thing, but they do it in different ways.

## tf.tidy()

This executes the provided function function and after it is executed, cleans up all intermediate tensors allocated by function except those returned by function.

tf.tidy() helps avoid memory leaks. In general, it wraps calls to operations in [tf.tidy()](https://js.tensorflow.org/api/0.12.0/#tidy) for automatic memory cleanup.

Code example:

    const y = tf.tidy(() =&gt; {
        // aa, b, and two will be cleaned up when the tidy ends.
    
        const two= tf.scalar(2); 
        const aa = tf.scalar(2); 
        const b = aa.square();
    
        console.log('numTensors (in tidy): ' + tf.memory().numTensors);
    
        // The value returned inside the tidy function will return // through the tidy,     in this case to the variable y. 
    
        return b.add(two); 
    });
    
    console.log('numTensors (outside tidy): ' + tf.memory().numTensors); y.print(); 

# tf.dispose()

Disposes any [tf.Tensor](https://js.tensorflow.org/api/0.12.0/#class:Tensor)s found within the mentioned object.

Code example:

    const two= tf.scalar(2);    
    two.dispose()    

## LayersAPI

Layers are the primary building block for constructing a ML/DL Model. Each layer will typically perform some computation to transform its input to its output. Under the hood, every layer uses the CoreAPI of Tensorflow.js.

Layers will automatically take care of creating and initializing the various internal variables/weights they need to function. So, basically it makes life easier by increasing the level of abstraction.

We will make a simple example feed forward network using the LayerAPI. The Feed Forward network we will build is as below:

[Neural Network](https://i.redd.it/sekifpiaz7d11.gif)

## Code:

**Index.html**

    &lt;html&gt;
    &lt;head&gt;
    &lt;title&gt;
    &lt;/title&gt;    
       &lt;script src=”https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.12.0""&gt; &lt;/script&gt;
    &lt;script src=”main.js” type=”text/javascript”&gt;&lt;/script&gt;
    &lt;/head&gt;
    &lt;body&gt;
    Tensorflow JS Demo
    &lt;/body&gt;
    &lt;/html&gt;

**main.js**

    const model = tf.sequential();
    
    //config for layer
    const config_hidden = {
      inputShape:[3],
      activation:'sigmoid',
      units:4
    }
    const config_output={
      units:2,
      activation:'sigmoid'
    }
    
    //defining the hidden and output layer
    const hidden = tf.layers.dense(config_hidden);
    const output = tf.layers.dense(config_output);
    
    //adding layers to model
    model.add(hidden);
    model.add(output);
    
    //define an optimizer
    const optimize=tf.train.sgd(0.1);
    
    //config for model
    const config={
    optimizer:optimize,
    loss:'meanSquaredError'
    }
    
    //compiling the model
    model.compile(config);
    
    console.log('Model Successfully Compiled');
    
    //Dummy training data
    const x_train = tf.tensor([
      [0.1,0.5,0.1],
      [0.9,0.3,0.4],
      [0.4,0.5,0.5],
      [0.7,0.1,0.9]
    ])
    
    //Dummy training labels
    const y_train = tf.tensor([
      [0.2,0.8],
      [0.9,0.10],
      [0.4,0.6],
      [0.5,0.5]
    ])
    
    //Dummy testing data
    const x_test = tf.tensor([
      [0.9,0.1,0.5]
    ])
    
    train_data().then(function(){
      console.log('Training is Complete');
      console.log('Predictions :');
      model.predict(x_test).print();
    })
    
    async function train_data(){
      for(let i=0;i&lt;10;i++){
      const res = await model.fit(x_train,y_train,epoch=1000,batch_size=10);
       console.log(res.history.loss[0]);
      }
    }

Output:

[Output of The code](https://i.redd.it/xm6j2hm5z7d11.png)

Pls watch the below videos for deep insight and code explanation:

[https://youtu.be/z2u-s3NzHhY](https://youtu.be/z2u-s3NzHhY)

[https://youtu.be/lKWUSkwOR5s](https://youtu.be/lKWUSkwOR5s)

## My take on this

This is excellent for coders who are familiar with JavaScript and are trying to find their way in the ML/DL world!

It makes things a lot simpler for people coming from a non-ML/DL background, but who are looking to understand this field. The use cases for this are many, and I personally think it’s something we need at the moment.

What do you think about TensorFlow.js? Let me know in the comments section below.

**Thanks For Reading and Giving your Precious Time**",neuralnetworks,i_am_adl,False,/r/neuralnetworks/comments/93bsic/experimenting_with_tensorflowjs_for_deep_learning/
Python Neural Network Help For Digit Recognition,1532958846,"Hello,
I am developing my own feed-forward neural network to recognize 8x8 hand written digits from the sklearn.dataset package.
The training process takes about a hour and a half to get a total cost of about 1.0-1.5 and when I am trying to input a number the network didn't see in the training batch it's totally wrong. I was considering to use a softmax layer at the end but I am not so sure how to implement it in code.

	import numpy as np
	from matplotlib import pylab as plt
	from matplotlib import pyplot as Plt
	from sklearn.datasets import load_digits
	import sys
	import pygame

	finishProgram = False
	mouseDown = False
	interruptLearning = False

	mouse_x, mouse_y = 0, 0
	pygame.init()

	userInputArray = np.zeros((8, 8))

	gameDisplay = pygame.display.set_mode((480, 240))
	pygame.display.set_caption(""Input Layer"")
	clock = pygame.time.Clock()

	def sigmoidScalar(x):
		return 1 / (1 + np.exp(-x))

	def sigmoid_derivativeScalar(x):
		return sigmoidS(x) * (1 - sigmoidS(x))

	sigmoid = np.vectorize(sigmoidScalar)
	sigmoid_derivative = np.vectorize(sigmoid_derivativeScalar)

	class NeuralNetwork:
		def __init__(self, inputCount, hiddenCount, outputCount):
			self.inputCount = inputCount
			self.hiddenCount = hiddenCount
			self.outputCount = outputCount
			self.weights1 = np.random.randn(inputCount, hiddenCount)
			self.weights2 = np.random.randn(hiddenCount, outputCount)
			self.bias1 = np.random.randn()
			self.bias2 = np.random.randn()
			
			self.plotX = []
			self.plotY = []
			
			self.zeroBackpropagationVariables()
			
		def feedForward(self, x):
			self.inputLayer = x / 16
			
			#print(self.inputLayer.shape, self.weights1.shape)
			self.preHiddenLayer = np.dot(self.inputLayer, self.weights1) + self.bias1
			self.hiddenLayer = sigmoid(np.asarray(self.preHiddenLayer))
			self.preOutputLayer = np.dot(self.hiddenLayer, self.weights2) + self.bias2
			self.outputLayer = sigmoid(np.asarray(self.preOutputLayer))
			
			#print(self.inputLayer)
			#print(self.weights1)
			#print(self.preHiddenLayer)
			#print(self.hiddenLayer)
			#print(self.weights2)
			#print(self.preOutputLayer)
			print(self.outputLayer)
			
		def backpropagation(self, y, index):
			self.diff = np.subtract(self.outputLayer, y)
			self.cost = np.sum(np.square(self.diff))
			
			self.dcost_dpred = 2 * self.diff
			self.dpred_dpout = sigmoid_derivative(self.preOutputLayer)
			self.dpout_dw2 = self.hiddenLayer
			
			self.outputLayerDelta = np.multiply(self.dcost_dpred, self.dpred_dpout)
			self.dpout_dw2.shape = [1, self.hiddenCount]
			self.outputLayerDelta.shape = [1, self.outputCount]
			self.dcost_dw2 += np.dot(self.dpout_dw2.T, self.outputLayerDelta)
			
			self.dpout_dhidden = self.weights2
			self.dhidden_dphidden = sigmoid_derivative(self.preHiddenLayer)
			self.dphidden_dw1 = self.inputLayer
			
			self.hiddenLayerDelta = np.multiply(np.dot(self.outputLayerDelta, self.dpout_dhidden.T), self.dhidden_dphidden)
			self.dphidden_dw1.shape = [1, self.inputCount]
			self.dcost_dw1 += np.dot(self.dphidden_dw1.T, self.hiddenLayerDelta)
			
			self.dcost_dbias2 += self.outputLayerDelta
			self.dcost_dbias1 += self.hiddenLayerDelta
			
			print(y)
			print(self.cost)
			#print(""==============="")
			#print(self.dcost_dpred)
			#print(self.dpred_dpout)
			#print(self.dpout_dw2)
			#print(""==============="")
			#print(self.dcost_dw2)
			#print(self.dcost_dw1)
			#print(self.bias1)
			#print(self.bias2)
			print(""\n\n\n"")
			
		def updateWeights(self, batchSize):
			self.weights1 -= np.multiply(self.dcost_dw1 / batchSize, learningRate)
			self.weights2 -= np.multiply(self.dcost_dw2 / batchSize, learningRate)
			self.bias1 -= (self.dcost_dbias1 / batchSize) * learningRate
			self.bias2 -= (self.dcost_dbias2 / batchSize) * learningRate
			
			self.zeroBackpropagationVariables()
			
		def zeroBackpropagationVariables(self):
			self.dcost_dw1 = np.zeros((self.inputCount, self.hiddenCount))
			self.dcost_dw2 = np.zeros((self.hiddenCount, self.outputCount))
			self.dcost_dbias1 = 0
			self.dcost_dbias2 = 0

	def gameLoop(currentEpoch = -1):
		gameDisplay.fill((255, 255, 255))

		global mouse_x, mouse_y
		mouse_x, mouse_y = -1, -1
		global mouseDown
		global finishProgram

		for event in pygame.event.get():
			if event.type == pygame.QUIT:
				finishProgram = True
				print(""Requesting Quit"")
			if event.type == pygame.MOUSEBUTTONDOWN:
				mouseDown = True
			if event.type == pygame.MOUSEBUTTONUP:
				mouseDown = False
			if event.type == pygame.KEYDOWN:
				if event.key == pygame.K_KP_ENTER:
					print(""Try this one!"")
			if event.type == pygame.KEYUP:
				if event.key == pygame.K_KP_ENTER:
					print(""JUST TRYING!"")
					global userInputArray
					userInputArray.shape = [1, 64]
					nn.feedForward(userInputArray)
					print(userInputArray)
					userInputArray = np.zeros((8, 8))
					print(""Lets try another one!"")
				if event.key == pygame.K_c:
					userInputArray = np.zeros((8, 8))
				if event.key == pygame.K_x:
					interruptLearning = True

		if mouseDown == True:
			mouse_x, mouse_y = pygame.mouse.get_pos()
			print(""mouse pos: ("" + str(mouse_x) + "", "" + str(mouse_y) + "")\n"")

		for k in range(8):
			for l in range(8):
				if currentEpoch == -1:
					grayscaleColor = int(userInputArray[k][l]) / 16
					rgbColor = (grayscaleColor * 255, grayscaleColor * 255, grayscaleColor * 255)
					pygame.draw.rect(gameDisplay, rgbColor, [k * 30, l * 30, 30, 30])
				else:
					grayscaleColor = int(dataset[currentEpoch][l * 8 + k]) / 16
					rgbColor = (grayscaleColor * 255, grayscaleColor * 255, grayscaleColor * 255)
					pygame.draw.rect(gameDisplay, rgbColor, [k * 30, l * 30, 30, 30])

		if mouse_x &gt; 240:
			x_pos = int(np.floor((mouse_x - 240) / 30))
			y_pos = int(np.floor(mouse_y / 30))

			userInputArray[x_pos][y_pos] += 1

			
			up = True
			down = True
			left = True
			right = True

			if x_pos == 0:
				left = False
			if x_pos == 7:
				right = False
			if y_pos == 0:
				up = False
			if y_pos == 7:
				down = False

			if up:
				userInputArray[x_pos][y_pos - 1] += 0.3
			if down:
				userInputArray[x_pos][y_pos + 1] += 0.3
			if left:
				userInputArray[x_pos - 1][y_pos] += 0.3
			if right:
				userInputArray[x_pos + 1][y_pos] += 0.3
			
		for k in range(8):
			for l in range(8):
				if userInputArray[k][l] &gt; 16:
					userInputArray[k][l] = 16
				grayscaleColor = userInputArray[k][l] / 16
				rgbColor = (grayscaleColor * 255, grayscaleColor * 255, grayscaleColor * 255)
				pygame.draw.rect(gameDisplay, rgbColor, [k * 30 + 240, l * 30, 30, 30])

		pygame.display.update()
		clock.tick(60)

		if finishProgram == True:
			pygame.quit()
			sys.exit()

	datasetRaw = load_digits()
	dataset = datasetRaw.data
	datasetLabels = datasetRaw.target
	mini_batch_count = 4

	epochs = 300
	learningRate = 0.25

	costPlot = np.zeros(epochs)
	#print(datasetLabels)

	nn = NeuralNetwork(64, 32, 10)

	for i in range(epochs):
		if interruptLearning == False:
			averageCost = 0
			for j in range(4):
				for minibatchCounter in range(int(dataset.shape[0] / 4)):
					nn.feedForward(dataset[j * 4 + minibatchCounter])
				
					tempTarget = np.ndarray(nn.outputCount)
					if datasetLabels[j * 4 + minibatchCounter] == 0:
						tempTarget = ([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 1:
						tempTarget = ([0, 1, 0, 0, 0, 0, 0, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 2:
						tempTarget = ([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 3:
						tempTarget = ([0, 0, 0, 1, 0, 0, 0, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 4:
						tempTarget = ([0, 0, 0, 0, 1, 0, 0, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 5:
						tempTarget = ([0, 0, 0, 0, 0, 1, 0, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 6:
						tempTarget = ([0, 0, 0, 0, 0, 0, 1, 0, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 7:
						tempTarget = ([0, 0, 0, 0, 0, 0, 0, 1, 0, 0])
					elif datasetLabels[j * 4 + minibatchCounter] == 8:
						tempTarget = ([0, 0, 0, 0, 0, 0, 0, 0, 1, 0])
					else:
						tempTarget = ([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])
				
					nn.backpropagation(tempTarget, i)
					averageCost += nn.cost
					print(str(i) + ""("" + str(j) + "")("" + str(minibatchCounter) + "")  "" + str(pygame.time.get_ticks() / 1000) + ""sec"")
					gameLoop(j * 4 + minibatchCounter)

				nn.updateWeights(dataset.shape[0])

				costPlot[i] = averageCost / int(dataset.shape[0] / 4)

	finished = False
	finishProgram = False

	while finishProgram == False:
		#do stuff
		if finished == False:
			print()
			print(""finished in "" + str(pygame.time.get_ticks() / 1000) + ""sec"")
			costPlot = np.asarray(costPlot)
			costPlot.reshape((-1, epochs))
			plt.plot(np.arange(epochs), costPlot)
			plt.title(""loss vs time - learningRate = "" + str(learningRate))
			plt.xlabel(""currect epoch"")
			plt.ylabel(""total cost"")
			plt.show()
			finished = True
		gameLoop()

What am I doing wrong? is it hyper parameters? or some kind of misspelled code line? maybe slow computer? (anyways I think it will be helpful to train my future networks on the GPU so if you know how to run python like that with sublime it will be great).

Thank you for ANY help,
Arad.",neuralnetworks,Aradarbel10,False,/r/neuralnetworks/comments/9348st/python_neural_network_help_for_digit_recognition/
IGLOO: A Different Paradigm For Processing Sequences without Recurrent Neural Networks,1532957030,,neuralnetworks,molode,False,/r/neuralnetworks/comments/9340tl/igloo_a_different_paradigm_for_processing/
The Gibraltarian Football Players Will Receive Salary in Cryptocurrency,1532952837,,neuralnetworks,StivJonson12DD,False,/r/neuralnetworks/comments/933jom/the_gibraltarian_football_players_will_receive/
Non Python Books,1532923241,"Can anyone point me to some books that explain in depth neural network architecture for a beginner. I dont want a book that uses tensorflow because I want to learn how neural networks work, not how to operate a black box.",neuralnetworks,SlickLikeOwl,False,/r/neuralnetworks/comments/930vat/non_python_books/
Need a bit more help,1532896650,"I posted a request for suggestions as to what lessons are the best for neural networks. Sadly, most of the resources are about the math, but not about the implementation. I did find a few that were helpful, but I'm looking for more than just data processing. Nothing that was recommended went beyond the basics, and I really want to do something with images and games, rather than just plain data. I was also looking for information on on genetic algorithms, but I found very little of that. 
Another downside was that the better courses cost money, but I don't want to spend money on this. If anyone knows of any course what so ever, I'd really appreciate the help. thanks. ",neuralnetworks,GamingTimelord19,False,/r/neuralnetworks/comments/92xttl/need_a_bit_more_help/
Georgia Tech Solves 'Texture Fill' Problem with Machine Learning,1532880072,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/92vrhu/georgia_tech_solves_texture_fill_problem_with/
Questions about modeling human perception of 1 dimensional tactile motion patterns,1532860409,"I've taken on a project that involves building a computational model (a neural network of 'some sort' was suggested) that reproduces the psychophysical findings of certain experiments in tactile perception. These experiments reveal 'filling-in' effects in human perception of touch (akin to filling in of the physiological blind spot in vision: [https://en.wikipedia.org/wiki/Filling-in](https://en.wikipedia.org/wiki/Filling-in)). Ideally, by modelling these experiments, we will confirm/refute hypotheses that certain neural mechanisms underpin filling-in (e.g. lateral disinhibition of neurons, synaptic plasticity) and potentially form new hypotheses. Ultimately, the broader project is investigating the idea that *stimulus motion is the organising principle of sensory maps in the cortex* (think this [https://en.wikipedia.org/wiki/Cortical\_homunculus](https://en.wikipedia.org/wiki/Cortical_homunculus) and how it's plastic).   


The two studies that my model will be based on are:   
1. [http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090892](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090892)

2. [https://www.ncbi.nlm.nih.gov/pubmed/26609112](https://www.ncbi.nlm.nih.gov/pubmed/26609112)

https://i.redd.it/mofnmsp00vc11.png

In sum, either 'Single' or 'Double' apparatus brushes repeatedly up and down the arm, over a metal occluder. The studies simulate surgical manipulation / suturing of the skin (in the Double condition) on naive participants, who report no spatial fragmentation in the motion path (even though there clearly is one). This effect is immediate. In the Single condition, over time, the perceived size of the occluder shrinks. Localisation tasks also show that repeated exposure to these stimuli (moreso the Double condition) cause increasing compressive mislocalisation of stationary test stimulus at locations marked with letters on the arm.  In the second study, which uses only the Double stimulus, greater mislocalisation is found for slower stimulus speeds. 

After 4 months of reading into all types of neural networks, I feel like I've learnt a lot but at the same time feel more lost than I was upon taking on the project, with respect to what my model will look like, and still struggling with the most fundamental of questions like ""*How should I encode motion (the input) and how can control velocity?""*    
Another problem I'm having is that I seem attached to some false dilemma between the use of neural networks for data science and for computational neuroscience, while I realise the scope of this project is somewhere in between both; in other words, I am not trying to simply train something like a backprop network with the independent variables as inputs and the results as outputs. There are neurophysiological features that should be incorporated (such as lateral and feedback connections at upper layers, which will facilitate self-organisation) and a degree of biological realism needs to be maintained (e.g. the input layer should represent the skin surface). Because of this I have read into things like dynamic neural field self-organising maps ([http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0040257)) which are more on the side of computational neuroscience. However, I think that the biological realism criterion for these kinds of models is too stringent for my purposes and they fall closer to the implementation level in Marr's hierarchical analysis, whereas my model will be closer to the algorithmic level (see here if you're unfamiliar: [https://en.wikipedia.org/wiki/David\_Marr\_(neuroscientist)#Levels\_of\_analysis](https://en.wikipedia.org/wiki/David_Marr_(neuroscientist)#Levels_of_analysis))  


**tl;dr / question**

I am trying to make a neural network where the input represents tactile stimulation moving in a one dimensional motion path. The graphs below clearly show the kind of effect I am investigating. The output of the network will be the human percept. In case (a) (below, corresponding to the 'Single' brush above), repeated exposure will cause reorganisation such that higher layer neurons 'forget' about the numb spot (occluded part of skin), the perceived gap shrinks and subsequent stationary stimuli reveal some degree of compressive mislocalisaton, as in the case of skin lesions or amputation (where receptive fields have been shown to expand). In the case of (c) (corresponding to the 'Double' brush), the perceived gap is immediately abridged (to reconcile the spatio-temporal incongruity of the stimulus input) and the compressive mislocalisation effects are accelerated and more pronounced compared to the case of (a).

https://i.redd.it/m5yloxdx3vc11.png

I have considered and started working on dynamic neural fields, self-organising maps, LSTM networks, ""self-organising recurrent networks"" and have even tried making an array of Reichardt detectors for the input layer because the encoding of motion is still confusing. Sorry if this post is a bit all over the place or unclear but I just need some guidance in terms of what kind of architecture to use, how to encode my input and the best tools to use? I'm currently using Simbrain ([http://simbrain.net/](http://simbrain.net/)) mostly but have been working a bit in Python as well, and have been recommended PyTorch but I'm yet to try it out. Again, sorry for the word salad and I can clarify anything that's unclear if needed. Cheers",neuralnetworks,nazuri33,False,/r/neuralnetworks/comments/92twhn/questions_about_modeling_human_perception_of_1/
A Newbie’s Guide to Stochastic Gradient Descent With Restarts,1532800072,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/92nyyr/a_newbies_guide_to_stochastic_gradient_descent/
Understanding Tensorflow's tensors shape: static and dynamic,1532786278,,neuralnetworks,pgaleone,False,/r/neuralnetworks/comments/92mako/understanding_tensorflows_tensors_shape_static/
Deep learning and free software: will trained neural net weights ever be considered free software?,1532635011,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/925ljs/deep_learning_and_free_software_will_trained/
Machine Learning and Materials Science,1532552985,"To begin with, I am quite new to working with neural networks. I studied materials engineering and currently doing a master ‘s degree on materials science. I am about to conclude an internship where I was given a goal to develop a neural network which can interpret X-Ray Fluorescence spectra and identify elements present in a sample. I had absolutely zero prior knowledge on machine learning, and limited experience on coding in general, so this internship definitely was fascinating for me. After messing around quite a lot with conventional nets, I ended up having success with convolutional nets on this task.

I definitely would like to dig deeper into neural nets, and machine learning in general, especially into applications thereof in materials science. I have been using Keras with Tensorflow backend during my internship. Any recommendations on what my next step should be? I am in particular interesting in learning more about RNN/LSTM’s and Decision Trees, my dilemma is that I find the papers difficult to read through because of my limited background on computer science and its vocabulary, so I’ve been trying to find a healthy equilibrium between papers and blogs.

Any recommendations on thorough books on machine learning? When I ask this question to my  friends studying computer science, they usually refer me to some websites with tutorials and so on (like Kaggle), but maybe because of my background in physics, I really prefer reading well structured books/pdfs instead of going through tutorials/blogs.

Hope I’ve made myself clear, I’d appreciate any direction you can give me :)",neuralnetworks,Dobix,False,/r/neuralnetworks/comments/91vvxz/machine_learning_and_materials_science/
"Stealing Bitcoins. Basic ways, How it happens and How to protect yourself | The Coin Shark",1532523932,,neuralnetworks,StivJonson12DD,False,/r/neuralnetworks/comments/91rnh6/stealing_bitcoins_basic_ways_how_it_happens_and/
Deep learning cracks the code of messenger RNAs and protein-coding potential,1532446492,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/91iflg/deep_learning_cracks_the_code_of_messenger_rnas/
Using machine learning to detect software vulnerabilities,1532446455,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/91ifea/using_machine_learning_to_detect_software/
Developing brain atlas using deep learning algorithms,1532446422,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/91if7o/developing_brain_atlas_using_deep_learning/
Biometric Mirror highlights flaws in artificial intelligence,1532446375,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/91iexv/biometric_mirror_highlights_flaws_in_artificial/
Google Became a Partner of the Blockchain Startup Digital Asset and Added Cryptocurrency Converter to its Search,1532441794,,neuralnetworks,JamesCooper12R1,False,/r/neuralnetworks/comments/91hrk8/google_became_a_partner_of_the_blockchain_startup/
Supercharging Classification - The Value of Multi-task Learning,1532419842,,neuralnetworks,jackblun,False,/r/neuralnetworks/comments/91fhyv/supercharging_classification_the_value_of/
What is the Difference Between a Batch and an Epoch in a Neural Network?,1532419304,,neuralnetworks,magneticono,False,/r/neuralnetworks/comments/91fgex/what_is_the_difference_between_a_batch_and_an/
What’s New in Deep Learning Research: Reinforcement Learning with Infinite Agents Using Mean Field Games,1532353541,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9176o1/whats_new_in_deep_learning_research_reinforcement/
Recommend Book on Neural Networks,1532350756,"Hi

I am looking for an introductory book on neural networks and deep learning.

Something in-depth both also for the absolute beginner (easy to follow) with some (but not much) programming experience.

Thanks in advance.",neuralnetworks,Phreakasa,False,/r/neuralnetworks/comments/916uk0/recommend_book_on_neural_networks/
GOTURN : Deep Learning based Object Tracking,1532349864,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/916qx0/goturn_deep_learning_based_object_tracking/
"When to Use MLP, CNN, and RNN Neural Networks",1532348330,,neuralnetworks,jackblun,False,/r/neuralnetworks/comments/916kbl/when_to_use_mlp_cnn_and_rnn_neural_networks/
Need a new laptop for Deep Learning,1532344224,"Hey guys,

I am heading to Grad School and I need a new laptop to learn/play around with Deep Learning.

I currently use a mac, but since I wouldn't get the GPU compatibility with CUDA, I am looking for an Ubuntu based laptop.

What do you guys think about using an eGPU with a spec'd out XPS 13?",neuralnetworks,abhinavgoel95,False,/r/neuralnetworks/comments/91653e/need_a_new_laptop_for_deep_learning/
FPGA Maker Snaps Up Deep Learning Chip Startup,1532285664,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/9101i2/fpga_maker_snaps_up_deep_learning_chip_startup/
Latest TensorFlow Release 1.9 is Out! Let us upgrade,1532238902,,neuralnetworks,DecipherTechnic,False,/r/neuralnetworks/comments/90vr83/latest_tensorflow_release_19_is_out_let_us_upgrade/
AI Weekly 21 July 2018,1532196133,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/90r6z7/ai_weekly_21_july_2018/
Eagle-Eyed Machine Learning Algorithm Outdoes Human Experts,1532182807,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/90plp6/eagleeyed_machine_learning_algorithm_outdoes/
What strategy/algorithm/library should I use to recognize and parse this pattern?,1532109143,"If you didn't find out already from the title, I'm new to machine learning and I would like to get some directions on where to start.

I marked the pattern with red and blue boxes and I want to parse and return the data in a structured way that the item name (red box) would go with it's price tag (blue box).",neuralnetworks,ForArtanis,False,/r/neuralnetworks/comments/90i63i/what_strategyalgorithmlibrary_should_i_use_to/
RNN,1532059856,Is RNN good for forecasting?,neuralnetworks,Vinceeeent,False,/r/neuralnetworks/comments/90ct5y/rnn/
Researchers move closer to completely optical artificial neural network,1532011791,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/906hwv/researchers_move_closer_to_completely_optical/
Stellar Became the First Blockchain that Complied With the “Sharia Law”,1532008656,,neuralnetworks,StivJonson12DD,False,/r/neuralnetworks/comments/9062b2/stellar_became_the_first_blockchain_that_complied/
Help,1531982767,How do I write code for asking for the data set from the user in python? (I'm trying to code a generalised neural network from scratch. So number of layers and activations are to be input from the user.,neuralnetworks,_reeefs,False,/r/neuralnetworks/comments/903hm1/help/
What Is the Process of Tokenization or How Can Any Person Create His/Her Own Token?,1531921918,,neuralnetworks,StivJonson12DD,False,/r/neuralnetworks/comments/8zvoan/what_is_the_process_of_tokenization_or_how_can/
"For any machine engineers / data scientists on here, IBM is looking to pay $100 for a short interview with you if you are able to describe the Deep Learing workflow w/ the tools involved to them!",1531889396,"Description from Respondent's website (the company they are using to source people for these interviews):

"" We at IBM would like to better understand the different stages of work involved with Deep Learning and what tools are used throughout the entire process. We welcome Data Scientists / Machine Learning Engineers that are savvy with the end to end process of deep learning. ""

I've used Respondent before, and its all pretty straight forward. They pay through paypal after the interview. Let me know if you have any questions!

Link to interview: [https://app.respondent.io/projects/view/5b4eb886c00db10023e78964/seeking-data-scientists-who-can-describe-the-deep-learing-workflow-w-the-tools-involved/brianroy-4abcac313e7a](https://app.respondent.io/projects/view/5b4eb886c00db10023e78964/seeking-data-scientists-who-can-describe-the-deep-learing-workflow-w-the-tools-involved/brianroy-4abcac313e7a)

(Referral link btw, in case that matters to anybody)",neuralnetworks,Momordicas,False,/r/neuralnetworks/comments/8zsgnc/for_any_machine_engineers_data_scientists_on_here/
Question: (new to NN) is there a way to automatically generate the Layers and Weights?,1531840042,"Hello there,

So I am playing around with NN and for example trying to find out which number is represented in a picture.

The first layer is the input of the image in 0,1s which is automated, but then I do not understand how the other layers are generated and their weights.

Thank you",neuralnetworks,data_driven_approach,False,/r/neuralnetworks/comments/8zlwp7/question_new_to_nn_is_there_a_way_to/
I tried to teach a network to play blackjack using genetic algorithms. It didn’t exactly work. Any suggestions?,1531807693,"So I made a simplified game of blackjack where all you can do is hit or stay, and you lose when you either overshoot 21 or stay and the dealer gets a higher score. Regardless of how I create the reward function, the network either always hits or always stays. It depends on whether is more “advantageous” to hit until your get 21 or to just stay on the first move hoping to get a higher score than the dealer. Any suggestions to improve on this? I eventually want to teach it how to raise but I suppose I should figure out how to do this well. ",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8zioaw/i_tried_to_teach_a_network_to_play_blackjack/
Future of the Science – ISH 'International Science Hub' on Blockchain,1531747481,,neuralnetworks,StivJonson12DD,False,/r/neuralnetworks/comments/8zavsh/future_of_the_science_ish_international_science/
AI Weekly 16 July 2018,1531730407,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8z99au/ai_weekly_16_july_2018/
"Are you interested in Computer Science and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1531716411,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/8z82pi/are_you_interested_in_computer_science_and_want/
Train Neural Network to respond to questions based of text fole containing conversation between two people.,1531694598,"I have a text file containing a text conversation with my friend. The file is a couple of megabytes in size. I have managed to successfully train a Neural Network (python library textgenrnn) to produce its own conversations based of of ours.

Now I would like to train a Neural Network so that I input some text, and it tries to guess what my friend would respond.

Would this be possible to make? Is the training data large enough? If yes, how would I do it?

I am completely new to machine learning, so sorry about my unprofessional terminology and noobish assumptions.",neuralnetworks,Tikolu43,False,/r/neuralnetworks/comments/8z5ptd/train_neural_network_to_respond_to_questions/
Can a CNN be made such that it can render what a view behind a rain drop covered window would look like if there was no window?,1531624512,"So imagine it’s raining and you’re looking out a window at some building. The window is covered with raindrops. Is it possible to make a CNN that would be able to accept a photo of the rain drop window and return a photo of the building, as if the window was clear? Has something similar to this been done before? I would love to read a research paper on something similar to this. ",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8yysx9/can_a_cnn_be_made_such_that_it_can_render_what_a/
Difference between ensemble ANNs and BNN?,1531604275,"I'm using ANN ensembles for cross-validation, which produces a outputs as a distribution. The distribution of outputs can then be used to calculate risk or generate confidence intervals. I'm wondering how a Bayesian neural network differs from this with respect to output uncertainty?",neuralnetworks,half_a_lime,False,/r/neuralnetworks/comments/8ywok9/difference_between_ensemble_anns_and_bnn/
Neural Network From Scratch — Tumour Diagnosis – Coinmonks – Medium,1531561891,,neuralnetworks,coinmonks,False,/r/neuralnetworks/comments/8ys7n8/neural_network_from_scratch_tumour_diagnosis/
Would phonetic languages like Arabic benefit from end to end speech synthesis (TTS) or since it does not need a word to phone dictionary from the beginning makes conventional systems good enough?,1531488668,,neuralnetworks,ysharief,False,/r/neuralnetworks/comments/8yjzls/would_phonetic_languages_like_arabic_benefit_from/
Awesome Youtube Channel for learning ML &amp; NN,1531474101,"I have visited a very nice youtube channel which explains topics of NN, ML &amp; AI really awesome.

[https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A](https://www.youtube.com/channel/UCWN3xxRkmTPmbKwht9FuE5A)

Do tell your comments...",neuralnetworks,Visalini23,False,/r/neuralnetworks/comments/8yij3a/awesome_youtube_channel_for_learning_ml_nn/
Where to start analyzing matrices and predicting new ones?,1531472955,"Hey,

in the past I noticed that I can learn new things best if there's some kind of application I have, so I want to get into neural networks and related things using a small project related to my research. It's not the most efficient and definitely not best way to learn but for me it's the most fun.

All I would need are some keywords so I can start moving in the right direction.

What I've got would be potentially thousands of n by n+1 matrices, each associated with a score from 0 to infinity. 

What I ultimately would like to get are new matrices with a high score (or even a certain score), but if that's too complicated then just predicting the score of a new matrix would be fine too.

What kind of neural networks should I look into? Are there prebuild tools I can use for such problems?

Thanks a lot.",neuralnetworks,FalconX88,False,/r/neuralnetworks/comments/8yifku/where_to_start_analyzing_matrices_and_predicting/
Laptop for Deep Learning,1531452876,"Hey guys! I am buying a new laptop before I head to grad school.
I will be working with Deep Neural Networks, although I will have AWS cloud instances to work with.
I would like to be able to run small Neural Networks on my laptop, mainly to learn and experiment.

Are the 8th Gen CPUs much better than the 7th Gen, that I mustn’t miss out on the performance upgrade for my needs?
I prefer using a mac, so I don’t have the option to use a GPU for these small experiments.

Thanks!",neuralnetworks,abhinavgoel95,False,/r/neuralnetworks/comments/8ygkuk/laptop_for_deep_learning/
DeepMind's AI Learns To See | Two Minute Papers #263,1531416166,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8ybrma/deepminds_ai_learns_to_see_two_minute_papers_263/
An Intriguing Failing of Convolutional Neural Networks and the CoordConv Solution,1531413642,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8ybdsd/an_intriguing_failing_of_convolutional_neural/
Novel synaptic architecture for brain inspired computing,1531409528,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8yaryc/novel_synaptic_architecture_for_brain_inspired/
Artificial intelligence helps design an ultra-aerodynamic bike,1531409485,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8yarpu/artificial_intelligence_helps_design_an/
Should I Buy Bitcoin Today?,1531406985,,neuralnetworks,JamesCooper12R1,False,/r/neuralnetworks/comments/8yaf2e/should_i_buy_bitcoin_today/
"Artificial life simulation, version: 0.x",1531297744,,neuralnetworks,Thomas-Arys,False,/r/neuralnetworks/comments/8xxtzz/artificial_life_simulation_version_0x/
Learn AI/ML/Deep Learning with new iOS App!,1531282358,"Hey everyone! First a bit about me: I did my undergrad and master's in Stanford's AI program.

I know firsthand that learning AI is challenging, but it doesn't have to be! I've released an iOS app that is a full-fledged curriculum for learning the basics of AI/ML/Deep Learning.

Check it out: [https://itunes.apple.com/us/app/artificial-intelligence-school/id1369987569?mt=8](https://itunes.apple.com/us/app/artificial-intelligence-school/id1369987569?mt=8)",neuralnetworks,MusingEtMachina,False,/r/neuralnetworks/comments/8xwfub/learn_aimldeep_learning_with_new_ios_app/
Best Tutorial for Neural Network Programming,1531245391,"I'm a skilled programmer. I have good experience and I'm great at programming software. I have had minimal success with AI and neural networks. I am looking for a tutorial for programming a neural network. I have looked at the work of carykh and codebullet and I want to do programming that can be applied to that kind of system. I have very little knowledge of neural networks, so I would prefer a tutorial that starts from scratch. I prefer the languages Python(2.7 or 3.x), Java, and JavaScript. Thanks for any help you can offer. ",neuralnetworks,GamingTimelord19,False,/r/neuralnetworks/comments/8xrnw2/best_tutorial_for_neural_network_programming/
Batch Normalization in Deep Networks,1531230207,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8xp87a/batch_normalization_in_deep_networks/
Backpropagation Explained,1531230123,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8xp7rv/backpropagation_explained/
Researchers have developed an artificial neural network made out of DNA that can solve a classic machine learning problem: correctly identifying handwritten numbers. The work is a significant step in demonstrating the capacity to program artificial intelligence into synthetic biomolecular circuits.,1531213077,,neuralnetworks,asiamnesis,False,/r/neuralnetworks/comments/8xn5yo/researchers_have_developed_an_artificial_neural/
"Are you interested in Neural Network and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1531196329,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/8xlfln/are_you_interested_in_neural_network_and_want_to/
"For teaching a XOR gate emulator network, is genetic algorithms faster than backpropagation? If so, why?",1531177330,"I made an XOR gate emulator and taught it using backpropagation, and then using a genetic algorithm. The GA was simply making a batch of 20 networks, choosing the top two, and mutating them to create 10 new networks each (thus 20 new networks in total)
Backpropagation took about 1000-4000 iterations to get the error below .0001, while the GA took about 800. I’m kinda surprised since GA and backpropagation in this scenario are essentially the same thing: changing the weights such that it lowers the cost function. And since backpropagation is a non-random mathematical process, shouldn’t it be faster than a ‘randomized’ version of itself? Why is it that GA is quicker than backpropagation? 

It’s also possible that I’m miscalculating the true number of iterations. ",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8xhefl/for_teaching_a_xor_gate_emulator_network_is/
CaffeNet a kind of convolutional neural network?,1531166062,"Hi, just checking my understanding. Is CaffeNet a kind of CNN? And can someone explain the distinction between DNNs and CNNs? What is each best for? ",neuralnetworks,GreySlate,False,/r/neuralnetworks/comments/8xfh58/caffenet_a_kind_of_convolutional_neural_network/
jGeneticNeuralNet: A Java library that trains neural networks with a genetic algorithm.,1531164224,,neuralnetworks,blank89,False,/r/neuralnetworks/comments/8xf5g3/jgeneticneuralnet_a_java_library_that_trains/
Would it be possible/make sense to apply machine learning to rendering?,1531153291,"Just a ""shower thought"". I'm a programmer, but I've never done anything related to neural networks. I was wondering if it would be possible to teach an AI to render a scene. The learning process seems more-or-less straightforward, with raw scene files and traditionally rendered results as input.

I was wondering whether it could be possible, and if it would have any benefits (maybe smaller files, as AI'd need less info to produce a realistic image, or maybe ""infinite"" resolution and poly-count, as it would predict how the image should look like).",neuralnetworks,MartinKardis,False,/r/neuralnetworks/comments/8xd856/would_it_be_possiblemake_sense_to_apply_machine/
A dive into the deep end of deep neural networks for recommender engines.,1531146272,,neuralnetworks,cptAwesome_070,False,/r/neuralnetworks/comments/8xc2m2/a_dive_into_the_deep_end_of_deep_neural_networks/
Second output always 1,1531082206,"I'm creating a network using C#. It has a 2000+ inputs, 2 hidden layer with 80 neurons and a 2 outputs, both close to 1 (min and max). Everything is well except for one thing: for some reason training always changes only 1st input. 2nd input always returns ""1"". I've tried two network implementations: Accord.Net and FANN. My code for Accord:
    var teacher = new ResilientBackpropagationLearning(this.Network)
    {
        LearningRate = 0.1
    };

    var i = 0;
    double error;
    do
    {
        error = teacher.RunEpoch(this.exampleInputs, this.exampleOutputs) / this.exampleInputs.Length;
        Console.WriteLine(i + "", Error = "" + error);
        if (i % 10 == 0)
        {
            this.Network.Save(this.tempFolder + $""\\Last.net"");
        }

        if (i % 50 == 0)
        {
            var realout = this.Network.Compute(this.exampleInputs[0]);
            Console.WriteLine(
                $""desired: {this.exampleOutputs[0][0]:0.00}, {this.exampleOutputs[0][1]:0.00}"");
            Console.WriteLine($""got: {realout[0]:0.00}, {realout[1]:0.00}"");
            this.TestNetwork();
            if (i % 500 == 0)
            {
                this.Network.Save(this.tempFolder + $""\\Run{i}.net"");
            }
        }

        ++i;
    }
    while (error &gt; 0.0000001);

What am I doing wrong?",neuralnetworks,BufferOverflow_x86,False,/r/neuralnetworks/comments/8x4jag/second_output_always_1/
Remember that neat networks learning to play pac man it's back,1531077613,It is back at twitch.tv/err0rz_404 sorry for it going last time my pc was shut off while I was away from home hope you guys enjoy also gonna release some files when it finishes this one forbpeole to make their own levels and then send them to me to use for the network ,neuralnetworks,thomascook04040,False,/r/neuralnetworks/comments/8x3xvd/remember_that_neat_networks_learning_to_play_pac/
AI Weekly 8 July 2018,1531044611,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8x0j0n/ai_weekly_8_july_2018/
Hyperparameter grid search,1531015943,,neuralnetworks,bwllc,False,/r/neuralnetworks/comments/8wybo5/hyperparameter_grid_search/
Reading a Neural Networks book.,1530995823,"Hi all, after finishing the videos by 3blue1brown on Youtube, I started the reading [this awesome book](http://neuralnetworksanddeeplearning.com/) by Michael Nielsen and was wondering that if there's anyone else who has read/reading/interested to read the book hoping we could compare our approaches to it's exercise problems and help each other if we're stuck somewhere. Cheers. ",neuralnetworks,styx97,False,/r/neuralnetworks/comments/8ww5h9/reading_a_neural_networks_book/
Reading a Neural Networks book,1530995219,,neuralnetworks,styx97,False,/r/neuralnetworks/comments/8ww301/reading_a_neural_networks_book/
GitHub - ewimberley/jGeneticNeuralNet: A Java library that implements neural networks with a genetic training algorithm.,1530992730,,neuralnetworks,blank89,False,/r/neuralnetworks/comments/8wvsn4/github_ewimberleyjgeneticneuralnet_a_java_library/
Audio Analysis. What type of network for a problem like this?,1530950593,"I'm creating a model to analyze the audio from many sample snippets of music and return the beats and sound heard on each beat. My current idea is to have a network with a few thousand inputs (to represent the audio) collapse down through hidden layers to 64 output neurons. The outputs are all numbers that are the estimate of the sound heard at that beat. (returns 11.08 because the model predicts a snare drum there and the number for snare is 11, so a relu activation)

Any ideas of what type of net to frame this around? I've been looking into LSTM's, but I fear I might be misunderstanding their use cases. Any advice appreciated, thanks.",neuralnetworks,FinelyTaylored,False,/r/neuralnetworks/comments/8wro8h/audio_analysis_what_type_of_network_for_a_problem/
How are weights optimized when feeding a neural net with 'm' training example ?,1530937109,"Hi everyone, the architecture of the neural network I want to build using numpy is a 3 by 1 neural net (an input layer with 3 neurons and an output layer with one neuron), I find no problem coding a forward and back propagation when feeding the neural net with one training sample (namely feature1, feature2, feature3 inputs) and i know exactly how my initial weights get optimized, the problem i find is when feeding the NN with multiple training inputs each time, here, I don't know exactly how the initial weights get optimized.
I would be grateful if anyone can explain how the initial weights get modified when feeding the NN with multiple training inputs.
(For example training inputs of 3 × 3 Matrix.
[[195, 90, 41],
[140, 50, 30],
[180, 85, 43]]
the first column is the height, 2nd: the weight, 3rd: shoe size, where we feed the NN with the first row then the second and the third row)
Thanks in advance.",neuralnetworks,Al-Khazrajy,False,/r/neuralnetworks/comments/8wqk02/how_are_weights_optimized_when_feeding_a_neural/
Deep Learning - Learn Recurrent Neural Networks in Python,1530901854,,neuralnetworks,frstnm,False,/r/neuralnetworks/comments/8wmhu1/deep_learning_learn_recurrent_neural_networks_in/
Neural Networks Integration into Searching Places Algorithm,1530886014,,neuralnetworks,deadjdona,False,/r/neuralnetworks/comments/8wkcf0/neural_networks_integration_into_searching_places/
Genann: simple neural network library in ANSI C,1530857025,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/8whpf1/genann_simple_neural_network_library_in_ansi_c/
The unreasonable effectiveness of Deep Learning Representations,1530856949,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/8whp63/the_unreasonable_effectiveness_of_deep_learning/
"The Lottery Ticket Hypothesis: Finding Small, Trainable Neural Networks",1530856906,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/8whp0s/the_lottery_ticket_hypothesis_finding_small/
XIAOMI:Will the IPO of Xiaomi Be Tokenized by a Russian Company or Is It Another Big Scam? | The Coin Shark,1530800162,,neuralnetworks,JamesCooper12R1,False,/r/neuralnetworks/comments/8warx3/xiaomiwill_the_ipo_of_xiaomi_be_tokenized_by_a/
Is genetic algorithms the only way to teach a neural network to play Snake or Asteroids? Is there no trick to using backpropagation?,1530732560,"I’m kinda interested in building a network that can play Snake, and I understand backpropagation, but my gut is telling me there’s no way you can use that to teach a network to play it. ",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8w4aud/is_genetic_algorithms_the_only_way_to_teach_a/
Wierd Ass NeuralNetwork,1530717471,"I've always felt like current network architectures are limited because of how static they are. I'm in no position to explore actual solutions to this but it can't hurt to experiment a bit. 

I started of by implementing a architecture that works like a multi-graph and propagation is done by just assigning a few nodes to be input and a few to be output and then pulsate every node X times and use classic gradient decent to update all weights with respect to the final output nodes values. And thereafter randomly switch the ""weakest"" connections to new nodes.

https://github.com/JonasRSV/SET_NETWORK_kindof 

This approach did not work very good at all! I don't actually know why it didn't work or why it even should work. The issue appeared to be that the input didn't always manage to propagate to the output and sometimes the gradients got fucked up because of this. 

Anyway the point of this post is to ask for advice on alternative approaches. I'm thinking of making it into a DAG to guarantee that input flows through the graph and to the output. Or maybe keep it as a multigraph but instead of ""pulsating"" each node maybe propagate with some form of BFS or DFS? 

I'm using https://networkx.github.io/ to visualize the network. Green nodes are inputs, red nodes are outputs and black nodes are just generic nodes. Red connections are positive, Blue connections are Negative. The size of the nodes correspond to their ""activity"". ",neuralnetworks,Jonas_SV,False,/r/neuralnetworks/comments/8w2e4i/wierd_ass_neuralnetwork/
"My understanding of why we chose to model Neural Networks after our own brains, and why it's good.",1530711506,,neuralnetworks,Sushrit_Lawliet,False,/r/neuralnetworks/comments/8w1p0j/my_understanding_of_why_we_chose_to_model_neural/
How were Kohonen Self Organizing Maps (SOMs) neural nets inspired by neuroscience?,1530701196,Kohonen Self Organizing Maps (SOMs) are neural nets inspired by neuroscience.,neuralnetworks,JoeHobbit,False,/r/neuralnetworks/comments/8w0q1p/how_were_kohonen_self_organizing_maps_soms_neural/
Wine vs Sparkling Wine: A Neural Network image classification explained,1530692941,,neuralnetworks,rennytech,False,/r/neuralnetworks/comments/8w02s4/wine_vs_sparkling_wine_a_neural_network_image/
How expensive are very large neural networks to run once they have been trained?,1530595656,"Computationally speaking of course...

I would assume that since they're just math functions that even a raspberry pi should be able to run a trained neural network fairly efficiently?

Does anybody have any insight on the matter?

Still learning to train my first neural net right now and I'm curious.",neuralnetworks,grumpyThrifter,False,/r/neuralnetworks/comments/8vpo10/how_expensive_are_very_large_neural_networks_to/
Debugging vectorised Neural Net,1530470978,"Hi,

im currently working on a simple vectorised neural network in C++. Wrote one a while ago, but completely object oriented and now i want to take it a step further and create a better/faster “base” to build upon. Unfortunately im a bit stuck, because it works perfectly fine with simple tasks (AND, XOR, ...) but fails miserably when it gets more complex (MNIST Dataset).

&amp;nbsp;

Some debugging help and of course general tips, tricks and techniques will be greatly appreciated. Still very new to this field, so thanks in advance for any kind of help.

&amp;nbsp;

    
    typedef std::vector&lt;double&gt;         Vector;
    typedef std::vector&lt;double&gt;         Matrix;
    typedef std::vector&lt;long&gt;           Topology;
    
    // Generate Random Number between 0.0f and 1.0f
    #define random_0_1                  ((rand() % 10000 + 1)/10000-0.5)
    
    Vector CalculateDotSigmoid(const Matrix&amp; weights, const Vector&amp; values, const Vector&amp; bias, const long matRows) {
        const long matColumns = values.size();
        Vector result(matRows);
        
        for (long r = 0; r &lt; matRows; r++) {
            for (long c = 0; c &lt; matColumns; c++) {
                // Get the dot product (sum of multiplications)
                // of the weight-matix and value-vector for each row
                result[r] += (weights[r * matColumns + c] * values[c]);
            }
            // Calculate the sigmoid function f(x) = 1/(1 + e^-x) based on the sum and added bias
            result[r] = 1 / (1 + exp(-(result[r] + bias[r])));
        }
        
        return result;
    }
    
    Vector CalculateDotSigmoidPrime(const Matrix&amp; weights, const Vector&amp; values, const Vector&amp; bias, const long matRows) {
        const long matColumns = values.size();
        Vector result(matRows);
        double tmp = 0.0f;
        
        for (long r = 0; r &lt; matRows; r++) {
            for (long c = 0; c &lt; matColumns; c++) {
                // Get the dot product (sum of multiplications)
                // of the weight-matix and value-vector for each row
                result[r] += (weights[r * matColumns + c] * values[c]);
            }
            // Calculate the sigmoid prime function based on the sum and added bias
            tmp = (result[r] + bias[r]);
            result[r] = exp(-tmp) / (pow(1 + exp(-tmp), 2));
        }
        
        return result;
    }
    
    // Calculate the bias delta between the nets output and the expected output: dEdB = (H - Y) * sigPrime(H * W + B)
    Vector CalculateLastBiasDelta(const Vector&amp; netOutput, const Vector&amp; expectedOutput, const Vector&amp; sigPrimeResult) {
        Vector result(netOutput.size());
        
        for (long i = 0; i &lt; netOutput.size(); i++) {
            result[i] = (netOutput[i] - expectedOutput[i]) * sigPrimeResult[i];
        }
        
        return result;
    }
    
    // Calculate the other layer bias deltas: dEdB = (dEdB+1 * (W+1 transposed) * sigPrime(H * W + B)
    Vector CalculateBiasDelta(const Vector&amp; nextBiasDelta, const Matrix&amp; nextWeights, const long rows, const long columns, const Vector&amp; dotSigmoidPrime) {
        Vector result(dotSigmoidPrime.size());
        
        // Transpose weight mat
        Matrix transposedWeights(nextWeights.size());
        for (long n = 0; n != nextWeights.size(); n++) {
            transposedWeights[n] = nextWeights[rows * (n % columns) + (n / columns)];
        }
        
        // Calculate nextBiasDelta DOT transposedWeights (column count is now row count)
        const long matColumns = nextBiasDelta.size();
        const long matRows = columns;
        Vector dotResult(matRows);
        
        for (long r = 0; r &lt; matRows; r++) {
            for (long c = 0; c &lt; matColumns; c++) {
                // Get the dot product (sum of multiplications)
                dotResult[r] += (transposedWeights[r * matColumns + c] * nextBiasDelta[c]);
            }
        }
        
        // dotResult * dotSigmoidPrime
        for (long i = 0; i &lt; dotSigmoidPrime.size(); i++) {
            result[i] = dotResult[i] * dotSigmoidPrime[i];
        }
        
        return result;
    }
    
    
    Matrix CalculateWeightDelta(const Vector&amp; neurons, const Vector&amp; biasDelta) {
        Matrix result(neurons.size() * biasDelta.size());
        
        // No transpose needed ... just dot the vecs
        for (int row = 0; row &lt; neurons.size(); row++) {
            for (int col = 0; col &lt; biasDelta.size(); col++) {
                result[row * biasDelta.size() + col] = (neurons[row] * biasDelta[col]);
            }
        }
        
        return result;
    }
    
    // Calculate the new weights updated based on the delta and learning rate: W = (W - (dEdW * a))
    Matrix UpdateWeight(const Matrix&amp; weight, const Matrix&amp; weightDelta, const double learnRate) {
        Matrix result(weight.size());
        for (long i = 0; i &lt; weight.size(); i++) {
            result[i] = (weight[i] - (weightDelta[i] * learnRate));
        }
        return result;
    }
    
    // Calculate the new biases updated based on the delta and learning rate: B = (B - (dEdB * a))
    Vector UpdateBias(const Vector&amp; bias, const Vector&amp; biasDelta, const double learnRate) {
        Vector result(bias.size());
        for (long i = 0; i &lt; bias.size(); i++) {
            result[i] = (bias[i] - (biasDelta[i] * learnRate));
        }
        return result;
    }
    
    
    class NeuralNet {
    private:
        const Topology _layers;
        const long _layerCount, _lastLayer, _hiddenLayerCount;
        const double _learningRate;             // a
        std::vector&lt;Vector&gt; _neuronVectors;     // H
        std::vector&lt;Matrix&gt; _weights;           // W
        std::vector&lt;Vector&gt; _biases;            // B
        std::vector&lt;Matrix&gt; _weightDeltas;      // dEdW
        std::vector&lt;Vector&gt; _biasDeltas;        // dEdB
        
    public:
        NeuralNet(const Topology&amp; layers, double learningRate)
        : _layers(layers), _layerCount(layers.size()), _lastLayer(_layerCount - 1), _hiddenLayerCount(_layerCount - 2), _learningRate(learningRate) {
            _neuronVectors = std::vector&lt;Vector&gt;(_layerCount);      // Each Layer has a neuron vector (Input/Hidden/Output)
            _weights = std::vector&lt;Matrix&gt;(_lastLayer);             // Each Layer holds the weights for the next layers neurons (-1 for Output)
            _biases = std::vector&lt;Vector&gt;(_lastLayer);              // Each Layer holds the biases for the next layers neurons (-1 for Output)
            _weightDeltas = std::vector&lt;Matrix&gt;(_lastLayer);
            _biasDeltas = std::vector&lt;Vector&gt;(_lastLayer);
            
            // Initialize all weight matrices and bias vectors with random values
            for (long i = 0; i &lt; _lastLayer; i++) {
                // Weight matrix (rows = nextLayerNeurons, columns = thisLayerNeurons)
                _weights[i] = Matrix(_layers[i + 1] * _layers[i]);
                for (long mc = 0; mc &lt; _weights[i].size(); mc++) {
                    _weights[i][mc] = random_0_1;
                }
                
                // Bias vectors (length = nextLayerNeutrons)
                _biases[i] = Vector(_layers[i + 1]);
                for (long bc = 0; bc &lt; _biases[i].size(); bc++) {
                    _biases[i][bc] = random_0_1;
                }
            }
        }
        
        void Train(long iterations, const std::vector&lt;Vector&gt;&amp; trainingInput, const std::vector&lt;Vector&gt;&amp; trainingOutput) {
            for (long i = 0; i &lt; iterations; i++) {
                for (long t = 0; t &lt; trainingInput.size(); t++) {
                    FeedForward(trainingInput[t]);
                    BackPropagate(trainingOutput[t]);
                }
            }
        }
        
        Vector FeedForward(const Vector&amp; input) {
            // Set the input layer == the input values
            _neuronVectors[0] = input;
            
            for (long i = 1; i &lt; _layerCount; i++) {
                // H = sigm((H-1 * W-1) + B-1)
                _neuronVectors[i] = CalculateDotSigmoid(_weights[i - 1], _neuronVectors[i - 1], _biases[i - 1], _layers[i]);
            }
            
            return _neuronVectors.back();
        }
        
        void BackPropagate(const Vector&amp; expectedOutput) {
            // Calculate Error here (MSE) ... (Not needed for now)
            
            // Calculate the bias delta between the nets output and the expected output: dEdB = (H - Y) * sigPrime(H * W + B)
            auto tmp = CalculateDotSigmoidPrime(_weights[_hiddenLayerCount], _neuronVectors[_hiddenLayerCount], _biases[_hiddenLayerCount], _layers[_lastLayer]);
            _biasDeltas[_hiddenLayerCount] = CalculateLastBiasDelta(_neuronVectors.back(), expectedOutput, tmp);
            
            for (long i = _hiddenLayerCount - 1; i &gt;= 0; i--)
            {
                // Calculate the other layer bias deltas: dEdB = (dEdB+1 * (W+1 transposed) * sigPrime(H * W + B)
                auto sigPrimeResult = CalculateDotSigmoidPrime(_weights[i], _neuronVectors[i], _biases[i], _layers[i + 1]);
                _biasDeltas[i] = CalculateBiasDelta(_biasDeltas[i + 1], _weights[i + 1], _layers[i + 2], _layers[i + 1], sigPrimeResult);
            }
            
            // Calculate the weight gradients and update all weights and biases
            for (long i = 0; i &lt; _lastLayer; i++) {
                // dEdW[i] = H[i].transpose().dot(dEdB[i])
                _weightDeltas[i] = CalculateWeightDelta(_neuronVectors[i], _biasDeltas[i]);
                
                _weights[i] = UpdateWeight(_weights[i], _weightDeltas[i], _learningRate);
                _biases[i] = UpdateBias(_biases[i], _biasDeltas[i], _learningRate);
            }
        }
        
    };
    ",neuralnetworks,_krck_,False,/r/neuralnetworks/comments/8vc33x/debugging_vectorised_neural_net/
Techniques for using known metadata about images to improve a retrained image recognition model?,1530409005,"Recently, I've built a model with ~97.5% identification accuracy by shamelessly stealing retraining techniques used in tutorials like Tensorflow For Poets for retraining a mobilenet. In validation, different categories of images with highly varying levels of accuracy. I'm hoping to improve the accuracy using information I already have about the contents of the image based on improved accuracy I see building customized models.

The images I am classifying are fairly consistent, and I have loads of structured metadata prior to classification detailing what will be in the image, but not about what needs to be classified. The ""generalized"" model appears to have segments of images with significantly higher classification accuracy than others (segmented based on the aforementioned metadata). If I build one-off models specifically for each segment, I'm able to get poorly performing segments of images (sub 90%) consistently above 97%.

However, building one-off models is 
1) fairly cumbersome as I have to build a process to build/retrain the models. and
2) The outputted models are each above 10MB and I have hundreds/thousands of possible segments.

Are there common patterns for adding known metadata into the feature vectors, or retraining models with metadata to improve training accuracy? I'm an software engineer by trade, and my google-foo is failing me in this regard as I don't think I know the right terminology.

Cheers",neuralnetworks,champloo11,False,/r/neuralnetworks/comments/8v6m0s/techniques_for_using_known_metadata_about_images/
AI Weekly 29 June 2018,1530361822,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8v1j74/ai_weekly_29_june_2018/
Where can I find a non-MNIST implementation of Capsule Networks for Tensorflow?,1530303195,"Hello, this is my first post here. I'm mostly used to Stack Overflow posting guidelines, so let me know what I can do to improve my posts here, or if this isn't the right place for a post like this. (Thanks!)

It looks like there are a [lot of](https://github.com/ageron/handson-ml/blob/master/extra_capsnets.ipynb) [different](https://github.com/bourdakos1/capsule-networks) [implementations](https://github.com/llSourcell/capsule_networks/blob/master/capsLayer.py) of Capsule Networks in TensorFlow. However, most of these seem to be versions of [https://github.com/naturomics/CapsNet-Tensorflow](https://github.com/naturomics/CapsNet-Tensorflow).

My issue with naturomic's (Huadong Liao) implementation is that it's hard-coded specifically for MNIST.

For example, there are assert statements like \`assert caps1.get\_shape() == \[cfg.batch\_size, 1152, 8, 1\]\` that only make sense if you're using a dataset with the same dimensions as MNIST.

Are there any implementations of Capsule Networks for TensorFlow that are more general?

If not, I'll try to go about making my own, probably by working from [naturomic's implementation](https://github.com/naturomics/CapsNet-Tensorflow) and replacing the MNIST constants with variables and editable properties.",neuralnetworks,Devenar,False,/r/neuralnetworks/comments/8uwa21/where_can_i_find_a_nonmnist_implementation_of/
Video of unsupervised Exemplar Network touch-to-search (warning: nudity),1530278221,,neuralnetworks,nooorol,False,/r/neuralnetworks/comments/8ut1pd/video_of_unsupervised_exemplar_network/
Explained: Artificial Neural Network,1530265653,,neuralnetworks,Sammycollins055,False,/r/neuralnetworks/comments/8urw57/explained_artificial_neural_network/
Deepmind's TensorFlow implementation of Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,1530117374,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/8ub3g2/deepminds_tensorflow_implementation_of_scalable/
IBM reduces computational strain using analog memory,1530077626,,neuralnetworks,anthonymysk,False,/r/neuralnetworks/comments/8u71qn/ibm_reduces_computational_strain_using_analog/
Backpropagation isn't working for this 3-layer sequential network I'm making.,1530060494,"Here's the link to my code: https://github.com/spencerkraisler/SimpleNeuralNetwork

I'm kinda new to coding so my commenting may be bad but I'm happy to answer any questions.",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8u5ajo/backpropagation_isnt_working_for_this_3layer/
LSTM tutor,1530029780,"Hi!

I have a kind of mathematical prediction problem and asking here and there someone told me that I could resolve it through LSTM neural networks. 

I have read tutorials about implementing LSTM in keras because they said it it easier but since this is my first approach to neural networks I can't understand even how to build my dataset  and I am looking for some help.",neuralnetworks,Yumeg12,False,/r/neuralnetworks/comments/8u1b76/lstm_tutor/
How to optimise a neural network?,1530029025,,neuralnetworks,aswath_madhu,False,/r/neuralnetworks/comments/8u17ey/how_to_optimise_a_neural_network/
Where can I learn programming Simulations with neuralnetworks as living creatures?,1529874984,"Hey guys
I am struggling so badly to just find anything to learn how to program simulations with learning living creatures... Ive Never Programmed before but i want to Start so badly... Any tips?
I would really appreaciate it^^
Ps Yeah i know my english sucks
",neuralnetworks,SpiritusHD,False,/r/neuralnetworks/comments/8tkz17/where_can_i_learn_programming_simulations_with/
"Back Propagation not Working, Code Review?",1529874974,"I honestly do not what where I have gone wrong in my neural network implementation. It may be the back propagation implementation, or a gradient checking implementation that is wrong, or propagating the inputs forward... I'd really appreciate if someone could review my **model.c** and **functions.c** file (even though it is in C, I have made each statement's intent extremely clear). 

[https://github.com/TheeNinja/NeuralNetwork](https://github.com/TheeNinja/NeuralNetwork)

If someone does not have much time to review those files, I'd appreciate review of this function specifically (the back propagation):

    void updateParameterGradients(struct Model *model, const double* targetOutput, double** weightGradients[],
 double* biasGradients[]) {
 int outputNeuronCount = model-&gt;neuronsPerLayer[OUTPUT_LAYER];

    // Entry indexed by [layerIndex][neuronIndex] gives
    // Δ C / Δ Z[layerIndex, neuronIndex]
 double* errors[NUMBER_OF_LAYERS];

       errors[OUTPUT_LAYER] = malloc(sizeof(double) * outputNeuronCount);

    // Fill errors of output layers
 for (int outputNeuronIndex = 0; outputNeuronIndex &lt; outputNeuronCount; outputNeuronIndex++) {
 double outputNeuronValue = model-&gt;values[OUTPUT_LAYER][outputNeuronIndex];
 double targetOutputNeuronValue = targetOutput[outputNeuronIndex];

    // Δ C_outputNeuronIndex / Δ A[OUTPUT_LAYER][outputNeuronIndex]
 double firstErrorComponent = model-&gt;getCostDerivative(outputNeuronValue, targetOutputNeuronValue);
 // Δ A[OUTPUT_LAYER][outputNeuronIndex] / Δ Z[OUTPUT_LAYER][outputNeuronIndex]
 double secondErrorComponent = model-&gt;getActivation(outputNeuronValue);
 // Δ C_outputNeuronIndex / Δ Z[OUTPUT_LAYER][outputNeuronIndex]
 double error = firstErrorComponent * secondErrorComponent;

           errors[OUTPUT_LAYER][outputNeuronIndex] = error;
    }

    // Fill errors of non-output layers
 for (int endLayerIndex = OUTPUT_LAYER; endLayerIndex &gt; INPUT_LAYER; endLayerIndex--) {
 int startLayerIndex = endLayerIndex - 1;
 int offsetEndLayerIndex = offsetLayer(endLayerIndex);

    int startNeuronsCount = model-&gt;neuronsPerLayer[startLayerIndex];
 int endNeuronsCount = model-&gt;neuronsPerLayer[endLayerIndex];

           errors[startLayerIndex] = malloc(sizeof(double) * startNeuronsCount);

    for (int startNeuronIndex = 0; startNeuronIndex &lt; startNeuronsCount; startNeuronIndex++) {
 double error = 0.0;

    for (int endNeuronIndex = 0; endNeuronIndex &lt; endNeuronsCount; endNeuronIndex++) {
 double nextError = errors[endLayerIndex][endNeuronIndex];
 double nextWeight = model-&gt;weights[offsetEndLayerIndex][endNeuronIndex][startNeuronIndex];

    double activationValue = model-&gt;values[startLayerIndex][startNeuronIndex];
 double activationValueDelta = model-&gt;getActivationChange(activationValue);

    double errorInfluence = nextWeight * nextError * activationValueDelta;
                error += errorInfluence;
            }

    // Take average of errors, not sum
 error /= endNeuronsCount;

               errors[startLayerIndex][startNeuronIndex] = error;
        }
    }

    // Update weights and biases of all layers based on errors
 for (int endLayerIndex = OUTPUT_LAYER; endLayerIndex &gt; INPUT_LAYER; endLayerIndex--) {
 int offsetEndLaterIndex = offsetLayer(endLayerIndex);
 int startLayerIndex = endLayerIndex - 1;

    int endNeuronCount = model-&gt;neuronsPerLayer[endLayerIndex];
 int startNeuronCount = model-&gt;neuronsPerLayer[startLayerIndex];

    for (int endNeuronIndex = 0; endNeuronIndex &lt; endNeuronCount; endNeuronIndex++) {
 for (int startNeuronIndex = 0; startNeuronIndex &lt; startNeuronCount; startNeuronIndex++) {
 double errorOfEndNeuronOfWeight = errors[endLayerIndex][endNeuronIndex];

    double valueOfStartNeuron = model-&gt;values[startLayerIndex][startNeuronIndex];

    double biasGradientInfluence = errorOfEndNeuronOfWeight;
 double weightGradientInfluence = errorOfEndNeuronOfWeight * valueOfStartNeuron;

                   biasGradientInfluence *= model-&gt;learningRate;
                weightGradientInfluence *= model-&gt;learningRate;

                   weightGradients[offsetEndLaterIndex][endNeuronIndex][startNeuronIndex] += weightGradientInfluence;
                biasGradients[offsetEndLaterIndex][endNeuronIndex] += biasGradientInfluence;
            }
        }
    }
}",neuralnetworks,TheeNinjaa,False,/r/neuralnetworks/comments/8tkyzc/back_propagation_not_working_code_review/
Why is used silicon in neuromorphing chips^,1529850436,Hello  I  was   wondering why is  used  silicon in neuromorphing chips...  what kind of  properties  does it  have compared to other materials?   I  have red that it speeds  up the  trasmission of  electric signal...   but  could  there be  others  materials  better  than silicon?  Which one?  I  was  thinking at  Germanene  or Graphene...   what  do you think about it?,neuralnetworks,luchins,False,/r/neuralnetworks/comments/8ti4qh/why_is_used_silicon_in_neuromorphing_chips/
There is currently a neural network playing pac-man the first level on twitch,1529841556,,neuralnetworks,thomascook04040,False,/r/neuralnetworks/comments/8thcyd/there_is_currently_a_neural_network_playing/
"Made a simple 2-3-3-1 XOR neural network from the ground up. After training, sometimes the NN's output values are .5-.5-.5-.5 as opposed to 0-1-1-0. Sometimes they're .3-.3-1-.3, and sometimes they are the values I want. Is it getting caught in local minima, or there some other issue?",1529795562,I can post my code if you want.,neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8tdqey/made_a_simple_2331_xor_neural_network_from_the/
"When finding the change in error w.r.t. the change in weight for a sequential neural network, does the pathway I take from the output node to the weight matter?",1529783257,"I'm making a SNN from the ground up and I'm making a method where the input is the weight, the output node, and the desired value of that output node. So does the pathway I take as I keep taking the derivatives of everything matter, as long as I arrive at the weight?
I'm looking at the math and it seems like it doesn't, but I want to make sure.",neuralnetworks,pocketMAD,False,/r/neuralnetworks/comments/8tcfnf/when_finding_the_change_in_error_wrt_the_change/
"Understanding how CNNs ""see"" via Topological Data Analysis",1529611760,,neuralnetworks,jtsymonds,False,/r/neuralnetworks/comments/8sv3p6/understanding_how_cnns_see_via_topological_data/
Getting started - sidebar for r/neuralnetworks?,1529599388,"I the last century when I started with ANNs, I used Usenet to get started. The FAQ of comp.ai.neural-nets was one of the best resources on ANNs I had the privilege to find. Unfortunately it has been updated the last time in 2002 and while I agree with their estimation that for feedforward NNs, the best reference book is: 

   Bishop, C.M. (1995), Neural Networks for Pattern Recognition,    Oxford: Oxford University Press.  

 If the answer isn't in Bishop, then for more theoretical questions try:  

   Ripley, B.D. (1996) Pattern Recognition and Neural    Networks, Cambridge: Cambridge University Press.  

these books have since  become a bit outdated. They especially don't cover some of the latest advances around activation functions like Relu or convolutional neural networks. The reason for their ""best of class"" status was that they discussed the mathematical underpinnings and worked out what specific items mean when you are using them, e.g. how and why activation functions are necessary, etc. Diving back into ANNs, I ran into the same issue I had back in the 90s with the subject that you have many introducing how things operate but never deigning to explain *why the hell you would need A or B*. Yes, to a certain extent it's art but to a certain extent it isn't and I would like to get an idea of what my design choices actually do.

Anyway, I'm sure there are good references and good basic resources in an updated form and thought this should be covered here as well in a similar fashion. What do you think?",neuralnetworks,YetAnotherGuy2,False,/r/neuralnetworks/comments/8stejh/getting_started_sidebar_for_rneuralnetworks/
Generating Atari images using Geneartive Adverserial Networks,1529590602,,neuralnetworks,satwik_,False,/r/neuralnetworks/comments/8ss7og/generating_atari_images_using_geneartive/
Activity Detector and Labeller,1529541661,"The idea of this is to create a kind of automated security monitoring software which works very similar to an image detector, but, detects the action the object in the image is taking. Here is an example.

There is inputs for for layer of images. The first image has maybe over 4050 inputs to represent an image, hue, and brightness. After the first frame, you have another 4050 inputs for the next image. Each activity that you can think of can probably be labeled in under a second, so you could cut down the number of inputs by depending on 15 frames. This equals 4050x15 inputs. Only the first layer of inputs for the current image being processed should care about image detection. The other layers are trained to detect how to label the differences between images over time. This means that an image detection like neural network is trained to process 15 frames at the same time in order to get an idea of what happened and time stamps the activity of interest, such as somebody getting into a car crash or stealing something from a store. Pretty interesting yes? You could set up cameras all over the player which have been trained to recognize certain types of activity and instead of sitting around behind a monitor watching the cameras all night, you can sleep like a baby. If the cameras witness something like someone trying to break or sneak into a house, a wild animal trying to hurt people, or anything else like that, it could be set up to alarm whoever needs to be alarmed.

What do you guys think?",neuralnetworks,Zigguraut,False,/r/neuralnetworks/comments/8snj3f/activity_detector_and_labeller/
New AI method increases the power of artificial neural networks,1529500856,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8si8s4/new_ai_method_increases_the_power_of_artificial/
How Convolutional Neural Network(CNN) Works? - Tutorial,1529483937,,neuralnetworks,pooja307,False,/r/neuralnetworks/comments/8sgpx0/how_convolutional_neural_networkcnn_works_tutorial/
The dark side of the neural network,1529474290,,neuralnetworks,koteth,False,/r/neuralnetworks/comments/8sfxlw/the_dark_side_of_the_neural_network/
Artificial life simulation.,1529454785,,neuralnetworks,Thomas-Arys,False,/r/neuralnetworks/comments/8sdza4/artificial_life_simulation/
MNIST dataset digit recognition help,1529422603,"Hello,
After a bunch of work time, I managed to program a handwritten digits neural net.
the net is feed forward with input layer of 28*28 pixels, a hidden layer of 40 neurons, and an output layer of 10 neurons (one for each digit).

my activation function is the logistic (sigmoid).

everything is good, except the network isn't working! the outputs of the net gets closer and closer to zero with time, and after about 150 epochs the network is outputting only 0's from all outputs. is this related to the net's structure? or the logistic function?  or is it a problem with my code? do you ever faced this problem before, and if you did; how do you fix it?

if it has something to do with my code, please tell me, and I will post it.

Thank you for ANY help,
Arad.",neuralnetworks,Aradarbel10,False,/r/neuralnetworks/comments/8s9t4o/mnist_dataset_digit_recognition_help/
Is Market Internal Data Enough To Predict the Future of Prices? A Short Investigation,1529421709,,neuralnetworks,HaikuHaiku,False,/r/neuralnetworks/comments/8s9p0o/is_market_internal_data_enough_to_predict_the/
Neural network training: going backward to go forward?,1529358253,"I am working on CNN models which are intended to predict a protein's structure from its amino acid sequence. I have a decently large data set, 750 protein structures containing over 100,000 amino acid residues. I am implementing my CNN's in Keras. When I perform 3X cross-validation to help select architectures and hyperparameters, I have 50K examples in my training folds and 25K samples in my validation folds. I am using the ADAM gradient descent algorithm, with the default settings. My backend is Tensorflow.

How many epochs should I train before giving up? I know, that's not an easy question to answer. The machine learning class that I took recommended that a good criterion for stopping is that the validation loss starts increasing. Defining what constitutes ""increasing"" can be a little vague. Some CNN architectures that I have tried can give noisy loss values for a couple of epochs. I assess my training and validation losses at the end of each epoch, and my ""patience value"" for validation loss is equal to the square root of the current epoch. I deliberately go a bit farther when the number of epochs is large, so that I have a decent snapshot of what happens after the minimum I chose. Here's an example from one of my typical runs.

*Processing img bjsfzlvbxt411...*

In addition to noticing the validation minimum, I also note that the training loss is still smoothly declining when I decide to stop. Although this particular architecture is not that large (about 7,000 trainable weights) relative to my data set, it is apparently capable of doing a better job of memorizing the training set than I allowed it to do.  So should I stop?

I have also looked at the [Tensorflow Playground web site](https://playground.tensorflow.org/), and I see something quite different. The Playground does not attempt to determine when a model is optimized, it just keeps running. Because the Playground data sets are so small, there is no great cost in time or computation to simply keep plugging away.

In the Two Spirals example, it is easy to choose conditions where test loss values climb higher, for tens of epochs, before they finally reverse. And soon after the reversal, a lower test error value is achieved than would have been achieved by using some form of early stopping as I've described above. I've even managed to arrange situations where both the training and test losses rise together for several epochs before they both start moving strongly lower again.

What exactly is happening in these examples? I haven't yet found the optimizer in the Playground's [source code](https://github.com/tensorflow/playground). (TypeScript isn't one of my programming languages, which may be slowing me down.)  If it's ADAM, what kind of momentum values are they using? How is it that the Two Spirals solver always seems to best its older local minimum when it swings back?

For my own work, I need to understand whether my early stopping criteria are too conservative. This could turn into a resource problem for me, because I am currently using close to an hour to perform a 3X cross-validation on a single CNN architecture with just three hidden layers. I believe that there are some optimizations I can make in Keras and/or Tensorflow to increase the throughput. However, optimizations of TF and/or Keras may amount to little, if I need to train for many more epochs.

Any advice you may have is appreciated, thanks!",neuralnetworks,bwllc,False,/r/neuralnetworks/comments/8s3d4c/neural_network_training_going_backward_to_go/
"Are you interested in AI and want to start learning more with Tutorials? Check out this new Youtube Channel, called Discover Artificial Intelligence. :)",1529341452,,neuralnetworks,DiscoverAI,False,/r/neuralnetworks/comments/8s155w/are_you_interested_in_ai_and_want_to_start/
Best Laptop for Machine Learning,1529340781,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8s11t7/best_laptop_for_machine_learning/
Enkronos: An App Paradise on Blockchain,1529314936,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8ry8io/enkronos_an_app_paradise_on_blockchain/
HELP! textgenrnn &amp; python newbie having trouble with weights files.,1529264231,"Hi all. 
First, super sorry to say I'm teaching myself this stuff, and probably don't have the proper foundation. I definitely don't know all the jargon that well yet. 
My question is probably so insanely basic. Anyway: I am trying to train on a new dataset. What weights file do I use? I am assuming not the one that I have from a previous training? I was under the impression I didn't have to have a weights file if I'm training from a new file. 

Any help is appreciated -- as is patience for my inevitable confusion. Thank you!",neuralnetworks,Commodore_Kangaroo,False,/r/neuralnetworks/comments/8rtfmh/help_textgenrnn_python_newbie_having_trouble_with/
Video explaining some common neural networks that are used today in many technologies we love such as google translate as well as other concepts of ai,1529254579,,neuralnetworks,TheScienceVerse,False,/r/neuralnetworks/comments/8rsba3/video_explaining_some_common_neural_networks_that/
The Hybercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) Users Page,1529186622,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8rmn97/the_hybercubebased_neuroevolution_of_augmenting/
"Need some help, newbie here",1529159685,"Hi, is there anyone there that could give me some directions.   
**The context:** I'm a computer science student and I'm starting to research neural networks and such, to solving some problems. The problem that I'm starting to tackle at the moment is dealing with large amounts of documents that we have in one of the departments on my university.  

I got to a point, using google vision api that I can ocr the documents, but now I have to classify the data and them classify the document it self.    


I'm trying to figure out how to model the data to classify and them create the model it self.  Sorry if something doesn't sound natural, I'm from Brazil, English ins't my first language.",neuralnetworks,Hakkon8065,False,/r/neuralnetworks/comments/8rjpvm/need_some_help_newbie_here/
Deep Learning with Intel,1529159244,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8rjob6/deep_learning_with_intel/
Train a neural network in real time,1529127598,"Hi, I hope you can help me. I want to use matlab or python to create a neural network with 10 inputs (come from sensor)  and 2 outputs,  the data will send each 4 seconds.  The input value vary from 20 to 80 and the output should be between 0 and 150. the question is, what kind of NN should I use?  Can it run in real time? what I need to take care of?

thanks ",neuralnetworks,Lorenzo9196,False,/r/neuralnetworks/comments/8rhez1/train_a_neural_network_in_real_time/
AI Weekly 15 June 2018,1529084669,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8rcvay/ai_weekly_15_june_2018/
A Tech Personality Became the Author of Mona Lisa Using Blockchain,1529052217,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8r9fqo/a_tech_personality_became_the_author_of_mona_lisa/
Natural Language to SQL conversion in 5 Steps,1528997478,,neuralnetworks,JayaYellowAnt,False,/r/neuralnetworks/comments/8r3qvo/natural_language_to_sql_conversion_in_5_steps/
[ NIPS 2011 / Andrew Ng ] First Glance of Sparse Filtering in Tensorflow with Interactive Code,1528995583,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8r3h6t/nips_2011_andrew_ng_first_glance_of_sparse/
Ethereum Holders Lost $20 Million Due to a Hacker Attack,1528965854,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8r0a1y/ethereum_holders_lost_20_million_due_to_a_hacker/
Encoder Decoder Network - Computerphile,1528933493,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8qx7k8/encoder_decoder_network_computerphile/
Deep Q-Learning using your computer screen?,1528908796,"Hi! I'm wondering if that sort of thing exists and would like your opinion on the matter.

Imagine a program that can take snapshots of your computer screen (with variable resolution, delta_t and number of snapshots used in the network) and use that to train a deep Q-Learning network, based on rewards you would have to define yourself using operations that may or may not involve future snapshots (i.e train a network to close all current windows, give it a reward if my screen looks like my wallpaper with an empty taskbar, meaning all windows are now closed).

Is there such a piece of software I could use? If not, do you reckon there may be some useful libraries I could use for this?

Thanks for your time!",neuralnetworks,TyzoneLyraNature,False,/r/neuralnetworks/comments/8qtzrh/deep_qlearning_using_your_computer_screen/
TensorFlow.js- A Practical Guide,1528902763,,neuralnetworks,JayaYellowAnt,False,/r/neuralnetworks/comments/8qt6ns/tensorflowjs_a_practical_guide/
European Union Will Host a Blockchain Q&amp;A Session,1528878651,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8qqtfe/european_union_will_host_a_blockchain_qa_session/
What Is a Bounty of Cryptocurrency?,1528796466,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8qhn3d/what_is_a_bounty_of_cryptocurrency/
Apple CreateML vs Kaggle Competitions,1528741255,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8qbntc/apple_createml_vs_kaggle_competitions/
World Models Experiments,1528732040,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8qakiz/world_models_experiments/
American Cryptocurrency Exchanges Are Not Happy About Complying with the SEC,1528701694,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8q7m8i/american_cryptocurrency_exchanges_are_not_happy/
What book for NN would be best for a beginner?,1528673772,"I have just finished my AP computer science course and decided to use the summer to learn about neural networks, the problem is I don’t know where to start. I am also one of those people who like a hard copy book, and am trying to find a good introductory book for beginners. It would also be preferable if the book was hands on, I.e actually had you do labs and Programming instead of just being complete theory. Any suggestions?",neuralnetworks,madaradeath9,False,/r/neuralnetworks/comments/8q4wti/what_book_for_nn_would_be_best_for_a_beginner/
AI Weekly 9 June 2018,1528566921,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8pujrm/ai_weekly_9_june_2018/
"I've set seeds, but different results every time. Keras + tensorflow.",1528560318,"Hi, 
I've been running some neural networks for the past week and just haven't been able to get identical results. It's very close, but I'm aiming for 5th or 6th decimal place results, and having it change by even a little bit can be quite a big deal.

I tried a few suggestions I've read online:

from numpy.random import seed
seed(9)

from tensorflow import set_random_seed
set_random_seed(9)

And
numpy.random.seed(9)

And running the network again with those set, keeping everything else the same, results in different results.

For code I basically just have :
----------------------------
model = Sequential()
model.add(Dense(14, input_dim=13, activation='relu'))
model.add(Dense(9, activation='relu'))
model.add(Dense(9, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(X, Y, epochs=3, batch_size=14)

-----------------------------
That's basically it. I've tried different optimizers, epochs and batch size of course.

Any ideas?
Thanks in advance.
",neuralnetworks,roushrsh,False,/r/neuralnetworks/comments/8ptrrs/ive_set_seeds_but_different_results_every_time/
Removing hard subtitles from video,1528532086,"Hey all!

I haven't played with nn in a while but I would like to get back into it, especially now that a lot of tools are available for us to use.

I think it would be a cool project to train a deep learning model to hide hard subtitles from video files.

First of all, have you heard of something similar yet?

Generating training data should be easy, take a normal video file without subtitles, download the corresponding subtitles and generate a file with subtitles hard embedded into the images. The first one is the output, the latter the input.

The algorithm could be optimized to only look at the part of the image where subtitles are expected (lower quarter or so). Might it be a good idea to also use audio data in this or would it be overkill? The idea is that a subtitle is usually linked with an audio input (but not always).

Lastly, any recommendation on a method that has proven to work for similar tasks?

Thanks a lot! ",neuralnetworks,_beeb,False,/r/neuralnetworks/comments/8preq3/removing_hard_subtitles_from_video/
DeepMind: First major AI patent filings revealed,1528481748,,neuralnetworks,nickb,False,/r/neuralnetworks/comments/8pm7f2/deepmind_first_major_ai_patent_filings_revealed/
Deep Learning - Computerphile,1528465757,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8pk0g0/deep_learning_computerphile/
Cryptocurrency Trading Terminals: What They Are Meant For and How to Use Them,1528443897,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8pi24j/cryptocurrency_trading_terminals_what_they_are/
What Is Cognitive Computing (How AI Will Think),1528385876,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8pbah1/what_is_cognitive_computing_how_ai_will_think/
ICO Marketing: Where to Find and How to Attract Target Audience,1528366834,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8p96bp/ico_marketing_where_to_find_and_how_to_attract/
Please help me understand weights,1528308194,"DISCLAIMER: I have NO knowledge of neural networks/AI/machine learning, but the topic interests me, so there might be lot's of things I mixed up here. Ok here we go:

As I understand a basic network (or rather simple neuron) works like this (in huge simplification):

1. You take inputs (x1, x2, x3)
2. Multiply them by weights (w1, w2, w3) and add them
3. Put the multiplied sum through a function (activator) that basically limits it to range between \-1 (false) and 1 (truth)

Assuming that is correct (right?) my question is how this works in a program. To be more specific how the weights are calculated? It it like this:

1. I give a single set of data with expected output 1
2. A program runs and randomizes weights 1, 2 ... 30 times in a loop until it gets the expected result
3. Then uses those weights for an input I might give it to predict the result

Is my understanding of this correct? Of course this would probably be horrible with single data set so what happens if I get for example 3 (or 1000) sets? Does it:

1. Go through the process I described above for first set
2. Then takes the weights that were correct for first set and tries them with 2nd and so on...
3. until it finds a set that the weights did not give expected output and then... what? It alters them a little (by how much?) so they are correct again for that set and goes on and on like so until it finds correct weights?

Or if it finds a set the weights are incorrect dues it randomizes the weights again? This probably wouldn't make sense to just get a random set again so... this is the part that confuses me basically.",neuralnetworks,borek87,False,/r/neuralnetworks/comments/8p2v1u/please_help_me_understand_weights/
Neural network for decision making,1528301560,"I would like to make a neural network that as input takes a number of objects containing symbols, and returns one of the objects. This keeps repeating until a result of 0, 1 or 2 is achieved. How would I go about this?",neuralnetworks,1LJA,False,/r/neuralnetworks/comments/8p1wjh/neural_network_for_decision_making/
What Is Necessary for Cryptocurrency Mining in 2018?,1528272210,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8oyuut/what_is_necessary_for_cryptocurrency_mining_in/
Neural network factorization?,1528241374,Is NN simplification or factorization a thing somewhere? e.g. deleting nodes that do not contribute (simplification) or replacing/combining a group of nodes of two or more different networks (factorization)?,neuralnetworks,redwins,False,/r/neuralnetworks/comments/8ovowx/neural_network_factorization/
Neural Network Tutorial - Simplified for you,1528194053,,neuralnetworks,pooja307,False,/r/neuralnetworks/comments/8opoi9/neural_network_tutorial_simplified_for_you/
It's Official: Microsoft Pays Out $7.5 Billion For GitHub,1528131264,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8oiurz/its_official_microsoft_pays_out_75_billion_for/
EOS Has Completed Its ICO with a Record of $4 Billion,1528115281,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8ogv7z/eos_has_completed_its_ico_with_a_record_of_4/
The result of mean absolute percentage error and mean absolute error is strange,1528097743,"While I'm doing an forecasting task, the two error performance MAE and MAPE shows the strange result. For example, I got MAE = 6.03, and MAPE = 1.04. However, in the paper I read, its result is about MAE = 6.9 while MAPE = 11%. It can be seen that my MAPE result is too big, while transform to percentage is 104%... But this result is strange, while my MAE value is not bad, the MAPE is too big! I can't figure out why?

My implementation is like:

N = len(result)

for i in range(N):

    _mape += abs(result[i] - y_test[i])/y_test[i]

    _mae += abs(result[i] - y_test[i])

    _rmse += (result[i] - y_test[i])**2

    if abs(result[i] - y_test[i])/y_test[i] &gt; 1:

        print(result[i])

        print(y_test[i])

        print(abs(result[i] - y_test[i])/y_test[i])

print(_mape)

mape = _mape/N

rmse = np.sqrt(_rmse/N)

mae = _mae/N

",neuralnetworks,Laurence-Lin,False,/r/neuralnetworks/comments/8ofh2i/the_result_of_mean_absolute_percentage_error_and/
AI Weekly 2 June 2018,1527960428,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8o23vr/ai_weekly_2_june_2018/
Mastercard Received a Patent on a Coupon Authenticating System on Blockchain,1527936227,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8nzrok/mastercard_received_a_patent_on_a_coupon/
"Simple CNN implementation from scratch, in tensorflow to classify images.",1527916077,,neuralnetworks,Perseus784,False,/r/neuralnetworks/comments/8nyf4j/simple_cnn_implementation_from_scratch_in/
Webcam Tracking with Tensorflow.js,1527877193,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8nu6ou/webcam_tracking_with_tensorflowjs/
"For Some Hard-To-Find Tumors, Doctors See Promise In Artificial Intelligence",1527859828,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8nrw19/for_some_hardtofind_tumors_doctors_see_promise_in/
NVIDIA Develops Monkey-See Monkey-Do Style Machine Learning Tech So AI Can Watch And Train,1527858788,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8nrrqy/nvidia_develops_monkeysee_monkeydo_style_machine/
URUNIT: Revolution in the Sphere of Gambling,1527839302,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8nq4q5/urunit_revolution_in_the_sphere_of_gambling/
SEC Uncovered a $21 Million ICO Scam Project,1527754341,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8ngoxc/sec_uncovered_a_21_million_ico_scam_project/
Simple matrix to visual network example--do you know how it works?,1527707602,"How can there be four rows and two columns, when there are three nodes?

https://i.imgur.com/h6bEwrt.png",neuralnetworks,jinnyjuice,False,/r/neuralnetworks/comments/8nbbvk/simple_matrix_to_visual_network_exampledo_you/
ARM Details Project Trillium Machine Learning Processor To Drive AI To the Edge,1527687781,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8n8o1s/arm_details_project_trillium_machine_learning/
ARM Details Project Trillium Machine Learning Processor To Drive AI To the Edge Read more at https://hothardware.com/reviews/arm-details-project-trillium-machine-learning-processor#K2kxbJV8YjLwBjei.99,1527686753,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8n8js0/arm_details_project_trillium_machine_learning/
[Project Help] NN learn Pathfinding,1527681508,"Hy, I am trying to do a neural network project, is consists in teaching organisms to reach an objective point. 
I already made something like that, without neural networks, you an check here (https://www.youtube.com/watch?v=MgBFUuuy2iI), the objective in the new project is doing the same thing with NN, that way I can change the map and it can get the goal anyway.

The code is already running however it not works very well, and I would love to hear your tips and opinions.

So, the inputs of the network are:
* 3 distance sensores (like self driving cars) to avoid collisions, this works really great in the current code.
* The direction of the objective point (0 is the right direction, &gt;0 is right, &lt;0 is left, 1 or -1 is the max where the objective is behind)

The algorithm learn easily to not collide with wall, and reach the goal if there was a clean path to the objective, like a race course... However if the goal is in open space after the course, it will never go there. Because he learn to get close to walls, and not go towards objective. So I try to add a new input (distance to objective, not euclidian distance, the distance for the closest path to objective) hoping that he learns something like, if it close to the goal it go towards it... However it didn't work very well. 
At the moment I have 1 hidden layer, and 1 output do you think I need more layers or the problem os pathfinding is just to hard for a neural network like this?

Thank you,
macarrony00

PS: sorry the bad english :'(",neuralnetworks,macarrony00,False,/r/neuralnetworks/comments/8n7z5q/project_help_nn_learn_pathfinding/
The Future of Cryptocurrency and What Can Happen to Bitcoin,1527673970,,neuralnetworks,GladKindheartedness,False,/r/neuralnetworks/comments/8n7ba0/the_future_of_cryptocurrency_and_what_can_happen/
Deep Learning based Human Pose Estimation using OpenCV ( C++ / Python ),1527610985,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8n0eps/deep_learning_based_human_pose_estimation_using/
Why activate the network more than once in MultiNEAT evaluation function?,1527609843,"I've just started looking into NEAT algorithms again, the MultiNEAT library in particular, and have found that the examples will activate the network multiple times in the evaluation function:

https://github.com/peter-ch/MultiNEAT/blob/1985a6e7c0b7ede3ce5d20ff88cbece057eb35c8/examples/TestESHyperNEAT_xor.py#L122

https://github.com/peter-ch/MultiNEAT/blob/1985a6e7c0b7ede3ce5d20ff88cbece057eb35c8/examples/TestNEAT_xor.py#L36

It seems to correspond with the depth of the NN (number of layers right?).  I'll keep looking for the answer in the code myself today, but I thought I'd ask here!  Thanks
",neuralnetworks,UpstairsTeacher,False,/r/neuralnetworks/comments/8n09bk/why_activate_the_network_more_than_once_in/
This AI Learned To See In The Dark | Two Minute Papers #253,1527609675,,neuralnetworks,keghn,False,/r/neuralnetworks/comments/8n08hq/this_ai_learned_to_see_in_the_dark_two_minute/
Automate License Plate Recognition in 3 Simple Steps,1527609507,,neuralnetworks,TopBrokenheartedness,False,/r/neuralnetworks/comments/8n07pb/automate_license_plate_recognition_in_3_simple/
Neural network programming,1527601179,,neuralnetworks,jothu123,False,/r/neuralnetworks/comments/8mz610/neural_network_programming/
The Price of Bitcoin Will Be $15k – A Prediction for the June of 2018 by John McAfee,1527591745,,neuralnetworks,icoinvest,False,/r/neuralnetworks/comments/8my7tb/the_price_of_bitcoin_will_be_15k_a_prediction_for/
Need volunteers for co-developing Chess Neural Networks (half of the work is already done by me),1527535070,"Hello guys, I'm trying to reimplement a research paper regarding Chess Neural Networks. Paper's name is:

`Giraffe: Using Deep Reinforcement Learning to Play Chess` by `Matthew Lai`. (https://arxiv.org/abs/1509.01549v2)

I've built a custom minimax based chess engine and will take it from there. Anyone wants to help and learn deep learning along the way?",neuralnetworks,AdnaanZaahid,False,/r/neuralnetworks/comments/8mslot/need_volunteers_for_codeveloping_chess_neural/
"TensorFlow latest release 1.8 is out! Hence, I made crisp tutorial to Install its GPU version on Windows PC",1527476293,,neuralnetworks,DecipherTechnic,False,/r/neuralnetworks/comments/8mmwrl/tensorflow_latest_release_18_is_out_hence_i_made/
Looking for checklist on basic neural networks.,1527455410,"Hey guys. I'm woking on a project on neural network introduction. It's about having a document which contains brief introduction to important aspects of a basic neural network algorithm (for example perceptrons, sigmoid function etc.). I'm not looking for going into the advanced stuff like recurring neural network etc. Can you guys help me figure out things I should have on my checklist to make my document for beginners (including me).

Thanks!",neuralnetworks,Tiki_taka_toko,False,/r/neuralnetworks/comments/8mkrxn/looking_for_checklist_on_basic_neural_networks/
How to train siamese network using triplet loss,1527404978,"triplet loss need 3 images\(anchor,positive,negative\),and then network backprops.

1\)So should I make 3 shared input layers connected to same network and then forward propagate. OR

2\)should I make 2 forward props 1st with \(anchor,positive\) and 2nd with\(anchor , negative\) and then backprop?

Please help me,I really need some help.",neuralnetworks,MayurNewase,False,/r/neuralnetworks/comments/8mg935/how_to_train_siamese_network_using_triplet_loss/
AI Weekly 26 May 2018,1527358763,,neuralnetworks,TomekB,False,/r/neuralnetworks/comments/8mbzs1/ai_weekly_26_may_2018/
Assistance de-blurring photo with CNN?,1527208473,"Hello /r/neuralnetworks! An odd question to throw out to this subreddit.  


Fulham v Aston Villa this weekend is one of the biggest football games of the year, and Fulham’s manager posted this image of his team’s confidential pre\-match report on Twitter:  


[https://twitter.com/Jokanovic/status/997207217618243584](https://twitter.com/Jokanovic/status/997207217618243584)  


I’m curious if anyone here knows whether a CNN for deblurring, such as the one below, might be able to declassify any of the hidden text? Does anyone have some tools to take a stab at it?  


[http://www.fit.vutbr.cz/\~ihradis/CNN\-Deblur/](http://www.fit.vutbr.cz/~ihradis/CNN-Deblur/)  


Here are some higher\-res versions of the original image:  


[https://imgur.com/a/P0B3lFO](https://imgur.com/a/P0B3lFO)  
[https://imgur.com/a/g6YRZs3](https://imgur.com/a/g6YRZs3)",neuralnetworks,overwhelmed_dev,False,/r/neuralnetworks/comments/8lxmq3/assistance_deblurring_photo_with_cnn/
Looking for a Hiragana Handwritten Database,1527202876,"Hello all,
I'm trying to make an app that can recognize handwritten hiragana characters. I can't seem to find a good database that I can use to train my network though. Does anyone know where to find a good database for hiragana? Katakana would also work too but Kanji would be much/hard for me.",neuralnetworks,Djinn_Aki,False,/r/neuralnetworks/comments/8lx083/looking_for_a_hiragana_handwritten_database/
"In the derivation of BPTT algorithm, is softmax function necessary?",1527044774,"In the derivation of BPTT algorithm, lots of examples use softmax function at the output state: y_output = softmax(weight*hidden state)

However, if I'm working on a task such as forecasting or text processing, I don't need softmax function to product the output. In this case, how could I derive the update equation of BPTT? Is there any reference or help?

Thanks a lot!",neuralnetworks,Laurence-Lin,False,/r/neuralnetworks/comments/8lg3bs/in_the_derivation_of_bptt_algorithm_is_softmax/
ICML accepted papers,1527015501,[removed],neuralnetworks,vector_machines,,/r/neuralnetworks/comments/8lckpo/icml_accepted_papers/
"[question] If two instances of the same AI are trained with exactly the same examples in the same order, will their nodes be the same?",1526986711,"Sup guys,

Question in the title pretty much. Say you train an AI, then train a second instance of that AI not only on the same training data, but exactly the same data in the same order, will their nodes be the same? Similar? Totally different?

Thanks in advance,
The0thArcana",neuralnetworks,The0thArcana,,/r/neuralnetworks/comments/8l93l7/question_if_two_instances_of_the_same_ai_are/
Using Saliency maps in place of attention,1526981470,Does anyone know whether it could work better than just using soft or hard attention? Has it been tried before and with what results. Thank you.,neuralnetworks,utsavsing,,/r/neuralnetworks/comments/8l8p9v/using_saliency_maps_in_place_of_attention/
НОТ BЕАUТIЕS WISНING LОVЕ ТНАТ'S WАIТING FОR УОU НЕRЕ.,1526956550,НОТ BЕАUТIЕS WISНING LОVЕ ТНАТ'S WАIТING FОR УОU НЕRЕ. \-\-\- http://redit.kdkvf.website,neuralnetworks,Jamwam3000,,/r/neuralnetworks/comments/8l6jkg/нот_bеаuтiеs_wisнing_lоvе_тнатs_wаiтing_fоr_уоu/
Image stuff,1526870688,"So I installed [this](https://github.com/solivr/tf-crnn) thing off of github for picture to text machine learning, and I installed all the dependencies but when I run it for training it says this:
`/home/mrmola/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""train.py"", line 14, in &lt;module&gt;
    from src.model import crnn_fn
ImportError: No module named src.model`
So I go to the src directory. I run model.py because I figured that would fix the problem and I get:
`  File ""model.py"", line 25
    def deep_cnn(input_imgs: tf.Tensor, is_training: bool, summaries: bool=True) -&gt; tf.Tensor:
                           ^
SyntaxError: invalid syntax`
Help?",neuralnetworks,mrmola,,/r/neuralnetworks/comments/8kxmyc/image_stuff/
AI Weekly 19 May 2018,1526752888,,neuralnetworks,TomekB,,/r/neuralnetworks/comments/8kmx2q/ai_weekly_19_may_2018/
Five Steps to TensorFlow on the Raspberry Pi,1526748455,,neuralnetworks,keghn,,/r/neuralnetworks/comments/8kmfpk/five_steps_to_tensorflow_on_the_raspberry_pi/
"Artificial life simulation 2.0: Better, faster, stronger, .exe available for the innocent",1526745273,,neuralnetworks,Thomas-Arys,,/r/neuralnetworks/comments/8km3np/artificial_life_simulation_20_better_faster/
3D Coordinates to Voxel to 3DCNN,1526587483,"I have a list of 3D coordinates and am trying to take it to a voxel for a 3DCNN in Python.

1. I re\-orgin the coordinates around a point.
2. Convert/round raw coordinates so that 1 voxel equal 1 original X,Y, Z coordinate.
3. Trim down to \+/\- a certain dimension value in X Y and Z direction. I.E. only a certain distance away from the origin is used, going from 500x500x500 to 30x30x30.
4. Go from the new coordinates to 0s and 1s in an 3d array.

I am having trouble with 3 things.

1. Rotating the voxel/array so that each one will have the same rotation according to a point\(s\) near the origin.
2. There will need to be multiple channels, probably 4, of multiple voxels that feed into the network. 
3. Gaussian or Interpolation to make the voxels spherical.

I've looked for info on the last 3 parts and can't seem to find any sort of information on how to accomplish it. Any help would be appreciated. ",neuralnetworks,VoxelHelp,,/r/neuralnetworks/comments/8k7aab/3d_coordinates_to_voxel_to_3dcnn/
Backpropagation in ReLU layer and convolutional layer,1526499640,"Hello, I am trying to build a convolutional neural network from scratch in Python (in order to get a deeper understanding of how every part of them works) and I am stuck on a couple things. I understand how to propagate the signals forward, and how to get the final error. But I am a little confused on the ReLU and convolutional layers backpropagation. Does anyone understand (or know a decent online source, I was unable to find a decent one) that explains how to compute the change in weights for the filters and how to propagate the error backwards through the convolutional and ReLU layers?",neuralnetworks,Wso333,,/r/neuralnetworks/comments/8jxuh6/backpropagation_in_relu_layer_and_convolutional/
"Deep Text Summarization: on Amazon reviews, Github issues and news articles",1526494507,,neuralnetworks,tttttm,,/r/neuralnetworks/comments/8jx678/deep_text_summarization_on_amazon_reviews_github/
